{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from models import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "BATCH_SIZE = 16\n",
    "CONTEXT_SIZE = 32\n",
    "EPOCH = 1000000\n",
    "\n",
    "LOG_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115393 characters in the dataset\n",
      "65 unique characters in the dataset\n"
     ]
    }
   ],
   "source": [
    "# setting up datasets\n",
    "# we use TinyShakespeare, the collection of all of Shakespeare's works\n",
    "# that has about 1 million characters\n",
    "\n",
    "path = \"./tinyshakespeare.txt\"\n",
    "\n",
    "lines = open(path,\"r\").read()\n",
    "print(f\"{len(lines)} characters in the dataset\")\n",
    "\n",
    "# 今回は文字単位でvocablaryを作る\n",
    "vocab = sorted(list(set(lines)))\n",
    "\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "\n",
    "print(f\"{len(vocab)} unique characters in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# simple tokenizer\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "e = encode(\"hello\")\n",
    "print(e)\n",
    "print(decode(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "# create tokenized dataset\n",
    "dataset = torch.tensor(encode(lines),dtype=torch.int8)\n",
    "dataset = dataset[:3000]\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "tensor([[ 1, 58, 46, 43,  1, 41, 47, 58, 63,  0],\n",
      "        [ 1, 46, 59, 52, 45, 43, 56,  1, 44, 53]])\n",
      "tensor([[58, 46, 43,  1, 41, 47, 58, 63,  0, 47],\n",
      "        [46, 59, 52, 45, 43, 56,  1, 44, 53, 56]])\n",
      "[(' the city\\n', 'the city\\ni'), (' hunger fo', 'hunger for')]\n"
     ]
    }
   ],
   "source": [
    "def get_batches(data,split,batch_size,context_window):\n",
    "    train = data[:int(.8*len(data))]\n",
    "    val = data[int(.8*len(data)):int(.9*len(data))]\n",
    "    test = data[int(.9*len(data)):]\n",
    "\n",
    "    if split == \"val\":\n",
    "        batch_data = val\n",
    "    elif split == \"test\":\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = train\n",
    "    \n",
    "    # pick random starting points\n",
    "    sp = torch.randint(0,batch_data.size(0) - context_window - 1,(batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in sp]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in sp]).long()\n",
    "\n",
    "    return x,y\n",
    "\n",
    "xs,ys = get_batches(dataset,\"train\",batch_size=2,context_window=10)\n",
    "print(xs.shape)\n",
    "print(ys.shape)\n",
    "print(xs)\n",
    "print(ys)\n",
    "\n",
    "print([(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用のutil関数ども\n",
    "# モデルの損失を確認する関数\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(model,use_cuda=False):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\",\"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb,yb = get_batches(dataset,split,batch_size=BATCH_SIZE,context_window=CONTEXT_SIZE)\n",
    "            if use_cuda:\n",
    "                xb,yb = xb.cuda(),yb.cuda()\n",
    "            _, loss = model(xb,yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def evaluate_text(model,xs,ys):\n",
    "    model.eval()\n",
    "    preds = model(xs)\n",
    "    if type(preds) == tuple:\n",
    "        preds = preds[0]\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    model.train()\n",
    "    return [(decode(xs[i].tolist()), decode(ys[i].tolist()), decode(preds[i].tolist())) for i in range(len(xs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 10])\n",
      "embedding shape: torch.Size([2, 10, 64])\n",
      "linear shape: torch.Size([2, 10, 65])\n",
      "logits view shape: torch.Size([20, 65])\n",
      "targets view shape: torch.Size([20])\n",
      "torch.Size([2, 10, 65])\n",
      "tensor(4.1753, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, 64)\n",
    "        self.linear = nn.Linear(64, vocab_size)\n",
    "        \n",
    "    def forward(self,x,targets=None,debug=False):\n",
    "        if debug: print(f\"input shape: {x.shape}\")\n",
    "        x = self.emb(x)\n",
    "        if debug: print(f\"embedding shape: {x.shape}\")\n",
    "        x = self.linear(x)\n",
    "        if debug: print(f\"linear shape: {x.shape}\")\n",
    "        logits = F.softmax(x,dim=-1)\n",
    "\n",
    "        if targets is not None:\n",
    "            if debug: \n",
    "                print(f\"logits view shape: {logits.view(-1,self.vocab_size).shape}\")\n",
    "                print(f\"targets view shape: {targets.view(-1).shape}\")\n",
    "            loss = F.cross_entropy(logits.view(-1,self.vocab_size),targets.view(-1))\n",
    "            return logits, loss\n",
    "        else:\n",
    "            return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            if idx.size(1) < CONTEXT_SIZE:\n",
    "                idx = idx[:,-CONTEXT_SIZE:]\n",
    "            x = idx.clone().detach().view(1,-1)\n",
    "            y = self(x)\n",
    "\n",
    "            idx_next = y.argmax(-1)[:,-1]\n",
    "            idx_next = idx_next.unsqueeze(-1).clone().detach()\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "\n",
    "        return idx\n",
    "        \n",
    "test_model = SimpleModel(len(vocab))\n",
    "xs,ys = get_batches(dataset,\"train\",batch_size=2,context_window=10)\n",
    "\n",
    "logits, loss = test_model(xs,ys,debug=True)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 4.173341131210327, val loss: 4.174221467971802, ETA in seconds: 193.338\n",
      "epoch: 100, train loss: 4.0263324737548825, val loss: 4.061368322372436, ETA in seconds: 981.481\n",
      "epoch: 200, train loss: 3.954658031463623, val loss: 4.001058220863342, ETA in seconds: 1715.641\n",
      "epoch: 300, train loss: 3.944222640991211, val loss: 3.9827446699142457, ETA in seconds: 2470.012\n",
      "epoch: 400, train loss: 3.9051242113113402, val loss: 3.9618775606155396, ETA in seconds: 3211.832\n",
      "epoch: 500, train loss: 3.8899839162826537, val loss: 3.9484106302261353, ETA in seconds: 3954.402\n",
      "epoch: 600, train loss: 3.9059818029403686, val loss: 3.953130102157593, ETA in seconds: 4709.497\n",
      "epoch: 700, train loss: 3.902489447593689, val loss: 3.9500710725784303, ETA in seconds: 5455.835\n",
      "epoch: 800, train loss: 3.898013782501221, val loss: 3.943277668952942, ETA in seconds: 6198.507\n",
      "epoch: 900, train loss: 3.9045109510421754, val loss: 3.947786545753479, ETA in seconds: 6956.059\n",
      "epoch: 1000, train loss: 3.8933572292327883, val loss: 3.942058801651001, ETA in seconds: 7730.949\n",
      "epoch: 1100, train loss: 3.901885437965393, val loss: 3.948422837257385, ETA in seconds: 8506.370\n",
      "epoch: 1200, train loss: 3.90438551902771, val loss: 3.946221399307251, ETA in seconds: 9274.538\n",
      "epoch: 1300, train loss: 3.89206063747406, val loss: 3.948136901855469, ETA in seconds: 10035.196\n",
      "epoch: 1400, train loss: 3.8859967947006226, val loss: 3.9375775814056397, ETA in seconds: 10791.053\n",
      "epoch: 1500, train loss: 3.901183009147644, val loss: 3.941269564628601, ETA in seconds: 11594.561\n",
      "epoch: 1600, train loss: 3.8964861154556276, val loss: 3.9451549530029295, ETA in seconds: 12474.756\n",
      "epoch: 1700, train loss: 3.902718758583069, val loss: 3.940254831314087, ETA in seconds: 13316.069\n",
      "epoch: 1800, train loss: 3.9114930629730225, val loss: 3.9579962491989136, ETA in seconds: 14082.922\n",
      "epoch: 1900, train loss: 3.8939175605773926, val loss: 3.958182215690613, ETA in seconds: 14808.498\n",
      "epoch: 2000, train loss: 3.8985945701599123, val loss: 3.953495669364929, ETA in seconds: 15533.812\n",
      "epoch: 2100, train loss: 3.901322770118713, val loss: 3.9429731130599976, ETA in seconds: 16270.256\n",
      "epoch: 2200, train loss: 3.8936471939086914, val loss: 3.95384361743927, ETA in seconds: 17023.556\n",
      "epoch: 2300, train loss: 3.9026904582977293, val loss: 3.9458694219589234, ETA in seconds: 17783.948\n",
      "epoch: 2400, train loss: 3.9034595489501953, val loss: 3.9415683269500734, ETA in seconds: 18539.033\n",
      "epoch: 2500, train loss: 3.9028682708740234, val loss: 3.96127610206604, ETA in seconds: 19278.667\n",
      "epoch: 2600, train loss: 3.892717409133911, val loss: 3.9472164154052733, ETA in seconds: 20010.004\n",
      "epoch: 2700, train loss: 3.886265754699707, val loss: 3.953854775428772, ETA in seconds: 20764.668\n",
      "epoch: 2800, train loss: 3.8946568727493287, val loss: 3.93822979927063, ETA in seconds: 21522.310\n",
      "epoch: 2900, train loss: 3.9038291215896606, val loss: 3.953452730178833, ETA in seconds: 22272.748\n",
      "epoch: 3000, train loss: 3.8819573879241944, val loss: 3.936670446395874, ETA in seconds: 23029.188\n",
      "epoch: 3100, train loss: 3.9010860204696653, val loss: 3.9395867824554442, ETA in seconds: 23789.679\n",
      "epoch: 3200, train loss: 3.9060921907424926, val loss: 3.9455767393112184, ETA in seconds: 24529.781\n",
      "epoch: 3300, train loss: 3.9077426195144653, val loss: 3.938574457168579, ETA in seconds: 25267.871\n",
      "epoch: 3400, train loss: 3.8965256929397585, val loss: 3.933209276199341, ETA in seconds: 26007.821\n",
      "epoch: 3500, train loss: 3.8940645456314087, val loss: 3.9462968349456786, ETA in seconds: 26747.633\n",
      "epoch: 3600, train loss: 3.8913344383239745, val loss: 3.9369898557662966, ETA in seconds: 27482.585\n",
      "epoch: 3700, train loss: 3.8913243770599366, val loss: 3.9598556756973267, ETA in seconds: 28216.653\n",
      "epoch: 3800, train loss: 3.8962018966674803, val loss: 3.940523099899292, ETA in seconds: 28944.837\n",
      "epoch: 3900, train loss: 3.887409210205078, val loss: 3.9583044767379763, ETA in seconds: 29660.091\n",
      "epoch: 4000, train loss: 3.8913138151168822, val loss: 3.950690197944641, ETA in seconds: 30433.065\n",
      "epoch: 4100, train loss: 3.8952251434326173, val loss: 3.950484347343445, ETA in seconds: 31249.915\n",
      "epoch: 4200, train loss: 3.9024458646774294, val loss: 3.944044804573059, ETA in seconds: 32069.644\n",
      "epoch: 4300, train loss: 3.9071311950683594, val loss: 3.9458065271377563, ETA in seconds: 32900.192\n",
      "epoch: 4400, train loss: 3.89072527885437, val loss: 3.957914614677429, ETA in seconds: 33707.939\n",
      "epoch: 4500, train loss: 3.899100661277771, val loss: 3.955951976776123, ETA in seconds: 34540.345\n",
      "epoch: 4600, train loss: 3.896001935005188, val loss: 3.939166784286499, ETA in seconds: 35306.759\n",
      "epoch: 4700, train loss: 3.896193289756775, val loss: 3.957520341873169, ETA in seconds: 36126.764\n",
      "epoch: 4800, train loss: 3.880574083328247, val loss: 3.939355230331421, ETA in seconds: 36942.141\n",
      "epoch: 4900, train loss: 3.8899449825286867, val loss: 3.9504907369613647, ETA in seconds: 37727.419\n",
      "epoch: 5000, train loss: 3.9034164190292358, val loss: 3.948533225059509, ETA in seconds: 38477.000\n",
      "epoch: 5100, train loss: 3.8983383417129516, val loss: 3.9405308961868286, ETA in seconds: 39213.319\n",
      "epoch: 5200, train loss: 3.8956035375595093, val loss: 3.945799684524536, ETA in seconds: 39944.221\n",
      "epoch: 5300, train loss: 3.8987295627593994, val loss: 3.951463484764099, ETA in seconds: 40667.005\n",
      "epoch: 5400, train loss: 3.8854482650756834, val loss: 3.937210512161255, ETA in seconds: 41392.871\n",
      "epoch: 5500, train loss: 3.8897441387176515, val loss: 3.9292012214660645, ETA in seconds: 42113.586\n",
      "epoch: 5600, train loss: 3.88837525844574, val loss: 3.934083271026611, ETA in seconds: 42845.349\n",
      "epoch: 5700, train loss: 3.87568678855896, val loss: 3.9432610988616945, ETA in seconds: 43588.998\n",
      "epoch: 5800, train loss: 3.905736970901489, val loss: 3.950285768508911, ETA in seconds: 44322.068\n",
      "epoch: 5900, train loss: 3.8943921327590942, val loss: 3.9496482849121093, ETA in seconds: 45248.088\n",
      "epoch: 6000, train loss: 3.885105776786804, val loss: 3.947885608673096, ETA in seconds: 46066.061\n",
      "epoch: 6100, train loss: 3.890278959274292, val loss: 3.9371633291244508, ETA in seconds: 47197.091\n",
      "epoch: 6200, train loss: 3.892565441131592, val loss: 3.948429822921753, ETA in seconds: 48220.301\n",
      "epoch: 6300, train loss: 3.901971125602722, val loss: 3.950158929824829, ETA in seconds: 49123.242\n",
      "epoch: 6400, train loss: 3.898842692375183, val loss: 3.9543784141540526, ETA in seconds: 50043.443\n",
      "epoch: 6500, train loss: 3.8941608667373657, val loss: 3.949472713470459, ETA in seconds: 50934.358\n",
      "epoch: 6600, train loss: 3.900174355506897, val loss: 3.955661749839783, ETA in seconds: 51824.143\n",
      "epoch: 6700, train loss: 3.9000415802001953, val loss: 3.944520092010498, ETA in seconds: 52565.763\n",
      "epoch: 6800, train loss: 3.908225154876709, val loss: 3.9459786415100098, ETA in seconds: 53302.177\n",
      "epoch: 6900, train loss: 3.8899415254592897, val loss: 3.949830484390259, ETA in seconds: 54042.737\n",
      "epoch: 7000, train loss: 3.9022072553634644, val loss: 3.953916239738464, ETA in seconds: 54737.575\n",
      "epoch: 7100, train loss: 3.8987713098526, val loss: 3.9531763553619386, ETA in seconds: 55461.365\n",
      "epoch: 7200, train loss: 3.900115442276001, val loss: 3.9429527044296266, ETA in seconds: 56240.699\n",
      "epoch: 7300, train loss: 3.891766309738159, val loss: 3.944216561317444, ETA in seconds: 57095.852\n",
      "epoch: 7400, train loss: 3.903672432899475, val loss: 3.94686598777771, ETA in seconds: 58045.190\n",
      "epoch: 7500, train loss: 3.88983154296875, val loss: 3.9469969272613525, ETA in seconds: 58970.655\n",
      "epoch: 7600, train loss: 3.895050525665283, val loss: 3.9520546436309814, ETA in seconds: 59697.707\n",
      "epoch: 7700, train loss: 3.901558017730713, val loss: 3.952642011642456, ETA in seconds: 60433.302\n",
      "epoch: 7800, train loss: 3.8958114862442015, val loss: 3.9592220067977903, ETA in seconds: 61182.699\n",
      "epoch: 7900, train loss: 3.9003841876983643, val loss: 3.9506708860397337, ETA in seconds: 61929.650\n",
      "epoch: 8000, train loss: 3.899553108215332, val loss: 3.9503901958465577, ETA in seconds: 62687.671\n",
      "epoch: 8100, train loss: 3.8955018520355225, val loss: 3.9523646593093873, ETA in seconds: 63436.576\n",
      "epoch: 8200, train loss: 3.892483639717102, val loss: 3.9453728437423705, ETA in seconds: 64160.981\n",
      "epoch: 8300, train loss: 3.9087523221969604, val loss: 3.953943967819214, ETA in seconds: 64925.738\n",
      "epoch: 8400, train loss: 3.889166736602783, val loss: 3.962179946899414, ETA in seconds: 65804.925\n",
      "epoch: 8500, train loss: 3.896822190284729, val loss: 3.944886350631714, ETA in seconds: 66684.255\n",
      "epoch: 8600, train loss: 3.8940757751464843, val loss: 3.9382593393325807, ETA in seconds: 67512.070\n",
      "epoch: 8700, train loss: 3.9063090324401855, val loss: 3.9477782249450684, ETA in seconds: 68238.538\n",
      "epoch: 8800, train loss: 3.899683046340942, val loss: 3.9500569820404055, ETA in seconds: 68959.601\n",
      "epoch: 8900, train loss: 3.892707943916321, val loss: 3.9438977956771852, ETA in seconds: 69719.335\n",
      "epoch: 9000, train loss: 3.9065990924835203, val loss: 3.9381933927536013, ETA in seconds: 70542.667\n",
      "epoch: 9100, train loss: 3.8975072860717774, val loss: 3.9480082035064696, ETA in seconds: 71228.162\n",
      "epoch: 9200, train loss: 3.8929962635040285, val loss: 3.9668734550476072, ETA in seconds: 71928.692\n",
      "epoch: 9300, train loss: 3.8841503620147706, val loss: 3.952240800857544, ETA in seconds: 72679.386\n",
      "epoch: 9400, train loss: 3.8961127519607546, val loss: 3.946358323097229, ETA in seconds: 73421.599\n",
      "epoch: 9500, train loss: 3.8953105211257935, val loss: 3.951105761528015, ETA in seconds: 74261.717\n",
      "epoch: 9600, train loss: 3.89734263420105, val loss: 3.9653730392456055, ETA in seconds: 75386.721\n",
      "epoch: 9700, train loss: 3.895050597190857, val loss: 3.9479921579360964, ETA in seconds: 76557.221\n",
      "epoch: 9800, train loss: 3.891633129119873, val loss: 3.952781891822815, ETA in seconds: 77478.030\n",
      "epoch: 9900, train loss: 3.8942826271057127, val loss: 3.9471431016921996, ETA in seconds: 78198.440\n",
      "epoch: 10000, train loss: 3.887278413772583, val loss: 3.9445154666900635, ETA in seconds: 78937.787\n",
      "epoch: 10100, train loss: 3.8905502557754517, val loss: 3.967059111595154, ETA in seconds: 79664.510\n",
      "epoch: 10200, train loss: 3.9016039848327635, val loss: 3.9475120306015015, ETA in seconds: 80386.822\n",
      "epoch: 10300, train loss: 3.8950547695159914, val loss: 3.9475403785705567, ETA in seconds: 81095.128\n",
      "epoch: 10400, train loss: 3.901513195037842, val loss: 3.953255844116211, ETA in seconds: 81834.706\n",
      "epoch: 10500, train loss: 3.898173522949219, val loss: 3.956629419326782, ETA in seconds: 82566.117\n",
      "epoch: 10600, train loss: 3.905080556869507, val loss: 3.963016128540039, ETA in seconds: 83337.821\n",
      "epoch: 10700, train loss: 3.894701433181763, val loss: 3.9597873210906984, ETA in seconds: 84124.359\n",
      "epoch: 10800, train loss: 3.904111886024475, val loss: 3.9596452951431274, ETA in seconds: 84855.180\n",
      "epoch: 10900, train loss: 3.9043591737747194, val loss: 3.943946695327759, ETA in seconds: 85743.116\n",
      "epoch: 11000, train loss: 3.905780529975891, val loss: 3.944468593597412, ETA in seconds: 86459.958\n",
      "epoch: 11100, train loss: 3.8968283414840696, val loss: 3.9540225744247435, ETA in seconds: 87153.381\n",
      "epoch: 11200, train loss: 3.8839826822280883, val loss: 3.941340613365173, ETA in seconds: 87871.981\n",
      "epoch: 11300, train loss: 3.8970566749572755, val loss: 3.9345889329910277, ETA in seconds: 88631.575\n",
      "epoch: 11400, train loss: 3.896393084526062, val loss: 3.9480082511901857, ETA in seconds: 89334.749\n",
      "epoch: 11500, train loss: 3.895378041267395, val loss: 3.943210554122925, ETA in seconds: 90052.434\n",
      "epoch: 11600, train loss: 3.8926549196243285, val loss: 3.943340516090393, ETA in seconds: 90797.651\n",
      "epoch: 11700, train loss: 3.889319896697998, val loss: 3.9471232652664185, ETA in seconds: 91536.773\n",
      "epoch: 11800, train loss: 3.8990957021713255, val loss: 3.957659912109375, ETA in seconds: 92281.156\n",
      "epoch: 11900, train loss: 3.8946362733840942, val loss: 3.9440555572509766, ETA in seconds: 92972.691\n",
      "epoch: 12000, train loss: 3.8886224746704103, val loss: 3.9414004564285277, ETA in seconds: 93701.880\n",
      "epoch: 12100, train loss: 3.902091121673584, val loss: 3.9446590900421143, ETA in seconds: 94433.689\n",
      "epoch: 12200, train loss: 3.899624967575073, val loss: 3.948618531227112, ETA in seconds: 95191.800\n",
      "epoch: 12300, train loss: 3.8980772733688354, val loss: 3.958655023574829, ETA in seconds: 95938.886\n",
      "epoch: 12400, train loss: 3.901969075202942, val loss: 3.9573012351989747, ETA in seconds: 96687.335\n",
      "epoch: 12500, train loss: 3.8892906665802003, val loss: 3.9541239976882934, ETA in seconds: 97417.779\n",
      "epoch: 12600, train loss: 3.8944875240325927, val loss: 3.9559323310852053, ETA in seconds: 98129.900\n",
      "epoch: 12700, train loss: 3.8952924251556396, val loss: 3.96094126701355, ETA in seconds: 98850.701\n",
      "epoch: 12800, train loss: 3.8874109268188475, val loss: 3.946628379821777, ETA in seconds: 99585.063\n",
      "epoch: 12900, train loss: 3.8896728277206423, val loss: 3.944562554359436, ETA in seconds: 100301.027\n",
      "epoch: 13000, train loss: 3.9041226625442507, val loss: 3.949531602859497, ETA in seconds: 101023.092\n",
      "epoch: 13100, train loss: 3.8962777137756346, val loss: 3.9492666006088255, ETA in seconds: 101741.265\n",
      "epoch: 13200, train loss: 3.9003385305404663, val loss: 3.9617287158966064, ETA in seconds: 102453.693\n",
      "epoch: 13300, train loss: 3.901039457321167, val loss: 3.9478464841842653, ETA in seconds: 103173.176\n",
      "epoch: 13400, train loss: 3.8870010375976562, val loss: 3.9402032613754274, ETA in seconds: 103887.437\n",
      "epoch: 13500, train loss: 3.89762487411499, val loss: 3.947061634063721, ETA in seconds: 104597.625\n",
      "epoch: 13600, train loss: 3.909626817703247, val loss: 3.9391178607940676, ETA in seconds: 105309.215\n",
      "epoch: 13700, train loss: 3.890100383758545, val loss: 3.950578236579895, ETA in seconds: 106017.851\n",
      "epoch: 13800, train loss: 3.8891437530517576, val loss: 3.951214241981506, ETA in seconds: 106733.493\n",
      "epoch: 13900, train loss: 3.892047166824341, val loss: 3.940701699256897, ETA in seconds: 107456.764\n",
      "epoch: 14000, train loss: 3.897664999961853, val loss: 3.9596149206161497, ETA in seconds: 108205.203\n",
      "epoch: 14100, train loss: 3.8982494354248045, val loss: 3.953791785240173, ETA in seconds: 108932.018\n",
      "epoch: 14200, train loss: 3.885610580444336, val loss: 3.952888822555542, ETA in seconds: 109659.222\n",
      "epoch: 14300, train loss: 3.899340605735779, val loss: 3.95092887878418, ETA in seconds: 110500.216\n",
      "epoch: 14400, train loss: 3.8910746335983277, val loss: 3.951860499382019, ETA in seconds: 111216.624\n",
      "epoch: 14500, train loss: 3.891438388824463, val loss: 3.9493423223495485, ETA in seconds: 111943.307\n",
      "epoch: 14600, train loss: 3.899919891357422, val loss: 3.9410393476486205, ETA in seconds: 112654.909\n",
      "epoch: 14700, train loss: 3.889385533332825, val loss: 3.9552964210510253, ETA in seconds: 113363.920\n",
      "epoch: 14800, train loss: 3.8927656173706056, val loss: 3.950087141990662, ETA in seconds: 114085.147\n",
      "epoch: 14900, train loss: 3.8960650682449343, val loss: 3.9476367235183716, ETA in seconds: 114800.144\n",
      "epoch: 15000, train loss: 3.886484694480896, val loss: 3.939187788963318, ETA in seconds: 115509.074\n",
      "epoch: 15100, train loss: 3.90174024105072, val loss: 3.9472431421279905, ETA in seconds: 116217.948\n",
      "epoch: 15200, train loss: 3.8875766515731813, val loss: 3.9459378242492678, ETA in seconds: 116934.225\n",
      "epoch: 15300, train loss: 3.904268264770508, val loss: 3.949812412261963, ETA in seconds: 117635.155\n",
      "epoch: 15400, train loss: 3.8977333545684814, val loss: 3.945164513587952, ETA in seconds: 118339.166\n",
      "epoch: 15500, train loss: 3.8868942737579344, val loss: 3.9407143354415894, ETA in seconds: 119015.552\n",
      "epoch: 15600, train loss: 3.906237745285034, val loss: 3.957394337654114, ETA in seconds: 119713.799\n",
      "epoch: 15700, train loss: 3.889253854751587, val loss: 3.9581073045730593, ETA in seconds: 120437.799\n",
      "epoch: 15800, train loss: 3.8882803916931152, val loss: 3.9436042308807373, ETA in seconds: 121162.207\n",
      "epoch: 15900, train loss: 3.8992066621780395, val loss: 3.9469722270965577, ETA in seconds: 121869.030\n",
      "epoch: 16000, train loss: 3.9057333946228026, val loss: 3.943681335449219, ETA in seconds: 122576.453\n",
      "epoch: 16100, train loss: 3.8984993934631347, val loss: 3.9455464124679565, ETA in seconds: 123281.724\n",
      "epoch: 16200, train loss: 3.8863628625869753, val loss: 3.9544206619262696, ETA in seconds: 123993.187\n",
      "epoch: 16300, train loss: 3.9011229991912844, val loss: 3.9590915203094483, ETA in seconds: 124700.903\n",
      "epoch: 16400, train loss: 3.894754719734192, val loss: 3.939600706100464, ETA in seconds: 125404.157\n",
      "epoch: 16500, train loss: 3.891574192047119, val loss: 3.956007146835327, ETA in seconds: 126108.420\n",
      "epoch: 16600, train loss: 3.904581356048584, val loss: 3.9558403730392455, ETA in seconds: 126822.970\n",
      "epoch: 16700, train loss: 3.896120858192444, val loss: 3.9500964164733885, ETA in seconds: 127529.307\n",
      "epoch: 16800, train loss: 3.8954986572265624, val loss: 3.9508801698684692, ETA in seconds: 128232.440\n",
      "epoch: 16900, train loss: 3.896263861656189, val loss: 3.9494609117507933, ETA in seconds: 128940.287\n",
      "epoch: 17000, train loss: 3.8921003103256226, val loss: 3.9606951475143433, ETA in seconds: 129643.660\n",
      "epoch: 17100, train loss: 3.8932649374008177, val loss: 3.9303298950195313, ETA in seconds: 130349.312\n",
      "epoch: 17200, train loss: 3.899489951133728, val loss: 3.949181914329529, ETA in seconds: 131059.359\n",
      "epoch: 17300, train loss: 3.8966453313827514, val loss: 3.956514811515808, ETA in seconds: 131762.400\n",
      "epoch: 17400, train loss: 3.893105125427246, val loss: 3.938684272766113, ETA in seconds: 132465.500\n",
      "epoch: 17500, train loss: 3.902576732635498, val loss: 3.9529205322265626, ETA in seconds: 133170.188\n",
      "epoch: 17600, train loss: 3.903718876838684, val loss: 3.9525882244110107, ETA in seconds: 133877.879\n",
      "epoch: 17700, train loss: 3.8932934522628786, val loss: 3.9470107316970826, ETA in seconds: 134579.735\n",
      "epoch: 17800, train loss: 3.8987780094146727, val loss: 3.94302442073822, ETA in seconds: 135284.331\n",
      "epoch: 17900, train loss: 3.901478481292725, val loss: 3.954926872253418, ETA in seconds: 135994.294\n",
      "epoch: 18000, train loss: 3.899786281585693, val loss: 3.943573522567749, ETA in seconds: 136692.364\n",
      "epoch: 18100, train loss: 3.8979691743850706, val loss: 3.9381231307983398, ETA in seconds: 137396.279\n",
      "epoch: 18200, train loss: 3.8906871557235716, val loss: 3.951974630355835, ETA in seconds: 138095.149\n",
      "epoch: 18300, train loss: 3.8997530698776246, val loss: 3.946590542793274, ETA in seconds: 138796.025\n",
      "epoch: 18400, train loss: 3.8999919652938844, val loss: 3.950429153442383, ETA in seconds: 139497.331\n",
      "epoch: 18500, train loss: 3.8924582958221436, val loss: 3.9341151475906373, ETA in seconds: 140197.749\n",
      "epoch: 18600, train loss: 3.8937223672866823, val loss: 3.9513101100921633, ETA in seconds: 140897.135\n",
      "epoch: 18700, train loss: 3.8828534364700316, val loss: 3.9513213872909545, ETA in seconds: 141604.206\n",
      "epoch: 18800, train loss: 3.8941324234008787, val loss: 3.9409026384353636, ETA in seconds: 142311.762\n",
      "epoch: 18900, train loss: 3.892049860954285, val loss: 3.9385465145111085, ETA in seconds: 143011.858\n",
      "epoch: 19000, train loss: 3.902701735496521, val loss: 3.945751261711121, ETA in seconds: 143757.745\n",
      "epoch: 19100, train loss: 3.893103528022766, val loss: 3.9483259439468386, ETA in seconds: 144548.605\n",
      "epoch: 19200, train loss: 3.886990213394165, val loss: 3.946919560432434, ETA in seconds: 145217.550\n",
      "epoch: 19300, train loss: 3.8940272092819215, val loss: 3.9539769172668455, ETA in seconds: 145902.100\n",
      "epoch: 19400, train loss: 3.8919119119644163, val loss: 3.961435914039612, ETA in seconds: 146614.659\n",
      "epoch: 19500, train loss: 3.892841410636902, val loss: 3.9449636459350588, ETA in seconds: 147328.439\n",
      "epoch: 19600, train loss: 3.893720602989197, val loss: 3.967877674102783, ETA in seconds: 148033.620\n",
      "epoch: 19700, train loss: 3.8864386320114135, val loss: 3.964750599861145, ETA in seconds: 148743.554\n",
      "epoch: 19800, train loss: 3.8958865880966185, val loss: 3.9550556421279905, ETA in seconds: 149463.312\n",
      "epoch: 19900, train loss: 3.886907458305359, val loss: 3.967493987083435, ETA in seconds: 150153.408\n",
      "epoch: 20000, train loss: 3.891665291786194, val loss: 3.9666762590408324, ETA in seconds: 150855.915\n",
      "epoch: 20100, train loss: 3.8992453813552856, val loss: 3.96201491355896, ETA in seconds: 151557.098\n",
      "epoch: 20200, train loss: 3.899545454978943, val loss: 3.959450364112854, ETA in seconds: 152285.203\n",
      "epoch: 20300, train loss: 3.8799180030822753, val loss: 3.9799264430999757, ETA in seconds: 153143.756\n",
      "epoch: 20400, train loss: 3.893935227394104, val loss: 3.968355822563171, ETA in seconds: 153874.052\n",
      "epoch: 20500, train loss: 3.8957215785980224, val loss: 3.9732194423675535, ETA in seconds: 154550.228\n",
      "epoch: 20600, train loss: 3.888875198364258, val loss: 3.956276702880859, ETA in seconds: 155258.481\n",
      "epoch: 20700, train loss: 3.8925065755844117, val loss: 3.968748688697815, ETA in seconds: 155967.403\n",
      "epoch: 20800, train loss: 3.896496319770813, val loss: 3.966906762123108, ETA in seconds: 156698.874\n",
      "epoch: 20900, train loss: 3.8905033826828004, val loss: 3.969520998001099, ETA in seconds: 157582.128\n",
      "epoch: 21000, train loss: 3.8890588521957397, val loss: 3.962165904045105, ETA in seconds: 158435.102\n",
      "epoch: 21100, train loss: 3.885713791847229, val loss: 3.970025157928467, ETA in seconds: 159323.181\n",
      "epoch: 21200, train loss: 3.8996337413787843, val loss: 3.9611551761627197, ETA in seconds: 160150.710\n",
      "epoch: 21300, train loss: 3.895282340049744, val loss: 3.9583009004592897, ETA in seconds: 160974.069\n",
      "epoch: 21400, train loss: 3.9004096508026125, val loss: 3.970227861404419, ETA in seconds: 161797.132\n",
      "epoch: 21500, train loss: 3.890725326538086, val loss: 3.9752019882202148, ETA in seconds: 162518.444\n",
      "epoch: 21600, train loss: 3.897296643257141, val loss: 3.9821252107620237, ETA in seconds: 163236.326\n",
      "epoch: 21700, train loss: 3.900457262992859, val loss: 3.9724316596984863, ETA in seconds: 163997.276\n",
      "epoch: 21800, train loss: 3.8976861476898192, val loss: 3.9677695512771605, ETA in seconds: 164688.644\n",
      "epoch: 21900, train loss: 3.897291564941406, val loss: 3.9708122968673707, ETA in seconds: 165405.740\n",
      "epoch: 22000, train loss: 3.8896443843841553, val loss: 3.9635913610458373, ETA in seconds: 166151.530\n",
      "epoch: 22100, train loss: 3.883081078529358, val loss: 3.9700643539428713, ETA in seconds: 166836.419\n",
      "epoch: 22200, train loss: 3.891047978401184, val loss: 3.97330584526062, ETA in seconds: 167510.910\n",
      "epoch: 22300, train loss: 3.8947492361068727, val loss: 3.976045751571655, ETA in seconds: 168227.720\n",
      "epoch: 22400, train loss: 3.890423607826233, val loss: 3.975966191291809, ETA in seconds: 168962.904\n",
      "epoch: 22500, train loss: 3.8905574798583986, val loss: 3.9775672435760496, ETA in seconds: 169863.182\n",
      "epoch: 22600, train loss: 3.894964361190796, val loss: 3.9725171089172364, ETA in seconds: 170751.300\n",
      "epoch: 22700, train loss: 3.8905408143997193, val loss: 3.9630420207977295, ETA in seconds: 171560.558\n",
      "epoch: 22800, train loss: 3.901104950904846, val loss: 3.965086269378662, ETA in seconds: 172270.853\n",
      "epoch: 22900, train loss: 3.890404295921326, val loss: 3.9731785535812376, ETA in seconds: 172983.603\n",
      "epoch: 23000, train loss: 3.890702414512634, val loss: 3.96799373626709, ETA in seconds: 173686.246\n",
      "epoch: 23100, train loss: 3.894327974319458, val loss: 3.9739704370498656, ETA in seconds: 174410.372\n",
      "epoch: 23200, train loss: 3.9080420970916747, val loss: 3.977709484100342, ETA in seconds: 175161.699\n",
      "epoch: 23300, train loss: 3.900543475151062, val loss: 3.9703867435455322, ETA in seconds: 176031.200\n",
      "epoch: 23400, train loss: 3.907444977760315, val loss: 3.9776023387908936, ETA in seconds: 176813.314\n",
      "epoch: 23500, train loss: 3.8882044315338136, val loss: 3.966499948501587, ETA in seconds: 177599.480\n",
      "epoch: 23600, train loss: 3.9027824878692625, val loss: 3.970277762413025, ETA in seconds: 178364.512\n",
      "epoch: 23700, train loss: 3.887506604194641, val loss: 3.9661065101623536, ETA in seconds: 179071.568\n",
      "epoch: 23800, train loss: 3.89687180519104, val loss: 3.983180546760559, ETA in seconds: 179779.553\n",
      "epoch: 23900, train loss: 3.893153738975525, val loss: 3.9691150903701784, ETA in seconds: 180500.264\n",
      "epoch: 24000, train loss: 3.899031710624695, val loss: 3.9783030271530153, ETA in seconds: 181414.890\n",
      "epoch: 24100, train loss: 3.8983994722366333, val loss: 3.9744343519210816, ETA in seconds: 182351.089\n",
      "epoch: 24200, train loss: 3.899380016326904, val loss: 3.9651113748550415, ETA in seconds: 183283.034\n",
      "epoch: 24300, train loss: 3.897023820877075, val loss: 3.9677157163619996, ETA in seconds: 184200.658\n",
      "epoch: 24400, train loss: 3.8984025001525877, val loss: 3.9724502086639406, ETA in seconds: 184939.045\n",
      "epoch: 24500, train loss: 3.8951106309890746, val loss: 3.9756121397018434, ETA in seconds: 185789.367\n",
      "epoch: 24600, train loss: 3.8936166524887086, val loss: 3.9632198333740236, ETA in seconds: 186600.630\n",
      "epoch: 24700, train loss: 3.8971280097961425, val loss: 3.97377028465271, ETA in seconds: 187360.730\n",
      "epoch: 24800, train loss: 3.89535539150238, val loss: 3.9766143560409546, ETA in seconds: 188126.389\n",
      "epoch: 24900, train loss: 3.8979795217514037, val loss: 3.9680163860321045, ETA in seconds: 188868.711\n",
      "epoch: 25000, train loss: 3.8942359685897827, val loss: 3.9757224798202513, ETA in seconds: 189583.082\n",
      "epoch: 25100, train loss: 3.896506977081299, val loss: 3.9644359827041624, ETA in seconds: 190293.091\n",
      "epoch: 25200, train loss: 3.8907593965530394, val loss: 3.974667549133301, ETA in seconds: 191017.382\n",
      "epoch: 25300, train loss: 3.886487340927124, val loss: 3.9653314113616944, ETA in seconds: 191801.570\n",
      "epoch: 25400, train loss: 3.888578939437866, val loss: 3.9703147411346436, ETA in seconds: 192674.518\n",
      "epoch: 25500, train loss: 3.885174870491028, val loss: 3.967280554771423, ETA in seconds: 193547.929\n",
      "epoch: 25600, train loss: 3.8945683240890503, val loss: 3.9708372354507446, ETA in seconds: 194430.954\n",
      "epoch: 25700, train loss: 3.8930843591690065, val loss: 3.971303868293762, ETA in seconds: 195303.432\n",
      "epoch: 25800, train loss: 3.8855329751968384, val loss: 3.9711846590042112, ETA in seconds: 196167.956\n",
      "epoch: 25900, train loss: 3.8837392568588256, val loss: 3.9703507661819457, ETA in seconds: 197022.909\n",
      "epoch: 26000, train loss: 3.89764347076416, val loss: 3.963666796684265, ETA in seconds: 197793.126\n",
      "epoch: 26100, train loss: 3.9028712272644044, val loss: 3.961634039878845, ETA in seconds: 198558.553\n",
      "epoch: 26200, train loss: 3.8909933090209963, val loss: 3.962912392616272, ETA in seconds: 199349.846\n",
      "epoch: 26300, train loss: 3.8974199533462524, val loss: 3.9606039047241213, ETA in seconds: 200117.754\n",
      "epoch: 26400, train loss: 3.905376434326172, val loss: 3.952854800224304, ETA in seconds: 200920.463\n",
      "epoch: 26500, train loss: 3.8897536277770994, val loss: 3.9721248626708983, ETA in seconds: 201671.584\n",
      "epoch: 26600, train loss: 3.8973261594772337, val loss: 3.971065640449524, ETA in seconds: 202444.430\n",
      "epoch: 26700, train loss: 3.889985203742981, val loss: 3.9746271133422852, ETA in seconds: 203215.522\n",
      "epoch: 26800, train loss: 3.9035648822784426, val loss: 3.963671064376831, ETA in seconds: 203948.997\n",
      "epoch: 26900, train loss: 3.9030280113220215, val loss: 3.981073784828186, ETA in seconds: 204702.008\n",
      "epoch: 27000, train loss: 3.896459364891052, val loss: 3.9691505432128906, ETA in seconds: 205433.697\n",
      "epoch: 27100, train loss: 3.900605082511902, val loss: 3.9832037687301636, ETA in seconds: 206209.214\n",
      "epoch: 27200, train loss: 3.894482660293579, val loss: 3.971086931228638, ETA in seconds: 207021.565\n",
      "epoch: 27300, train loss: 3.890812373161316, val loss: 3.9593833923339843, ETA in seconds: 207830.718\n",
      "epoch: 27400, train loss: 3.902199721336365, val loss: 3.9686620473861693, ETA in seconds: 208639.350\n",
      "epoch: 27500, train loss: 3.877810907363892, val loss: 3.9646689176559446, ETA in seconds: 209405.056\n",
      "epoch: 27600, train loss: 3.9068993091583253, val loss: 3.9645405292510985, ETA in seconds: 210128.505\n",
      "epoch: 27700, train loss: 3.9006063461303713, val loss: 3.967500638961792, ETA in seconds: 210883.908\n",
      "epoch: 27800, train loss: 3.9010369300842287, val loss: 3.9701038599014282, ETA in seconds: 211732.367\n",
      "epoch: 27900, train loss: 3.8959712982177734, val loss: 3.965855669975281, ETA in seconds: 212579.308\n",
      "epoch: 28000, train loss: 3.89220073223114, val loss: 3.9701476097106934, ETA in seconds: 213378.681\n",
      "epoch: 28100, train loss: 3.9052417993545534, val loss: 3.9644924879074095, ETA in seconds: 214221.284\n",
      "epoch: 28200, train loss: 3.8952399253845216, val loss: 3.9658883810043335, ETA in seconds: 215061.971\n",
      "epoch: 28300, train loss: 3.888907790184021, val loss: 3.9639271974563597, ETA in seconds: 215812.821\n",
      "epoch: 28400, train loss: 3.902590608596802, val loss: 3.9680138349533083, ETA in seconds: 216569.781\n",
      "epoch: 28500, train loss: 3.8986870765686037, val loss: 3.967938017845154, ETA in seconds: 217276.133\n",
      "epoch: 28600, train loss: 3.901315140724182, val loss: 3.9718814373016356, ETA in seconds: 217963.688\n",
      "epoch: 28700, train loss: 3.8943416833877564, val loss: 3.9562987804412844, ETA in seconds: 218654.351\n",
      "epoch: 28800, train loss: 3.889883780479431, val loss: 3.972035121917725, ETA in seconds: 219359.082\n",
      "epoch: 28900, train loss: 3.894412136077881, val loss: 3.9665477514266967, ETA in seconds: 220087.833\n",
      "epoch: 29000, train loss: 3.891921782493591, val loss: 3.9683170557022094, ETA in seconds: 220780.555\n",
      "epoch: 29100, train loss: 3.8936508893966675, val loss: 3.964822220802307, ETA in seconds: 221459.620\n",
      "epoch: 29200, train loss: 3.8973703622817992, val loss: 3.9628681898117066, ETA in seconds: 222203.942\n",
      "epoch: 29300, train loss: 3.8877347707748413, val loss: 3.9769856691360475, ETA in seconds: 222940.343\n",
      "epoch: 29400, train loss: 3.895121717453003, val loss: 3.967820334434509, ETA in seconds: 223673.195\n",
      "epoch: 29500, train loss: 3.8887808084487916, val loss: 3.9633042097091673, ETA in seconds: 224416.267\n",
      "epoch: 29600, train loss: 3.8946019887924193, val loss: 3.9532236576080324, ETA in seconds: 225124.603\n",
      "epoch: 29700, train loss: 3.8902570009231567, val loss: 3.9682059049606324, ETA in seconds: 225838.534\n",
      "epoch: 29800, train loss: 3.898673176765442, val loss: 3.961893892288208, ETA in seconds: 226546.214\n",
      "epoch: 29900, train loss: 3.9015389919281005, val loss: 3.9660083293914794, ETA in seconds: 227299.588\n",
      "epoch: 30000, train loss: 3.883862090110779, val loss: 3.9779123783111574, ETA in seconds: 228056.128\n",
      "epoch: 30100, train loss: 3.893261027336121, val loss: 3.9737378120422364, ETA in seconds: 228819.136\n",
      "epoch: 30200, train loss: 3.8871275663375853, val loss: 3.9662894725799562, ETA in seconds: 229526.338\n",
      "epoch: 30300, train loss: 3.8857776880264283, val loss: 3.9668336391448973, ETA in seconds: 230203.317\n",
      "epoch: 30400, train loss: 3.889822244644165, val loss: 3.970070648193359, ETA in seconds: 230903.146\n",
      "epoch: 30500, train loss: 3.895791840553284, val loss: 3.977678632736206, ETA in seconds: 231671.285\n",
      "epoch: 30600, train loss: 3.8906319379806518, val loss: 3.976543998718262, ETA in seconds: 232428.751\n",
      "epoch: 30700, train loss: 3.8943833351135253, val loss: 3.975157690048218, ETA in seconds: 233172.675\n",
      "epoch: 30800, train loss: 3.8896815538406373, val loss: 3.9751301527023317, ETA in seconds: 233866.207\n",
      "epoch: 30900, train loss: 3.88524215221405, val loss: 3.9621705770492555, ETA in seconds: 234546.639\n",
      "epoch: 31000, train loss: 3.8896063804626464, val loss: 3.969988775253296, ETA in seconds: 235288.519\n",
      "epoch: 31100, train loss: 3.8868992567062377, val loss: 3.9643946409225466, ETA in seconds: 236038.290\n",
      "epoch: 31200, train loss: 3.89898567199707, val loss: 3.982188653945923, ETA in seconds: 236748.509\n",
      "epoch: 31300, train loss: 3.8992878437042235, val loss: 3.9663776397705077, ETA in seconds: 237459.655\n",
      "epoch: 31400, train loss: 3.8896207332611086, val loss: 3.9535181283950807, ETA in seconds: 238180.716\n",
      "epoch: 31500, train loss: 3.896763777732849, val loss: 3.9693616390228272, ETA in seconds: 238867.432\n",
      "epoch: 31600, train loss: 3.887446475028992, val loss: 3.970432472229004, ETA in seconds: 239558.490\n",
      "epoch: 31700, train loss: 3.8912671566009522, val loss: 3.9765658617019652, ETA in seconds: 240266.340\n",
      "epoch: 31800, train loss: 3.891864800453186, val loss: 3.975291204452515, ETA in seconds: 240950.981\n",
      "epoch: 31900, train loss: 3.891294002532959, val loss: 3.972152304649353, ETA in seconds: 241641.446\n",
      "epoch: 32000, train loss: 3.8900071382522583, val loss: 3.9646841526031493, ETA in seconds: 242330.119\n",
      "epoch: 32100, train loss: 3.886231565475464, val loss: 3.9650619506835936, ETA in seconds: 243018.733\n",
      "epoch: 32200, train loss: 3.8959901571273803, val loss: 3.9820491790771486, ETA in seconds: 243721.112\n",
      "epoch: 32300, train loss: 3.8985437154769897, val loss: 3.9653409242630007, ETA in seconds: 244443.692\n",
      "epoch: 32400, train loss: 3.897282910346985, val loss: 3.974814772605896, ETA in seconds: 245135.162\n",
      "epoch: 32500, train loss: 3.89846556186676, val loss: 3.97363862991333, ETA in seconds: 245882.893\n",
      "epoch: 32600, train loss: 3.885792541503906, val loss: 3.9765279293060303, ETA in seconds: 246598.252\n",
      "epoch: 32700, train loss: 3.8958166599273683, val loss: 3.968004083633423, ETA in seconds: 247312.115\n",
      "epoch: 32800, train loss: 3.8930623054504396, val loss: 3.9796857118606566, ETA in seconds: 248068.738\n",
      "epoch: 32900, train loss: 3.9001861810684204, val loss: 3.973754334449768, ETA in seconds: 248776.629\n",
      "epoch: 33000, train loss: 3.8954647302627565, val loss: 3.9721639156341553, ETA in seconds: 249524.278\n",
      "epoch: 33100, train loss: 3.8868521213531495, val loss: 3.9632431745529173, ETA in seconds: 250262.406\n",
      "epoch: 33200, train loss: 3.8848329067230223, val loss: 3.964804530143738, ETA in seconds: 251080.653\n",
      "epoch: 33300, train loss: 3.8973981618881224, val loss: 3.9857988119125367, ETA in seconds: 251915.578\n",
      "epoch: 33400, train loss: 3.8933852672576905, val loss: 3.9764487981796264, ETA in seconds: 252751.691\n",
      "epoch: 33500, train loss: 3.8947551012039185, val loss: 3.967123866081238, ETA in seconds: 253593.576\n",
      "epoch: 33600, train loss: 3.888198447227478, val loss: 3.9821181774139403, ETA in seconds: 254427.800\n",
      "epoch: 33700, train loss: 3.891455888748169, val loss: 3.9605268478393554, ETA in seconds: 255168.416\n",
      "epoch: 33800, train loss: 3.8942572355270384, val loss: 3.969947361946106, ETA in seconds: 255877.650\n",
      "epoch: 33900, train loss: 3.904171514511108, val loss: 3.977229928970337, ETA in seconds: 256567.849\n",
      "epoch: 34000, train loss: 3.8922775745391847, val loss: 3.9702805042266847, ETA in seconds: 257263.600\n",
      "epoch: 34100, train loss: 3.899812030792236, val loss: 3.98072407245636, ETA in seconds: 257930.159\n",
      "epoch: 34200, train loss: 3.889849615097046, val loss: 3.984458255767822, ETA in seconds: 258629.153\n",
      "epoch: 34300, train loss: 3.8835986852645874, val loss: 3.957053518295288, ETA in seconds: 259329.300\n",
      "epoch: 34400, train loss: 3.895149564743042, val loss: 3.9617894887924194, ETA in seconds: 260018.644\n",
      "epoch: 34500, train loss: 3.8917144536972046, val loss: 3.9717641115188598, ETA in seconds: 260707.564\n",
      "epoch: 34600, train loss: 3.899186396598816, val loss: 3.962411975860596, ETA in seconds: 261404.531\n",
      "epoch: 34700, train loss: 3.8894774436950685, val loss: 3.9670416831970217, ETA in seconds: 262102.092\n",
      "epoch: 34800, train loss: 3.887690877914429, val loss: 3.9700484752655028, ETA in seconds: 262787.808\n",
      "epoch: 34900, train loss: 3.8945085763931275, val loss: 3.9632442951202393, ETA in seconds: 263472.040\n",
      "epoch: 35000, train loss: 3.893184781074524, val loss: 3.966419744491577, ETA in seconds: 264127.585\n",
      "epoch: 35100, train loss: 3.889688801765442, val loss: 3.9746595859527587, ETA in seconds: 264776.175\n",
      "epoch: 35200, train loss: 3.9001544713974, val loss: 3.972142481803894, ETA in seconds: 265512.760\n",
      "epoch: 35300, train loss: 3.895636487007141, val loss: 3.9880730152130126, ETA in seconds: 266362.699\n",
      "epoch: 35400, train loss: 3.8915920734405516, val loss: 3.9585631608963014, ETA in seconds: 267093.175\n",
      "epoch: 35500, train loss: 3.8951821327209473, val loss: 3.97441680431366, ETA in seconds: 267967.387\n",
      "epoch: 35600, train loss: 3.891116404533386, val loss: 3.9713544130325316, ETA in seconds: 268719.196\n",
      "epoch: 35700, train loss: 3.904209327697754, val loss: 3.9781863689422607, ETA in seconds: 269366.562\n",
      "epoch: 35800, train loss: 3.8942198038101195, val loss: 3.967175006866455, ETA in seconds: 270085.181\n",
      "epoch: 35900, train loss: 3.901306629180908, val loss: 3.9623520612716674, ETA in seconds: 270918.287\n",
      "epoch: 36000, train loss: 3.89295494556427, val loss: 3.9746718406677246, ETA in seconds: 271778.259\n",
      "epoch: 36100, train loss: 3.893436908721924, val loss: 3.9719566345214843, ETA in seconds: 272513.873\n",
      "epoch: 36200, train loss: 3.8861932039260862, val loss: 3.9819499254226685, ETA in seconds: 273187.674\n",
      "epoch: 36300, train loss: 3.892001485824585, val loss: 3.975599765777588, ETA in seconds: 273861.074\n",
      "epoch: 36400, train loss: 3.8959465265274047, val loss: 3.9730402708053587, ETA in seconds: 274538.271\n",
      "epoch: 36500, train loss: 3.8936803102493287, val loss: 3.9722028493881227, ETA in seconds: 275204.645\n",
      "epoch: 36600, train loss: 3.896546745300293, val loss: 3.9683205127716064, ETA in seconds: 276037.895\n",
      "epoch: 36700, train loss: 3.9004236459732056, val loss: 3.964331793785095, ETA in seconds: 276827.115\n",
      "epoch: 36800, train loss: 3.8937074661254885, val loss: 3.9750515937805178, ETA in seconds: 277516.583\n",
      "epoch: 36900, train loss: 3.8852606058120727, val loss: 3.962483620643616, ETA in seconds: 278204.879\n",
      "epoch: 37000, train loss: 3.8959057569503783, val loss: 3.9623239517211912, ETA in seconds: 278883.726\n",
      "epoch: 37100, train loss: 3.895201325416565, val loss: 3.9739325523376463, ETA in seconds: 279569.191\n",
      "epoch: 37200, train loss: 3.898301100730896, val loss: 3.961261487007141, ETA in seconds: 280251.384\n",
      "epoch: 37300, train loss: 3.8957781076431273, val loss: 3.9628178119659423, ETA in seconds: 280941.735\n",
      "epoch: 37400, train loss: 3.901458191871643, val loss: 3.9774384260177613, ETA in seconds: 281631.134\n",
      "epoch: 37500, train loss: 3.8896358489990233, val loss: 3.9518110513687135, ETA in seconds: 282306.139\n",
      "epoch: 37600, train loss: 3.894784426689148, val loss: 3.9599013805389403, ETA in seconds: 283028.744\n",
      "epoch: 37700, train loss: 3.8852718591690065, val loss: 3.9644810438156126, ETA in seconds: 283757.503\n",
      "epoch: 37800, train loss: 3.8972822189331056, val loss: 3.9664607763290407, ETA in seconds: 284501.271\n",
      "epoch: 37900, train loss: 3.8969796895980835, val loss: 3.958489751815796, ETA in seconds: 285286.633\n",
      "epoch: 38000, train loss: 3.898672604560852, val loss: 3.9740081787109376, ETA in seconds: 286014.928\n",
      "epoch: 38100, train loss: 3.8919128656387327, val loss: 3.9643231868743896, ETA in seconds: 286715.768\n",
      "epoch: 38200, train loss: 3.903664493560791, val loss: 3.962764310836792, ETA in seconds: 287457.143\n",
      "epoch: 38300, train loss: 3.896631121635437, val loss: 3.9769577026367187, ETA in seconds: 288328.139\n",
      "epoch: 38400, train loss: 3.8933770656585693, val loss: 3.960060954093933, ETA in seconds: 289033.373\n",
      "epoch: 38500, train loss: 3.8919312238693236, val loss: 3.966903305053711, ETA in seconds: 289749.737\n",
      "epoch: 38600, train loss: 3.897670841217041, val loss: 3.961098837852478, ETA in seconds: 290441.941\n",
      "epoch: 38700, train loss: 3.894929885864258, val loss: 3.972175121307373, ETA in seconds: 291130.675\n",
      "epoch: 38800, train loss: 3.8918184995651246, val loss: 3.965330696105957, ETA in seconds: 291843.811\n",
      "epoch: 38900, train loss: 3.8958160161972044, val loss: 3.982814979553223, ETA in seconds: 292682.681\n",
      "epoch: 39000, train loss: 3.8974429845809935, val loss: 3.9660879373550415, ETA in seconds: 293510.137\n",
      "epoch: 39100, train loss: 3.8932719469070434, val loss: 3.968303108215332, ETA in seconds: 294281.167\n",
      "epoch: 39200, train loss: 3.895881175994873, val loss: 3.967639374732971, ETA in seconds: 294984.142\n",
      "epoch: 39300, train loss: 3.889108943939209, val loss: 3.9565979719161986, ETA in seconds: 295681.494\n",
      "epoch: 39400, train loss: 3.8849963903427125, val loss: 3.971223258972168, ETA in seconds: 296419.317\n",
      "epoch: 39500, train loss: 3.9007970809936525, val loss: 3.9788097143173218, ETA in seconds: 297105.736\n",
      "epoch: 39600, train loss: 3.8928064346313476, val loss: 3.9691141843795776, ETA in seconds: 297772.648\n",
      "epoch: 39700, train loss: 3.886489200592041, val loss: 3.9678694009780884, ETA in seconds: 298450.533\n",
      "epoch: 39800, train loss: 3.906198740005493, val loss: 3.976993942260742, ETA in seconds: 299124.269\n",
      "epoch: 39900, train loss: 3.8901799440383913, val loss: 3.96914279460907, ETA in seconds: 299803.805\n",
      "epoch: 40000, train loss: 3.897447609901428, val loss: 3.958565616607666, ETA in seconds: 300487.578\n",
      "epoch: 40100, train loss: 3.89706437587738, val loss: 3.972742033004761, ETA in seconds: 301190.522\n",
      "epoch: 40200, train loss: 3.892104077339172, val loss: 3.9664892673492433, ETA in seconds: 301897.730\n",
      "epoch: 40300, train loss: 3.8902477025985718, val loss: 3.9557374000549315, ETA in seconds: 302596.702\n",
      "epoch: 40400, train loss: 3.8949976682662966, val loss: 3.9608085632324217, ETA in seconds: 303304.365\n",
      "epoch: 40500, train loss: 3.8925959348678587, val loss: 3.9640241384506227, ETA in seconds: 303976.585\n",
      "epoch: 40600, train loss: 3.890931177139282, val loss: 3.950597143173218, ETA in seconds: 304647.249\n",
      "epoch: 40700, train loss: 3.9011979341506957, val loss: 3.9633076429367065, ETA in seconds: 305320.518\n",
      "epoch: 40800, train loss: 3.8905635595321657, val loss: 3.965969133377075, ETA in seconds: 305989.151\n",
      "epoch: 40900, train loss: 3.892877531051636, val loss: 3.9688591957092285, ETA in seconds: 306673.502\n",
      "epoch: 41000, train loss: 3.8897285223007203, val loss: 3.9711231231689452, ETA in seconds: 307317.788\n",
      "epoch: 41100, train loss: 3.8865490674972536, val loss: 3.968252968788147, ETA in seconds: 308007.424\n",
      "epoch: 41200, train loss: 3.8988237619400024, val loss: 3.9648933887481688, ETA in seconds: 308707.221\n",
      "epoch: 41300, train loss: 3.8944005489349367, val loss: 3.981231427192688, ETA in seconds: 309404.362\n",
      "epoch: 41400, train loss: 3.8889270782470704, val loss: 3.986857223510742, ETA in seconds: 310103.294\n",
      "epoch: 41500, train loss: 3.8876959800720217, val loss: 3.9761554479598997, ETA in seconds: 310876.830\n",
      "epoch: 41600, train loss: 3.8871100902557374, val loss: 3.974585843086243, ETA in seconds: 311622.075\n",
      "epoch: 41700, train loss: 3.890837073326111, val loss: 3.966303300857544, ETA in seconds: 312397.364\n",
      "epoch: 41800, train loss: 3.8891287088394164, val loss: 3.9673546075820925, ETA in seconds: 313235.297\n",
      "epoch: 41900, train loss: 3.8838485717773437, val loss: 3.955300045013428, ETA in seconds: 314046.211\n",
      "epoch: 42000, train loss: 3.9029820203781127, val loss: 3.9731176137924193, ETA in seconds: 314806.339\n",
      "epoch: 42100, train loss: 3.893157696723938, val loss: 3.972472143173218, ETA in seconds: 315466.142\n",
      "epoch: 42200, train loss: 3.8952107667922973, val loss: 3.975739669799805, ETA in seconds: 316199.110\n",
      "epoch: 42300, train loss: 3.895429825782776, val loss: 3.974214220046997, ETA in seconds: 317020.676\n",
      "epoch: 42400, train loss: 3.8940871238708494, val loss: 3.9674546003341673, ETA in seconds: 317914.885\n",
      "epoch: 42500, train loss: 3.89112229347229, val loss: 3.9821563243865965, ETA in seconds: 318758.059\n",
      "epoch: 42600, train loss: 3.8918280601501465, val loss: 3.9719504833221437, ETA in seconds: 319447.791\n",
      "epoch: 42700, train loss: 3.8959251642227173, val loss: 3.970065140724182, ETA in seconds: 320207.947\n",
      "epoch: 42800, train loss: 3.8971301078796388, val loss: 3.963758182525635, ETA in seconds: 321053.839\n",
      "epoch: 42900, train loss: 3.891805124282837, val loss: 3.9764200925827025, ETA in seconds: 321894.142\n",
      "epoch: 43000, train loss: 3.899313545227051, val loss: 3.9634835481643678, ETA in seconds: 322743.510\n",
      "epoch: 43100, train loss: 3.8931285619735716, val loss: 3.9717597723007203, ETA in seconds: 323534.816\n",
      "epoch: 43200, train loss: 3.8858423471450805, val loss: 3.9679189682006837, ETA in seconds: 324219.299\n",
      "epoch: 43300, train loss: 3.9026352882385256, val loss: 3.972463059425354, ETA in seconds: 324946.831\n",
      "epoch: 43400, train loss: 3.89220507144928, val loss: 3.9676888942718507, ETA in seconds: 325758.606\n",
      "epoch: 43500, train loss: 3.89993793964386, val loss: 3.9730904340744018, ETA in seconds: 326567.904\n",
      "epoch: 43600, train loss: 3.8940397977828978, val loss: 3.9784826040267944, ETA in seconds: 327410.523\n",
      "epoch: 43700, train loss: 3.883482313156128, val loss: 3.9716542959213257, ETA in seconds: 328297.119\n",
      "epoch: 43800, train loss: 3.889545226097107, val loss: 3.9675004959106444, ETA in seconds: 329204.683\n",
      "epoch: 43900, train loss: 3.891845679283142, val loss: 3.96189546585083, ETA in seconds: 330053.251\n",
      "epoch: 44000, train loss: 3.8909372091293335, val loss: 3.9725926876068116, ETA in seconds: 330836.003\n",
      "epoch: 44100, train loss: 3.8928826570510866, val loss: 3.9575172424316407, ETA in seconds: 331499.644\n",
      "epoch: 44200, train loss: 3.898740291595459, val loss: 3.9680289030075073, ETA in seconds: 332255.548\n",
      "epoch: 44300, train loss: 3.8981515884399416, val loss: 3.9626543045043947, ETA in seconds: 332943.253\n",
      "epoch: 44400, train loss: 3.9062631845474245, val loss: 3.962069296836853, ETA in seconds: 333615.597\n",
      "epoch: 44500, train loss: 3.904424285888672, val loss: 3.961489772796631, ETA in seconds: 334283.991\n",
      "epoch: 44600, train loss: 3.8932478427886963, val loss: 3.9826793670654297, ETA in seconds: 334955.807\n",
      "epoch: 44700, train loss: 3.8996158123016356, val loss: 3.9566524505615233, ETA in seconds: 335787.696\n",
      "epoch: 44800, train loss: 3.897239637374878, val loss: 3.974350333213806, ETA in seconds: 336536.136\n",
      "epoch: 44900, train loss: 3.8951953411102296, val loss: 3.972525644302368, ETA in seconds: 337302.995\n",
      "epoch: 45000, train loss: 3.8906404256820677, val loss: 3.965084505081177, ETA in seconds: 337978.192\n",
      "epoch: 45100, train loss: 3.8937814950942995, val loss: 3.9808438301086424, ETA in seconds: 338707.741\n",
      "epoch: 45200, train loss: 3.897333574295044, val loss: 3.9773321628570555, ETA in seconds: 339531.048\n",
      "epoch: 45300, train loss: 3.8955106496810914, val loss: 3.9622328996658327, ETA in seconds: 340350.419\n",
      "epoch: 45400, train loss: 3.896822381019592, val loss: 3.965126085281372, ETA in seconds: 341161.135\n",
      "epoch: 45500, train loss: 3.884025979042053, val loss: 3.9716520071029664, ETA in seconds: 341968.546\n",
      "epoch: 45600, train loss: 3.89033305644989, val loss: 3.971963667869568, ETA in seconds: 342781.115\n",
      "epoch: 45700, train loss: 3.8845485925674437, val loss: 3.974734139442444, ETA in seconds: 343608.232\n",
      "epoch: 45800, train loss: 3.889917016029358, val loss: 3.9680817127227783, ETA in seconds: 344435.312\n",
      "epoch: 45900, train loss: 3.8844335079193115, val loss: 3.9756900787353517, ETA in seconds: 345349.586\n",
      "epoch: 46000, train loss: 3.893687534332275, val loss: 3.9621912956237795, ETA in seconds: 346169.978\n",
      "epoch: 46100, train loss: 3.893421173095703, val loss: 3.9721227407455446, ETA in seconds: 346891.921\n",
      "epoch: 46200, train loss: 3.8984706878662108, val loss: 3.9612141609191895, ETA in seconds: 347577.423\n",
      "epoch: 46300, train loss: 3.8871901512145994, val loss: 3.977555847167969, ETA in seconds: 348272.440\n",
      "epoch: 46400, train loss: 3.89532470703125, val loss: 3.9847405195236205, ETA in seconds: 348996.514\n",
      "epoch: 46500, train loss: 3.8956167936325072, val loss: 3.972858691215515, ETA in seconds: 349738.101\n",
      "epoch: 46600, train loss: 3.8987300157547, val loss: 3.96640031337738, ETA in seconds: 350445.697\n",
      "epoch: 46700, train loss: 3.8813628435134886, val loss: 3.973986053466797, ETA in seconds: 351112.806\n",
      "epoch: 46800, train loss: 3.894954037666321, val loss: 3.970842456817627, ETA in seconds: 351780.146\n",
      "epoch: 46900, train loss: 3.884731125831604, val loss: 3.9591228008270263, ETA in seconds: 352468.983\n",
      "epoch: 47000, train loss: 3.8903610944747924, val loss: 3.9792580127716066, ETA in seconds: 353154.688\n",
      "epoch: 47100, train loss: 3.897139620780945, val loss: 3.9726718425750733, ETA in seconds: 353896.254\n",
      "epoch: 47200, train loss: 3.8949598550796507, val loss: 3.960954475402832, ETA in seconds: 354616.104\n",
      "epoch: 47300, train loss: 3.88999183177948, val loss: 3.967269468307495, ETA in seconds: 355362.219\n",
      "epoch: 47400, train loss: 3.8978718757629394, val loss: 3.9564590215682984, ETA in seconds: 356113.180\n",
      "epoch: 47500, train loss: 3.900468182563782, val loss: 3.9699806213378905, ETA in seconds: 356862.513\n",
      "epoch: 47600, train loss: 3.88585479259491, val loss: 3.972773551940918, ETA in seconds: 357584.319\n",
      "epoch: 47700, train loss: 3.8959736824035645, val loss: 3.96914746761322, ETA in seconds: 358261.811\n",
      "epoch: 47800, train loss: 3.8948567390441893, val loss: 3.9622186422348022, ETA in seconds: 358969.123\n",
      "epoch: 47900, train loss: 3.891597938537598, val loss: 3.974004125595093, ETA in seconds: 359639.248\n",
      "epoch: 48000, train loss: 3.8886980295181273, val loss: 3.9682708978652954, ETA in seconds: 360329.012\n",
      "epoch: 48100, train loss: 3.8961820363998414, val loss: 3.976208972930908, ETA in seconds: 361054.976\n",
      "epoch: 48200, train loss: 3.888440561294556, val loss: 3.962264561653137, ETA in seconds: 361772.919\n",
      "epoch: 48300, train loss: 3.8909313917160033, val loss: 3.9649226665496826, ETA in seconds: 362465.204\n",
      "epoch: 48400, train loss: 3.8921472311019896, val loss: 3.9648674726486206, ETA in seconds: 363197.143\n",
      "epoch: 48500, train loss: 3.8925935506820677, val loss: 3.9620291948318482, ETA in seconds: 363884.752\n",
      "epoch: 48600, train loss: 3.8963002920150758, val loss: 3.9685127973556518, ETA in seconds: 364545.224\n",
      "epoch: 48700, train loss: 3.896313762664795, val loss: 3.9543224811553954, ETA in seconds: 365201.315\n",
      "epoch: 48800, train loss: 3.8927848100662232, val loss: 3.9727535009384156, ETA in seconds: 365869.298\n",
      "epoch: 48900, train loss: 3.8936978816986083, val loss: 3.9666133880615235, ETA in seconds: 366525.533\n",
      "epoch: 49000, train loss: 3.888603925704956, val loss: 3.9776379585266115, ETA in seconds: 367354.668\n",
      "epoch: 49100, train loss: 3.892832064628601, val loss: 3.969714307785034, ETA in seconds: 368160.964\n",
      "epoch: 49200, train loss: 3.891363263130188, val loss: 3.9817394018173218, ETA in seconds: 368924.905\n",
      "epoch: 49300, train loss: 3.889608144760132, val loss: 3.9789525270462036, ETA in seconds: 369657.552\n",
      "epoch: 49400, train loss: 3.8950993061065673, val loss: 3.98419668674469, ETA in seconds: 370490.844\n",
      "epoch: 49500, train loss: 3.89887592792511, val loss: 3.9629714488983154, ETA in seconds: 371164.589\n",
      "epoch: 49600, train loss: 3.8944419384002686, val loss: 3.9655125379562377, ETA in seconds: 371896.443\n",
      "epoch: 49700, train loss: 3.8952263593673706, val loss: 3.980044651031494, ETA in seconds: 372700.489\n",
      "epoch: 49800, train loss: 3.8917818546295164, val loss: 3.978139114379883, ETA in seconds: 373506.166\n",
      "epoch: 49900, train loss: 3.8960994482040405, val loss: 3.9782981395721437, ETA in seconds: 374325.667\n",
      "epoch: 50000, train loss: 3.894455981254578, val loss: 3.9738728523254396, ETA in seconds: 375021.876\n",
      "epoch: 50100, train loss: 3.893925738334656, val loss: 3.9729198217391968, ETA in seconds: 375717.924\n",
      "epoch: 50200, train loss: 3.90110502243042, val loss: 3.9627804517745973, ETA in seconds: 376409.025\n",
      "epoch: 50300, train loss: 3.8933246612548826, val loss: 3.975889205932617, ETA in seconds: 377099.347\n",
      "epoch: 50400, train loss: 3.8993104219436647, val loss: 3.9804677963256836, ETA in seconds: 377780.706\n",
      "epoch: 50500, train loss: 3.888810086250305, val loss: 3.953898811340332, ETA in seconds: 378502.492\n",
      "epoch: 50600, train loss: 3.89486620426178, val loss: 3.9629719257354736, ETA in seconds: 379222.238\n",
      "epoch: 50700, train loss: 3.8940452337265015, val loss: 3.956937575340271, ETA in seconds: 379970.926\n",
      "epoch: 50800, train loss: 3.8888865232467653, val loss: 3.973912215232849, ETA in seconds: 380780.818\n",
      "epoch: 50900, train loss: 3.8893333196640016, val loss: 3.9713345527648927, ETA in seconds: 381582.244\n",
      "epoch: 51000, train loss: 3.89409921169281, val loss: 3.9678293466567993, ETA in seconds: 382402.436\n",
      "epoch: 51100, train loss: 3.892883539199829, val loss: 3.9772766828536987, ETA in seconds: 383216.645\n",
      "epoch: 51200, train loss: 3.884527158737183, val loss: 3.976285696029663, ETA in seconds: 384032.534\n",
      "epoch: 51300, train loss: 3.887878942489624, val loss: 3.9621070623397827, ETA in seconds: 384844.641\n",
      "epoch: 51400, train loss: 3.8979243516921995, val loss: 3.9699901580810546, ETA in seconds: 385660.377\n",
      "epoch: 51500, train loss: 3.897393226623535, val loss: 3.9731123447418213, ETA in seconds: 386460.765\n",
      "epoch: 51600, train loss: 3.8906744003295897, val loss: 3.962181544303894, ETA in seconds: 387111.082\n",
      "epoch: 51700, train loss: 3.9017026662826537, val loss: 3.956802749633789, ETA in seconds: 387876.514\n",
      "epoch: 51800, train loss: 3.896980047225952, val loss: 3.969300127029419, ETA in seconds: 388731.757\n",
      "epoch: 51900, train loss: 3.8918071746826173, val loss: 3.9655372142791747, ETA in seconds: 389584.737\n",
      "epoch: 52000, train loss: 3.8823971509933473, val loss: 3.9741885900497436, ETA in seconds: 390430.710\n",
      "epoch: 52100, train loss: 3.898187041282654, val loss: 3.9768306493759153, ETA in seconds: 391248.851\n",
      "epoch: 52200, train loss: 3.888357639312744, val loss: 3.9727287530899047, ETA in seconds: 391966.206\n",
      "epoch: 52300, train loss: 3.90004723072052, val loss: 3.962178444862366, ETA in seconds: 392691.165\n",
      "epoch: 52400, train loss: 3.8994084119796755, val loss: 3.966713237762451, ETA in seconds: 393412.356\n",
      "epoch: 52500, train loss: 3.8949290990829466, val loss: 3.964441204071045, ETA in seconds: 394129.426\n",
      "epoch: 52600, train loss: 3.8905923128128053, val loss: 3.9700915575027467, ETA in seconds: 394845.409\n",
      "epoch: 52700, train loss: 3.8967820405960083, val loss: 3.9725939989089967, ETA in seconds: 395529.747\n",
      "epoch: 52800, train loss: 3.899221968650818, val loss: 3.9811447143554686, ETA in seconds: 396212.628\n",
      "epoch: 52900, train loss: 3.9007762908935546, val loss: 3.9721106767654417, ETA in seconds: 396895.297\n",
      "epoch: 53000, train loss: 3.887710189819336, val loss: 3.972116208076477, ETA in seconds: 397606.228\n",
      "epoch: 53100, train loss: 3.8820728778839113, val loss: 3.964125156402588, ETA in seconds: 398306.546\n",
      "epoch: 53200, train loss: 3.893798494338989, val loss: 3.9703946113586426, ETA in seconds: 399002.462\n",
      "epoch: 53300, train loss: 3.8927741765975954, val loss: 3.9762129306793215, ETA in seconds: 399660.730\n",
      "epoch: 53400, train loss: 3.8938429832458494, val loss: 3.9672556638717653, ETA in seconds: 400334.075\n",
      "epoch: 53500, train loss: 3.8946603775024413, val loss: 3.969100570678711, ETA in seconds: 400997.768\n",
      "epoch: 53600, train loss: 3.890870714187622, val loss: 3.9695895433425905, ETA in seconds: 401660.340\n",
      "epoch: 53700, train loss: 3.8961887836456297, val loss: 3.972980237007141, ETA in seconds: 402319.570\n",
      "epoch: 53800, train loss: 3.9011380672454834, val loss: 3.97020845413208, ETA in seconds: 402985.821\n",
      "epoch: 53900, train loss: 3.8951433658599854, val loss: 3.9755716800689695, ETA in seconds: 403636.029\n",
      "epoch: 54000, train loss: 3.899877119064331, val loss: 3.9706828355789185, ETA in seconds: 404285.396\n",
      "epoch: 54100, train loss: 3.896922469139099, val loss: 3.9732290744781493, ETA in seconds: 404924.183\n",
      "epoch: 54200, train loss: 3.8978519678115844, val loss: 3.973621106147766, ETA in seconds: 405568.352\n",
      "epoch: 54300, train loss: 3.9011822938919067, val loss: 3.9632275342941283, ETA in seconds: 406198.657\n",
      "epoch: 54400, train loss: 3.8889416456222534, val loss: 3.9660564422607423, ETA in seconds: 406861.361\n",
      "epoch: 54500, train loss: 3.8858086824417115, val loss: 3.970696496963501, ETA in seconds: 407523.587\n",
      "epoch: 54600, train loss: 3.903474307060242, val loss: 3.9799360275268554, ETA in seconds: 408164.089\n",
      "epoch: 54700, train loss: 3.8981093883514406, val loss: 3.9708793640136717, ETA in seconds: 408839.032\n",
      "epoch: 54800, train loss: 3.898850917816162, val loss: 3.9658282518386843, ETA in seconds: 409635.991\n",
      "epoch: 54900, train loss: 3.8967585802078246, val loss: 3.9690722703933714, ETA in seconds: 410435.892\n",
      "epoch: 55000, train loss: 3.896178388595581, val loss: 3.967864203453064, ETA in seconds: 411239.471\n",
      "epoch: 55100, train loss: 3.896658992767334, val loss: 3.966265916824341, ETA in seconds: 412040.046\n",
      "epoch: 55200, train loss: 3.8934593915939333, val loss: 3.9713638544082643, ETA in seconds: 412780.550\n",
      "epoch: 55300, train loss: 3.894059753417969, val loss: 3.9794982194900514, ETA in seconds: 413436.911\n",
      "epoch: 55400, train loss: 3.8937721490859984, val loss: 3.9733990907669066, ETA in seconds: 414089.528\n",
      "epoch: 55500, train loss: 3.894904375076294, val loss: 3.975077247619629, ETA in seconds: 414745.020\n",
      "epoch: 55600, train loss: 3.8969218730926514, val loss: 3.9594242572784424, ETA in seconds: 415394.778\n",
      "epoch: 55700, train loss: 3.882054901123047, val loss: 3.969428610801697, ETA in seconds: 416041.397\n",
      "epoch: 55800, train loss: 3.8837358474731447, val loss: 3.973180055618286, ETA in seconds: 416684.542\n",
      "epoch: 55900, train loss: 3.8902789831161497, val loss: 3.973888611793518, ETA in seconds: 417331.812\n",
      "epoch: 56000, train loss: 3.893021035194397, val loss: 3.975532627105713, ETA in seconds: 417974.828\n",
      "epoch: 56100, train loss: 3.890450143814087, val loss: 3.9747636318206787, ETA in seconds: 418618.281\n",
      "epoch: 56200, train loss: 3.890804481506348, val loss: 3.9736082315444947, ETA in seconds: 419271.091\n",
      "epoch: 56300, train loss: 3.8944730281829836, val loss: 3.97079176902771, ETA in seconds: 419924.997\n",
      "epoch: 56400, train loss: 3.895308089256287, val loss: 3.966333508491516, ETA in seconds: 420578.139\n",
      "epoch: 56500, train loss: 3.9003775119781494, val loss: 3.960526394844055, ETA in seconds: 421257.245\n",
      "epoch: 56600, train loss: 3.8884612321853638, val loss: 3.975730800628662, ETA in seconds: 422057.524\n",
      "epoch: 56700, train loss: 3.89045672416687, val loss: 3.967415976524353, ETA in seconds: 422743.001\n",
      "epoch: 56800, train loss: 3.889890670776367, val loss: 3.9714492082595827, ETA in seconds: 423381.993\n",
      "epoch: 56900, train loss: 3.8936983346939087, val loss: 3.972490119934082, ETA in seconds: 424051.673\n",
      "epoch: 57000, train loss: 3.901623773574829, val loss: 3.969876503944397, ETA in seconds: 424728.148\n",
      "epoch: 57100, train loss: 3.903651261329651, val loss: 3.975961947441101, ETA in seconds: 425394.209\n",
      "epoch: 57200, train loss: 3.887067198753357, val loss: 3.9783621072769164, ETA in seconds: 426054.034\n",
      "epoch: 57300, train loss: 3.889720630645752, val loss: 3.9784914016723634, ETA in seconds: 426697.079\n",
      "epoch: 57400, train loss: 3.8928880214691164, val loss: 3.9771075963974, ETA in seconds: 427352.413\n",
      "epoch: 57500, train loss: 3.8953041315078734, val loss: 3.9710431337356566, ETA in seconds: 428019.126\n",
      "epoch: 57600, train loss: 3.8887303113937377, val loss: 3.96025869846344, ETA in seconds: 428672.512\n",
      "epoch: 57700, train loss: 3.8906243801116944, val loss: 3.9789247512817383, ETA in seconds: 429359.930\n",
      "epoch: 57800, train loss: 3.8933536052703857, val loss: 3.9627787113189696, ETA in seconds: 430165.617\n",
      "epoch: 57900, train loss: 3.889725685119629, val loss: 3.9672454357147218, ETA in seconds: 430812.130\n",
      "epoch: 58000, train loss: 3.885234236717224, val loss: 3.971317410469055, ETA in seconds: 431489.993\n",
      "epoch: 58100, train loss: 3.8935728073120117, val loss: 3.975341558456421, ETA in seconds: 432141.397\n",
      "epoch: 58200, train loss: 3.893359565734863, val loss: 3.970257067680359, ETA in seconds: 432789.183\n",
      "epoch: 58300, train loss: 3.8915483713150025, val loss: 3.96561975479126, ETA in seconds: 433443.733\n",
      "epoch: 58400, train loss: 3.8864604234695435, val loss: 3.9644917011260987, ETA in seconds: 434108.180\n",
      "epoch: 58500, train loss: 3.896367073059082, val loss: 3.97771213054657, ETA in seconds: 434760.249\n",
      "epoch: 58600, train loss: 3.897749638557434, val loss: 3.9614783048629763, ETA in seconds: 435432.893\n",
      "epoch: 58700, train loss: 3.8969824075698853, val loss: 3.963266634941101, ETA in seconds: 436083.609\n",
      "epoch: 58800, train loss: 3.8964019298553465, val loss: 3.968455123901367, ETA in seconds: 436734.481\n",
      "epoch: 58900, train loss: 3.9023100614547728, val loss: 3.9667736291885376, ETA in seconds: 437390.396\n",
      "epoch: 59000, train loss: 3.894813895225525, val loss: 3.9607782125473023, ETA in seconds: 438037.460\n",
      "epoch: 59100, train loss: 3.8937244176864625, val loss: 3.972234916687012, ETA in seconds: 438683.743\n",
      "epoch: 59200, train loss: 3.8905940771102907, val loss: 3.9568902254104614, ETA in seconds: 439328.072\n",
      "epoch: 59300, train loss: 3.8927648305892943, val loss: 3.9724643230438232, ETA in seconds: 439984.568\n",
      "epoch: 59400, train loss: 3.8806824684143066, val loss: 3.9726919889450074, ETA in seconds: 440619.383\n",
      "epoch: 59500, train loss: 3.8903743267059325, val loss: 3.972590446472168, ETA in seconds: 441288.931\n",
      "epoch: 59600, train loss: 3.8982930183410645, val loss: 3.973207402229309, ETA in seconds: 441932.627\n",
      "epoch: 59700, train loss: 3.896689510345459, val loss: 3.963843035697937, ETA in seconds: 442579.300\n",
      "epoch: 59800, train loss: 3.9041502714157104, val loss: 3.965438795089722, ETA in seconds: 443238.725\n",
      "epoch: 59900, train loss: 3.8919365644454955, val loss: 3.9609482049942017, ETA in seconds: 443893.219\n",
      "epoch: 60000, train loss: 3.8818398475646974, val loss: 3.973877954483032, ETA in seconds: 444642.202\n",
      "epoch: 60100, train loss: 3.8977879524230956, val loss: 3.9625523567199705, ETA in seconds: 445297.486\n",
      "epoch: 60200, train loss: 3.8958914279937744, val loss: 3.969456362724304, ETA in seconds: 445953.183\n",
      "epoch: 60300, train loss: 3.895353627204895, val loss: 3.9579307079315185, ETA in seconds: 446602.687\n",
      "epoch: 60400, train loss: 3.8938083171844484, val loss: 3.965602159500122, ETA in seconds: 447263.269\n",
      "epoch: 60500, train loss: 3.8845316648483275, val loss: 3.969253349304199, ETA in seconds: 447938.120\n",
      "epoch: 60600, train loss: 3.8955042600631713, val loss: 3.9605443716049193, ETA in seconds: 448587.897\n",
      "epoch: 60700, train loss: 3.8890591621398927, val loss: 3.9737477540969848, ETA in seconds: 449247.000\n",
      "epoch: 60800, train loss: 3.8986358642578125, val loss: 3.983127236366272, ETA in seconds: 449908.878\n",
      "epoch: 60900, train loss: 3.8995591402053833, val loss: 3.9751136779785154, ETA in seconds: 450560.817\n",
      "epoch: 61000, train loss: 3.8951404809951784, val loss: 3.9749761819839478, ETA in seconds: 451219.342\n",
      "epoch: 61100, train loss: 3.8927648305892943, val loss: 3.9764102697372437, ETA in seconds: 451886.523\n",
      "epoch: 61200, train loss: 3.8976327419281005, val loss: 3.9625658273696898, ETA in seconds: 452764.510\n",
      "epoch: 61300, train loss: 3.897841143608093, val loss: 3.972598433494568, ETA in seconds: 453634.571\n",
      "epoch: 61400, train loss: 3.895723581314087, val loss: 3.9652246475219726, ETA in seconds: 454520.272\n",
      "epoch: 61500, train loss: 3.8976943254470826, val loss: 3.97032253742218, ETA in seconds: 455190.595\n",
      "epoch: 61600, train loss: 3.8923124074935913, val loss: 3.967814874649048, ETA in seconds: 455844.278\n",
      "epoch: 61700, train loss: 3.897807812690735, val loss: 3.9677767753601074, ETA in seconds: 456488.360\n",
      "epoch: 61800, train loss: 3.8921557664871216, val loss: 3.9617253303527833, ETA in seconds: 457146.691\n",
      "epoch: 61900, train loss: 3.891439700126648, val loss: 3.95365047454834, ETA in seconds: 457814.179\n",
      "epoch: 62000, train loss: 3.897585868835449, val loss: 3.9525177478790283, ETA in seconds: 458477.568\n",
      "epoch: 62100, train loss: 3.893266534805298, val loss: 3.9685883283615113, ETA in seconds: 459143.982\n",
      "epoch: 62200, train loss: 3.892980933189392, val loss: 3.966407370567322, ETA in seconds: 459784.330\n",
      "epoch: 62300, train loss: 3.884938669204712, val loss: 3.9653183221817017, ETA in seconds: 460443.744\n",
      "epoch: 62400, train loss: 3.888369846343994, val loss: 3.9642388105392454, ETA in seconds: 461114.122\n",
      "epoch: 62500, train loss: 3.8872905731201173, val loss: 3.9535373210906983, ETA in seconds: 461785.450\n",
      "epoch: 62600, train loss: 3.897241306304932, val loss: 3.9828824520111086, ETA in seconds: 462468.447\n",
      "epoch: 62700, train loss: 3.8926521062850954, val loss: 3.963710403442383, ETA in seconds: 463130.387\n",
      "epoch: 62800, train loss: 3.898621368408203, val loss: 3.9651508808135985, ETA in seconds: 463879.290\n",
      "epoch: 62900, train loss: 3.900056314468384, val loss: 3.9705492496490478, ETA in seconds: 464634.589\n",
      "epoch: 63000, train loss: 3.8929805994033813, val loss: 3.9571505784988403, ETA in seconds: 465435.064\n",
      "epoch: 63100, train loss: 3.889190340042114, val loss: 3.9665844678878783, ETA in seconds: 466102.206\n",
      "epoch: 63200, train loss: 3.899046611785889, val loss: 3.962648129463196, ETA in seconds: 466795.970\n",
      "epoch: 63300, train loss: 3.8984336614608766, val loss: 3.972540259361267, ETA in seconds: 467627.960\n",
      "epoch: 63400, train loss: 3.901484560966492, val loss: 3.967612075805664, ETA in seconds: 468388.843\n",
      "epoch: 63500, train loss: 3.8963629007339478, val loss: 3.962979245185852, ETA in seconds: 469062.568\n",
      "epoch: 63600, train loss: 3.8975883960723876, val loss: 3.955374026298523, ETA in seconds: 469730.807\n",
      "epoch: 63700, train loss: 3.8893876552581785, val loss: 3.9684054613113404, ETA in seconds: 470410.536\n",
      "epoch: 63800, train loss: 3.899362850189209, val loss: 3.974199891090393, ETA in seconds: 471084.887\n",
      "epoch: 63900, train loss: 3.8998935222625732, val loss: 3.9708133935928345, ETA in seconds: 471738.201\n",
      "epoch: 64000, train loss: 3.9044352531433106, val loss: 3.965949273109436, ETA in seconds: 472425.221\n",
      "epoch: 64100, train loss: 3.9008777379989623, val loss: 3.96703565120697, ETA in seconds: 473094.312\n",
      "epoch: 64200, train loss: 3.891423535346985, val loss: 3.9699697971343992, ETA in seconds: 473757.025\n",
      "epoch: 64300, train loss: 3.896731376647949, val loss: 3.966197371482849, ETA in seconds: 474425.022\n",
      "epoch: 64400, train loss: 3.8921292781829835, val loss: 3.972836995124817, ETA in seconds: 475074.450\n",
      "epoch: 64500, train loss: 3.885448527336121, val loss: 3.9693878412246706, ETA in seconds: 475719.684\n",
      "epoch: 64600, train loss: 3.8925880908966066, val loss: 3.9759999752044677, ETA in seconds: 476401.281\n",
      "epoch: 64700, train loss: 3.90116868019104, val loss: 3.971064329147339, ETA in seconds: 477185.163\n",
      "epoch: 64800, train loss: 3.8918421506881713, val loss: 3.9779616832733153, ETA in seconds: 477968.891\n",
      "epoch: 64900, train loss: 3.8938308000564574, val loss: 3.970014238357544, ETA in seconds: 478745.352\n",
      "epoch: 65000, train loss: 3.8831140041351317, val loss: 3.9612042903900146, ETA in seconds: 479559.007\n",
      "epoch: 65100, train loss: 3.9005377769470213, val loss: 3.973767566680908, ETA in seconds: 480269.129\n",
      "epoch: 65200, train loss: 3.8846851587295532, val loss: 3.973840761184692, ETA in seconds: 481061.722\n",
      "epoch: 65300, train loss: 3.8938632726669313, val loss: 3.972615957260132, ETA in seconds: 481856.063\n",
      "epoch: 65400, train loss: 3.8908141374588014, val loss: 3.9611184358596803, ETA in seconds: 482654.835\n",
      "epoch: 65500, train loss: 3.8966375827789306, val loss: 3.965205430984497, ETA in seconds: 483448.902\n",
      "epoch: 65600, train loss: 3.893004870414734, val loss: 3.973682928085327, ETA in seconds: 484243.101\n",
      "epoch: 65700, train loss: 3.885259246826172, val loss: 3.981017327308655, ETA in seconds: 485024.182\n",
      "epoch: 65800, train loss: 3.8997581005096436, val loss: 3.9694199323654176, ETA in seconds: 485688.475\n",
      "epoch: 65900, train loss: 3.8942461252212524, val loss: 3.9816133975982666, ETA in seconds: 486324.957\n",
      "epoch: 66000, train loss: 3.901923990249634, val loss: 3.976583480834961, ETA in seconds: 486968.717\n",
      "epoch: 66100, train loss: 3.8965727567672728, val loss: 3.9653130531311036, ETA in seconds: 487608.437\n",
      "epoch: 66200, train loss: 3.8968976736068726, val loss: 3.9795450925827027, ETA in seconds: 488242.921\n",
      "epoch: 66300, train loss: 3.8899198532104493, val loss: 3.9566751956939696, ETA in seconds: 488891.022\n",
      "epoch: 66400, train loss: 3.886130928993225, val loss: 3.9729693651199343, ETA in seconds: 489527.731\n",
      "epoch: 66500, train loss: 3.8867565393447876, val loss: 3.9776005268096926, ETA in seconds: 490162.259\n",
      "epoch: 66600, train loss: 3.8951765060424806, val loss: 3.9711130619049073, ETA in seconds: 490794.417\n",
      "epoch: 66700, train loss: 3.89116530418396, val loss: 3.975782108306885, ETA in seconds: 491452.027\n",
      "epoch: 66800, train loss: 3.886869525909424, val loss: 3.9755017518997193, ETA in seconds: 492072.460\n",
      "epoch: 66900, train loss: 3.8949623584747313, val loss: 3.9732329130172728, ETA in seconds: 492707.229\n",
      "epoch: 67000, train loss: 3.891394853591919, val loss: 3.9572431087493896, ETA in seconds: 493456.589\n",
      "epoch: 67100, train loss: 3.8930935859680176, val loss: 3.965775179862976, ETA in seconds: 494088.220\n",
      "epoch: 67200, train loss: 3.889171600341797, val loss: 3.9767178297042847, ETA in seconds: 494725.239\n",
      "epoch: 67300, train loss: 3.8931032180786134, val loss: 3.973822522163391, ETA in seconds: 495363.900\n",
      "epoch: 67400, train loss: 3.8996947765350343, val loss: 3.9753745555877686, ETA in seconds: 496017.297\n",
      "epoch: 67500, train loss: 3.892502689361572, val loss: 3.971128058433533, ETA in seconds: 496671.455\n",
      "epoch: 67600, train loss: 3.902986979484558, val loss: 3.969311022758484, ETA in seconds: 497338.936\n",
      "epoch: 67700, train loss: 3.8914905309677126, val loss: 3.9729655742645265, ETA in seconds: 497986.633\n",
      "epoch: 67800, train loss: 3.88879599571228, val loss: 3.9701170682907105, ETA in seconds: 498633.347\n",
      "epoch: 67900, train loss: 3.9022688627243043, val loss: 3.9630994319915773, ETA in seconds: 499278.168\n",
      "epoch: 68000, train loss: 3.893202066421509, val loss: 3.9665572166442873, ETA in seconds: 499915.348\n",
      "epoch: 68100, train loss: 3.8914178371429444, val loss: 3.9770386695861815, ETA in seconds: 500556.271\n",
      "epoch: 68200, train loss: 3.896482753753662, val loss: 3.9704927682876585, ETA in seconds: 501202.865\n",
      "epoch: 68300, train loss: 3.8987274408340453, val loss: 3.9553731203079225, ETA in seconds: 501874.555\n",
      "epoch: 68400, train loss: 3.8993168115615844, val loss: 3.9753026723861695, ETA in seconds: 502523.189\n",
      "epoch: 68500, train loss: 3.893876624107361, val loss: 3.9721657991409303, ETA in seconds: 503186.317\n",
      "epoch: 68600, train loss: 3.899403429031372, val loss: 3.9569485902786257, ETA in seconds: 503972.456\n",
      "epoch: 68700, train loss: 3.8853333234786986, val loss: 3.9722644567489622, ETA in seconds: 504832.786\n",
      "epoch: 68800, train loss: 3.8984379529953004, val loss: 3.96157763004303, ETA in seconds: 505522.903\n",
      "epoch: 68900, train loss: 3.8919780969619753, val loss: 3.964399814605713, ETA in seconds: 506171.186\n",
      "epoch: 69000, train loss: 3.8964356422424316, val loss: 3.974044108390808, ETA in seconds: 506823.209\n",
      "epoch: 69100, train loss: 3.8867573499679566, val loss: 3.969602608680725, ETA in seconds: 507477.169\n",
      "epoch: 69200, train loss: 3.8883114099502563, val loss: 3.9708102464675905, ETA in seconds: 508127.607\n",
      "epoch: 69300, train loss: 3.891684722900391, val loss: 3.972359871864319, ETA in seconds: 508779.011\n",
      "epoch: 69400, train loss: 3.8935810327529907, val loss: 3.979039931297302, ETA in seconds: 509436.969\n",
      "epoch: 69500, train loss: 3.8971614122390745, val loss: 3.9712201595306396, ETA in seconds: 510101.579\n",
      "epoch: 69600, train loss: 3.894053268432617, val loss: 3.9673575162887573, ETA in seconds: 510779.976\n",
      "epoch: 69700, train loss: 3.8965196132659914, val loss: 3.965397000312805, ETA in seconds: 511509.786\n",
      "epoch: 69800, train loss: 3.8945224285125732, val loss: 3.9529695749282836, ETA in seconds: 512170.838\n",
      "epoch: 69900, train loss: 3.8962290048599244, val loss: 3.979154682159424, ETA in seconds: 512837.205\n",
      "epoch: 70000, train loss: 3.901470446586609, val loss: 3.9623132944107056, ETA in seconds: 513502.286\n",
      "epoch: 70100, train loss: 3.8961081743240356, val loss: 3.9582937479019167, ETA in seconds: 514144.224\n",
      "epoch: 70200, train loss: 3.8940780401229858, val loss: 3.969694209098816, ETA in seconds: 514884.644\n",
      "epoch: 70300, train loss: 3.8911487102508544, val loss: 3.973027491569519, ETA in seconds: 515585.932\n",
      "epoch: 70400, train loss: 3.8949440717697144, val loss: 3.9766497135162355, ETA in seconds: 516338.165\n",
      "epoch: 70500, train loss: 3.8911191701889036, val loss: 3.961037445068359, ETA in seconds: 516986.434\n",
      "epoch: 70600, train loss: 3.8934112548828126, val loss: 3.9779851913452147, ETA in seconds: 517664.003\n",
      "epoch: 70700, train loss: 3.8964441061019897, val loss: 3.969576597213745, ETA in seconds: 518324.241\n",
      "epoch: 70800, train loss: 3.888689422607422, val loss: 3.969172239303589, ETA in seconds: 518974.850\n",
      "epoch: 70900, train loss: 3.901563024520874, val loss: 3.9614714622497558, ETA in seconds: 519608.755\n",
      "epoch: 71000, train loss: 3.889456868171692, val loss: 3.9729013681411742, ETA in seconds: 520260.344\n",
      "epoch: 71100, train loss: 3.885967469215393, val loss: 3.9699791193008425, ETA in seconds: 520915.519\n",
      "epoch: 71200, train loss: 3.8882275819778442, val loss: 3.9726706743240356, ETA in seconds: 521571.865\n",
      "epoch: 71300, train loss: 3.8958573579788207, val loss: 3.971106243133545, ETA in seconds: 522229.851\n",
      "epoch: 71400, train loss: 3.8961030721664427, val loss: 3.9726408958435058, ETA in seconds: 522870.631\n",
      "epoch: 71500, train loss: 3.8875885248184203, val loss: 3.9733832120895385, ETA in seconds: 523522.507\n",
      "epoch: 71600, train loss: 3.891377067565918, val loss: 3.977332520484924, ETA in seconds: 524251.433\n",
      "epoch: 71700, train loss: 3.8946157932281493, val loss: 3.9746796607971193, ETA in seconds: 525031.757\n",
      "epoch: 71800, train loss: 3.8977328300476075, val loss: 3.9679046630859376, ETA in seconds: 525812.317\n",
      "epoch: 71900, train loss: 3.8947287082672117, val loss: 3.9751504182815554, ETA in seconds: 526591.288\n",
      "epoch: 72000, train loss: 3.8991961240768434, val loss: 3.987368106842041, ETA in seconds: 527368.657\n",
      "epoch: 72100, train loss: 3.888794994354248, val loss: 3.9760141372680664, ETA in seconds: 528159.533\n",
      "epoch: 72200, train loss: 3.897003436088562, val loss: 3.965423083305359, ETA in seconds: 528937.939\n",
      "epoch: 72300, train loss: 3.8986212491989134, val loss: 3.9684840202331544, ETA in seconds: 529595.737\n",
      "epoch: 72400, train loss: 3.8847402334213257, val loss: 3.970252823829651, ETA in seconds: 530226.947\n",
      "epoch: 72500, train loss: 3.897020721435547, val loss: 3.9636678218841555, ETA in seconds: 530855.566\n",
      "epoch: 72600, train loss: 3.8992800951004027, val loss: 3.9742956876754763, ETA in seconds: 531488.758\n",
      "epoch: 72700, train loss: 3.887334418296814, val loss: 3.9679996728897096, ETA in seconds: 532133.020\n",
      "epoch: 72800, train loss: 3.8889384031295777, val loss: 3.977156901359558, ETA in seconds: 532786.005\n",
      "epoch: 72900, train loss: 3.8890180587768555, val loss: 3.9742629528045654, ETA in seconds: 533424.603\n",
      "epoch: 73000, train loss: 3.9005333185195923, val loss: 3.9639633893966675, ETA in seconds: 534077.177\n",
      "epoch: 73100, train loss: 3.890048050880432, val loss: 3.969300961494446, ETA in seconds: 534721.181\n",
      "epoch: 73200, train loss: 3.898590064048767, val loss: 3.9772017002105713, ETA in seconds: 535363.820\n",
      "epoch: 73300, train loss: 3.906611132621765, val loss: 3.9744656324386596, ETA in seconds: 536010.047\n",
      "epoch: 73400, train loss: 3.8897356748580934, val loss: 3.9749311447143554, ETA in seconds: 536644.002\n",
      "epoch: 73500, train loss: 3.8867345809936524, val loss: 3.9641976594924926, ETA in seconds: 537285.397\n",
      "epoch: 73600, train loss: 3.8970267534255982, val loss: 3.955218267440796, ETA in seconds: 537921.246\n",
      "epoch: 73700, train loss: 3.887590742111206, val loss: 3.9706830739974976, ETA in seconds: 538552.592\n",
      "epoch: 73800, train loss: 3.8960768699646, val loss: 3.9669235944747925, ETA in seconds: 539184.854\n",
      "epoch: 73900, train loss: 3.8941518306732177, val loss: 3.9913360834121705, ETA in seconds: 539817.309\n",
      "epoch: 74000, train loss: 3.8947775840759276, val loss: 3.976461172103882, ETA in seconds: 540466.168\n",
      "epoch: 74100, train loss: 3.9009405374526978, val loss: 3.9714169263839723, ETA in seconds: 541093.550\n",
      "epoch: 74200, train loss: 3.898275637626648, val loss: 3.97187340259552, ETA in seconds: 541722.086\n",
      "epoch: 74300, train loss: 3.894940161705017, val loss: 3.9726688385009767, ETA in seconds: 542377.347\n",
      "epoch: 74400, train loss: 3.8963744401931764, val loss: 3.9644696950912475, ETA in seconds: 543001.412\n",
      "epoch: 74500, train loss: 3.887030816078186, val loss: 3.971761202812195, ETA in seconds: 543627.687\n",
      "epoch: 74600, train loss: 3.893565821647644, val loss: 3.96588819026947, ETA in seconds: 544264.384\n",
      "epoch: 74700, train loss: 3.8943013191223144, val loss: 3.975123167037964, ETA in seconds: 544911.381\n",
      "epoch: 74800, train loss: 3.895634651184082, val loss: 3.961661434173584, ETA in seconds: 545572.189\n",
      "epoch: 74900, train loss: 3.891598272323608, val loss: 3.9635384321212768, ETA in seconds: 546350.649\n",
      "epoch: 75000, train loss: 3.8932066440582274, val loss: 3.958315920829773, ETA in seconds: 546962.648\n",
      "epoch: 75100, train loss: 3.9017396926879884, val loss: 3.9767630338668822, ETA in seconds: 547603.100\n",
      "epoch: 75200, train loss: 3.8921205520629885, val loss: 3.970941495895386, ETA in seconds: 548264.292\n",
      "epoch: 75300, train loss: 3.8873458385467528, val loss: 3.96449191570282, ETA in seconds: 548874.030\n",
      "epoch: 75400, train loss: 3.9083924531936645, val loss: 3.9662488222122194, ETA in seconds: 549561.543\n",
      "epoch: 75500, train loss: 3.8994922399520875, val loss: 3.9687696933746337, ETA in seconds: 550216.965\n",
      "epoch: 75600, train loss: 3.894249153137207, val loss: 3.9576488733291626, ETA in seconds: 550856.866\n",
      "epoch: 75700, train loss: 3.8876402139663697, val loss: 3.9633801221847533, ETA in seconds: 551450.753\n",
      "epoch: 75800, train loss: 3.901333141326904, val loss: 3.976718521118164, ETA in seconds: 552128.078\n",
      "epoch: 75900, train loss: 3.8939621210098267, val loss: 3.9682588815689086, ETA in seconds: 552807.325\n",
      "epoch: 76000, train loss: 3.897340750694275, val loss: 3.976346659660339, ETA in seconds: 553438.217\n",
      "epoch: 76100, train loss: 3.899631977081299, val loss: 3.966060185432434, ETA in seconds: 554082.032\n",
      "epoch: 76200, train loss: 3.8870766162872314, val loss: 3.966971278190613, ETA in seconds: 554734.711\n",
      "epoch: 76300, train loss: 3.891819477081299, val loss: 3.9660618782043455, ETA in seconds: 555424.998\n",
      "epoch: 76400, train loss: 3.8924281120300295, val loss: 3.96447491645813, ETA in seconds: 556108.963\n",
      "epoch: 76500, train loss: 3.896507239341736, val loss: 3.9745942831039427, ETA in seconds: 556740.746\n",
      "epoch: 76600, train loss: 3.895214796066284, val loss: 3.964616394042969, ETA in seconds: 557388.932\n",
      "epoch: 76700, train loss: 3.900726556777954, val loss: 3.964348864555359, ETA in seconds: 558034.382\n",
      "epoch: 76800, train loss: 3.892190408706665, val loss: 3.9658523082733153, ETA in seconds: 558790.564\n",
      "epoch: 76900, train loss: 3.893625855445862, val loss: 3.957580542564392, ETA in seconds: 559467.195\n",
      "epoch: 77000, train loss: 3.894625687599182, val loss: 3.964882469177246, ETA in seconds: 560140.900\n",
      "epoch: 77100, train loss: 3.898905634880066, val loss: 3.984052228927612, ETA in seconds: 560996.610\n",
      "epoch: 77200, train loss: 3.903652882575989, val loss: 3.975437307357788, ETA in seconds: 561828.741\n",
      "epoch: 77300, train loss: 3.891395401954651, val loss: 3.971937346458435, ETA in seconds: 562669.363\n",
      "epoch: 77400, train loss: 3.894792890548706, val loss: 3.968324589729309, ETA in seconds: 563486.822\n",
      "epoch: 77500, train loss: 3.882471036911011, val loss: 3.9660964965820313, ETA in seconds: 564323.299\n",
      "epoch: 77600, train loss: 3.8823076486587524, val loss: 3.9706286191940308, ETA in seconds: 565124.298\n",
      "epoch: 77700, train loss: 3.893834447860718, val loss: 3.9705015659332275, ETA in seconds: 565834.871\n",
      "epoch: 77800, train loss: 3.900235962867737, val loss: 3.9676936864852905, ETA in seconds: 566498.611\n",
      "epoch: 77900, train loss: 3.8959806680679323, val loss: 3.9776556491851807, ETA in seconds: 567162.268\n",
      "epoch: 78000, train loss: 3.891102337837219, val loss: 3.9677698373794557, ETA in seconds: 567774.062\n",
      "epoch: 78100, train loss: 3.897243690490723, val loss: 3.9653398990631104, ETA in seconds: 568392.906\n",
      "epoch: 78200, train loss: 3.9038103342056276, val loss: 3.965179991722107, ETA in seconds: 569009.892\n",
      "epoch: 78300, train loss: 3.8885821104049683, val loss: 3.97258198261261, ETA in seconds: 569694.968\n",
      "epoch: 78400, train loss: 3.8992119789123536, val loss: 3.965834951400757, ETA in seconds: 570353.724\n",
      "epoch: 78500, train loss: 3.885026955604553, val loss: 3.9691757917404176, ETA in seconds: 570954.425\n",
      "epoch: 78600, train loss: 3.8816001176834107, val loss: 3.973284935951233, ETA in seconds: 571596.000\n",
      "epoch: 78700, train loss: 3.8837104558944704, val loss: 3.9665992498397826, ETA in seconds: 572227.732\n",
      "epoch: 78800, train loss: 3.903612661361694, val loss: 3.9617688179016115, ETA in seconds: 572862.520\n",
      "epoch: 78900, train loss: 3.9015143394470213, val loss: 3.9719402313232424, ETA in seconds: 573472.130\n",
      "epoch: 79000, train loss: 3.887680768966675, val loss: 3.966000533103943, ETA in seconds: 574193.711\n",
      "epoch: 79100, train loss: 3.8981220006942747, val loss: 3.9732983112335205, ETA in seconds: 574961.559\n",
      "epoch: 79200, train loss: 3.8931717395782472, val loss: 3.961143660545349, ETA in seconds: 575734.805\n",
      "epoch: 79300, train loss: 3.8919779539108275, val loss: 3.9627921342849732, ETA in seconds: 576396.993\n",
      "epoch: 79400, train loss: 3.902684950828552, val loss: 3.9697009325027466, ETA in seconds: 577039.635\n",
      "epoch: 79500, train loss: 3.8977194309234617, val loss: 3.9549533843994142, ETA in seconds: 577680.039\n",
      "epoch: 79600, train loss: 3.8896264314651487, val loss: 3.9614348649978637, ETA in seconds: 578323.639\n",
      "epoch: 79700, train loss: 3.8995067834854127, val loss: 3.966185522079468, ETA in seconds: 578992.450\n",
      "epoch: 79800, train loss: 3.896131658554077, val loss: 3.964547896385193, ETA in seconds: 579678.381\n",
      "epoch: 79900, train loss: 3.8881285429000854, val loss: 3.970927095413208, ETA in seconds: 580344.788\n",
      "epoch: 80000, train loss: 3.887807011604309, val loss: 3.9658334016799928, ETA in seconds: 580993.087\n",
      "epoch: 80100, train loss: 3.890198588371277, val loss: 3.963547945022583, ETA in seconds: 581637.400\n",
      "epoch: 80200, train loss: 3.8920836210250855, val loss: 3.962671732902527, ETA in seconds: 582273.005\n",
      "epoch: 80300, train loss: 3.883478355407715, val loss: 3.970342826843262, ETA in seconds: 582916.711\n",
      "epoch: 80400, train loss: 3.891216778755188, val loss: 3.94770724773407, ETA in seconds: 583569.320\n",
      "epoch: 80500, train loss: 3.888387846946716, val loss: 3.962065005302429, ETA in seconds: 584207.765\n",
      "epoch: 80600, train loss: 3.891596269607544, val loss: 3.967239332199097, ETA in seconds: 584838.241\n",
      "epoch: 80700, train loss: 3.886562633514404, val loss: 3.9722193479537964, ETA in seconds: 585468.323\n",
      "epoch: 80800, train loss: 3.898926520347595, val loss: 3.9728854179382322, ETA in seconds: 586110.167\n",
      "epoch: 80900, train loss: 3.890029263496399, val loss: 3.9711601972579955, ETA in seconds: 586736.056\n",
      "epoch: 81000, train loss: 3.8941089868545533, val loss: 3.9740930080413817, ETA in seconds: 587363.707\n",
      "epoch: 81100, train loss: 3.897402548789978, val loss: 3.9736223220825195, ETA in seconds: 588005.549\n",
      "epoch: 81200, train loss: 3.8906885385513306, val loss: 3.9617950677871705, ETA in seconds: 588612.601\n",
      "epoch: 81300, train loss: 3.8915908336639404, val loss: 3.971344470977783, ETA in seconds: 589220.084\n",
      "epoch: 81400, train loss: 3.9017568826675415, val loss: 3.9598392248153687, ETA in seconds: 589890.529\n",
      "epoch: 81500, train loss: 3.8936075925827027, val loss: 3.985328507423401, ETA in seconds: 590511.038\n",
      "epoch: 81600, train loss: 3.898606204986572, val loss: 3.9610397577285767, ETA in seconds: 591122.912\n",
      "epoch: 81700, train loss: 3.893899989128113, val loss: 3.967946267127991, ETA in seconds: 591746.018\n",
      "epoch: 81800, train loss: 3.891202211380005, val loss: 3.9773837327957153, ETA in seconds: 592355.413\n",
      "epoch: 81900, train loss: 3.891721272468567, val loss: 3.959422159194946, ETA in seconds: 592963.022\n",
      "epoch: 82000, train loss: 3.896245741844177, val loss: 3.965405988693237, ETA in seconds: 593551.696\n",
      "epoch: 82100, train loss: 3.8977989196777343, val loss: 3.977045106887817, ETA in seconds: 594226.066\n",
      "epoch: 82200, train loss: 3.8987767934799193, val loss: 3.9607336282730103, ETA in seconds: 594907.679\n",
      "epoch: 82300, train loss: 3.898387598991394, val loss: 3.96652090549469, ETA in seconds: 595588.631\n",
      "epoch: 82400, train loss: 3.9013354778289795, val loss: 3.9782901287078856, ETA in seconds: 596268.508\n",
      "epoch: 82500, train loss: 3.8948915004730225, val loss: 3.9788219928741455, ETA in seconds: 596944.603\n",
      "epoch: 82600, train loss: 3.8913336515426638, val loss: 3.966497850418091, ETA in seconds: 597609.468\n",
      "epoch: 82700, train loss: 3.900718665122986, val loss: 3.9719897508621216, ETA in seconds: 598211.843\n",
      "epoch: 82800, train loss: 3.8942479372024534, val loss: 3.9671563148498534, ETA in seconds: 598825.425\n",
      "epoch: 82900, train loss: 3.887464451789856, val loss: 3.964240074157715, ETA in seconds: 599542.609\n",
      "epoch: 83000, train loss: 3.899687576293945, val loss: 3.97339985370636, ETA in seconds: 600299.729\n",
      "epoch: 83100, train loss: 3.897331976890564, val loss: 3.9778321266174315, ETA in seconds: 601056.517\n",
      "epoch: 83200, train loss: 3.8966311693191527, val loss: 3.9737263917922974, ETA in seconds: 601850.519\n",
      "epoch: 83300, train loss: 3.8990687131881714, val loss: 3.9736751556396483, ETA in seconds: 602660.348\n",
      "epoch: 83400, train loss: 3.895932149887085, val loss: 3.9799856901168824, ETA in seconds: 603269.373\n",
      "epoch: 83500, train loss: 3.88899188041687, val loss: 3.9770336627960203, ETA in seconds: 603893.299\n",
      "epoch: 83600, train loss: 3.8932354927062987, val loss: 3.9711217641830445, ETA in seconds: 604498.000\n",
      "epoch: 83700, train loss: 3.894920229911804, val loss: 3.9671745777130125, ETA in seconds: 605123.722\n",
      "epoch: 83800, train loss: 3.9075044870376585, val loss: 3.972140598297119, ETA in seconds: 605774.460\n",
      "epoch: 83900, train loss: 3.8937833309173584, val loss: 3.965053582191467, ETA in seconds: 606457.082\n",
      "epoch: 84000, train loss: 3.888935685157776, val loss: 3.9615994453430177, ETA in seconds: 607080.794\n",
      "epoch: 84100, train loss: 3.8794439315795897, val loss: 3.9843647956848143, ETA in seconds: 607775.613\n",
      "epoch: 84200, train loss: 3.891397476196289, val loss: 3.9675741672515867, ETA in seconds: 608570.459\n",
      "epoch: 84300, train loss: 3.8972533464431764, val loss: 3.9731268644332887, ETA in seconds: 609413.324\n",
      "epoch: 84400, train loss: 3.8899795770645142, val loss: 3.9814854860305786, ETA in seconds: 610049.081\n",
      "epoch: 84500, train loss: 3.896781301498413, val loss: 3.968833327293396, ETA in seconds: 610711.135\n",
      "epoch: 84600, train loss: 3.880720019340515, val loss: 3.9644518852233888, ETA in seconds: 611383.060\n",
      "epoch: 84700, train loss: 3.8951475858688354, val loss: 3.9712082624435423, ETA in seconds: 612039.806\n",
      "epoch: 84800, train loss: 3.902233910560608, val loss: 3.977177929878235, ETA in seconds: 612686.156\n",
      "epoch: 84900, train loss: 3.891118955612183, val loss: 3.9695186376571656, ETA in seconds: 613322.640\n",
      "epoch: 85000, train loss: 3.904239797592163, val loss: 3.971899724006653, ETA in seconds: 614046.163\n",
      "epoch: 85100, train loss: 3.8978450298309326, val loss: 3.9664684772491454, ETA in seconds: 614764.201\n",
      "epoch: 85200, train loss: 3.891655135154724, val loss: 3.9716188430786135, ETA in seconds: 615516.810\n",
      "epoch: 85300, train loss: 3.899131989479065, val loss: 3.957665038108826, ETA in seconds: 616337.309\n",
      "epoch: 85400, train loss: 3.8898743867874144, val loss: 3.96870596408844, ETA in seconds: 617046.056\n",
      "epoch: 85500, train loss: 3.8942312240600585, val loss: 3.971580743789673, ETA in seconds: 617736.667\n",
      "epoch: 85600, train loss: 3.8971497058868407, val loss: 3.9659356832504273, ETA in seconds: 618398.645\n",
      "epoch: 85700, train loss: 3.8930845499038695, val loss: 3.9696482181549073, ETA in seconds: 619146.829\n",
      "epoch: 85800, train loss: 3.8990527629852294, val loss: 3.970984935760498, ETA in seconds: 619884.996\n",
      "epoch: 85900, train loss: 3.893193221092224, val loss: 3.9754387378692626, ETA in seconds: 620579.416\n",
      "epoch: 86000, train loss: 3.8865980386734007, val loss: 3.9691045761108397, ETA in seconds: 621256.652\n",
      "epoch: 86100, train loss: 3.8904799461364745, val loss: 3.9833160400390626, ETA in seconds: 621938.901\n",
      "epoch: 86200, train loss: 3.898258924484253, val loss: 3.972403573989868, ETA in seconds: 622604.185\n",
      "epoch: 86300, train loss: 3.8995149612426756, val loss: 3.9693459033966065, ETA in seconds: 623312.806\n",
      "epoch: 86400, train loss: 3.893096375465393, val loss: 3.9691092491149904, ETA in seconds: 623995.961\n",
      "epoch: 86500, train loss: 3.903539228439331, val loss: 3.967618465423584, ETA in seconds: 624572.670\n",
      "epoch: 86600, train loss: 3.898357129096985, val loss: 3.9790164470672607, ETA in seconds: 625153.212\n",
      "epoch: 86700, train loss: 3.897936797142029, val loss: 3.9576972007751463, ETA in seconds: 625773.996\n",
      "epoch: 86800, train loss: 3.8938936948776246, val loss: 3.969756317138672, ETA in seconds: 626427.191\n",
      "epoch: 86900, train loss: 3.893904423713684, val loss: 3.968881344795227, ETA in seconds: 627045.614\n",
      "epoch: 87000, train loss: 3.8985100030899047, val loss: 3.9621746301651, ETA in seconds: 627710.989\n",
      "epoch: 87100, train loss: 3.8904385566711426, val loss: 3.970294642448425, ETA in seconds: 628353.614\n",
      "epoch: 87200, train loss: 3.896603226661682, val loss: 3.959468126296997, ETA in seconds: 628983.715\n",
      "epoch: 87300, train loss: 3.8978633165359495, val loss: 3.9782782316207888, ETA in seconds: 629796.476\n",
      "epoch: 87400, train loss: 3.893793511390686, val loss: 3.974844193458557, ETA in seconds: 630555.437\n",
      "epoch: 87500, train loss: 3.8883813619613647, val loss: 3.9665054798126222, ETA in seconds: 631309.554\n",
      "epoch: 87600, train loss: 3.8991657495498657, val loss: 3.9685431718826294, ETA in seconds: 631933.554\n",
      "epoch: 87700, train loss: 3.8901007175445557, val loss: 3.960629105567932, ETA in seconds: 632591.903\n",
      "epoch: 87800, train loss: 3.8993117809295654, val loss: 3.9774862051010134, ETA in seconds: 633239.473\n",
      "epoch: 87900, train loss: 3.903783893585205, val loss: 3.9624539613723755, ETA in seconds: 633893.090\n",
      "epoch: 88000, train loss: 3.8918734073638914, val loss: 3.973110389709473, ETA in seconds: 634535.677\n",
      "epoch: 88100, train loss: 3.9016445875167847, val loss: 3.95797860622406, ETA in seconds: 635176.033\n",
      "epoch: 88200, train loss: 3.9004929780960085, val loss: 3.9830817222595214, ETA in seconds: 635827.201\n",
      "epoch: 88300, train loss: 3.8878153562545776, val loss: 3.972426009178162, ETA in seconds: 636500.058\n",
      "epoch: 88400, train loss: 3.892833042144775, val loss: 3.967054295539856, ETA in seconds: 637158.369\n",
      "epoch: 88500, train loss: 3.889192223548889, val loss: 3.9781244516372682, ETA in seconds: 637806.145\n",
      "epoch: 88600, train loss: 3.893426775932312, val loss: 3.963457775115967, ETA in seconds: 638504.369\n",
      "epoch: 88700, train loss: 3.896312618255615, val loss: 3.969278025627136, ETA in seconds: 639184.139\n",
      "epoch: 88800, train loss: 3.893932247161865, val loss: 3.9633949518203737, ETA in seconds: 639831.729\n",
      "epoch: 88900, train loss: 3.8956568241119385, val loss: 3.965402293205261, ETA in seconds: 640522.283\n",
      "epoch: 89000, train loss: 3.8999460697174073, val loss: 3.957152771949768, ETA in seconds: 641266.687\n",
      "epoch: 89100, train loss: 3.887908601760864, val loss: 3.9688610553741457, ETA in seconds: 642007.217\n",
      "epoch: 89200, train loss: 3.898662638664246, val loss: 3.973443841934204, ETA in seconds: 642747.790\n",
      "epoch: 89300, train loss: 3.897804021835327, val loss: 3.9672391176223756, ETA in seconds: 643485.014\n",
      "epoch: 89400, train loss: 3.8860261917114256, val loss: 3.9716266632080077, ETA in seconds: 644235.961\n",
      "epoch: 89500, train loss: 3.8909433841705323, val loss: 3.9801844358444214, ETA in seconds: 645000.772\n",
      "epoch: 89600, train loss: 3.898546552658081, val loss: 3.9678175926208494, ETA in seconds: 645776.596\n",
      "epoch: 89700, train loss: 3.8944684982299806, val loss: 3.9728113174438477, ETA in seconds: 646577.725\n",
      "epoch: 89800, train loss: 3.884481596946716, val loss: 3.9737042427062987, ETA in seconds: 647207.152\n",
      "epoch: 89900, train loss: 3.8904725313186646, val loss: 3.9622928380966185, ETA in seconds: 647902.888\n",
      "epoch: 90000, train loss: 3.8847272872924803, val loss: 3.967735171318054, ETA in seconds: 648478.905\n",
      "epoch: 90100, train loss: 3.897123336791992, val loss: 3.972905707359314, ETA in seconds: 649072.043\n",
      "epoch: 90200, train loss: 3.892391800880432, val loss: 3.9694426536560057, ETA in seconds: 649675.166\n",
      "epoch: 90300, train loss: 3.900075006484985, val loss: 3.9718204021453856, ETA in seconds: 650288.364\n",
      "epoch: 90400, train loss: 3.8860082387924195, val loss: 3.9662270307540894, ETA in seconds: 650907.905\n",
      "epoch: 90500, train loss: 3.894828701019287, val loss: 3.9609775066375734, ETA in seconds: 651526.655\n",
      "epoch: 90600, train loss: 3.8949723720550535, val loss: 3.969220685958862, ETA in seconds: 652133.470\n",
      "epoch: 90700, train loss: 3.890977621078491, val loss: 3.9731590270996096, ETA in seconds: 652747.460\n",
      "epoch: 90800, train loss: 3.8918073177337646, val loss: 3.960974907875061, ETA in seconds: 653357.778\n",
      "epoch: 90900, train loss: 3.88279185295105, val loss: 3.9639986038208006, ETA in seconds: 653964.409\n",
      "epoch: 91000, train loss: 3.8997616529464723, val loss: 3.970084547996521, ETA in seconds: 654636.714\n",
      "epoch: 91100, train loss: 3.8861692190170287, val loss: 3.9710801362991335, ETA in seconds: 655263.411\n",
      "epoch: 91200, train loss: 3.896491003036499, val loss: 3.9613487243652346, ETA in seconds: 655880.536\n",
      "epoch: 91300, train loss: 3.896811318397522, val loss: 3.9688172817230223, ETA in seconds: 656527.903\n",
      "epoch: 91400, train loss: 3.8910726070404054, val loss: 3.97008421421051, ETA in seconds: 657305.231\n",
      "epoch: 91500, train loss: 3.900185990333557, val loss: 3.96686646938324, ETA in seconds: 658021.434\n",
      "epoch: 91600, train loss: 3.8992244720458986, val loss: 3.982453727722168, ETA in seconds: 658634.880\n",
      "epoch: 91700, train loss: 3.897385811805725, val loss: 3.967359471321106, ETA in seconds: 659240.980\n",
      "epoch: 91800, train loss: 3.893997597694397, val loss: 3.9572344064712524, ETA in seconds: 659856.768\n",
      "epoch: 91900, train loss: 3.8858845949172975, val loss: 3.980033850669861, ETA in seconds: 660506.075\n",
      "epoch: 92000, train loss: 3.8907586336135864, val loss: 3.9677663087844848, ETA in seconds: 661245.487\n",
      "epoch: 92100, train loss: 3.8935600996017454, val loss: 3.965086483955383, ETA in seconds: 661863.205\n",
      "epoch: 92200, train loss: 3.8925219774246216, val loss: 3.975465488433838, ETA in seconds: 662525.050\n",
      "epoch: 92300, train loss: 3.8979504823684694, val loss: 3.953683114051819, ETA in seconds: 663259.460\n",
      "epoch: 92400, train loss: 3.893288230895996, val loss: 3.967500019073486, ETA in seconds: 664000.358\n",
      "epoch: 92500, train loss: 3.8852776288986206, val loss: 3.965693736076355, ETA in seconds: 664737.586\n",
      "epoch: 92600, train loss: 3.8960001707077025, val loss: 3.9630026817321777, ETA in seconds: 665471.022\n",
      "epoch: 92700, train loss: 3.8835039854049684, val loss: 3.9755537509918213, ETA in seconds: 666202.274\n",
      "epoch: 92800, train loss: 3.888385701179504, val loss: 3.979053020477295, ETA in seconds: 666930.886\n",
      "epoch: 92900, train loss: 3.9027504682540894, val loss: 3.9672956228256226, ETA in seconds: 667498.939\n",
      "epoch: 93000, train loss: 3.898572087287903, val loss: 3.9785558223724364, ETA in seconds: 668086.589\n",
      "epoch: 93100, train loss: 3.8910821199417116, val loss: 3.97107048034668, ETA in seconds: 668693.778\n",
      "epoch: 93200, train loss: 3.891605567932129, val loss: 3.9721978425979616, ETA in seconds: 669329.205\n",
      "epoch: 93300, train loss: 3.8985076427459715, val loss: 3.9726861000061033, ETA in seconds: 669909.452\n",
      "epoch: 93400, train loss: 3.895322155952454, val loss: 3.9697497367858885, ETA in seconds: 670511.456\n",
      "epoch: 93500, train loss: 3.892941212654114, val loss: 3.9728974103927612, ETA in seconds: 671258.752\n",
      "epoch: 93600, train loss: 3.89911425113678, val loss: 3.975542759895325, ETA in seconds: 672004.916\n",
      "epoch: 93700, train loss: 3.8987369537353516, val loss: 3.9694729328155516, ETA in seconds: 672763.928\n",
      "epoch: 93800, train loss: 3.8907551050186155, val loss: 3.969570684432983, ETA in seconds: 673512.967\n",
      "epoch: 93900, train loss: 3.904059910774231, val loss: 3.970527696609497, ETA in seconds: 674263.483\n",
      "epoch: 94000, train loss: 3.8967663049697876, val loss: 3.9695329427719117, ETA in seconds: 675012.983\n",
      "epoch: 94100, train loss: 3.8979644536972047, val loss: 3.967552161216736, ETA in seconds: 675668.576\n",
      "epoch: 94200, train loss: 3.8951611518859863, val loss: 3.9813650608062745, ETA in seconds: 676267.454\n",
      "epoch: 94300, train loss: 3.8895354509353637, val loss: 3.9707716703414917, ETA in seconds: 676909.269\n",
      "epoch: 94400, train loss: 3.8828806400299074, val loss: 3.9772687911987306, ETA in seconds: 677639.480\n",
      "epoch: 94500, train loss: 3.8961092948913576, val loss: 3.9702776432037354, ETA in seconds: 678364.064\n",
      "epoch: 94600, train loss: 3.900321531295776, val loss: 3.9668842792510985, ETA in seconds: 679090.252\n",
      "epoch: 94700, train loss: 3.8891055822372436, val loss: 3.96933331489563, ETA in seconds: 679736.008\n",
      "epoch: 94800, train loss: 3.8959847688674927, val loss: 3.9779303073883057, ETA in seconds: 680300.949\n",
      "epoch: 94900, train loss: 3.891138029098511, val loss: 3.9720103263854982, ETA in seconds: 680880.052\n",
      "epoch: 95000, train loss: 3.8946099281311035, val loss: 3.97348952293396, ETA in seconds: 681468.292\n",
      "epoch: 95100, train loss: 3.893354630470276, val loss: 3.9798004627227783, ETA in seconds: 682045.230\n",
      "epoch: 95200, train loss: 3.8949456453323363, val loss: 3.9642319440841676, ETA in seconds: 682625.425\n",
      "epoch: 95300, train loss: 3.8995539426803587, val loss: 3.979402041435242, ETA in seconds: 683204.609\n",
      "epoch: 95400, train loss: 3.9056384801864623, val loss: 3.97873170375824, ETA in seconds: 683770.647\n",
      "epoch: 95500, train loss: 3.891756296157837, val loss: 3.9720683336257934, ETA in seconds: 684354.148\n",
      "epoch: 95600, train loss: 3.8966058254241944, val loss: 3.968039894104004, ETA in seconds: 684953.484\n",
      "epoch: 95700, train loss: 3.892953562736511, val loss: 3.9743784427642823, ETA in seconds: 685534.222\n",
      "epoch: 95800, train loss: 3.890656065940857, val loss: 3.9701321363449096, ETA in seconds: 686124.893\n",
      "epoch: 95900, train loss: 3.8961058616638184, val loss: 3.97847101688385, ETA in seconds: 686702.338\n",
      "epoch: 96000, train loss: 3.8974302768707276, val loss: 3.973415470123291, ETA in seconds: 687264.742\n",
      "epoch: 96100, train loss: 3.8945932388305664, val loss: 3.973554563522339, ETA in seconds: 687830.765\n",
      "epoch: 96200, train loss: 3.895124816894531, val loss: 3.9637018918991087, ETA in seconds: 688406.729\n",
      "epoch: 96300, train loss: 3.90333890914917, val loss: 3.969172739982605, ETA in seconds: 688983.989\n",
      "epoch: 96400, train loss: 3.8967478275299072, val loss: 3.968419909477234, ETA in seconds: 689562.187\n",
      "epoch: 96500, train loss: 3.89230101108551, val loss: 3.972143268585205, ETA in seconds: 690151.548\n",
      "epoch: 96600, train loss: 3.8937222957611084, val loss: 3.9754170894622805, ETA in seconds: 690728.567\n",
      "epoch: 96700, train loss: 3.9036734819412233, val loss: 3.964906668663025, ETA in seconds: 691310.610\n",
      "epoch: 96800, train loss: 3.9031891584396363, val loss: 3.969971489906311, ETA in seconds: 691891.071\n",
      "epoch: 96900, train loss: 3.888548469543457, val loss: 3.980186128616333, ETA in seconds: 692465.123\n",
      "epoch: 97000, train loss: 3.896047592163086, val loss: 3.9641119718551634, ETA in seconds: 693041.735\n",
      "epoch: 97100, train loss: 3.89735860824585, val loss: 3.9740544319152833, ETA in seconds: 693618.528\n",
      "epoch: 97200, train loss: 3.8960546016693116, val loss: 3.978006362915039, ETA in seconds: 694207.604\n",
      "epoch: 97300, train loss: 3.899392914772034, val loss: 3.9726343870162966, ETA in seconds: 694783.098\n",
      "epoch: 97400, train loss: 3.902654266357422, val loss: 3.969766044616699, ETA in seconds: 695359.769\n",
      "epoch: 97500, train loss: 3.892411160469055, val loss: 3.9696874380111695, ETA in seconds: 695937.545\n",
      "epoch: 97600, train loss: 3.8963114261627196, val loss: 3.9684557914733887, ETA in seconds: 696516.471\n",
      "epoch: 97700, train loss: 3.8882078886032105, val loss: 3.9851200580596924, ETA in seconds: 697086.294\n",
      "epoch: 97800, train loss: 3.8882801055908205, val loss: 3.981973338127136, ETA in seconds: 697654.918\n",
      "epoch: 97900, train loss: 3.8942928314208984, val loss: 3.962502932548523, ETA in seconds: 698230.991\n",
      "epoch: 98000, train loss: 3.8906916618347167, val loss: 3.978003907203674, ETA in seconds: 698818.144\n",
      "epoch: 98100, train loss: 3.904396104812622, val loss: 3.9640305757522585, ETA in seconds: 699397.076\n",
      "epoch: 98200, train loss: 3.8938071966171264, val loss: 3.981937861442566, ETA in seconds: 699975.647\n",
      "epoch: 98300, train loss: 3.8921431064605714, val loss: 3.9736708641052245, ETA in seconds: 700547.420\n",
      "epoch: 98400, train loss: 3.9001972913742065, val loss: 3.97236909866333, ETA in seconds: 701112.034\n",
      "epoch: 98500, train loss: 3.889741849899292, val loss: 3.9732454299926756, ETA in seconds: 701692.316\n",
      "epoch: 98600, train loss: 3.8985676765441895, val loss: 3.9735090494155885, ETA in seconds: 702259.175\n",
      "epoch: 98700, train loss: 3.887482738494873, val loss: 3.9763402938842773, ETA in seconds: 702840.862\n",
      "epoch: 98800, train loss: 3.8925699472427366, val loss: 3.970812273025513, ETA in seconds: 703414.925\n",
      "epoch: 98900, train loss: 3.8918677806854247, val loss: 3.9700154781341555, ETA in seconds: 703981.941\n",
      "epoch: 99000, train loss: 3.8943835735321044, val loss: 3.972596597671509, ETA in seconds: 704547.705\n",
      "epoch: 99100, train loss: 3.8911041498184202, val loss: 3.965575408935547, ETA in seconds: 705117.571\n",
      "epoch: 99200, train loss: 3.8875197887420656, val loss: 3.971631073951721, ETA in seconds: 705694.075\n",
      "epoch: 99300, train loss: 3.8941015005111694, val loss: 3.9554460763931276, ETA in seconds: 706273.726\n",
      "epoch: 99400, train loss: 3.8876130104064943, val loss: 3.971455764770508, ETA in seconds: 706905.538\n",
      "epoch: 99500, train loss: 3.888723611831665, val loss: 3.9755341053009032, ETA in seconds: 707653.395\n",
      "epoch: 99600, train loss: 3.8928085803985595, val loss: 3.966990280151367, ETA in seconds: 708392.939\n",
      "epoch: 99700, train loss: 3.8934367179870604, val loss: 3.9631312131881713, ETA in seconds: 709144.701\n",
      "epoch: 99800, train loss: 3.8859908103942873, val loss: 3.9532074451446535, ETA in seconds: 709829.269\n",
      "epoch: 99900, train loss: 3.8883318424224855, val loss: 3.966230535507202, ETA in seconds: 710463.068\n",
      "epoch: 100000, train loss: 3.8989378929138185, val loss: 3.9689666271209716, ETA in seconds: 711202.290\n",
      "epoch: 100100, train loss: 3.8941763401031495, val loss: 3.981224036216736, ETA in seconds: 711974.356\n",
      "epoch: 100200, train loss: 3.8985904693603515, val loss: 3.963593769073486, ETA in seconds: 712554.766\n",
      "epoch: 100300, train loss: 3.892962384223938, val loss: 3.972836995124817, ETA in seconds: 713207.237\n",
      "epoch: 100400, train loss: 3.9000982999801637, val loss: 3.972660040855408, ETA in seconds: 713946.124\n",
      "epoch: 100500, train loss: 3.888287091255188, val loss: 3.9617082357406614, ETA in seconds: 714560.553\n",
      "epoch: 100600, train loss: 3.8971938848495484, val loss: 3.9677204132080077, ETA in seconds: 715137.533\n",
      "epoch: 100700, train loss: 3.8864816665649413, val loss: 3.967450761795044, ETA in seconds: 715717.153\n",
      "epoch: 100800, train loss: 3.890564727783203, val loss: 3.971644425392151, ETA in seconds: 716309.191\n",
      "epoch: 100900, train loss: 3.888218092918396, val loss: 3.9808348178863526, ETA in seconds: 716957.517\n",
      "epoch: 101000, train loss: 3.8921460151672362, val loss: 3.960869312286377, ETA in seconds: 717527.563\n",
      "epoch: 101100, train loss: 3.89253089427948, val loss: 3.9679099798202513, ETA in seconds: 718099.520\n",
      "epoch: 101200, train loss: 3.893482494354248, val loss: 3.9500063180923464, ETA in seconds: 718676.962\n",
      "epoch: 101300, train loss: 3.8972008228302, val loss: 3.964268612861633, ETA in seconds: 719256.340\n",
      "epoch: 101400, train loss: 3.8940476894378664, val loss: 3.97026469707489, ETA in seconds: 719846.698\n",
      "epoch: 101500, train loss: 3.8888416290283203, val loss: 3.967724347114563, ETA in seconds: 720431.415\n",
      "epoch: 101600, train loss: 3.8916243076324464, val loss: 3.96294686794281, ETA in seconds: 721002.083\n",
      "epoch: 101700, train loss: 3.9008291482925417, val loss: 3.9660749435424805, ETA in seconds: 721626.595\n",
      "epoch: 101800, train loss: 3.9043264627456664, val loss: 3.967244601249695, ETA in seconds: 722235.350\n",
      "epoch: 101900, train loss: 3.883149242401123, val loss: 3.957471227645874, ETA in seconds: 722844.137\n",
      "epoch: 102000, train loss: 3.893392729759216, val loss: 3.97517728805542, ETA in seconds: 723430.731\n",
      "epoch: 102100, train loss: 3.8899889469146727, val loss: 3.9743934154510496, ETA in seconds: 724033.683\n",
      "epoch: 102200, train loss: 3.8948401689529417, val loss: 3.9584487676620483, ETA in seconds: 724631.556\n",
      "epoch: 102300, train loss: 3.8967179536819456, val loss: 3.963102412223816, ETA in seconds: 725238.842\n",
      "epoch: 102400, train loss: 3.898767685890198, val loss: 3.971164178848267, ETA in seconds: 725861.012\n",
      "epoch: 102500, train loss: 3.8996469497680666, val loss: 3.9670451641082765, ETA in seconds: 726575.440\n",
      "epoch: 102600, train loss: 3.8987743854522705, val loss: 3.963042664527893, ETA in seconds: 727299.656\n",
      "epoch: 102700, train loss: 3.894728255271912, val loss: 3.958612561225891, ETA in seconds: 727931.217\n",
      "epoch: 102800, train loss: 3.890484619140625, val loss: 3.9614519834518434, ETA in seconds: 728517.202\n",
      "epoch: 102900, train loss: 3.8816048383712767, val loss: 3.975136876106262, ETA in seconds: 729123.741\n",
      "epoch: 103000, train loss: 3.8908291101455688, val loss: 3.962039279937744, ETA in seconds: 729772.616\n",
      "epoch: 103100, train loss: 3.886148762702942, val loss: 3.970860147476196, ETA in seconds: 730491.119\n",
      "epoch: 103200, train loss: 3.887726593017578, val loss: 3.9604594469070435, ETA in seconds: 731115.968\n",
      "epoch: 103300, train loss: 3.9038180351257323, val loss: 3.9734393119812013, ETA in seconds: 731710.064\n",
      "epoch: 103400, train loss: 3.890759325027466, val loss: 3.974557328224182, ETA in seconds: 732315.461\n",
      "epoch: 103500, train loss: 3.894552135467529, val loss: 3.96962571144104, ETA in seconds: 732976.716\n",
      "epoch: 103600, train loss: 3.890784502029419, val loss: 3.981092858314514, ETA in seconds: 733683.788\n",
      "epoch: 103700, train loss: 3.8917603969573973, val loss: 3.9689395904541014, ETA in seconds: 734403.684\n",
      "epoch: 103800, train loss: 3.895655941963196, val loss: 3.974294924736023, ETA in seconds: 735031.559\n",
      "epoch: 103900, train loss: 3.890978693962097, val loss: 3.9673410415649415, ETA in seconds: 735608.051\n",
      "epoch: 104000, train loss: 3.8928293228149413, val loss: 3.969817543029785, ETA in seconds: 736185.913\n",
      "epoch: 104100, train loss: 3.8923181295394897, val loss: 3.964158058166504, ETA in seconds: 736860.019\n",
      "epoch: 104200, train loss: 3.8964821815490724, val loss: 3.965904402732849, ETA in seconds: 737477.969\n",
      "epoch: 104300, train loss: 3.890701508522034, val loss: 3.9565038204193117, ETA in seconds: 738192.571\n",
      "epoch: 104400, train loss: 3.89607834815979, val loss: 3.9670685529708862, ETA in seconds: 738909.505\n",
      "epoch: 104500, train loss: 3.8938996315002443, val loss: 3.970977234840393, ETA in seconds: 739542.932\n",
      "epoch: 104600, train loss: 3.8845515966415407, val loss: 3.971385049819946, ETA in seconds: 740112.999\n",
      "epoch: 104700, train loss: 3.8895372629165648, val loss: 3.9666972875595095, ETA in seconds: 740685.389\n",
      "epoch: 104800, train loss: 3.889612627029419, val loss: 3.9657135725021364, ETA in seconds: 741265.804\n",
      "epoch: 104900, train loss: 3.901187515258789, val loss: 3.967970442771912, ETA in seconds: 741836.486\n",
      "epoch: 105000, train loss: 3.8960442781448363, val loss: 3.9670051097869874, ETA in seconds: 742425.661\n",
      "epoch: 105100, train loss: 3.899015188217163, val loss: 3.9682934045791627, ETA in seconds: 743016.122\n",
      "epoch: 105200, train loss: 3.892376518249512, val loss: 3.9856149911880494, ETA in seconds: 743620.227\n",
      "epoch: 105300, train loss: 3.88701491355896, val loss: 3.9703836679458617, ETA in seconds: 744162.503\n",
      "epoch: 105400, train loss: 3.8955103158950806, val loss: 3.9613171577453614, ETA in seconds: 744735.863\n",
      "epoch: 105500, train loss: 3.9033514738082884, val loss: 3.9794723749160767, ETA in seconds: 745410.129\n",
      "epoch: 105600, train loss: 3.9005715370178224, val loss: 3.9640208005905153, ETA in seconds: 745998.154\n",
      "epoch: 105700, train loss: 3.90094735622406, val loss: 3.971711802482605, ETA in seconds: 746579.416\n",
      "epoch: 105800, train loss: 3.901913595199585, val loss: 3.9725122451782227, ETA in seconds: 747156.279\n",
      "epoch: 105900, train loss: 3.8875338315963743, val loss: 3.962394857406616, ETA in seconds: 747736.675\n",
      "epoch: 106000, train loss: 3.8901898145675657, val loss: 3.959631586074829, ETA in seconds: 748351.651\n",
      "epoch: 106100, train loss: 3.896567940711975, val loss: 3.9774455070495605, ETA in seconds: 749047.973\n",
      "epoch: 106200, train loss: 3.8901240825653076, val loss: 3.975274348258972, ETA in seconds: 749772.745\n",
      "epoch: 106300, train loss: 3.894191765785217, val loss: 3.972555947303772, ETA in seconds: 750521.621\n",
      "epoch: 106400, train loss: 3.899090552330017, val loss: 3.9614968061447144, ETA in seconds: 751094.619\n",
      "epoch: 106500, train loss: 3.8935090065002442, val loss: 3.961677598953247, ETA in seconds: 751652.689\n",
      "epoch: 106600, train loss: 3.8859207153320314, val loss: 3.973562169075012, ETA in seconds: 752251.525\n",
      "epoch: 106700, train loss: 3.891356897354126, val loss: 3.9640291929244995, ETA in seconds: 752873.210\n",
      "epoch: 106800, train loss: 3.8961243629455566, val loss: 3.9629887104034425, ETA in seconds: 753562.968\n",
      "epoch: 106900, train loss: 3.8926607847213743, val loss: 3.9661581754684447, ETA in seconds: 754141.134\n",
      "epoch: 107000, train loss: 3.8893832206726073, val loss: 3.9651296615600584, ETA in seconds: 754710.824\n",
      "epoch: 107100, train loss: 3.902728223800659, val loss: 3.971274733543396, ETA in seconds: 755281.771\n",
      "epoch: 107200, train loss: 3.8995240926742554, val loss: 3.9687670469284058, ETA in seconds: 755908.352\n",
      "epoch: 107300, train loss: 3.8951860666275024, val loss: 3.972083020210266, ETA in seconds: 756575.948\n",
      "epoch: 107400, train loss: 3.891601324081421, val loss: 3.97703275680542, ETA in seconds: 757289.168\n",
      "epoch: 107500, train loss: 3.8894707441329954, val loss: 3.9839011669158935, ETA in seconds: 758059.419\n",
      "epoch: 107600, train loss: 3.8902969121932984, val loss: 3.957585000991821, ETA in seconds: 758654.499\n",
      "epoch: 107700, train loss: 3.889492392539978, val loss: 3.968890571594238, ETA in seconds: 759216.217\n",
      "epoch: 107800, train loss: 3.8874719619750975, val loss: 3.986755681037903, ETA in seconds: 759824.051\n",
      "epoch: 107900, train loss: 3.890258717536926, val loss: 3.966150188446045, ETA in seconds: 760462.664\n",
      "epoch: 108000, train loss: 3.894986796379089, val loss: 3.973803162574768, ETA in seconds: 761052.000\n",
      "epoch: 108100, train loss: 3.892588758468628, val loss: 3.9600861072540283, ETA in seconds: 761627.794\n",
      "epoch: 108200, train loss: 3.897509288787842, val loss: 3.9713223695755007, ETA in seconds: 762303.307\n",
      "epoch: 108300, train loss: 3.896452856063843, val loss: 3.9767109632492064, ETA in seconds: 762894.622\n",
      "epoch: 108400, train loss: 3.9000112295150755, val loss: 3.9598382472991944, ETA in seconds: 763532.250\n",
      "epoch: 108500, train loss: 3.888529396057129, val loss: 3.965944504737854, ETA in seconds: 764247.297\n",
      "epoch: 108600, train loss: 3.896393966674805, val loss: 3.9813365936279297, ETA in seconds: 764957.996\n",
      "epoch: 108700, train loss: 3.8925557374954223, val loss: 3.968436431884766, ETA in seconds: 765664.673\n",
      "epoch: 108800, train loss: 3.9029041051864626, val loss: 3.9722443342208864, ETA in seconds: 766377.656\n",
      "epoch: 108900, train loss: 3.8903624534606935, val loss: 3.971734714508057, ETA in seconds: 767005.808\n",
      "epoch: 109000, train loss: 3.9068147420883177, val loss: 3.97535126209259, ETA in seconds: 767578.132\n",
      "epoch: 109100, train loss: 3.886855387687683, val loss: 3.980156087875366, ETA in seconds: 768161.381\n",
      "epoch: 109200, train loss: 3.891553258895874, val loss: 3.975258493423462, ETA in seconds: 768743.182\n",
      "epoch: 109300, train loss: 3.8944761753082275, val loss: 3.96857373714447, ETA in seconds: 769354.607\n",
      "epoch: 109400, train loss: 3.8908401012420653, val loss: 3.970248317718506, ETA in seconds: 769975.644\n",
      "epoch: 109500, train loss: 3.894765090942383, val loss: 3.973304867744446, ETA in seconds: 770662.704\n",
      "epoch: 109600, train loss: 3.891873335838318, val loss: 3.9729613304138183, ETA in seconds: 771297.099\n",
      "epoch: 109700, train loss: 3.8935999870300293, val loss: 3.964191031455994, ETA in seconds: 771898.449\n",
      "epoch: 109800, train loss: 3.8964148283004763, val loss: 3.9691550731658936, ETA in seconds: 772494.412\n",
      "epoch: 109900, train loss: 3.894967865943909, val loss: 3.9739774227142335, ETA in seconds: 773092.111\n",
      "epoch: 110000, train loss: 3.8942621469497682, val loss: 3.9676901578903196, ETA in seconds: 773688.813\n",
      "epoch: 110100, train loss: 3.8980854988098144, val loss: 3.9776522874832154, ETA in seconds: 774282.831\n",
      "epoch: 110200, train loss: 3.89831964969635, val loss: 3.9631818294525147, ETA in seconds: 774886.236\n",
      "epoch: 110300, train loss: 3.893709397315979, val loss: 3.9769598722457884, ETA in seconds: 775484.851\n",
      "epoch: 110400, train loss: 3.8908777475357055, val loss: 3.979568529129028, ETA in seconds: 776072.155\n",
      "epoch: 110500, train loss: 3.8974475860595703, val loss: 3.986308789253235, ETA in seconds: 776663.710\n",
      "epoch: 110600, train loss: 3.8947428703308105, val loss: 3.9726864814758303, ETA in seconds: 777245.141\n",
      "epoch: 110700, train loss: 3.894531798362732, val loss: 3.964570927619934, ETA in seconds: 777823.511\n",
      "epoch: 110800, train loss: 3.894248867034912, val loss: 3.9759586095809936, ETA in seconds: 778416.831\n",
      "epoch: 110900, train loss: 3.8969603061676024, val loss: 3.9825581550598144, ETA in seconds: 779101.087\n",
      "epoch: 111000, train loss: 3.8880272626876833, val loss: 3.9651434659957885, ETA in seconds: 779679.457\n",
      "epoch: 111100, train loss: 3.893964719772339, val loss: 3.972355914115906, ETA in seconds: 780282.373\n",
      "epoch: 111200, train loss: 3.89315721988678, val loss: 3.9797276973724367, ETA in seconds: 780849.603\n",
      "epoch: 111300, train loss: 3.893524980545044, val loss: 3.9760658502578736, ETA in seconds: 781403.635\n",
      "epoch: 111400, train loss: 3.8987365484237673, val loss: 3.9731255054473875, ETA in seconds: 781973.036\n",
      "epoch: 111500, train loss: 3.8937092542648317, val loss: 3.9719457626342773, ETA in seconds: 782542.198\n",
      "epoch: 111600, train loss: 3.8947962284088136, val loss: 3.9659077644348146, ETA in seconds: 783117.651\n",
      "epoch: 111700, train loss: 3.896736907958984, val loss: 3.9720317840576174, ETA in seconds: 783693.417\n",
      "epoch: 111800, train loss: 3.8887880086898803, val loss: 3.9681167602539062, ETA in seconds: 784392.540\n",
      "epoch: 111900, train loss: 3.890948987007141, val loss: 3.9628166913986207, ETA in seconds: 785106.894\n",
      "epoch: 112000, train loss: 3.9032681941986085, val loss: 3.966732954978943, ETA in seconds: 785850.857\n",
      "epoch: 112100, train loss: 3.903981256484985, val loss: 3.9776326179504395, ETA in seconds: 786571.485\n",
      "epoch: 112200, train loss: 3.8999754905700685, val loss: 3.972140669822693, ETA in seconds: 787136.162\n",
      "epoch: 112300, train loss: 3.9056877851486207, val loss: 3.9732911825180053, ETA in seconds: 787728.927\n",
      "epoch: 112400, train loss: 3.896336531639099, val loss: 3.973198127746582, ETA in seconds: 788352.050\n",
      "epoch: 112500, train loss: 3.9023604869842528, val loss: 3.9578868627548216, ETA in seconds: 788938.876\n",
      "epoch: 112600, train loss: 3.894332838058472, val loss: 3.9690627098083495, ETA in seconds: 789581.900\n",
      "epoch: 112700, train loss: 3.8943009853363035, val loss: 3.9788385391235352, ETA in seconds: 790219.487\n",
      "epoch: 112800, train loss: 3.884247827529907, val loss: 3.9672903060913085, ETA in seconds: 790855.263\n",
      "epoch: 112900, train loss: 3.8963608026504515, val loss: 3.9799085378646852, ETA in seconds: 791479.330\n",
      "epoch: 113000, train loss: 3.8974579334259034, val loss: 3.972498059272766, ETA in seconds: 792118.737\n",
      "epoch: 113100, train loss: 3.900408887863159, val loss: 3.97236909866333, ETA in seconds: 792754.624\n",
      "epoch: 113200, train loss: 3.8819242238998415, val loss: 3.9642544984817505, ETA in seconds: 793376.881\n",
      "epoch: 113300, train loss: 3.903158354759216, val loss: 3.966206169128418, ETA in seconds: 794013.238\n",
      "epoch: 113400, train loss: 3.8944202184677126, val loss: 3.967988443374634, ETA in seconds: 794599.439\n",
      "epoch: 113500, train loss: 3.89792320728302, val loss: 3.964189624786377, ETA in seconds: 795232.844\n",
      "epoch: 113600, train loss: 3.8892168760299684, val loss: 3.979513239860535, ETA in seconds: 795871.114\n",
      "epoch: 113700, train loss: 3.894860291481018, val loss: 3.9793522119522096, ETA in seconds: 796509.993\n",
      "epoch: 113800, train loss: 3.9047379732131957, val loss: 3.9818762302398683, ETA in seconds: 797120.017\n",
      "epoch: 113900, train loss: 3.881452798843384, val loss: 3.9689146280288696, ETA in seconds: 797688.574\n",
      "epoch: 114000, train loss: 3.8855738401412965, val loss: 3.9735769271850585, ETA in seconds: 798289.199\n",
      "epoch: 114100, train loss: 3.8881123542785643, val loss: 3.9714343547821045, ETA in seconds: 798873.775\n",
      "epoch: 114200, train loss: 3.8926045894622803, val loss: 3.9723053932189942, ETA in seconds: 799497.923\n",
      "epoch: 114300, train loss: 3.893588423728943, val loss: 3.975518250465393, ETA in seconds: 800082.506\n",
      "epoch: 114400, train loss: 3.8895900726318358, val loss: 3.9723939657211305, ETA in seconds: 800683.988\n",
      "epoch: 114500, train loss: 3.8875538110733032, val loss: 3.969243049621582, ETA in seconds: 801253.452\n",
      "epoch: 114600, train loss: 3.8970295429229735, val loss: 3.9710469007492066, ETA in seconds: 801834.903\n",
      "epoch: 114700, train loss: 3.9016916036605833, val loss: 3.974264931678772, ETA in seconds: 802418.261\n",
      "epoch: 114800, train loss: 3.8935870885849, val loss: 3.961813473701477, ETA in seconds: 803031.192\n",
      "epoch: 114900, train loss: 3.889001417160034, val loss: 3.9690939664840696, ETA in seconds: 803605.217\n",
      "epoch: 115000, train loss: 3.891124153137207, val loss: 3.964093542098999, ETA in seconds: 804195.809\n",
      "epoch: 115100, train loss: 3.8945249557495116, val loss: 3.968885326385498, ETA in seconds: 804787.929\n",
      "epoch: 115200, train loss: 3.893503212928772, val loss: 3.9715213060379027, ETA in seconds: 805367.616\n",
      "epoch: 115300, train loss: 3.8956011295318604, val loss: 3.973263072967529, ETA in seconds: 805991.932\n",
      "epoch: 115400, train loss: 3.892634630203247, val loss: 3.9650055408477782, ETA in seconds: 806628.427\n",
      "epoch: 115500, train loss: 3.9063488721847532, val loss: 3.9603911638259888, ETA in seconds: 807215.883\n",
      "epoch: 115600, train loss: 3.894143486022949, val loss: 3.970511317253113, ETA in seconds: 807912.312\n",
      "epoch: 115700, train loss: 3.8986346244812013, val loss: 3.976273512840271, ETA in seconds: 808673.156\n",
      "epoch: 115800, train loss: 3.8972193956375123, val loss: 3.977328872680664, ETA in seconds: 809332.077\n",
      "epoch: 115900, train loss: 3.893475604057312, val loss: 3.9737035751342775, ETA in seconds: 809985.063\n",
      "epoch: 116000, train loss: 3.883979630470276, val loss: 3.982233452796936, ETA in seconds: 810662.154\n",
      "epoch: 116100, train loss: 3.893305253982544, val loss: 3.9711420059204103, ETA in seconds: 811301.685\n",
      "epoch: 116200, train loss: 3.898166823387146, val loss: 3.9683047771453857, ETA in seconds: 811950.186\n",
      "epoch: 116300, train loss: 3.9034478425979615, val loss: 3.975106406211853, ETA in seconds: 812632.376\n",
      "epoch: 116400, train loss: 3.889464282989502, val loss: 3.96682755947113, ETA in seconds: 813309.326\n",
      "epoch: 116500, train loss: 3.9000673055648805, val loss: 3.973502826690674, ETA in seconds: 813990.563\n",
      "epoch: 116600, train loss: 3.8924445629119875, val loss: 3.9760955333709718, ETA in seconds: 814672.967\n",
      "epoch: 116700, train loss: 3.890346145629883, val loss: 3.970976138114929, ETA in seconds: 815345.755\n",
      "epoch: 116800, train loss: 3.8922284841537476, val loss: 3.963751292228699, ETA in seconds: 816003.960\n",
      "epoch: 116900, train loss: 3.8847582817077635, val loss: 3.972470188140869, ETA in seconds: 816617.999\n",
      "epoch: 117000, train loss: 3.9046894788742064, val loss: 3.9708297729492186, ETA in seconds: 817236.455\n",
      "epoch: 117100, train loss: 3.891498255729675, val loss: 3.9613891363143923, ETA in seconds: 817871.904\n",
      "epoch: 117200, train loss: 3.897924280166626, val loss: 3.9681970357894896, ETA in seconds: 818566.367\n",
      "epoch: 117300, train loss: 3.8995007038116456, val loss: 3.9744178295135497, ETA in seconds: 819138.169\n",
      "epoch: 117400, train loss: 3.893707013130188, val loss: 3.9791325092315675, ETA in seconds: 819724.623\n",
      "epoch: 117500, train loss: 3.8936666011810304, val loss: 3.9743754386901857, ETA in seconds: 820286.608\n",
      "epoch: 117600, train loss: 3.9071990251541138, val loss: 3.9688560485839846, ETA in seconds: 820912.431\n",
      "epoch: 117700, train loss: 3.897867798805237, val loss: 3.976044225692749, ETA in seconds: 821541.644\n",
      "epoch: 117800, train loss: 3.893940806388855, val loss: 3.9653860092163087, ETA in seconds: 822142.438\n",
      "epoch: 117900, train loss: 3.8949822902679445, val loss: 3.9758843421936034, ETA in seconds: 822815.250\n",
      "epoch: 118000, train loss: 3.897490072250366, val loss: 3.9606924772262575, ETA in seconds: 823495.225\n",
      "epoch: 118100, train loss: 3.8971226692199705, val loss: 3.9855894565582277, ETA in seconds: 824027.728\n",
      "epoch: 118200, train loss: 3.8871011257171633, val loss: 3.9692126750946044, ETA in seconds: 824599.713\n",
      "epoch: 118300, train loss: 3.889512634277344, val loss: 3.980729508399963, ETA in seconds: 825168.140\n",
      "epoch: 118400, train loss: 3.8859246969223022, val loss: 3.9634363412857057, ETA in seconds: 825834.926\n",
      "epoch: 118500, train loss: 3.899180459976196, val loss: 3.9629562854766847, ETA in seconds: 826516.488\n",
      "epoch: 118600, train loss: 3.9007576942443847, val loss: 3.9763003826141357, ETA in seconds: 827233.454\n",
      "epoch: 118700, train loss: 3.8990308046340942, val loss: 3.9742228269577025, ETA in seconds: 827930.752\n",
      "epoch: 118800, train loss: 3.8954469919204713, val loss: 3.9771734952926634, ETA in seconds: 828461.023\n",
      "epoch: 118900, train loss: 3.8972423791885378, val loss: 3.9583298206329345, ETA in seconds: 829048.694\n",
      "epoch: 119000, train loss: 3.8939911842346193, val loss: 3.966373014450073, ETA in seconds: 829740.456\n",
      "epoch: 119100, train loss: 3.89696581363678, val loss: 3.9815932750701903, ETA in seconds: 830435.341\n",
      "epoch: 119200, train loss: 3.900978136062622, val loss: 3.9728577613830565, ETA in seconds: 831139.318\n",
      "epoch: 119300, train loss: 3.884683895111084, val loss: 3.977742886543274, ETA in seconds: 831673.271\n",
      "epoch: 119400, train loss: 3.896456265449524, val loss: 3.9772374868392943, ETA in seconds: 832237.945\n",
      "epoch: 119500, train loss: 3.892605018615723, val loss: 3.9667009592056273, ETA in seconds: 832815.929\n",
      "epoch: 119600, train loss: 3.8980125188827515, val loss: 3.963848114013672, ETA in seconds: 833364.139\n",
      "epoch: 119700, train loss: 3.883687472343445, val loss: 3.9664363622665406, ETA in seconds: 833898.504\n",
      "epoch: 119800, train loss: 3.894210410118103, val loss: 3.9774966716766356, ETA in seconds: 834437.356\n",
      "epoch: 119900, train loss: 3.887546420097351, val loss: 3.971226763725281, ETA in seconds: 834988.332\n",
      "epoch: 120000, train loss: 3.89931206703186, val loss: 3.979907584190369, ETA in seconds: 835535.671\n",
      "epoch: 120100, train loss: 3.8837602615356444, val loss: 3.951710605621338, ETA in seconds: 836091.610\n",
      "epoch: 120200, train loss: 3.891795206069946, val loss: 3.962241196632385, ETA in seconds: 836653.045\n",
      "epoch: 120300, train loss: 3.89269859790802, val loss: 3.984556293487549, ETA in seconds: 837206.629\n",
      "epoch: 120400, train loss: 3.8915426969528197, val loss: 3.969349575042725, ETA in seconds: 837771.240\n",
      "epoch: 120500, train loss: 3.8876319885253907, val loss: 3.9794275999069213, ETA in seconds: 838335.499\n",
      "epoch: 120600, train loss: 3.887111020088196, val loss: 3.9761401176452638, ETA in seconds: 838892.264\n",
      "epoch: 120700, train loss: 3.8918736219406127, val loss: 3.97738037109375, ETA in seconds: 839463.830\n",
      "epoch: 120800, train loss: 3.8872875213623046, val loss: 3.9724992752075194, ETA in seconds: 840040.855\n",
      "epoch: 120900, train loss: 3.8957099676132203, val loss: 3.9732263803482057, ETA in seconds: 840615.789\n",
      "epoch: 121000, train loss: 3.8933471918106077, val loss: 3.9650277853012086, ETA in seconds: 841196.183\n",
      "epoch: 121100, train loss: 3.8898043632507324, val loss: 3.9853155851364135, ETA in seconds: 841744.360\n",
      "epoch: 121200, train loss: 3.8991052150726317, val loss: 3.966506266593933, ETA in seconds: 842295.423\n",
      "epoch: 121300, train loss: 3.8987712144851683, val loss: 3.9614214181900023, ETA in seconds: 842852.382\n",
      "epoch: 121400, train loss: 3.890551447868347, val loss: 3.972663068771362, ETA in seconds: 843420.733\n",
      "epoch: 121500, train loss: 3.8975104093551636, val loss: 3.9701260566711425, ETA in seconds: 843978.273\n",
      "epoch: 121600, train loss: 3.9004759311676027, val loss: 3.9688257932662965, ETA in seconds: 844548.957\n",
      "epoch: 121700, train loss: 3.8923475980758666, val loss: 3.9695666313171385, ETA in seconds: 845103.014\n",
      "epoch: 121800, train loss: 3.89033727645874, val loss: 3.9766477584838866, ETA in seconds: 845659.313\n",
      "epoch: 121900, train loss: 3.89194860458374, val loss: 3.9709751844406127, ETA in seconds: 846218.818\n",
      "epoch: 122000, train loss: 3.8923344373703004, val loss: 3.979715371131897, ETA in seconds: 846790.610\n",
      "epoch: 122100, train loss: 3.9023411750793455, val loss: 3.9742143630981444, ETA in seconds: 847351.866\n",
      "epoch: 122200, train loss: 3.8903000116348267, val loss: 3.9762057065963745, ETA in seconds: 847921.812\n",
      "epoch: 122300, train loss: 3.896626281738281, val loss: 3.967113971710205, ETA in seconds: 848500.058\n",
      "epoch: 122400, train loss: 3.8816389560699465, val loss: 3.971676301956177, ETA in seconds: 849070.878\n",
      "epoch: 122500, train loss: 3.8941418886184693, val loss: 3.9758176803588867, ETA in seconds: 849674.849\n",
      "epoch: 122600, train loss: 3.884479284286499, val loss: 3.9670711517333985, ETA in seconds: 850394.311\n",
      "epoch: 122700, train loss: 3.895797944068909, val loss: 3.9717063426971437, ETA in seconds: 851079.780\n",
      "epoch: 122800, train loss: 3.8931134700775147, val loss: 3.967799115180969, ETA in seconds: 851756.851\n",
      "epoch: 122900, train loss: 3.8994797468185425, val loss: 3.966419792175293, ETA in seconds: 852299.409\n",
      "epoch: 123000, train loss: 3.8928205013275146, val loss: 3.9685635566711426, ETA in seconds: 852865.904\n",
      "epoch: 123100, train loss: 3.8846920251846315, val loss: 3.967134475708008, ETA in seconds: 853429.110\n",
      "epoch: 123200, train loss: 3.8853559732437133, val loss: 3.9628392696380614, ETA in seconds: 853974.480\n",
      "epoch: 123300, train loss: 3.8866261720657347, val loss: 3.9738759279251097, ETA in seconds: 854517.231\n",
      "epoch: 123400, train loss: 3.8820677995681763, val loss: 3.9792715072631837, ETA in seconds: 855051.904\n",
      "epoch: 123500, train loss: 3.888074517250061, val loss: 3.9675787687301636, ETA in seconds: 855596.612\n",
      "epoch: 123600, train loss: 3.893798065185547, val loss: 3.9793857097625733, ETA in seconds: 856135.647\n",
      "epoch: 123700, train loss: 3.8964449405670165, val loss: 3.9715831756591795, ETA in seconds: 856737.068\n",
      "epoch: 123800, train loss: 3.894726276397705, val loss: 3.9752539157867433, ETA in seconds: 857446.520\n",
      "epoch: 123900, train loss: 3.886293816566467, val loss: 3.969852828979492, ETA in seconds: 858145.523\n",
      "epoch: 124000, train loss: 3.895665431022644, val loss: 3.9743324518203735, ETA in seconds: 858846.768\n",
      "epoch: 124100, train loss: 3.9043888568878176, val loss: 3.9688178300857544, ETA in seconds: 859545.875\n",
      "epoch: 124200, train loss: 3.9019126653671266, val loss: 3.9612290143966673, ETA in seconds: 860243.612\n",
      "epoch: 124300, train loss: 3.8968890428543093, val loss: 3.976104974746704, ETA in seconds: 860891.986\n",
      "epoch: 124400, train loss: 3.890485906600952, val loss: 3.9761473655700685, ETA in seconds: 861444.724\n",
      "epoch: 124500, train loss: 3.8989657878875734, val loss: 3.975688624382019, ETA in seconds: 862010.097\n",
      "epoch: 124600, train loss: 3.895867204666138, val loss: 3.9666579723358155, ETA in seconds: 862577.495\n",
      "epoch: 124700, train loss: 3.886599969863892, val loss: 3.9685295820236206, ETA in seconds: 863144.279\n",
      "epoch: 124800, train loss: 3.8872938632965086, val loss: 3.969654989242554, ETA in seconds: 863696.017\n",
      "epoch: 124900, train loss: 3.8907020568847654, val loss: 3.9777092695236207, ETA in seconds: 864239.830\n",
      "epoch: 125000, train loss: 3.8988346099853515, val loss: 3.974631404876709, ETA in seconds: 864791.478\n",
      "epoch: 125100, train loss: 3.8919975757598877, val loss: 3.97491135597229, ETA in seconds: 865330.395\n",
      "epoch: 125200, train loss: 3.9009113073349, val loss: 3.9801785230636595, ETA in seconds: 865921.038\n",
      "epoch: 125300, train loss: 3.8871115684509276, val loss: 3.9712215662002563, ETA in seconds: 866616.118\n",
      "epoch: 125400, train loss: 3.895499515533447, val loss: 3.9808614253997803, ETA in seconds: 867175.309\n",
      "epoch: 125500, train loss: 3.891808366775513, val loss: 3.982958507537842, ETA in seconds: 867790.642\n",
      "epoch: 125600, train loss: 3.8965056896209718, val loss: 3.974812078475952, ETA in seconds: 868472.909\n",
      "epoch: 125700, train loss: 3.895016074180603, val loss: 3.9606617212295534, ETA in seconds: 869148.405\n",
      "epoch: 125800, train loss: 3.898798108100891, val loss: 3.9735835075378416, ETA in seconds: 869876.115\n",
      "epoch: 125900, train loss: 3.888787579536438, val loss: 3.9627618312835695, ETA in seconds: 870535.979\n",
      "epoch: 126000, train loss: 3.8935316801071167, val loss: 3.968620753288269, ETA in seconds: 871065.713\n",
      "epoch: 126100, train loss: 3.8940441608428955, val loss: 3.9716765165328978, ETA in seconds: 871575.970\n",
      "epoch: 126200, train loss: 3.8888278722763063, val loss: 3.9799683570861815, ETA in seconds: 872181.162\n",
      "epoch: 126300, train loss: 3.8957534551620485, val loss: 3.9753337860107423, ETA in seconds: 872770.975\n",
      "epoch: 126400, train loss: 3.8974687576293947, val loss: 3.9746971845626833, ETA in seconds: 873479.375\n",
      "epoch: 126500, train loss: 3.889582633972168, val loss: 3.9730480670928956, ETA in seconds: 874204.260\n",
      "epoch: 126600, train loss: 3.8908215284347536, val loss: 3.972010087966919, ETA in seconds: 874787.631\n",
      "epoch: 126700, train loss: 3.8938929080963134, val loss: 3.969148349761963, ETA in seconds: 875300.195\n",
      "epoch: 126800, train loss: 3.8991873979568483, val loss: 3.9702183485031126, ETA in seconds: 875825.133\n",
      "epoch: 126900, train loss: 3.9045578718185423, val loss: 3.9739905118942263, ETA in seconds: 876359.483\n",
      "epoch: 127000, train loss: 3.89561288356781, val loss: 3.972003436088562, ETA in seconds: 876908.396\n",
      "epoch: 127100, train loss: 3.9000250339508056, val loss: 3.9689367055892943, ETA in seconds: 877486.145\n",
      "epoch: 127200, train loss: 3.9049902677536013, val loss: 3.9768476724624633, ETA in seconds: 878048.692\n",
      "epoch: 127300, train loss: 3.9033836841583254, val loss: 3.97710497379303, ETA in seconds: 878619.812\n",
      "epoch: 127400, train loss: 3.9017645835876467, val loss: 3.982740330696106, ETA in seconds: 879279.373\n",
      "epoch: 127500, train loss: 3.9039202451705934, val loss: 3.975216007232666, ETA in seconds: 879950.267\n",
      "epoch: 127600, train loss: 3.889366960525513, val loss: 3.9712377786636353, ETA in seconds: 880578.049\n",
      "epoch: 127700, train loss: 3.8908255815505983, val loss: 3.9686945915222167, ETA in seconds: 881131.886\n",
      "epoch: 127800, train loss: 3.893921136856079, val loss: 3.967144417762756, ETA in seconds: 881661.420\n",
      "epoch: 127900, train loss: 3.8984424352645872, val loss: 3.96289541721344, ETA in seconds: 882197.535\n",
      "epoch: 128000, train loss: 3.883119750022888, val loss: 3.9720181703567503, ETA in seconds: 882740.540\n",
      "epoch: 128100, train loss: 3.8926993131637575, val loss: 3.961937737464905, ETA in seconds: 883275.710\n",
      "epoch: 128200, train loss: 3.895116400718689, val loss: 3.9758513927459718, ETA in seconds: 883832.689\n",
      "epoch: 128300, train loss: 3.8920878171920776, val loss: 3.972190809249878, ETA in seconds: 884392.728\n",
      "epoch: 128400, train loss: 3.900361490249634, val loss: 3.9737767934799195, ETA in seconds: 884957.345\n",
      "epoch: 128500, train loss: 3.8948503732681274, val loss: 3.97192268371582, ETA in seconds: 885537.075\n",
      "epoch: 128600, train loss: 3.8902096271514894, val loss: 3.9621034622192384, ETA in seconds: 886214.985\n",
      "epoch: 128700, train loss: 3.9009413957595824, val loss: 3.974656891822815, ETA in seconds: 886890.424\n",
      "epoch: 128800, train loss: 3.8915478229522704, val loss: 3.9767847061157227, ETA in seconds: 887570.361\n",
      "epoch: 128900, train loss: 3.892028069496155, val loss: 3.9724939107894897, ETA in seconds: 888282.614\n",
      "epoch: 129000, train loss: 3.8981329679489134, val loss: 3.9711141109466555, ETA in seconds: 888832.125\n",
      "epoch: 129100, train loss: 3.890747570991516, val loss: 3.9688462018966675, ETA in seconds: 889370.762\n",
      "epoch: 129200, train loss: 3.8876131057739256, val loss: 3.984759211540222, ETA in seconds: 889976.346\n",
      "epoch: 129300, train loss: 3.8951915502548218, val loss: 3.9692034244537355, ETA in seconds: 890527.219\n",
      "epoch: 129400, train loss: 3.9021288394927978, val loss: 3.9810184240341187, ETA in seconds: 891081.201\n",
      "epoch: 129500, train loss: 3.902845597267151, val loss: 3.9627368688583373, ETA in seconds: 891621.523\n",
      "epoch: 129600, train loss: 3.8981558799743654, val loss: 3.9808836936950684, ETA in seconds: 892174.161\n",
      "epoch: 129700, train loss: 3.8935086250305178, val loss: 3.974388837814331, ETA in seconds: 892750.494\n",
      "epoch: 129800, train loss: 3.886069488525391, val loss: 3.9793002128601076, ETA in seconds: 893309.584\n",
      "epoch: 129900, train loss: 3.897179865837097, val loss: 3.973252296447754, ETA in seconds: 893863.358\n",
      "epoch: 130000, train loss: 3.892442226409912, val loss: 3.967471551895142, ETA in seconds: 894418.228\n",
      "epoch: 130100, train loss: 3.89259614944458, val loss: 3.9743184566497805, ETA in seconds: 894981.993\n",
      "epoch: 130200, train loss: 3.891750526428223, val loss: 3.960470986366272, ETA in seconds: 895523.028\n",
      "epoch: 130300, train loss: 3.896523928642273, val loss: 3.9825428247451784, ETA in seconds: 896069.220\n",
      "epoch: 130400, train loss: 3.8936635732650755, val loss: 3.9626317024230957, ETA in seconds: 896636.560\n",
      "epoch: 130500, train loss: 3.896716666221619, val loss: 3.97087619304657, ETA in seconds: 897211.241\n",
      "epoch: 130600, train loss: 3.8927226305007934, val loss: 3.984857964515686, ETA in seconds: 897902.697\n",
      "epoch: 130700, train loss: 3.895561695098877, val loss: 3.9837356567382813, ETA in seconds: 898570.864\n",
      "epoch: 130800, train loss: 3.8864595174789427, val loss: 3.9709661960601808, ETA in seconds: 899237.296\n",
      "epoch: 130900, train loss: 3.899488925933838, val loss: 3.968232798576355, ETA in seconds: 899915.416\n",
      "epoch: 131000, train loss: 3.896812891960144, val loss: 3.9646294355392455, ETA in seconds: 900615.250\n",
      "epoch: 131100, train loss: 3.8928681135177614, val loss: 3.9676262378692626, ETA in seconds: 901221.962\n",
      "epoch: 131200, train loss: 3.9017330408096313, val loss: 3.9622382164001464, ETA in seconds: 901770.950\n",
      "epoch: 131300, train loss: 3.8961947441101072, val loss: 3.964570665359497, ETA in seconds: 902330.095\n",
      "epoch: 131400, train loss: 3.892285394668579, val loss: 3.959746265411377, ETA in seconds: 902872.217\n",
      "epoch: 131500, train loss: 3.8888431787490845, val loss: 3.9706801652908323, ETA in seconds: 903443.341\n",
      "epoch: 131600, train loss: 3.9008926153182983, val loss: 3.9794827938079833, ETA in seconds: 903987.349\n",
      "epoch: 131700, train loss: 3.8939141511917112, val loss: 3.9784812688827516, ETA in seconds: 904635.262\n",
      "epoch: 131800, train loss: 3.903739666938782, val loss: 3.9671558856964113, ETA in seconds: 905228.882\n",
      "epoch: 131900, train loss: 3.8862988471984865, val loss: 3.9728073358535765, ETA in seconds: 905896.768\n",
      "epoch: 132000, train loss: 3.8965028524398804, val loss: 3.9720516920089723, ETA in seconds: 906559.080\n",
      "epoch: 132100, train loss: 3.8914269685745237, val loss: 3.968256711959839, ETA in seconds: 907232.064\n",
      "epoch: 132200, train loss: 3.8890053987503053, val loss: 3.968924307823181, ETA in seconds: 907906.921\n",
      "epoch: 132300, train loss: 3.894089412689209, val loss: 3.971797776222229, ETA in seconds: 908586.782\n",
      "epoch: 132400, train loss: 3.8954808473587037, val loss: 3.969765877723694, ETA in seconds: 909263.975\n",
      "epoch: 132500, train loss: 3.885274147987366, val loss: 3.972109293937683, ETA in seconds: 909953.471\n",
      "epoch: 132600, train loss: 3.8878379344940184, val loss: 3.973389506340027, ETA in seconds: 910526.891\n",
      "epoch: 132700, train loss: 3.8947593688964846, val loss: 3.969470191001892, ETA in seconds: 911163.020\n",
      "epoch: 132800, train loss: 3.8827728271484374, val loss: 3.9684974193572997, ETA in seconds: 911834.663\n",
      "epoch: 132900, train loss: 3.8899747848510744, val loss: 3.973659539222717, ETA in seconds: 912511.282\n",
      "epoch: 133000, train loss: 3.8889641523361207, val loss: 3.9717643737792967, ETA in seconds: 913079.897\n",
      "epoch: 133100, train loss: 3.8993613719940186, val loss: 3.97887499332428, ETA in seconds: 913631.917\n",
      "epoch: 133200, train loss: 3.9000144004821777, val loss: 3.96908221244812, ETA in seconds: 914178.929\n",
      "epoch: 133300, train loss: 3.892821264266968, val loss: 3.969780611991882, ETA in seconds: 914728.274\n",
      "epoch: 133400, train loss: 3.8986793041229246, val loss: 3.9553168773651124, ETA in seconds: 915296.679\n",
      "epoch: 133500, train loss: 3.8934114933013917, val loss: 3.9689029455184937, ETA in seconds: 915824.972\n",
      "epoch: 133600, train loss: 3.8990010261535644, val loss: 3.976651406288147, ETA in seconds: 916367.318\n",
      "epoch: 133700, train loss: 3.891306519508362, val loss: 3.9680755853652956, ETA in seconds: 916928.651\n",
      "epoch: 133800, train loss: 3.8915104627609254, val loss: 3.9766916751861574, ETA in seconds: 917454.839\n",
      "epoch: 133900, train loss: 3.907887172698975, val loss: 3.962053823471069, ETA in seconds: 917993.708\n",
      "epoch: 134000, train loss: 3.8892958879470827, val loss: 3.968982124328613, ETA in seconds: 918515.073\n",
      "epoch: 134100, train loss: 3.8993838310241697, val loss: 3.967616868019104, ETA in seconds: 919096.305\n",
      "epoch: 134200, train loss: 3.898637628555298, val loss: 3.970040488243103, ETA in seconds: 919690.606\n",
      "epoch: 134300, train loss: 3.8967511892318725, val loss: 3.9609039068222045, ETA in seconds: 920345.267\n",
      "epoch: 134400, train loss: 3.887171006202698, val loss: 3.971784520149231, ETA in seconds: 920889.552\n",
      "epoch: 134500, train loss: 3.8930095195770265, val loss: 3.976457214355469, ETA in seconds: 921424.862\n",
      "epoch: 134600, train loss: 3.8940022706985475, val loss: 3.9721853733062744, ETA in seconds: 921969.557\n",
      "epoch: 134700, train loss: 3.8959555864334106, val loss: 3.978459429740906, ETA in seconds: 922507.377\n",
      "epoch: 134800, train loss: 3.8945159912109375, val loss: 3.965742087364197, ETA in seconds: 923048.888\n",
      "epoch: 134900, train loss: 3.899276542663574, val loss: 3.9696862936019897, ETA in seconds: 923582.622\n",
      "epoch: 135000, train loss: 3.8850316286087034, val loss: 3.96731584072113, ETA in seconds: 924122.408\n",
      "epoch: 135100, train loss: 3.896608662605286, val loss: 3.969064545631409, ETA in seconds: 924664.669\n",
      "epoch: 135200, train loss: 3.9007883071899414, val loss: 3.9665398597717285, ETA in seconds: 925242.368\n",
      "epoch: 135300, train loss: 3.897281527519226, val loss: 3.9627415418624876, ETA in seconds: 925821.876\n",
      "epoch: 135400, train loss: 3.9012202978134156, val loss: 3.9630064725875855, ETA in seconds: 926397.889\n",
      "epoch: 135500, train loss: 3.8935849905014037, val loss: 3.9774582386016846, ETA in seconds: 926929.648\n",
      "epoch: 135600, train loss: 3.8935184717178344, val loss: 3.973991060256958, ETA in seconds: 927460.565\n",
      "epoch: 135700, train loss: 3.8882532835006716, val loss: 3.9721657752990724, ETA in seconds: 928010.692\n",
      "epoch: 135800, train loss: 3.8861541032791136, val loss: 3.957182455062866, ETA in seconds: 928556.603\n",
      "epoch: 135900, train loss: 3.8935375213623047, val loss: 3.9718210458755494, ETA in seconds: 929092.046\n",
      "epoch: 136000, train loss: 3.890865921974182, val loss: 3.974566173553467, ETA in seconds: 929636.033\n",
      "epoch: 136100, train loss: 3.8932879686355593, val loss: 3.9675686836242674, ETA in seconds: 930199.717\n",
      "epoch: 136200, train loss: 3.8918817758560182, val loss: 3.970745801925659, ETA in seconds: 930779.082\n",
      "epoch: 136300, train loss: 3.8779297351837156, val loss: 3.968022418022156, ETA in seconds: 931399.440\n",
      "epoch: 136400, train loss: 3.8940062522888184, val loss: 3.9713608026504517, ETA in seconds: 932072.383\n",
      "epoch: 136500, train loss: 3.892977738380432, val loss: 3.970476746559143, ETA in seconds: 932736.463\n",
      "epoch: 136600, train loss: 3.904552674293518, val loss: 3.9588412046432495, ETA in seconds: 933405.981\n",
      "epoch: 136700, train loss: 3.8819435834884644, val loss: 3.966065788269043, ETA in seconds: 934073.918\n",
      "epoch: 136800, train loss: 3.897014546394348, val loss: 3.9554973363876345, ETA in seconds: 934732.103\n",
      "epoch: 136900, train loss: 3.891320538520813, val loss: 3.9727217674255373, ETA in seconds: 935292.047\n",
      "epoch: 137000, train loss: 3.8967430114746096, val loss: 3.9626811742782593, ETA in seconds: 935866.171\n",
      "epoch: 137100, train loss: 3.8910413503646852, val loss: 3.965068483352661, ETA in seconds: 936443.560\n",
      "epoch: 137200, train loss: 3.8979517459869384, val loss: 3.97041802406311, ETA in seconds: 937054.919\n",
      "epoch: 137300, train loss: 3.8944061994552612, val loss: 3.9667940855026247, ETA in seconds: 937684.180\n",
      "epoch: 137400, train loss: 3.901831340789795, val loss: 3.9728732824325563, ETA in seconds: 938216.477\n",
      "epoch: 137500, train loss: 3.8948487997055055, val loss: 3.9731290102005006, ETA in seconds: 938816.071\n",
      "epoch: 137600, train loss: 3.8913379430770876, val loss: 3.9656795501708983, ETA in seconds: 939389.728\n",
      "epoch: 137700, train loss: 3.892642617225647, val loss: 3.969205927848816, ETA in seconds: 939936.362\n",
      "epoch: 137800, train loss: 3.8963172912597654, val loss: 3.979031300544739, ETA in seconds: 940473.740\n",
      "epoch: 137900, train loss: 3.9035651683807373, val loss: 3.9582185983657836, ETA in seconds: 940998.876\n",
      "epoch: 138000, train loss: 3.885590100288391, val loss: 3.9589251279830933, ETA in seconds: 941563.706\n",
      "epoch: 138100, train loss: 3.8885831594467164, val loss: 3.967946910858154, ETA in seconds: 942260.682\n",
      "epoch: 138200, train loss: 3.8982428550720214, val loss: 3.9623019218444826, ETA in seconds: 942775.220\n",
      "epoch: 138300, train loss: 3.8953082084655763, val loss: 3.9721049547195433, ETA in seconds: 943311.122\n",
      "epoch: 138400, train loss: 3.8858365058898925, val loss: 3.969568657875061, ETA in seconds: 943853.963\n",
      "epoch: 138500, train loss: 3.8935427904129027, val loss: 3.956821250915527, ETA in seconds: 944385.408\n",
      "epoch: 138600, train loss: 3.881825160980225, val loss: 3.9707802534103394, ETA in seconds: 944921.975\n",
      "epoch: 138700, train loss: 3.886651706695557, val loss: 3.9696744203567507, ETA in seconds: 945462.588\n",
      "epoch: 138800, train loss: 3.8935458421707154, val loss: 3.9667633533477784, ETA in seconds: 946030.493\n",
      "epoch: 138900, train loss: 3.8793789386749267, val loss: 3.9686091423034666, ETA in seconds: 946562.766\n",
      "epoch: 139000, train loss: 3.894831681251526, val loss: 3.955588674545288, ETA in seconds: 947097.884\n",
      "epoch: 139100, train loss: 3.896250319480896, val loss: 3.9627712965011597, ETA in seconds: 947728.594\n",
      "epoch: 139200, train loss: 3.894380879402161, val loss: 3.969137954711914, ETA in seconds: 948250.865\n",
      "epoch: 139300, train loss: 3.8963184356689453, val loss: 3.979959321022034, ETA in seconds: 948769.234\n",
      "epoch: 139400, train loss: 3.894807052612305, val loss: 3.979285144805908, ETA in seconds: 949289.141\n",
      "epoch: 139500, train loss: 3.8953328132629395, val loss: 3.978056216239929, ETA in seconds: 949809.920\n",
      "epoch: 139600, train loss: 3.903212881088257, val loss: 3.970418906211853, ETA in seconds: 950334.698\n",
      "epoch: 139700, train loss: 3.890876317024231, val loss: 3.9675387144088745, ETA in seconds: 950855.677\n",
      "epoch: 139800, train loss: 3.8969723701477053, val loss: 3.9706979513168337, ETA in seconds: 951387.321\n",
      "epoch: 139900, train loss: 3.899535632133484, val loss: 3.9625993967056274, ETA in seconds: 951913.279\n",
      "epoch: 140000, train loss: 3.894600677490234, val loss: 3.9668302297592164, ETA in seconds: 952434.884\n",
      "epoch: 140100, train loss: 3.8930776357650756, val loss: 3.9589020252227782, ETA in seconds: 952980.596\n",
      "epoch: 140200, train loss: 3.897545003890991, val loss: 3.963965320587158, ETA in seconds: 953502.179\n",
      "epoch: 140300, train loss: 3.8891175985336304, val loss: 3.9628852367401124, ETA in seconds: 954022.046\n",
      "epoch: 140400, train loss: 3.899600291252136, val loss: 3.967962455749512, ETA in seconds: 954532.580\n",
      "epoch: 140500, train loss: 3.9008187532424925, val loss: 3.969217133522034, ETA in seconds: 955043.897\n",
      "epoch: 140600, train loss: 3.89216628074646, val loss: 3.960048866271973, ETA in seconds: 955589.422\n",
      "epoch: 140700, train loss: 3.880923056602478, val loss: 3.965990948677063, ETA in seconds: 956117.163\n",
      "epoch: 140800, train loss: 3.901096796989441, val loss: 3.964481973648071, ETA in seconds: 956644.896\n",
      "epoch: 140900, train loss: 3.894305920600891, val loss: 3.9651374578475953, ETA in seconds: 957157.877\n",
      "epoch: 141000, train loss: 3.8812011003494264, val loss: 3.969378447532654, ETA in seconds: 957658.590\n",
      "epoch: 141100, train loss: 3.902289891242981, val loss: 3.9826656341552735, ETA in seconds: 958176.287\n",
      "epoch: 141200, train loss: 3.894926905632019, val loss: 3.9699915409088136, ETA in seconds: 958706.740\n",
      "epoch: 141300, train loss: 3.8858028173446657, val loss: 3.9669875621795656, ETA in seconds: 959299.245\n",
      "epoch: 141400, train loss: 3.8841596126556395, val loss: 3.971618580818176, ETA in seconds: 959846.381\n",
      "epoch: 141500, train loss: 3.8959471464157103, val loss: 3.970395398139954, ETA in seconds: 960388.612\n",
      "epoch: 141600, train loss: 3.8908405780792235, val loss: 3.979433560371399, ETA in seconds: 960908.230\n",
      "epoch: 141700, train loss: 3.8956549406051635, val loss: 3.9795178890228273, ETA in seconds: 961430.077\n",
      "epoch: 141800, train loss: 3.888103461265564, val loss: 3.977593111991882, ETA in seconds: 961950.824\n",
      "epoch: 141900, train loss: 3.89372456073761, val loss: 3.9691993236541747, ETA in seconds: 962476.661\n",
      "epoch: 142000, train loss: 3.887537956237793, val loss: 3.9696536540985106, ETA in seconds: 963010.295\n",
      "epoch: 142100, train loss: 3.8971906900405884, val loss: 3.9643887758255003, ETA in seconds: 963539.696\n",
      "epoch: 142200, train loss: 3.892291450500488, val loss: 3.9727664470672606, ETA in seconds: 964054.026\n",
      "epoch: 142300, train loss: 3.8855169296264647, val loss: 3.9770084381103517, ETA in seconds: 964553.300\n",
      "epoch: 142400, train loss: 3.8868372201919557, val loss: 3.966322922706604, ETA in seconds: 965049.919\n",
      "epoch: 142500, train loss: 3.892535090446472, val loss: 3.9704217195510862, ETA in seconds: 965575.316\n",
      "epoch: 142600, train loss: 3.8859197378158568, val loss: 3.973615837097168, ETA in seconds: 966091.270\n",
      "epoch: 142700, train loss: 3.8896108865737915, val loss: 3.968800687789917, ETA in seconds: 966586.136\n",
      "epoch: 142800, train loss: 3.884141731262207, val loss: 3.963303828239441, ETA in seconds: 967104.513\n",
      "epoch: 142900, train loss: 3.8859310150146484, val loss: 3.9790701150894163, ETA in seconds: 967620.419\n",
      "epoch: 143000, train loss: 3.89372341632843, val loss: 3.9750348567962646, ETA in seconds: 968134.538\n",
      "epoch: 143100, train loss: 3.8961326122283935, val loss: 3.975000286102295, ETA in seconds: 968668.317\n",
      "epoch: 143200, train loss: 3.8993804693222045, val loss: 3.9677937746047975, ETA in seconds: 969194.244\n",
      "epoch: 143300, train loss: 3.8932090282440184, val loss: 3.9696537017822267, ETA in seconds: 969715.300\n",
      "epoch: 143400, train loss: 3.887260580062866, val loss: 3.9784549951553343, ETA in seconds: 970232.515\n",
      "epoch: 143500, train loss: 3.893047332763672, val loss: 3.976085638999939, ETA in seconds: 970846.347\n",
      "epoch: 143600, train loss: 3.896113920211792, val loss: 3.9645522117614744, ETA in seconds: 971362.336\n",
      "epoch: 143700, train loss: 3.8902738809585573, val loss: 3.972123050689697, ETA in seconds: 971878.667\n",
      "epoch: 143800, train loss: 3.88441162109375, val loss: 3.969353675842285, ETA in seconds: 972395.470\n",
      "epoch: 143900, train loss: 3.8840970754623414, val loss: 3.9793198347091674, ETA in seconds: 972914.894\n",
      "epoch: 144000, train loss: 3.893566060066223, val loss: 3.9759564876556395, ETA in seconds: 973431.161\n",
      "epoch: 144100, train loss: 3.8849323749542237, val loss: 3.965341806411743, ETA in seconds: 973957.256\n",
      "epoch: 144200, train loss: 3.8968310356140137, val loss: 3.975987267494202, ETA in seconds: 974527.894\n",
      "epoch: 144300, train loss: 3.8939016103744506, val loss: 3.9715850830078123, ETA in seconds: 975059.281\n",
      "epoch: 144400, train loss: 3.897957134246826, val loss: 3.9772645473480224, ETA in seconds: 975626.486\n",
      "epoch: 144500, train loss: 3.8908191919326782, val loss: 3.9747822999954225, ETA in seconds: 976264.068\n",
      "epoch: 144600, train loss: 3.9003506183624266, val loss: 3.9692194938659666, ETA in seconds: 976827.698\n",
      "epoch: 144700, train loss: 3.888833427429199, val loss: 3.9760496854782104, ETA in seconds: 977397.424\n",
      "epoch: 144800, train loss: 3.8921443939208986, val loss: 3.972223234176636, ETA in seconds: 977971.141\n",
      "epoch: 144900, train loss: 3.8999324798583985, val loss: 3.975000739097595, ETA in seconds: 978513.440\n",
      "epoch: 145000, train loss: 3.9021557092666628, val loss: 3.975375533103943, ETA in seconds: 979031.431\n",
      "epoch: 145100, train loss: 3.8960816621780396, val loss: 3.9754551887512206, ETA in seconds: 979544.758\n",
      "epoch: 145200, train loss: 3.8919062852859496, val loss: 3.973638582229614, ETA in seconds: 980109.089\n",
      "epoch: 145300, train loss: 3.8912803888320924, val loss: 3.9721627473831176, ETA in seconds: 980680.443\n",
      "epoch: 145400, train loss: 3.8976935863494875, val loss: 3.9689969062805175, ETA in seconds: 981203.736\n",
      "epoch: 145500, train loss: 3.8953571796417235, val loss: 3.9838462352752684, ETA in seconds: 981720.417\n",
      "epoch: 145600, train loss: 3.9001935958862304, val loss: 3.9693724393844603, ETA in seconds: 982287.861\n",
      "epoch: 145700, train loss: 3.888472151756287, val loss: 3.9648627758026125, ETA in seconds: 982828.165\n",
      "epoch: 145800, train loss: 3.8960843324661254, val loss: 3.9790270566940307, ETA in seconds: 983364.790\n",
      "epoch: 145900, train loss: 3.883164477348328, val loss: 3.9763697147369386, ETA in seconds: 983899.066\n",
      "epoch: 146000, train loss: 3.893837738037109, val loss: 3.9596638679504395, ETA in seconds: 984419.912\n",
      "epoch: 146100, train loss: 3.8869157791137696, val loss: 3.96957471370697, ETA in seconds: 984934.929\n",
      "epoch: 146200, train loss: 3.887863802909851, val loss: 3.961963248252869, ETA in seconds: 985457.359\n",
      "epoch: 146300, train loss: 3.8980764389038085, val loss: 3.964820957183838, ETA in seconds: 985991.616\n",
      "epoch: 146400, train loss: 3.896300220489502, val loss: 3.9682396411895753, ETA in seconds: 986488.848\n",
      "epoch: 146500, train loss: 3.902319049835205, val loss: 3.981347441673279, ETA in seconds: 986990.201\n",
      "epoch: 146600, train loss: 3.902907657623291, val loss: 3.973620390892029, ETA in seconds: 987521.830\n",
      "epoch: 146700, train loss: 3.8860559701919555, val loss: 3.9736836433410643, ETA in seconds: 988025.161\n",
      "epoch: 146800, train loss: 3.8881926774978637, val loss: 3.978009343147278, ETA in seconds: 988546.948\n",
      "epoch: 146900, train loss: 3.8956458568573, val loss: 3.9705337047576905, ETA in seconds: 989057.669\n",
      "epoch: 147000, train loss: 3.8973772287368775, val loss: 3.976524329185486, ETA in seconds: 989612.460\n",
      "epoch: 147100, train loss: 3.895782780647278, val loss: 3.9753248929977416, ETA in seconds: 990173.910\n",
      "epoch: 147200, train loss: 3.9021241426467896, val loss: 3.9706740379333496, ETA in seconds: 990714.911\n",
      "epoch: 147300, train loss: 3.884249472618103, val loss: 3.9682883977890016, ETA in seconds: 991240.488\n",
      "epoch: 147400, train loss: 3.8962416648864746, val loss: 3.9715582370758056, ETA in seconds: 991753.912\n",
      "epoch: 147500, train loss: 3.894000744819641, val loss: 3.970353674888611, ETA in seconds: 992265.838\n",
      "epoch: 147600, train loss: 3.8890233755111696, val loss: 3.964667725563049, ETA in seconds: 992771.508\n",
      "epoch: 147700, train loss: 3.89709095954895, val loss: 3.9557599782943726, ETA in seconds: 993281.147\n",
      "epoch: 147800, train loss: 3.881973910331726, val loss: 3.969425916671753, ETA in seconds: 993843.236\n",
      "epoch: 147900, train loss: 3.893970322608948, val loss: 3.9694969177246096, ETA in seconds: 994354.887\n",
      "epoch: 148000, train loss: 3.9002562046051024, val loss: 3.9692503690719603, ETA in seconds: 994925.944\n",
      "epoch: 148100, train loss: 3.888296365737915, val loss: 3.9718862771987915, ETA in seconds: 995463.543\n",
      "epoch: 148200, train loss: 3.893026661872864, val loss: 3.9703792572021483, ETA in seconds: 995965.336\n",
      "epoch: 148300, train loss: 3.8920173168182375, val loss: 3.9669686555862427, ETA in seconds: 996474.912\n",
      "epoch: 148400, train loss: 3.899426794052124, val loss: 3.9755468368530273, ETA in seconds: 996989.055\n",
      "epoch: 148500, train loss: 3.9033327102661133, val loss: 3.9739159107208253, ETA in seconds: 997508.910\n",
      "epoch: 148600, train loss: 3.8981468200683596, val loss: 3.9725528955459595, ETA in seconds: 998056.241\n",
      "epoch: 148700, train loss: 3.889473295211792, val loss: 3.9792888164520264, ETA in seconds: 998639.386\n",
      "epoch: 148800, train loss: 3.8889652967453, val loss: 3.9784225463867187, ETA in seconds: 999160.300\n",
      "epoch: 148900, train loss: 3.8941715955734253, val loss: 3.9688204765319823, ETA in seconds: 999736.816\n",
      "epoch: 149000, train loss: 3.888308811187744, val loss: 3.965901827812195, ETA in seconds: 1000397.884\n",
      "epoch: 149100, train loss: 3.8937488555908204, val loss: 3.970393753051758, ETA in seconds: 1001077.245\n",
      "epoch: 149200, train loss: 3.9020957469940187, val loss: 3.9764883518218994, ETA in seconds: 1001634.972\n",
      "epoch: 149300, train loss: 3.892655611038208, val loss: 3.971241497993469, ETA in seconds: 1002143.586\n",
      "epoch: 149400, train loss: 3.8902881860733034, val loss: 3.974908947944641, ETA in seconds: 1002659.327\n",
      "epoch: 149500, train loss: 3.887983727455139, val loss: 3.9631484746932983, ETA in seconds: 1003221.931\n",
      "epoch: 149600, train loss: 3.891848349571228, val loss: 3.965039300918579, ETA in seconds: 1003731.874\n",
      "epoch: 149700, train loss: 3.89997661113739, val loss: 3.9806713819503785, ETA in seconds: 1004287.476\n",
      "epoch: 149800, train loss: 3.8940640687942505, val loss: 3.9690693855285644, ETA in seconds: 1004842.837\n",
      "epoch: 149900, train loss: 3.880397891998291, val loss: 3.9700231313705445, ETA in seconds: 1005368.798\n",
      "epoch: 150000, train loss: 3.8952045917510985, val loss: 3.9649781703948976, ETA in seconds: 1005895.840\n",
      "epoch: 150100, train loss: 3.8933184146881104, val loss: 3.9701929092407227, ETA in seconds: 1006420.469\n",
      "epoch: 150200, train loss: 3.902694058418274, val loss: 3.9745043992996214, ETA in seconds: 1006930.826\n",
      "epoch: 150300, train loss: 3.8972126960754396, val loss: 3.9697343826293947, ETA in seconds: 1007441.182\n",
      "epoch: 150400, train loss: 3.900334286689758, val loss: 3.975137996673584, ETA in seconds: 1007956.322\n",
      "epoch: 150500, train loss: 3.9010369539260865, val loss: 3.9780980348587036, ETA in seconds: 1008468.920\n",
      "epoch: 150600, train loss: 3.8950194120407104, val loss: 3.9612746715545653, ETA in seconds: 1008980.103\n",
      "epoch: 150700, train loss: 3.8962257623672487, val loss: 3.9786015510559083, ETA in seconds: 1009494.885\n",
      "epoch: 150800, train loss: 3.8866440057754517, val loss: 3.980635380744934, ETA in seconds: 1010003.867\n",
      "epoch: 150900, train loss: 3.8981242179870605, val loss: 3.971147155761719, ETA in seconds: 1010516.532\n",
      "epoch: 151000, train loss: 3.8962854385375976, val loss: 3.9685169219970704, ETA in seconds: 1011041.872\n",
      "epoch: 151100, train loss: 3.895817446708679, val loss: 3.9766225814819336, ETA in seconds: 1011547.300\n",
      "epoch: 151200, train loss: 3.8838831901550295, val loss: 3.9714826583862304, ETA in seconds: 1012064.572\n",
      "epoch: 151300, train loss: 3.8907860994338987, val loss: 3.9647011041641234, ETA in seconds: 1012577.144\n",
      "epoch: 151400, train loss: 3.8893359661102296, val loss: 3.9692240715026856, ETA in seconds: 1013101.751\n",
      "epoch: 151500, train loss: 3.890530729293823, val loss: 3.978866529464722, ETA in seconds: 1013631.335\n",
      "epoch: 151600, train loss: 3.891341543197632, val loss: 3.969432735443115, ETA in seconds: 1014163.263\n",
      "epoch: 151700, train loss: 3.8981546640396116, val loss: 3.968361163139343, ETA in seconds: 1014685.650\n",
      "epoch: 151800, train loss: 3.896717834472656, val loss: 3.968385601043701, ETA in seconds: 1015202.281\n",
      "epoch: 151900, train loss: 3.9068804740905763, val loss: 3.970614290237427, ETA in seconds: 1015764.173\n",
      "epoch: 152000, train loss: 3.8933043479919434, val loss: 3.9782623767852785, ETA in seconds: 1016399.775\n",
      "epoch: 152100, train loss: 3.8985286235809324, val loss: 3.9634363889694213, ETA in seconds: 1017032.489\n",
      "epoch: 152200, train loss: 3.8878474950790407, val loss: 3.9651303052902223, ETA in seconds: 1017668.566\n",
      "epoch: 152300, train loss: 3.882150435447693, val loss: 3.9541651010513306, ETA in seconds: 1018302.588\n",
      "epoch: 152400, train loss: 3.8875917196273804, val loss: 3.96306893825531, ETA in seconds: 1018934.757\n",
      "epoch: 152500, train loss: 3.892541527748108, val loss: 3.9763766527175903, ETA in seconds: 1019576.171\n",
      "epoch: 152600, train loss: 3.897809386253357, val loss: 3.9703900814056396, ETA in seconds: 1020207.246\n",
      "epoch: 152700, train loss: 3.8938083410263062, val loss: 3.960640621185303, ETA in seconds: 1020835.929\n",
      "epoch: 152800, train loss: 3.8988861560821535, val loss: 3.9616015434265135, ETA in seconds: 1021471.952\n",
      "epoch: 152900, train loss: 3.8968808889389037, val loss: 3.9750612497329714, ETA in seconds: 1022086.320\n",
      "epoch: 153000, train loss: 3.882671856880188, val loss: 3.964091491699219, ETA in seconds: 1022576.198\n",
      "epoch: 153100, train loss: 3.885431098937988, val loss: 3.97002592086792, ETA in seconds: 1023053.812\n",
      "epoch: 153200, train loss: 3.887253499031067, val loss: 3.9741953134536745, ETA in seconds: 1023545.299\n",
      "epoch: 153300, train loss: 3.8968392610549927, val loss: 3.9689515829086304, ETA in seconds: 1024033.165\n",
      "epoch: 153400, train loss: 3.8949017763137816, val loss: 3.9735177755355835, ETA in seconds: 1024538.684\n",
      "epoch: 153500, train loss: 3.898812699317932, val loss: 3.9697879552841187, ETA in seconds: 1025037.421\n",
      "epoch: 153600, train loss: 3.8994301080703737, val loss: 3.968882417678833, ETA in seconds: 1025535.474\n",
      "epoch: 153700, train loss: 3.897040033340454, val loss: 3.965843677520752, ETA in seconds: 1026050.466\n",
      "epoch: 153800, train loss: 3.8979299783706667, val loss: 3.9736578702926635, ETA in seconds: 1026558.615\n",
      "epoch: 153900, train loss: 3.890056872367859, val loss: 3.960196018218994, ETA in seconds: 1027059.704\n",
      "epoch: 154000, train loss: 3.889704465866089, val loss: 3.9706014156341554, ETA in seconds: 1027576.630\n",
      "epoch: 154100, train loss: 3.890606164932251, val loss: 3.97622971534729, ETA in seconds: 1028090.740\n",
      "epoch: 154200, train loss: 3.888773560523987, val loss: 3.9660805463790894, ETA in seconds: 1028595.032\n",
      "epoch: 154300, train loss: 3.894012117385864, val loss: 3.9648936986923218, ETA in seconds: 1029100.102\n",
      "epoch: 154400, train loss: 3.894761157035828, val loss: 3.9725605249404907, ETA in seconds: 1029591.393\n",
      "epoch: 154500, train loss: 3.8967809438705445, val loss: 3.9600380182266237, ETA in seconds: 1030077.044\n",
      "epoch: 154600, train loss: 3.897414469718933, val loss: 3.9843585729599, ETA in seconds: 1030592.102\n",
      "epoch: 154700, train loss: 3.899565052986145, val loss: 3.970812129974365, ETA in seconds: 1031087.214\n",
      "epoch: 154800, train loss: 3.8956277132034303, val loss: 3.974944734573364, ETA in seconds: 1031574.187\n",
      "epoch: 154900, train loss: 3.8896019220352174, val loss: 3.9767950057983397, ETA in seconds: 1032062.798\n",
      "epoch: 155000, train loss: 3.8935436010360718, val loss: 3.9638338565826414, ETA in seconds: 1032555.582\n",
      "epoch: 155100, train loss: 3.890420746803284, val loss: 3.974857807159424, ETA in seconds: 1033041.004\n",
      "epoch: 155200, train loss: 3.8965269327163696, val loss: 3.979219841957092, ETA in seconds: 1033534.485\n",
      "epoch: 155300, train loss: 3.891753339767456, val loss: 3.9603370428085327, ETA in seconds: 1034027.298\n",
      "epoch: 155400, train loss: 3.898888683319092, val loss: 3.96273877620697, ETA in seconds: 1034530.409\n",
      "epoch: 155500, train loss: 3.893074560165405, val loss: 3.9758561134338377, ETA in seconds: 1035093.018\n",
      "epoch: 155600, train loss: 3.8910938262939454, val loss: 3.976426863670349, ETA in seconds: 1035722.380\n",
      "epoch: 155700, train loss: 3.892150545120239, val loss: 3.9572723150253295, ETA in seconds: 1036347.019\n",
      "epoch: 155800, train loss: 3.899785375595093, val loss: 3.97076678276062, ETA in seconds: 1036978.090\n",
      "epoch: 155900, train loss: 3.8850117444992067, val loss: 3.9686729669570924, ETA in seconds: 1037517.789\n",
      "epoch: 156000, train loss: 3.8916242599487303, val loss: 3.9639726638793946, ETA in seconds: 1038103.864\n",
      "epoch: 156100, train loss: 3.89481406211853, val loss: 3.9704401969909666, ETA in seconds: 1038594.494\n",
      "epoch: 156200, train loss: 3.9035690307617186, val loss: 3.9754900217056273, ETA in seconds: 1039081.275\n",
      "epoch: 156300, train loss: 3.8903689861297606, val loss: 3.9740388870239256, ETA in seconds: 1039580.365\n",
      "epoch: 156400, train loss: 3.8921568155288697, val loss: 3.964951252937317, ETA in seconds: 1040093.119\n",
      "epoch: 156500, train loss: 3.8977394342422484, val loss: 3.9741862535476686, ETA in seconds: 1040587.526\n",
      "epoch: 156600, train loss: 3.8934491872787476, val loss: 3.9594849348068237, ETA in seconds: 1041073.194\n",
      "epoch: 156700, train loss: 3.896782159805298, val loss: 3.9586156368255616, ETA in seconds: 1041576.325\n",
      "epoch: 156800, train loss: 3.8919466972351073, val loss: 3.9689149141311644, ETA in seconds: 1042065.120\n",
      "epoch: 156900, train loss: 3.8839981317520142, val loss: 3.9862667322158813, ETA in seconds: 1042547.241\n",
      "epoch: 157000, train loss: 3.881921339035034, val loss: 3.9728623390197755, ETA in seconds: 1043151.006\n",
      "epoch: 157100, train loss: 3.8924401998519897, val loss: 3.9782489776611327, ETA in seconds: 1043809.182\n",
      "epoch: 157200, train loss: 3.89409019947052, val loss: 3.9616582870483397, ETA in seconds: 1044444.087\n",
      "epoch: 157300, train loss: 3.8857104778289795, val loss: 3.963795208930969, ETA in seconds: 1045077.805\n",
      "epoch: 157400, train loss: 3.889030623435974, val loss: 3.959231662750244, ETA in seconds: 1045626.282\n",
      "epoch: 157500, train loss: 3.8968997955322267, val loss: 3.9775811672210692, ETA in seconds: 1046126.301\n",
      "epoch: 157600, train loss: 3.900991153717041, val loss: 3.9725499868392946, ETA in seconds: 1046628.666\n",
      "epoch: 157700, train loss: 3.8949477672576904, val loss: 3.9722113370895387, ETA in seconds: 1047129.970\n",
      "epoch: 157800, train loss: 3.8941850423812867, val loss: 3.9730140924453736, ETA in seconds: 1047632.606\n",
      "epoch: 157900, train loss: 3.8942679882049562, val loss: 3.9577481746673584, ETA in seconds: 1048147.591\n",
      "epoch: 158000, train loss: 3.892993187904358, val loss: 3.9607093811035154, ETA in seconds: 1048660.142\n",
      "epoch: 158100, train loss: 3.884116697311401, val loss: 3.9652007102966307, ETA in seconds: 1049176.761\n",
      "epoch: 158200, train loss: 3.8911641120910643, val loss: 3.9600867509841917, ETA in seconds: 1049677.803\n",
      "epoch: 158300, train loss: 3.901555609703064, val loss: 3.979022765159607, ETA in seconds: 1050174.316\n",
      "epoch: 158400, train loss: 3.887215256690979, val loss: 3.956662106513977, ETA in seconds: 1050661.271\n",
      "epoch: 158500, train loss: 3.892335557937622, val loss: 3.9562224388122558, ETA in seconds: 1051154.073\n",
      "epoch: 158600, train loss: 3.888867712020874, val loss: 3.962454843521118, ETA in seconds: 1051651.545\n",
      "epoch: 158700, train loss: 3.894357919692993, val loss: 3.958792495727539, ETA in seconds: 1052167.244\n",
      "epoch: 158800, train loss: 3.891387462615967, val loss: 3.973369288444519, ETA in seconds: 1052758.989\n",
      "epoch: 158900, train loss: 3.8978084087371827, val loss: 3.970023441314697, ETA in seconds: 1053239.582\n",
      "epoch: 159000, train loss: 3.8913586854934694, val loss: 3.9637112617492676, ETA in seconds: 1053708.413\n",
      "epoch: 159100, train loss: 3.88877477645874, val loss: 3.972493052482605, ETA in seconds: 1054199.488\n",
      "epoch: 159200, train loss: 3.8936062812805177, val loss: 3.96684193611145, ETA in seconds: 1054682.601\n",
      "epoch: 159300, train loss: 3.8943165063858034, val loss: 3.9711055755615234, ETA in seconds: 1055157.417\n",
      "epoch: 159400, train loss: 3.8895132780075072, val loss: 3.9730042457580566, ETA in seconds: 1055650.884\n",
      "epoch: 159500, train loss: 3.88984272480011, val loss: 3.9865146398544313, ETA in seconds: 1056237.464\n",
      "epoch: 159600, train loss: 3.8949942111968996, val loss: 3.9657928705215455, ETA in seconds: 1056913.957\n",
      "epoch: 159700, train loss: 3.891276240348816, val loss: 3.9706975698471068, ETA in seconds: 1057574.907\n",
      "epoch: 159800, train loss: 3.892424702644348, val loss: 3.9741453170776366, ETA in seconds: 1058139.207\n",
      "epoch: 159900, train loss: 3.8974096059799193, val loss: 3.965473175048828, ETA in seconds: 1058599.512\n",
      "epoch: 160000, train loss: 3.896281123161316, val loss: 3.975973200798035, ETA in seconds: 1059062.108\n",
      "epoch: 160100, train loss: 3.8945164680480957, val loss: 3.9611172437667848, ETA in seconds: 1059523.021\n",
      "epoch: 160200, train loss: 3.8960467100143434, val loss: 3.9720602512359617, ETA in seconds: 1060006.274\n",
      "epoch: 160300, train loss: 3.886061763763428, val loss: 3.9641652345657348, ETA in seconds: 1060494.998\n",
      "epoch: 160400, train loss: 3.9032517433166505, val loss: 3.965805673599243, ETA in seconds: 1061008.208\n",
      "epoch: 160500, train loss: 3.890607237815857, val loss: 3.9771135091781615, ETA in seconds: 1061510.344\n",
      "epoch: 160600, train loss: 3.8948899507522583, val loss: 3.9674938440322878, ETA in seconds: 1062004.625\n",
      "epoch: 160700, train loss: 3.899535346031189, val loss: 3.976828193664551, ETA in seconds: 1062494.760\n",
      "epoch: 160800, train loss: 3.8895461797714233, val loss: 3.989778685569763, ETA in seconds: 1063008.576\n",
      "epoch: 160900, train loss: 3.895478844642639, val loss: 3.9594183683395388, ETA in seconds: 1063521.401\n",
      "epoch: 161000, train loss: 3.892231297492981, val loss: 3.974570870399475, ETA in seconds: 1064021.702\n",
      "epoch: 161100, train loss: 3.8964599847793577, val loss: 3.9683608770370484, ETA in seconds: 1064529.091\n",
      "epoch: 161200, train loss: 3.888070845603943, val loss: 3.9688841342926025, ETA in seconds: 1065036.715\n",
      "epoch: 161300, train loss: 3.8984012842178344, val loss: 3.97763876914978, ETA in seconds: 1065542.824\n",
      "epoch: 161400, train loss: 3.9007482290267945, val loss: 3.9667958974838258, ETA in seconds: 1066045.472\n",
      "epoch: 161500, train loss: 3.8975265502929686, val loss: 3.969627022743225, ETA in seconds: 1066525.221\n",
      "epoch: 161600, train loss: 3.8921560049057007, val loss: 3.9788583278656007, ETA in seconds: 1067026.711\n",
      "epoch: 161700, train loss: 3.8903001308441163, val loss: 3.974769926071167, ETA in seconds: 1067520.068\n",
      "epoch: 161800, train loss: 3.890752339363098, val loss: 3.967537593841553, ETA in seconds: 1068036.747\n",
      "epoch: 161900, train loss: 3.8950706481933595, val loss: 3.974691390991211, ETA in seconds: 1068530.163\n",
      "epoch: 162000, train loss: 3.8973528861999513, val loss: 3.9766172647476195, ETA in seconds: 1069010.750\n",
      "epoch: 162100, train loss: 3.889852738380432, val loss: 3.981082820892334, ETA in seconds: 1069488.283\n",
      "epoch: 162200, train loss: 3.8910696506500244, val loss: 3.9733179807662964, ETA in seconds: 1069967.412\n",
      "epoch: 162300, train loss: 3.898029065132141, val loss: 3.9822518110275267, ETA in seconds: 1070554.128\n",
      "epoch: 162400, train loss: 3.885320806503296, val loss: 3.9729114770889282, ETA in seconds: 1071061.244\n",
      "epoch: 162500, train loss: 3.894885778427124, val loss: 3.977281093597412, ETA in seconds: 1071553.308\n",
      "epoch: 162600, train loss: 3.903167152404785, val loss: 3.9826772928237917, ETA in seconds: 1072060.266\n",
      "epoch: 162700, train loss: 3.8859678506851196, val loss: 3.9763805866241455, ETA in seconds: 1072572.070\n",
      "epoch: 162800, train loss: 3.8965062856674195, val loss: 3.9786664247512817, ETA in seconds: 1073078.038\n",
      "epoch: 162900, train loss: 3.8952826499938964, val loss: 3.9732862234115602, ETA in seconds: 1073579.800\n",
      "epoch: 163000, train loss: 3.8801847457885743, val loss: 3.975008153915405, ETA in seconds: 1074178.710\n",
      "epoch: 163100, train loss: 3.895060992240906, val loss: 3.975744700431824, ETA in seconds: 1074718.775\n",
      "epoch: 163200, train loss: 3.896745800971985, val loss: 3.957239580154419, ETA in seconds: 1075245.662\n",
      "epoch: 163300, train loss: 3.8942349433898924, val loss: 3.9710213422775267, ETA in seconds: 1075740.456\n",
      "epoch: 163400, train loss: 3.899439001083374, val loss: 3.9689268350601195, ETA in seconds: 1076336.043\n",
      "epoch: 163500, train loss: 3.896487092971802, val loss: 3.9719494581222534, ETA in seconds: 1076984.955\n",
      "epoch: 163600, train loss: 3.8978748321533203, val loss: 3.9617602825164795, ETA in seconds: 1077677.073\n",
      "epoch: 163700, train loss: 3.888081741333008, val loss: 3.9784645318984984, ETA in seconds: 1078366.849\n",
      "epoch: 163800, train loss: 3.894686794281006, val loss: 3.9732747077941895, ETA in seconds: 1079053.796\n",
      "epoch: 163900, train loss: 3.890804386138916, val loss: 3.9659375190734862, ETA in seconds: 1079741.080\n",
      "epoch: 164000, train loss: 3.89845552444458, val loss: 3.968384122848511, ETA in seconds: 1080394.715\n",
      "epoch: 164100, train loss: 3.9032622575759888, val loss: 3.971384382247925, ETA in seconds: 1081008.517\n",
      "epoch: 164200, train loss: 3.891419529914856, val loss: 3.9781425476074217, ETA in seconds: 1081641.628\n",
      "epoch: 164300, train loss: 3.8922605991363524, val loss: 3.9642271041870116, ETA in seconds: 1082263.367\n",
      "epoch: 164400, train loss: 3.883297872543335, val loss: 3.9788267612457275, ETA in seconds: 1082875.239\n",
      "epoch: 164500, train loss: 3.8944507360458376, val loss: 3.956325650215149, ETA in seconds: 1083490.466\n",
      "epoch: 164600, train loss: 3.8963806867599486, val loss: 3.968855142593384, ETA in seconds: 1084108.443\n",
      "epoch: 164700, train loss: 3.894885444641113, val loss: 3.965073013305664, ETA in seconds: 1084730.748\n",
      "epoch: 164800, train loss: 3.9072969913482667, val loss: 3.9520715475082397, ETA in seconds: 1085361.841\n",
      "epoch: 164900, train loss: 3.902561068534851, val loss: 3.9651169538497926, ETA in seconds: 1085985.620\n",
      "epoch: 165000, train loss: 3.9031822204589846, val loss: 3.9692581415176393, ETA in seconds: 1086670.884\n",
      "epoch: 165100, train loss: 3.897961139678955, val loss: 3.964904618263245, ETA in seconds: 1087360.515\n",
      "epoch: 165200, train loss: 3.887810969352722, val loss: 3.9692671060562135, ETA in seconds: 1088024.165\n",
      "epoch: 165300, train loss: 3.8908194303512573, val loss: 3.975969672203064, ETA in seconds: 1088477.458\n",
      "epoch: 165400, train loss: 3.8917224407196045, val loss: 3.9625539779663086, ETA in seconds: 1088961.452\n",
      "epoch: 165500, train loss: 3.8980458974838257, val loss: 3.968728518486023, ETA in seconds: 1089481.720\n",
      "epoch: 165600, train loss: 3.896508073806763, val loss: 3.9741556644439697, ETA in seconds: 1089960.464\n",
      "epoch: 165700, train loss: 3.900457978248596, val loss: 3.9702285051345827, ETA in seconds: 1090649.093\n",
      "epoch: 165800, train loss: 3.900421953201294, val loss: 3.9681353092193605, ETA in seconds: 1091159.276\n",
      "epoch: 165900, train loss: 3.8881783723831176, val loss: 3.9695645332336427, ETA in seconds: 1091635.128\n",
      "epoch: 166000, train loss: 3.8923484802246096, val loss: 3.966174101829529, ETA in seconds: 1092106.417\n",
      "epoch: 166100, train loss: 3.8936354875564576, val loss: 3.970816946029663, ETA in seconds: 1092599.469\n",
      "epoch: 166200, train loss: 3.8991459131240847, val loss: 3.9706079721450807, ETA in seconds: 1093094.600\n",
      "epoch: 166300, train loss: 3.893065643310547, val loss: 3.969784140586853, ETA in seconds: 1093618.086\n",
      "epoch: 166400, train loss: 3.9025773286819456, val loss: 3.9690826654434206, ETA in seconds: 1094161.418\n",
      "epoch: 166500, train loss: 3.8942873001098635, val loss: 3.9718950986862183, ETA in seconds: 1094646.308\n",
      "epoch: 166600, train loss: 3.8935510396957396, val loss: 3.9736088037490847, ETA in seconds: 1095130.484\n",
      "epoch: 166700, train loss: 3.893330788612366, val loss: 3.975630497932434, ETA in seconds: 1095615.277\n",
      "epoch: 166800, train loss: 3.8934558153152468, val loss: 3.9787755250930785, ETA in seconds: 1096137.772\n",
      "epoch: 166900, train loss: 3.882242465019226, val loss: 3.9756101608276366, ETA in seconds: 1096669.526\n",
      "epoch: 167000, train loss: 3.8991695165634157, val loss: 3.960496735572815, ETA in seconds: 1097177.161\n",
      "epoch: 167100, train loss: 3.886681413650513, val loss: 3.9659464836120604, ETA in seconds: 1097671.272\n",
      "epoch: 167200, train loss: 3.8945011377334593, val loss: 3.9619915962219237, ETA in seconds: 1098166.740\n",
      "epoch: 167300, train loss: 3.900108218193054, val loss: 3.980892848968506, ETA in seconds: 1098671.978\n",
      "epoch: 167400, train loss: 3.8975486516952516, val loss: 3.9698026180267334, ETA in seconds: 1099148.708\n",
      "epoch: 167500, train loss: 3.8941210746765136, val loss: 3.9716853141784667, ETA in seconds: 1099731.995\n",
      "epoch: 167600, train loss: 3.8820767402648926, val loss: 3.978194165229797, ETA in seconds: 1100247.445\n",
      "epoch: 167700, train loss: 3.903165030479431, val loss: 3.989933443069458, ETA in seconds: 1100756.449\n",
      "epoch: 167800, train loss: 3.8893139362335205, val loss: 3.9634761810302734, ETA in seconds: 1101293.839\n",
      "epoch: 167900, train loss: 3.894699716567993, val loss: 3.96672203540802, ETA in seconds: 1101791.776\n",
      "epoch: 168000, train loss: 3.8841666698455812, val loss: 3.9640666246414185, ETA in seconds: 1102321.287\n",
      "epoch: 168100, train loss: 3.8995408535003664, val loss: 3.978746271133423, ETA in seconds: 1102849.363\n",
      "epoch: 168200, train loss: 3.8950012922286987, val loss: 3.9751117706298826, ETA in seconds: 1103370.689\n",
      "epoch: 168300, train loss: 3.8979103565216064, val loss: 3.977653741836548, ETA in seconds: 1103846.218\n",
      "epoch: 168400, train loss: 3.8895771503448486, val loss: 3.9774406433105467, ETA in seconds: 1104325.473\n",
      "epoch: 168500, train loss: 3.8972795248031615, val loss: 3.9711984157562257, ETA in seconds: 1104816.727\n",
      "epoch: 168600, train loss: 3.891218423843384, val loss: 3.9666330814361572, ETA in seconds: 1105351.254\n",
      "epoch: 168700, train loss: 3.900669980049133, val loss: 3.9742081642150877, ETA in seconds: 1105916.763\n",
      "epoch: 168800, train loss: 3.8848649740219114, val loss: 3.963883090019226, ETA in seconds: 1106530.712\n",
      "epoch: 168900, train loss: 3.890670728683472, val loss: 3.9687367916107177, ETA in seconds: 1107148.741\n",
      "epoch: 169000, train loss: 3.8881331205368044, val loss: 3.9665197849273683, ETA in seconds: 1107764.358\n",
      "epoch: 169100, train loss: 3.8763135433197022, val loss: 3.9735884428024293, ETA in seconds: 1108379.085\n",
      "epoch: 169200, train loss: 3.8900250673294066, val loss: 3.972393012046814, ETA in seconds: 1108991.880\n",
      "epoch: 169300, train loss: 3.8963879585266112, val loss: 3.978592371940613, ETA in seconds: 1109607.955\n",
      "epoch: 169400, train loss: 3.8827501058578493, val loss: 3.9697479963302613, ETA in seconds: 1110221.900\n",
      "epoch: 169500, train loss: 3.886703109741211, val loss: 3.966157817840576, ETA in seconds: 1110779.587\n",
      "epoch: 169600, train loss: 3.8923760890960692, val loss: 3.968699049949646, ETA in seconds: 1111374.621\n",
      "epoch: 169700, train loss: 3.896450996398926, val loss: 3.9658233404159544, ETA in seconds: 1112003.633\n",
      "epoch: 169800, train loss: 3.8993093729019166, val loss: 3.971487545967102, ETA in seconds: 1112552.904\n",
      "epoch: 169900, train loss: 3.8887411832809446, val loss: 3.9769471168518065, ETA in seconds: 1113075.872\n",
      "epoch: 170000, train loss: 3.897681379318237, val loss: 3.96828236579895, ETA in seconds: 1113572.581\n",
      "epoch: 170100, train loss: 3.8910950899124144, val loss: 3.9697919607162477, ETA in seconds: 1114078.259\n",
      "epoch: 170200, train loss: 3.8972616672515867, val loss: 3.9766647577285767, ETA in seconds: 1114580.568\n",
      "epoch: 170300, train loss: 3.8870472431182863, val loss: 3.968074369430542, ETA in seconds: 1115075.422\n",
      "epoch: 170400, train loss: 3.9017671585083007, val loss: 3.9594571590423584, ETA in seconds: 1115572.223\n",
      "epoch: 170500, train loss: 3.8925177574157717, val loss: 3.9675018787384033, ETA in seconds: 1116104.282\n",
      "epoch: 170600, train loss: 3.8896787405014037, val loss: 3.9649017572402956, ETA in seconds: 1116665.405\n",
      "epoch: 170700, train loss: 3.895784378051758, val loss: 3.974212956428528, ETA in seconds: 1117192.764\n",
      "epoch: 170800, train loss: 3.894192099571228, val loss: 3.9692244052886965, ETA in seconds: 1117813.228\n",
      "epoch: 170900, train loss: 3.8986171007156374, val loss: 3.9643544912338258, ETA in seconds: 1118340.007\n",
      "epoch: 171000, train loss: 3.884650707244873, val loss: 3.9818217277526857, ETA in seconds: 1118860.794\n",
      "epoch: 171100, train loss: 3.8945829391479494, val loss: 3.968406009674072, ETA in seconds: 1119395.784\n",
      "epoch: 171200, train loss: 3.9006247997283934, val loss: 3.97335205078125, ETA in seconds: 1119915.933\n",
      "epoch: 171300, train loss: 3.8855673551559446, val loss: 3.977380657196045, ETA in seconds: 1120415.765\n",
      "epoch: 171400, train loss: 3.895870304107666, val loss: 3.9610869884490967, ETA in seconds: 1120926.672\n",
      "epoch: 171500, train loss: 3.908073306083679, val loss: 3.97409451007843, ETA in seconds: 1121465.846\n",
      "epoch: 171600, train loss: 3.893281364440918, val loss: 3.9625738143920897, ETA in seconds: 1121962.644\n",
      "epoch: 171700, train loss: 3.8886506080627443, val loss: 3.9767756700515746, ETA in seconds: 1122529.274\n",
      "epoch: 171800, train loss: 3.9053948879241944, val loss: 3.9785770893096926, ETA in seconds: 1123095.370\n",
      "epoch: 171900, train loss: 3.889470863342285, val loss: 3.968908405303955, ETA in seconds: 1123600.900\n",
      "epoch: 172000, train loss: 3.9017151594161987, val loss: 3.97122700214386, ETA in seconds: 1124108.326\n",
      "epoch: 172100, train loss: 3.8947495937347414, val loss: 3.973818302154541, ETA in seconds: 1124609.705\n",
      "epoch: 172200, train loss: 3.8934252738952635, val loss: 3.962733793258667, ETA in seconds: 1125096.815\n",
      "epoch: 172300, train loss: 3.902469801902771, val loss: 3.9721296310424803, ETA in seconds: 1125607.835\n",
      "epoch: 172400, train loss: 3.8988482475280763, val loss: 3.9695658445358277, ETA in seconds: 1126126.979\n",
      "epoch: 172500, train loss: 3.8921233654022216, val loss: 3.959069776535034, ETA in seconds: 1126649.018\n",
      "epoch: 172600, train loss: 3.893289256095886, val loss: 3.974213433265686, ETA in seconds: 1127167.507\n",
      "epoch: 172700, train loss: 3.9043747186660767, val loss: 3.976492667198181, ETA in seconds: 1127672.775\n",
      "epoch: 172800, train loss: 3.892233657836914, val loss: 3.95439555644989, ETA in seconds: 1128173.624\n",
      "epoch: 172900, train loss: 3.8926397800445556, val loss: 3.973166799545288, ETA in seconds: 1128690.090\n",
      "epoch: 173000, train loss: 3.8904510259628298, val loss: 3.983086085319519, ETA in seconds: 1129234.604\n",
      "epoch: 173100, train loss: 3.8982236623764037, val loss: 3.978572630882263, ETA in seconds: 1129736.980\n",
      "epoch: 173200, train loss: 3.903153729438782, val loss: 3.9623653650283814, ETA in seconds: 1130220.126\n",
      "epoch: 173300, train loss: 3.8925596475601196, val loss: 3.96399667263031, ETA in seconds: 1130768.591\n",
      "epoch: 173400, train loss: 3.8906369686126707, val loss: 3.972384738922119, ETA in seconds: 1131325.321\n",
      "epoch: 173500, train loss: 3.8959369897842406, val loss: 3.9741184949874877, ETA in seconds: 1131900.234\n",
      "epoch: 173600, train loss: 3.8874531030654906, val loss: 3.9641517877578734, ETA in seconds: 1132496.306\n",
      "epoch: 173700, train loss: 3.8971436738967897, val loss: 3.9656936883926392, ETA in seconds: 1133088.459\n",
      "epoch: 173800, train loss: 3.9026448249816896, val loss: 3.9574193477630617, ETA in seconds: 1133561.298\n",
      "epoch: 173900, train loss: 3.903654623031616, val loss: 3.963099241256714, ETA in seconds: 1134044.148\n",
      "epoch: 174000, train loss: 3.891387367248535, val loss: 3.9544662952423097, ETA in seconds: 1134525.624\n",
      "epoch: 174100, train loss: 3.896415638923645, val loss: 3.9430543422698974, ETA in seconds: 1135058.136\n",
      "epoch: 174200, train loss: 3.890947437286377, val loss: 3.9673603057861326, ETA in seconds: 1135559.878\n",
      "epoch: 174300, train loss: 3.8949853658676146, val loss: 3.965849685668945, ETA in seconds: 1136032.219\n",
      "epoch: 174400, train loss: 3.8859899759292604, val loss: 3.972655940055847, ETA in seconds: 1136533.291\n",
      "epoch: 174500, train loss: 3.9050020217895507, val loss: 3.962861204147339, ETA in seconds: 1137144.564\n",
      "epoch: 174600, train loss: 3.893583655357361, val loss: 3.9743747234344484, ETA in seconds: 1137754.358\n",
      "epoch: 174700, train loss: 3.884890151023865, val loss: 3.9694109678268434, ETA in seconds: 1138285.324\n",
      "epoch: 174800, train loss: 3.9032588958740235, val loss: 3.9696505546569822, ETA in seconds: 1138800.878\n",
      "epoch: 174900, train loss: 3.8913525104522706, val loss: 3.9747961521148683, ETA in seconds: 1139286.432\n",
      "epoch: 175000, train loss: 3.889967846870422, val loss: 3.9701905965805055, ETA in seconds: 1139800.105\n",
      "epoch: 175100, train loss: 3.8911566972732543, val loss: 3.9615995407104494, ETA in seconds: 1140402.431\n",
      "epoch: 175200, train loss: 3.9016799926757812, val loss: 3.9721675157546996, ETA in seconds: 1141004.195\n",
      "epoch: 175300, train loss: 3.8895304679870604, val loss: 3.9682991981506346, ETA in seconds: 1141603.596\n",
      "epoch: 175400, train loss: 3.8866987466812133, val loss: 3.977664041519165, ETA in seconds: 1142209.965\n",
      "epoch: 175500, train loss: 3.89015588760376, val loss: 3.9730948925018312, ETA in seconds: 1142806.607\n",
      "epoch: 175600, train loss: 3.897913384437561, val loss: 3.983957862854004, ETA in seconds: 1143459.006\n",
      "epoch: 175700, train loss: 3.894287586212158, val loss: 3.9768479347229, ETA in seconds: 1143993.155\n",
      "epoch: 175800, train loss: 3.8981187105178834, val loss: 3.9713366508483885, ETA in seconds: 1144508.997\n",
      "epoch: 175900, train loss: 3.898862886428833, val loss: 3.9577303171157836, ETA in seconds: 1144996.147\n",
      "epoch: 176000, train loss: 3.891154742240906, val loss: 3.983240032196045, ETA in seconds: 1145479.133\n",
      "epoch: 176100, train loss: 3.8972031593322756, val loss: 3.9655041456222535, ETA in seconds: 1145989.387\n",
      "epoch: 176200, train loss: 3.893541955947876, val loss: 3.9674186229705812, ETA in seconds: 1146603.105\n",
      "epoch: 176300, train loss: 3.9039806604385374, val loss: 3.9636637210845946, ETA in seconds: 1147224.209\n",
      "epoch: 176400, train loss: 3.9047730922698975, val loss: 3.9769315242767336, ETA in seconds: 1147775.241\n",
      "epoch: 176500, train loss: 3.895029616355896, val loss: 3.971234345436096, ETA in seconds: 1148397.348\n",
      "epoch: 176600, train loss: 3.9005495071411134, val loss: 3.968245244026184, ETA in seconds: 1148962.438\n",
      "epoch: 176700, train loss: 3.9034758806228638, val loss: 3.9761259078979494, ETA in seconds: 1149450.503\n",
      "epoch: 176800, train loss: 3.892249846458435, val loss: 3.962175178527832, ETA in seconds: 1150112.143\n",
      "epoch: 176900, train loss: 3.8927316427230836, val loss: 3.9625015020370484, ETA in seconds: 1150757.992\n",
      "epoch: 177000, train loss: 3.8935970306396483, val loss: 3.9723260164260865, ETA in seconds: 1151422.107\n",
      "epoch: 177100, train loss: 3.8892300844192507, val loss: 3.97397403717041, ETA in seconds: 1152037.893\n",
      "epoch: 177200, train loss: 3.8945016622543336, val loss: 3.9568506479263306, ETA in seconds: 1152637.271\n",
      "epoch: 177300, train loss: 3.897783350944519, val loss: 3.976230525970459, ETA in seconds: 1153192.231\n",
      "epoch: 177400, train loss: 3.900786018371582, val loss: 3.9659049034118654, ETA in seconds: 1153671.020\n",
      "epoch: 177500, train loss: 3.895714354515076, val loss: 3.960233378410339, ETA in seconds: 1154141.173\n",
      "epoch: 177600, train loss: 3.891534662246704, val loss: 3.9661139011383058, ETA in seconds: 1154606.056\n",
      "epoch: 177700, train loss: 3.890263295173645, val loss: 3.9737836599349974, ETA in seconds: 1155067.177\n",
      "epoch: 177800, train loss: 3.9004321813583376, val loss: 3.9763232469558716, ETA in seconds: 1155544.977\n",
      "epoch: 177900, train loss: 3.9064969301223753, val loss: 3.9656898021697997, ETA in seconds: 1156019.185\n",
      "epoch: 178000, train loss: 3.8933308362960815, val loss: 3.9613736391067507, ETA in seconds: 1156499.779\n",
      "epoch: 178100, train loss: 3.896547222137451, val loss: 3.968510150909424, ETA in seconds: 1157159.754\n",
      "epoch: 178200, train loss: 3.890151333808899, val loss: 3.960837721824646, ETA in seconds: 1157764.753\n",
      "epoch: 178300, train loss: 3.895663833618164, val loss: 3.9504790782928465, ETA in seconds: 1158237.684\n",
      "epoch: 178400, train loss: 3.893253493309021, val loss: 3.9627859115600588, ETA in seconds: 1158705.091\n",
      "epoch: 178500, train loss: 3.903425431251526, val loss: 3.975036787986755, ETA in seconds: 1159159.405\n",
      "epoch: 178600, train loss: 3.8950756788253784, val loss: 3.973694396018982, ETA in seconds: 1159618.255\n",
      "epoch: 178700, train loss: 3.8921605587005614, val loss: 3.9647826194763183, ETA in seconds: 1160087.846\n",
      "epoch: 178800, train loss: 3.902198386192322, val loss: 3.975097155570984, ETA in seconds: 1160547.598\n",
      "epoch: 178900, train loss: 3.886738896369934, val loss: 3.977702760696411, ETA in seconds: 1161009.382\n",
      "epoch: 179000, train loss: 3.897256064414978, val loss: 3.9659730434417724, ETA in seconds: 1161472.543\n",
      "epoch: 179100, train loss: 3.893222618103027, val loss: 3.9683166265487673, ETA in seconds: 1161935.496\n",
      "epoch: 179200, train loss: 3.8919774770736693, val loss: 3.9610175132751464, ETA in seconds: 1162417.881\n",
      "epoch: 179300, train loss: 3.891666555404663, val loss: 3.9673918008804323, ETA in seconds: 1162887.873\n",
      "epoch: 179400, train loss: 3.895608162879944, val loss: 3.9675900459289553, ETA in seconds: 1163384.989\n",
      "epoch: 179500, train loss: 3.902392363548279, val loss: 3.9704368352890014, ETA in seconds: 1163868.601\n",
      "epoch: 179600, train loss: 3.900698661804199, val loss: 3.9616207122802733, ETA in seconds: 1164353.455\n",
      "epoch: 179700, train loss: 3.899717330932617, val loss: 3.9605228185653685, ETA in seconds: 1164826.014\n",
      "epoch: 179800, train loss: 3.8889840126037596, val loss: 3.9568448305130004, ETA in seconds: 1165311.114\n",
      "epoch: 179900, train loss: 3.8972007513046263, val loss: 3.974638032913208, ETA in seconds: 1165936.898\n",
      "epoch: 180000, train loss: 3.888340950012207, val loss: 3.983674144744873, ETA in seconds: 1166409.873\n",
      "epoch: 180100, train loss: 3.8932454109191896, val loss: 3.9708458185195923, ETA in seconds: 1166885.128\n",
      "epoch: 180200, train loss: 3.8952818393707274, val loss: 3.961509585380554, ETA in seconds: 1167346.161\n",
      "epoch: 180300, train loss: 3.878672480583191, val loss: 3.966484308242798, ETA in seconds: 1167796.846\n",
      "epoch: 180400, train loss: 3.8952716112136843, val loss: 3.9640905141830443, ETA in seconds: 1168324.255\n",
      "epoch: 180500, train loss: 3.8984038829803467, val loss: 3.9606900930404665, ETA in seconds: 1168917.371\n",
      "epoch: 180600, train loss: 3.8913719177246096, val loss: 3.9740997552871704, ETA in seconds: 1169497.337\n",
      "epoch: 180700, train loss: 3.8961535930633544, val loss: 3.968836450576782, ETA in seconds: 1170089.046\n",
      "epoch: 180800, train loss: 3.8906918287277223, val loss: 3.96824471950531, ETA in seconds: 1170549.088\n",
      "epoch: 180900, train loss: 3.9061028242111204, val loss: 3.9652817249298096, ETA in seconds: 1171028.923\n",
      "epoch: 181000, train loss: 3.8914186000823974, val loss: 3.9781725645065307, ETA in seconds: 1171492.339\n",
      "epoch: 181100, train loss: 3.88719699382782, val loss: 3.9805546522140505, ETA in seconds: 1171926.155\n",
      "epoch: 181200, train loss: 3.886836290359497, val loss: 3.971487259864807, ETA in seconds: 1172381.171\n",
      "epoch: 181300, train loss: 3.8975380182266237, val loss: 3.9621088027954103, ETA in seconds: 1172846.597\n",
      "epoch: 181400, train loss: 3.897097873687744, val loss: 3.965061831474304, ETA in seconds: 1173322.466\n",
      "epoch: 181500, train loss: 3.889897179603577, val loss: 3.972187566757202, ETA in seconds: 1173778.646\n",
      "epoch: 181600, train loss: 3.8997784852981567, val loss: 3.965777635574341, ETA in seconds: 1174238.558\n",
      "epoch: 181700, train loss: 3.8937919616699217, val loss: 3.9683805465698243, ETA in seconds: 1174771.405\n",
      "epoch: 181800, train loss: 3.889109420776367, val loss: 3.9657550573349, ETA in seconds: 1175256.773\n",
      "epoch: 181900, train loss: 3.891365575790405, val loss: 3.9858644247055053, ETA in seconds: 1175702.006\n",
      "epoch: 182000, train loss: 3.889513921737671, val loss: 3.9696536540985106, ETA in seconds: 1176159.356\n",
      "epoch: 182100, train loss: 3.8993412017822267, val loss: 3.9698272228240965, ETA in seconds: 1176640.266\n",
      "epoch: 182200, train loss: 3.896388363838196, val loss: 3.971723508834839, ETA in seconds: 1177123.359\n",
      "epoch: 182300, train loss: 3.90021550655365, val loss: 3.9657732009887696, ETA in seconds: 1177668.899\n",
      "epoch: 182400, train loss: 3.8942977666854857, val loss: 3.9719637393951417, ETA in seconds: 1178240.020\n",
      "epoch: 182500, train loss: 3.8905016660690306, val loss: 3.9740976095199585, ETA in seconds: 1178715.415\n",
      "epoch: 182600, train loss: 3.884405255317688, val loss: 3.97457959651947, ETA in seconds: 1179194.270\n",
      "epoch: 182700, train loss: 3.892385196685791, val loss: 3.9713456869125365, ETA in seconds: 1179673.524\n",
      "epoch: 182800, train loss: 3.8942614555358888, val loss: 3.9692044258117676, ETA in seconds: 1180157.157\n",
      "epoch: 182900, train loss: 3.896935319900513, val loss: 3.964788293838501, ETA in seconds: 1180619.010\n",
      "epoch: 183000, train loss: 3.897687554359436, val loss: 3.97362642288208, ETA in seconds: 1181087.591\n",
      "epoch: 183100, train loss: 3.88613646030426, val loss: 3.977142810821533, ETA in seconds: 1181561.530\n",
      "epoch: 183200, train loss: 3.894848656654358, val loss: 3.969355034828186, ETA in seconds: 1182022.684\n",
      "epoch: 183300, train loss: 3.889681339263916, val loss: 3.9762585401535033, ETA in seconds: 1182480.990\n",
      "epoch: 183400, train loss: 3.8992140769958494, val loss: 3.9637218952178954, ETA in seconds: 1182948.787\n",
      "epoch: 183500, train loss: 3.8942265272140504, val loss: 3.985962176322937, ETA in seconds: 1183420.164\n",
      "epoch: 183600, train loss: 3.8886491775512697, val loss: 3.9742843389511107, ETA in seconds: 1183875.078\n",
      "epoch: 183700, train loss: 3.8940643310546874, val loss: 3.973993182182312, ETA in seconds: 1184339.918\n",
      "epoch: 183800, train loss: 3.8982593536376955, val loss: 3.9792517900466917, ETA in seconds: 1184800.017\n",
      "epoch: 183900, train loss: 3.8857714176177978, val loss: 3.9781496047973635, ETA in seconds: 1185254.985\n",
      "epoch: 184000, train loss: 3.8959858894348143, val loss: 3.9791579246520996, ETA in seconds: 1185703.796\n",
      "epoch: 184100, train loss: 3.903774309158325, val loss: 3.9695079803466795, ETA in seconds: 1186162.566\n",
      "epoch: 184200, train loss: 3.9040101051330565, val loss: 3.966407823562622, ETA in seconds: 1186620.643\n",
      "epoch: 184300, train loss: 3.901930260658264, val loss: 3.9634847164154055, ETA in seconds: 1187085.683\n",
      "epoch: 184400, train loss: 3.89469792842865, val loss: 3.9785197496414186, ETA in seconds: 1187543.681\n",
      "epoch: 184500, train loss: 3.8917367696762084, val loss: 3.9772427558898924, ETA in seconds: 1188002.499\n",
      "epoch: 184600, train loss: 3.884066438674927, val loss: 3.9819056034088134, ETA in seconds: 1188460.333\n",
      "epoch: 184700, train loss: 3.8896140575408937, val loss: 3.9770276069641115, ETA in seconds: 1188917.127\n",
      "epoch: 184800, train loss: 3.8932504177093508, val loss: 3.9604260444641115, ETA in seconds: 1189373.514\n",
      "epoch: 184900, train loss: 3.898119020462036, val loss: 3.980674409866333, ETA in seconds: 1189839.546\n",
      "epoch: 185000, train loss: 3.891007900238037, val loss: 3.975423288345337, ETA in seconds: 1190276.855\n",
      "epoch: 185100, train loss: 3.8993885040283205, val loss: 3.962117600440979, ETA in seconds: 1190704.492\n",
      "epoch: 185200, train loss: 3.888720464706421, val loss: 3.9721322298049926, ETA in seconds: 1191135.369\n",
      "epoch: 185300, train loss: 3.897921419143677, val loss: 3.9681515216827394, ETA in seconds: 1191564.539\n",
      "epoch: 185400, train loss: 3.888464069366455, val loss: 3.9722450256347654, ETA in seconds: 1192002.466\n",
      "epoch: 185500, train loss: 3.893846869468689, val loss: 3.9831822395324705, ETA in seconds: 1192441.930\n",
      "epoch: 185600, train loss: 3.8901029348373415, val loss: 3.971526527404785, ETA in seconds: 1192876.165\n",
      "epoch: 185700, train loss: 3.8945303440093992, val loss: 3.9670098781585694, ETA in seconds: 1193324.247\n",
      "epoch: 185800, train loss: 3.8947163105010985, val loss: 3.9644042015075684, ETA in seconds: 1193769.966\n",
      "epoch: 185900, train loss: 3.8848798751831053, val loss: 3.9596981763839723, ETA in seconds: 1194215.856\n",
      "epoch: 186000, train loss: 3.894850754737854, val loss: 3.965507411956787, ETA in seconds: 1194790.892\n",
      "epoch: 186100, train loss: 3.8885703802108766, val loss: 3.9718459367752077, ETA in seconds: 1195376.421\n",
      "epoch: 186200, train loss: 3.9009146451950074, val loss: 3.971400022506714, ETA in seconds: 1195937.087\n",
      "epoch: 186300, train loss: 3.8951996088027956, val loss: 3.9563152313232424, ETA in seconds: 1196363.516\n",
      "epoch: 186400, train loss: 3.900992774963379, val loss: 3.9632282495498656, ETA in seconds: 1196811.731\n",
      "epoch: 186500, train loss: 3.8932003498077394, val loss: 3.976690888404846, ETA in seconds: 1197258.186\n",
      "epoch: 186600, train loss: 3.891853904724121, val loss: 3.96236047744751, ETA in seconds: 1197686.413\n",
      "epoch: 186700, train loss: 3.889281129837036, val loss: 3.9693944454193115, ETA in seconds: 1198129.814\n",
      "epoch: 186800, train loss: 3.89119074344635, val loss: 3.971920108795166, ETA in seconds: 1198578.393\n",
      "epoch: 186900, train loss: 3.881042003631592, val loss: 3.9626758098602295, ETA in seconds: 1199056.162\n",
      "epoch: 187000, train loss: 3.8903455257415773, val loss: 3.9687936544418334, ETA in seconds: 1199597.675\n",
      "epoch: 187100, train loss: 3.889267921447754, val loss: 3.9665171623229982, ETA in seconds: 1200044.294\n",
      "epoch: 187200, train loss: 3.898523044586182, val loss: 3.956356716156006, ETA in seconds: 1200488.956\n",
      "epoch: 187300, train loss: 3.895869255065918, val loss: 3.961674189567566, ETA in seconds: 1200933.967\n",
      "epoch: 187400, train loss: 3.898489546775818, val loss: 3.9621570110321045, ETA in seconds: 1201380.715\n",
      "epoch: 187500, train loss: 3.891624665260315, val loss: 3.965763807296753, ETA in seconds: 1201822.679\n",
      "epoch: 187600, train loss: 3.898309898376465, val loss: 3.953631114959717, ETA in seconds: 1202265.753\n",
      "epoch: 187700, train loss: 3.8889596462249756, val loss: 3.964855980873108, ETA in seconds: 1202702.093\n",
      "epoch: 187800, train loss: 3.8927793502807617, val loss: 3.9730824708938597, ETA in seconds: 1203143.725\n",
      "epoch: 187900, train loss: 3.884945201873779, val loss: 3.9736462354660036, ETA in seconds: 1203568.556\n",
      "epoch: 188000, train loss: 3.885479712486267, val loss: 3.9561755418777467, ETA in seconds: 1203994.149\n",
      "epoch: 188100, train loss: 3.8864784240722656, val loss: 3.960439109802246, ETA in seconds: 1204425.450\n",
      "epoch: 188200, train loss: 3.9009854078292845, val loss: 3.9638466119766234, ETA in seconds: 1204892.052\n",
      "epoch: 188300, train loss: 3.8891050815582275, val loss: 3.9744104623794554, ETA in seconds: 1205472.497\n",
      "epoch: 188400, train loss: 3.8903785943984985, val loss: 3.9676796197891235, ETA in seconds: 1205964.181\n",
      "epoch: 188500, train loss: 3.8921762704849243, val loss: 3.971345806121826, ETA in seconds: 1206436.322\n",
      "epoch: 188600, train loss: 3.8910628080368044, val loss: 3.982002377510071, ETA in seconds: 1206892.378\n",
      "epoch: 188700, train loss: 3.887675166130066, val loss: 3.9732065916061403, ETA in seconds: 1207342.500\n",
      "epoch: 188800, train loss: 3.891629385948181, val loss: 3.9625988245010375, ETA in seconds: 1207798.336\n",
      "epoch: 188900, train loss: 3.893707203865051, val loss: 3.9631343126296996, ETA in seconds: 1208257.414\n",
      "epoch: 189000, train loss: 3.8921276330947876, val loss: 3.961249041557312, ETA in seconds: 1208710.979\n",
      "epoch: 189100, train loss: 3.8975784301757814, val loss: 3.964490795135498, ETA in seconds: 1209162.337\n",
      "epoch: 189200, train loss: 3.897588086128235, val loss: 3.959020757675171, ETA in seconds: 1209614.390\n",
      "epoch: 189300, train loss: 3.890582299232483, val loss: 3.9691821098327638, ETA in seconds: 1210065.753\n",
      "epoch: 189400, train loss: 3.8823954820632935, val loss: 3.9655182361602783, ETA in seconds: 1210519.811\n",
      "epoch: 189500, train loss: 3.8917571783065794, val loss: 3.973827934265137, ETA in seconds: 1210961.730\n",
      "epoch: 189600, train loss: 3.8897423267364504, val loss: 3.9683484077453612, ETA in seconds: 1211407.637\n",
      "epoch: 189700, train loss: 3.8895832300186157, val loss: 3.957780122756958, ETA in seconds: 1211848.099\n",
      "epoch: 189800, train loss: 3.8961320161819457, val loss: 3.9689311504364015, ETA in seconds: 1212336.743\n",
      "epoch: 189900, train loss: 3.8998540878295898, val loss: 3.962776613235474, ETA in seconds: 1212909.000\n",
      "epoch: 190000, train loss: 3.886963677406311, val loss: 3.9708410263061524, ETA in seconds: 1213487.712\n",
      "epoch: 190100, train loss: 3.892147254943848, val loss: 3.9682717084884644, ETA in seconds: 1214058.157\n",
      "epoch: 190200, train loss: 3.8995113134384156, val loss: 3.97227942943573, ETA in seconds: 1214668.844\n",
      "epoch: 190300, train loss: 3.886371946334839, val loss: 3.967960810661316, ETA in seconds: 1215182.078\n",
      "epoch: 190400, train loss: 3.892478346824646, val loss: 3.9624125957489014, ETA in seconds: 1215630.387\n",
      "epoch: 190500, train loss: 3.8924792051315307, val loss: 3.9641343116760255, ETA in seconds: 1216091.328\n",
      "epoch: 190600, train loss: 3.8936840057373048, val loss: 3.9783099412918093, ETA in seconds: 1216594.744\n",
      "epoch: 190700, train loss: 3.8928810596466064, val loss: 3.962761640548706, ETA in seconds: 1217173.179\n",
      "epoch: 190800, train loss: 3.897996973991394, val loss: 3.964473247528076, ETA in seconds: 1217752.009\n",
      "epoch: 190900, train loss: 3.8998107433319094, val loss: 3.969515657424927, ETA in seconds: 1218296.523\n",
      "epoch: 191000, train loss: 3.889634537696838, val loss: 3.9763304233551025, ETA in seconds: 1218754.271\n",
      "epoch: 191100, train loss: 3.9023173809051515, val loss: 3.9755523681640623, ETA in seconds: 1219231.700\n",
      "epoch: 191200, train loss: 3.8939555406570436, val loss: 3.973806929588318, ETA in seconds: 1219720.675\n",
      "epoch: 191300, train loss: 3.9009790420532227, val loss: 3.969054865837097, ETA in seconds: 1220199.647\n",
      "epoch: 191400, train loss: 3.8942134857177733, val loss: 3.97452449798584, ETA in seconds: 1220677.251\n",
      "epoch: 191500, train loss: 3.887747883796692, val loss: 3.9640663146972654, ETA in seconds: 1221147.033\n",
      "epoch: 191600, train loss: 3.883662390708923, val loss: 3.9812068939208984, ETA in seconds: 1221598.866\n",
      "epoch: 191700, train loss: 3.892496037483215, val loss: 3.9655170679092406, ETA in seconds: 1222041.552\n",
      "epoch: 191800, train loss: 3.8923582077026366, val loss: 3.9809645414352417, ETA in seconds: 1222497.057\n",
      "epoch: 191900, train loss: 3.9001120805740355, val loss: 3.9617374658584597, ETA in seconds: 1222929.621\n",
      "epoch: 192000, train loss: 3.8845103979110718, val loss: 3.9672433376312255, ETA in seconds: 1223385.311\n",
      "epoch: 192100, train loss: 3.897334265708923, val loss: 3.951730155944824, ETA in seconds: 1223828.231\n",
      "epoch: 192200, train loss: 3.897741365432739, val loss: 3.970034432411194, ETA in seconds: 1224286.846\n",
      "epoch: 192300, train loss: 3.9014118194580076, val loss: 3.9690282344818115, ETA in seconds: 1224732.992\n",
      "epoch: 192400, train loss: 3.8883129358291626, val loss: 3.9737092018127442, ETA in seconds: 1225191.794\n",
      "epoch: 192500, train loss: 3.88996958732605, val loss: 3.970136594772339, ETA in seconds: 1225635.660\n",
      "epoch: 192600, train loss: 3.891514468193054, val loss: 3.9692230939865114, ETA in seconds: 1226055.741\n",
      "epoch: 192700, train loss: 3.8878395557403564, val loss: 3.96712384223938, ETA in seconds: 1226502.791\n",
      "epoch: 192800, train loss: 3.88662371635437, val loss: 3.9802745819091796, ETA in seconds: 1226932.882\n",
      "epoch: 192900, train loss: 3.898131084442139, val loss: 3.963561940193176, ETA in seconds: 1227363.851\n",
      "epoch: 193000, train loss: 3.8877270936965944, val loss: 3.9647178649902344, ETA in seconds: 1227797.322\n",
      "epoch: 193100, train loss: 3.886641836166382, val loss: 3.9719236373901365, ETA in seconds: 1228229.889\n",
      "epoch: 193200, train loss: 3.906055545806885, val loss: 3.9755589723587037, ETA in seconds: 1228648.742\n",
      "epoch: 193300, train loss: 3.8816277027130126, val loss: 3.9773866891860963, ETA in seconds: 1229080.811\n",
      "epoch: 193400, train loss: 3.9033129930496218, val loss: 3.9741402864456177, ETA in seconds: 1229518.429\n",
      "epoch: 193500, train loss: 3.8853988885879516, val loss: 3.971598815917969, ETA in seconds: 1229956.500\n",
      "epoch: 193600, train loss: 3.889817452430725, val loss: 3.970448613166809, ETA in seconds: 1230372.452\n",
      "epoch: 193700, train loss: 3.8955084323883056, val loss: 3.9667990684509276, ETA in seconds: 1230818.906\n",
      "epoch: 193800, train loss: 3.8958874940872192, val loss: 3.965013837814331, ETA in seconds: 1231266.935\n",
      "epoch: 193900, train loss: 3.8993510961532594, val loss: 3.980041480064392, ETA in seconds: 1231714.615\n",
      "epoch: 194000, train loss: 3.8859325885772704, val loss: 3.9766467571258546, ETA in seconds: 1232139.336\n",
      "epoch: 194100, train loss: 3.893406391143799, val loss: 3.9786125659942626, ETA in seconds: 1232668.840\n",
      "epoch: 194200, train loss: 3.8987839937210085, val loss: 3.971598410606384, ETA in seconds: 1233129.273\n",
      "epoch: 194300, train loss: 3.894424843788147, val loss: 3.97610285282135, ETA in seconds: 1233633.669\n",
      "epoch: 194400, train loss: 3.901076650619507, val loss: 3.974353384971619, ETA in seconds: 1234138.695\n",
      "epoch: 194500, train loss: 3.8919184684753416, val loss: 3.9635138511657715, ETA in seconds: 1234644.041\n",
      "epoch: 194600, train loss: 3.890669107437134, val loss: 3.9653300523757933, ETA in seconds: 1235148.226\n",
      "epoch: 194700, train loss: 3.8953235149383545, val loss: 3.97639741897583, ETA in seconds: 1235647.448\n",
      "epoch: 194800, train loss: 3.8911980628967284, val loss: 3.9680387496948244, ETA in seconds: 1236136.922\n",
      "epoch: 194900, train loss: 3.8942966222763062, val loss: 3.970247578620911, ETA in seconds: 1236602.023\n",
      "epoch: 195000, train loss: 3.893721580505371, val loss: 3.978429937362671, ETA in seconds: 1237047.462\n",
      "epoch: 195100, train loss: 3.8895565986633303, val loss: 3.960873174667358, ETA in seconds: 1237464.976\n",
      "epoch: 195200, train loss: 3.883859133720398, val loss: 3.9761040925979616, ETA in seconds: 1237907.816\n",
      "epoch: 195300, train loss: 3.894096612930298, val loss: 3.972250819206238, ETA in seconds: 1238366.660\n",
      "epoch: 195400, train loss: 3.8801812648773195, val loss: 3.9748759984970095, ETA in seconds: 1238844.303\n",
      "epoch: 195500, train loss: 3.8941476583480834, val loss: 3.9627788066864014, ETA in seconds: 1239346.777\n",
      "epoch: 195600, train loss: 3.8899015188217163, val loss: 3.969906044006348, ETA in seconds: 1239866.255\n",
      "epoch: 195700, train loss: 3.89234402179718, val loss: 3.9650807857513426, ETA in seconds: 1240498.610\n",
      "epoch: 195800, train loss: 3.892862319946289, val loss: 3.9724521160125734, ETA in seconds: 1241114.734\n",
      "epoch: 195900, train loss: 3.892666554450989, val loss: 3.963341760635376, ETA in seconds: 1241712.598\n",
      "epoch: 196000, train loss: 3.8936684846878054, val loss: 3.9622862100601197, ETA in seconds: 1242339.063\n",
      "epoch: 196100, train loss: 3.8786295890808105, val loss: 3.968404531478882, ETA in seconds: 1242978.162\n",
      "epoch: 196200, train loss: 3.8903809785842896, val loss: 3.9657980680465696, ETA in seconds: 1243607.009\n",
      "epoch: 196300, train loss: 3.8957550525665283, val loss: 3.965137577056885, ETA in seconds: 1244221.310\n",
      "epoch: 196400, train loss: 3.887795853614807, val loss: 3.971545433998108, ETA in seconds: 1244809.297\n",
      "epoch: 196500, train loss: 3.8925047397613524, val loss: 3.9752583265304566, ETA in seconds: 1245374.190\n",
      "epoch: 196600, train loss: 3.886793828010559, val loss: 3.97117133140564, ETA in seconds: 1245948.956\n",
      "epoch: 196700, train loss: 3.892776679992676, val loss: 3.971257448196411, ETA in seconds: 1246520.049\n",
      "epoch: 196800, train loss: 3.900458312034607, val loss: 3.9746114015579224, ETA in seconds: 1246936.219\n",
      "epoch: 196900, train loss: 3.891579008102417, val loss: 3.969079303741455, ETA in seconds: 1247458.877\n",
      "epoch: 197000, train loss: 3.888593602180481, val loss: 3.971496033668518, ETA in seconds: 1248041.995\n",
      "epoch: 197100, train loss: 3.8962611675262453, val loss: 3.9684555530548096, ETA in seconds: 1248621.174\n",
      "epoch: 197200, train loss: 3.8972503185272216, val loss: 3.975598907470703, ETA in seconds: 1249087.096\n",
      "epoch: 197300, train loss: 3.894944262504578, val loss: 3.9761556148529054, ETA in seconds: 1249524.419\n",
      "epoch: 197400, train loss: 3.8896048069000244, val loss: 3.9731127262115478, ETA in seconds: 1249950.598\n",
      "epoch: 197500, train loss: 3.8982803583145142, val loss: 3.982572627067566, ETA in seconds: 1250405.317\n",
      "epoch: 197600, train loss: 3.901142978668213, val loss: 3.979990839958191, ETA in seconds: 1250982.935\n",
      "epoch: 197700, train loss: 3.8899831056594847, val loss: 3.9638224124908445, ETA in seconds: 1251443.218\n",
      "epoch: 197800, train loss: 3.8928335666656495, val loss: 3.9707089185714723, ETA in seconds: 1251894.800\n",
      "epoch: 197900, train loss: 3.8979067325592043, val loss: 3.967828893661499, ETA in seconds: 1252313.745\n",
      "epoch: 198000, train loss: 3.8941050291061403, val loss: 3.9644850492477417, ETA in seconds: 1252730.634\n",
      "epoch: 198100, train loss: 3.8943433284759523, val loss: 3.973705506324768, ETA in seconds: 1253156.569\n",
      "epoch: 198200, train loss: 3.8939443588256837, val loss: 3.960554003715515, ETA in seconds: 1253645.941\n",
      "epoch: 198300, train loss: 3.8947542667388917, val loss: 3.961205816268921, ETA in seconds: 1254133.355\n",
      "epoch: 198400, train loss: 3.8870766401290893, val loss: 3.968272256851196, ETA in seconds: 1254608.195\n",
      "epoch: 198500, train loss: 3.8960071086883543, val loss: 3.9630133390426634, ETA in seconds: 1255098.446\n",
      "epoch: 198600, train loss: 3.9040858268737795, val loss: 3.9801043033599854, ETA in seconds: 1255579.100\n",
      "epoch: 198700, train loss: 3.9008044958114625, val loss: 3.9577462673187256, ETA in seconds: 1256060.842\n",
      "epoch: 198800, train loss: 3.8921737909317016, val loss: 3.9596354722976685, ETA in seconds: 1256501.186\n",
      "epoch: 198900, train loss: 3.8954612970352174, val loss: 3.962636685371399, ETA in seconds: 1256952.703\n",
      "epoch: 199000, train loss: 3.894098925590515, val loss: 3.9700074195861816, ETA in seconds: 1257403.756\n",
      "epoch: 199100, train loss: 3.887757444381714, val loss: 3.9721494913101196, ETA in seconds: 1257891.269\n",
      "epoch: 199200, train loss: 3.8979196310043336, val loss: 3.969666290283203, ETA in seconds: 1258382.761\n",
      "epoch: 199300, train loss: 3.8932181358337403, val loss: 3.9605732917785645, ETA in seconds: 1258880.543\n",
      "epoch: 199400, train loss: 3.898626279830933, val loss: 3.958932948112488, ETA in seconds: 1259382.340\n",
      "epoch: 199500, train loss: 3.8969027996063232, val loss: 3.9647859811782835, ETA in seconds: 1259846.557\n",
      "epoch: 199600, train loss: 3.894304132461548, val loss: 3.971109914779663, ETA in seconds: 1260397.599\n",
      "epoch: 199700, train loss: 3.892677664756775, val loss: 3.9587595224380494, ETA in seconds: 1261026.720\n",
      "epoch: 199800, train loss: 3.890661668777466, val loss: 3.975652503967285, ETA in seconds: 1261620.109\n",
      "epoch: 199900, train loss: 3.894028091430664, val loss: 3.969999647140503, ETA in seconds: 1262048.171\n",
      "epoch: 200000, train loss: 3.8936328172683714, val loss: 3.9714561700820923, ETA in seconds: 1262492.689\n",
      "epoch: 200100, train loss: 3.89501416683197, val loss: 3.9666347026824953, ETA in seconds: 1262982.095\n",
      "epoch: 200200, train loss: 3.8996157169342043, val loss: 3.9609430551528932, ETA in seconds: 1263425.411\n",
      "epoch: 200300, train loss: 3.9006752490997316, val loss: 3.9650956869125364, ETA in seconds: 1263871.594\n",
      "epoch: 200400, train loss: 3.893381118774414, val loss: 3.9659300565719606, ETA in seconds: 1264292.264\n",
      "epoch: 200500, train loss: 3.8976547479629517, val loss: 3.973614811897278, ETA in seconds: 1264739.384\n",
      "epoch: 200600, train loss: 3.8989110469818113, val loss: 3.959961462020874, ETA in seconds: 1265295.416\n",
      "epoch: 200700, train loss: 3.906736135482788, val loss: 3.976511740684509, ETA in seconds: 1265858.940\n",
      "epoch: 200800, train loss: 3.8925198554992675, val loss: 3.9737722158432005, ETA in seconds: 1266415.741\n",
      "epoch: 200900, train loss: 3.88147177696228, val loss: 3.96962456703186, ETA in seconds: 1266973.173\n",
      "epoch: 201000, train loss: 3.893335747718811, val loss: 3.9596825361251833, ETA in seconds: 1267528.819\n",
      "epoch: 201100, train loss: 3.888867163658142, val loss: 3.968945026397705, ETA in seconds: 1268085.466\n",
      "epoch: 201200, train loss: 3.8917499780654907, val loss: 3.96596245765686, ETA in seconds: 1268640.746\n",
      "epoch: 201300, train loss: 3.8945849895477296, val loss: 3.971622180938721, ETA in seconds: 1269144.054\n",
      "epoch: 201400, train loss: 3.888537549972534, val loss: 3.9756855249404905, ETA in seconds: 1269635.961\n",
      "epoch: 201500, train loss: 3.885161209106445, val loss: 3.964072275161743, ETA in seconds: 1270114.057\n",
      "epoch: 201600, train loss: 3.8904613971710207, val loss: 3.9758665800094604, ETA in seconds: 1270562.212\n",
      "epoch: 201700, train loss: 3.898644280433655, val loss: 3.972090721130371, ETA in seconds: 1271017.697\n",
      "epoch: 201800, train loss: 3.904096174240112, val loss: 3.977091145515442, ETA in seconds: 1271584.479\n",
      "epoch: 201900, train loss: 3.890982174873352, val loss: 3.9766703605651856, ETA in seconds: 1272009.229\n",
      "epoch: 202000, train loss: 3.888210344314575, val loss: 3.965673637390137, ETA in seconds: 1272440.840\n",
      "epoch: 202100, train loss: 3.9005206346511843, val loss: 3.975728750228882, ETA in seconds: 1272892.558\n",
      "epoch: 202200, train loss: 3.8955661773681642, val loss: 3.9678864240646363, ETA in seconds: 1273338.804\n",
      "epoch: 202300, train loss: 3.8944149732589723, val loss: 3.9674163341522215, ETA in seconds: 1273787.962\n",
      "epoch: 202400, train loss: 3.8916576623916628, val loss: 3.968546175956726, ETA in seconds: 1274227.449\n",
      "epoch: 202500, train loss: 3.9002875089645386, val loss: 3.958973217010498, ETA in seconds: 1274755.943\n",
      "epoch: 202600, train loss: 3.8963719606399536, val loss: 3.9587353229522706, ETA in seconds: 1275322.306\n",
      "epoch: 202700, train loss: 3.8853055000305177, val loss: 3.967725372314453, ETA in seconds: 1275882.428\n",
      "epoch: 202800, train loss: 3.892709517478943, val loss: 3.9770927906036375, ETA in seconds: 1276440.237\n",
      "epoch: 202900, train loss: 3.8862763404846192, val loss: 3.980144238471985, ETA in seconds: 1276999.362\n",
      "epoch: 203000, train loss: 3.8915664196014403, val loss: 3.9711217880249023, ETA in seconds: 1277556.692\n",
      "epoch: 203100, train loss: 3.8976693391799926, val loss: 3.974224853515625, ETA in seconds: 1278111.494\n",
      "epoch: 203200, train loss: 3.8971671581268312, val loss: 3.977344846725464, ETA in seconds: 1278740.608\n",
      "epoch: 203300, train loss: 3.890246343612671, val loss: 3.9645426750183104, ETA in seconds: 1279362.869\n",
      "epoch: 203400, train loss: 3.8858691215515138, val loss: 3.981057357788086, ETA in seconds: 1279982.421\n",
      "epoch: 203500, train loss: 3.88744854927063, val loss: 3.974033236503601, ETA in seconds: 1280602.561\n",
      "epoch: 203600, train loss: 3.8906058788299562, val loss: 3.968473029136658, ETA in seconds: 1281081.664\n",
      "epoch: 203700, train loss: 3.8899755477905273, val loss: 3.9732825994491576, ETA in seconds: 1281523.388\n",
      "epoch: 203800, train loss: 3.895994019508362, val loss: 3.9761342763900758, ETA in seconds: 1281983.716\n",
      "epoch: 203900, train loss: 3.884059524536133, val loss: 3.9801116228103637, ETA in seconds: 1282409.964\n",
      "epoch: 204000, train loss: 3.8898287534713747, val loss: 3.970549774169922, ETA in seconds: 1282849.003\n",
      "epoch: 204100, train loss: 3.8937421083450316, val loss: 3.9659339427947997, ETA in seconds: 1283292.397\n",
      "epoch: 204200, train loss: 3.8902000188827515, val loss: 3.973300242424011, ETA in seconds: 1283706.105\n",
      "epoch: 204300, train loss: 3.8992908239364623, val loss: 3.960040545463562, ETA in seconds: 1284116.792\n",
      "epoch: 204400, train loss: 3.8959317207336426, val loss: 3.981665825843811, ETA in seconds: 1284537.002\n",
      "epoch: 204500, train loss: 3.885618257522583, val loss: 3.97339186668396, ETA in seconds: 1285097.267\n",
      "epoch: 204600, train loss: 3.8890136957168577, val loss: 3.971868801116943, ETA in seconds: 1285642.082\n",
      "epoch: 204700, train loss: 3.88777916431427, val loss: 3.9718138694763185, ETA in seconds: 1286189.586\n",
      "epoch: 204800, train loss: 3.890825057029724, val loss: 3.965776038169861, ETA in seconds: 1286599.756\n",
      "epoch: 204900, train loss: 3.899474930763245, val loss: 3.9766658544540405, ETA in seconds: 1287028.229\n",
      "epoch: 205000, train loss: 3.888390564918518, val loss: 3.9615744590759276, ETA in seconds: 1287455.211\n",
      "epoch: 205100, train loss: 3.9002820253372192, val loss: 3.9714488983154297, ETA in seconds: 1287936.857\n",
      "epoch: 205200, train loss: 3.8940670251846314, val loss: 3.97426061630249, ETA in seconds: 1288529.018\n",
      "epoch: 205300, train loss: 3.890465760231018, val loss: 3.97863085269928, ETA in seconds: 1289126.820\n",
      "epoch: 205400, train loss: 3.896046280860901, val loss: 3.982409048080444, ETA in seconds: 1289708.792\n",
      "epoch: 205500, train loss: 3.9027827739715577, val loss: 3.9636242389678955, ETA in seconds: 1290150.086\n",
      "epoch: 205600, train loss: 3.888509726524353, val loss: 3.9724003314971923, ETA in seconds: 1290588.570\n",
      "epoch: 205700, train loss: 3.8967158079147337, val loss: 3.9665447950363157, ETA in seconds: 1291047.951\n",
      "epoch: 205800, train loss: 3.8961090326309202, val loss: 3.9744992017745973, ETA in seconds: 1291535.128\n",
      "epoch: 205900, train loss: 3.900082516670227, val loss: 3.9651061296463013, ETA in seconds: 1292005.454\n",
      "epoch: 206000, train loss: 3.894931268692017, val loss: 3.9671954870224, ETA in seconds: 1292428.513\n",
      "epoch: 206100, train loss: 3.885497784614563, val loss: 3.9630300045013427, ETA in seconds: 1292832.177\n",
      "epoch: 206200, train loss: 3.895154070854187, val loss: 3.9629098892211916, ETA in seconds: 1293243.074\n",
      "epoch: 206300, train loss: 3.900084900856018, val loss: 3.9777704954147337, ETA in seconds: 1293662.746\n",
      "epoch: 206400, train loss: 3.886430788040161, val loss: 3.9802277803421022, ETA in seconds: 1294074.694\n",
      "epoch: 206500, train loss: 3.8931774854660035, val loss: 3.970770311355591, ETA in seconds: 1294548.872\n",
      "epoch: 206600, train loss: 3.8950397968292236, val loss: 3.965488600730896, ETA in seconds: 1294975.754\n",
      "epoch: 206700, train loss: 3.8882941007614136, val loss: 3.974911022186279, ETA in seconds: 1295399.662\n",
      "epoch: 206800, train loss: 3.896261215209961, val loss: 3.982515740394592, ETA in seconds: 1295820.821\n",
      "epoch: 206900, train loss: 3.900802755355835, val loss: 3.965164542198181, ETA in seconds: 1296239.562\n",
      "epoch: 207000, train loss: 3.8854740381240847, val loss: 3.973503661155701, ETA in seconds: 1296645.850\n",
      "epoch: 207100, train loss: 3.897416186332703, val loss: 3.964379644393921, ETA in seconds: 1297068.308\n",
      "epoch: 207200, train loss: 3.889179515838623, val loss: 3.9666603326797487, ETA in seconds: 1297505.899\n",
      "epoch: 207300, train loss: 3.893065333366394, val loss: 3.971377658843994, ETA in seconds: 1297902.074\n",
      "epoch: 207400, train loss: 3.901003360748291, val loss: 3.964297914505005, ETA in seconds: 1298295.769\n",
      "epoch: 207500, train loss: 3.895562195777893, val loss: 3.9667784929275514, ETA in seconds: 1298723.529\n",
      "epoch: 207600, train loss: 3.8855396270751954, val loss: 3.972405767440796, ETA in seconds: 1299138.127\n",
      "epoch: 207700, train loss: 3.9105454206466677, val loss: 3.973511743545532, ETA in seconds: 1299534.294\n",
      "epoch: 207800, train loss: 3.9018708229064942, val loss: 3.977048873901367, ETA in seconds: 1299934.554\n",
      "epoch: 207900, train loss: 3.891952061653137, val loss: 3.9816603660583496, ETA in seconds: 1300336.557\n",
      "epoch: 208000, train loss: 3.9020026683807374, val loss: 3.9657752752304076, ETA in seconds: 1300830.938\n",
      "epoch: 208100, train loss: 3.894116020202637, val loss: 3.9791919708251955, ETA in seconds: 1301254.794\n",
      "epoch: 208200, train loss: 3.89840567111969, val loss: 3.9601888179779055, ETA in seconds: 1301685.453\n",
      "epoch: 208300, train loss: 3.900113320350647, val loss: 3.9687664270401, ETA in seconds: 1302112.335\n",
      "epoch: 208400, train loss: 3.883935046195984, val loss: 3.965245985984802, ETA in seconds: 1302538.709\n",
      "epoch: 208500, train loss: 3.8946120023727415, val loss: 3.976028084754944, ETA in seconds: 1302980.052\n",
      "epoch: 208600, train loss: 3.8980769872665406, val loss: 3.9685744047164917, ETA in seconds: 1303422.798\n",
      "epoch: 208700, train loss: 3.9013667821884157, val loss: 3.965470480918884, ETA in seconds: 1303911.365\n",
      "epoch: 208800, train loss: 3.8933702230453493, val loss: 3.965748119354248, ETA in seconds: 1304332.888\n",
      "epoch: 208900, train loss: 3.8855681896209715, val loss: 3.9784517765045164, ETA in seconds: 1304800.111\n",
      "epoch: 209000, train loss: 3.887273979187012, val loss: 3.9606395721435548, ETA in seconds: 1305352.369\n",
      "epoch: 209100, train loss: 3.898023319244385, val loss: 3.9631200551986696, ETA in seconds: 1305932.770\n",
      "epoch: 209200, train loss: 3.8863824367523194, val loss: 3.973031520843506, ETA in seconds: 1306529.462\n",
      "epoch: 209300, train loss: 3.893972325325012, val loss: 3.9655834913253782, ETA in seconds: 1307134.298\n",
      "epoch: 209400, train loss: 3.9060558795928957, val loss: 3.9833630084991456, ETA in seconds: 1307698.271\n",
      "epoch: 209500, train loss: 3.896352458000183, val loss: 3.9618115186691285, ETA in seconds: 1308245.119\n",
      "epoch: 209600, train loss: 3.896741032600403, val loss: 3.96829571723938, ETA in seconds: 1308781.822\n",
      "epoch: 209700, train loss: 3.897068452835083, val loss: 3.96173415184021, ETA in seconds: 1309329.555\n",
      "epoch: 209800, train loss: 3.890126442909241, val loss: 3.9648781061172484, ETA in seconds: 1309762.331\n",
      "epoch: 209900, train loss: 3.8920355558395388, val loss: 3.969712162017822, ETA in seconds: 1310207.240\n",
      "epoch: 210000, train loss: 3.8960369348526003, val loss: 3.9750797748565674, ETA in seconds: 1310640.608\n",
      "epoch: 210100, train loss: 3.888807201385498, val loss: 3.9707368135452272, ETA in seconds: 1311073.020\n",
      "epoch: 210200, train loss: 3.895559859275818, val loss: 3.974852275848389, ETA in seconds: 1311498.090\n",
      "epoch: 210300, train loss: 3.8952898502349855, val loss: 3.960948133468628, ETA in seconds: 1311921.439\n",
      "epoch: 210400, train loss: 3.89011869430542, val loss: 3.9747904539108276, ETA in seconds: 1312339.305\n",
      "epoch: 210500, train loss: 3.8874887228012085, val loss: 3.972734808921814, ETA in seconds: 1312755.256\n",
      "epoch: 210600, train loss: 3.8863826751708985, val loss: 3.970499134063721, ETA in seconds: 1313176.848\n",
      "epoch: 210700, train loss: 3.8987491607666014, val loss: 3.9631767749786375, ETA in seconds: 1313597.643\n",
      "epoch: 210800, train loss: 3.90080840587616, val loss: 3.9628482818603517, ETA in seconds: 1314010.304\n",
      "epoch: 210900, train loss: 3.893016505241394, val loss: 3.9708187103271486, ETA in seconds: 1314418.785\n",
      "epoch: 211000, train loss: 3.905679130554199, val loss: 3.9650692462921144, ETA in seconds: 1314834.469\n",
      "epoch: 211100, train loss: 3.8937237739562987, val loss: 3.9772046566009522, ETA in seconds: 1315289.117\n",
      "epoch: 211200, train loss: 3.88852322101593, val loss: 3.9549014568328857, ETA in seconds: 1315731.331\n",
      "epoch: 211300, train loss: 3.895824503898621, val loss: 3.965110421180725, ETA in seconds: 1316168.181\n",
      "epoch: 211400, train loss: 3.8940200805664062, val loss: 3.967225122451782, ETA in seconds: 1316592.022\n",
      "epoch: 211500, train loss: 3.898339867591858, val loss: 3.9556127071380613, ETA in seconds: 1317011.577\n",
      "epoch: 211600, train loss: 3.895404267311096, val loss: 3.961369824409485, ETA in seconds: 1317448.412\n",
      "epoch: 211700, train loss: 3.891996431350708, val loss: 3.9621712923049928, ETA in seconds: 1317855.752\n",
      "epoch: 211800, train loss: 3.894414281845093, val loss: 3.962141752243042, ETA in seconds: 1318264.658\n",
      "epoch: 211900, train loss: 3.896208167076111, val loss: 3.9640789270401, ETA in seconds: 1318670.847\n",
      "epoch: 212000, train loss: 3.8897838592529297, val loss: 3.9780614137649537, ETA in seconds: 1319121.518\n",
      "epoch: 212100, train loss: 3.894064521789551, val loss: 3.9610594272613526, ETA in seconds: 1319695.349\n",
      "epoch: 212200, train loss: 3.8955501079559327, val loss: 3.9598501205444334, ETA in seconds: 1320216.501\n",
      "epoch: 212300, train loss: 3.905391526222229, val loss: 3.9762179136276243, ETA in seconds: 1320779.958\n",
      "epoch: 212400, train loss: 3.8907476902008056, val loss: 3.9621505737304688, ETA in seconds: 1321321.400\n",
      "epoch: 212500, train loss: 3.8929099559783937, val loss: 3.9691818237304686, ETA in seconds: 1321890.419\n",
      "epoch: 212600, train loss: 3.9046479225158692, val loss: 3.9593863248825074, ETA in seconds: 1322470.158\n",
      "epoch: 212700, train loss: 3.891225266456604, val loss: 3.9678706884384156, ETA in seconds: 1323047.440\n",
      "epoch: 212800, train loss: 3.8918150424957276, val loss: 3.960332155227661, ETA in seconds: 1323582.514\n",
      "epoch: 212900, train loss: 3.893987202644348, val loss: 3.9750070571899414, ETA in seconds: 1323986.306\n",
      "epoch: 213000, train loss: 3.8958646774291994, val loss: 3.957336759567261, ETA in seconds: 1324373.877\n",
      "epoch: 213100, train loss: 3.882742238044739, val loss: 3.9722978830337525, ETA in seconds: 1324871.673\n",
      "epoch: 213200, train loss: 3.894511270523071, val loss: 3.9745237827301025, ETA in seconds: 1325402.936\n",
      "epoch: 213300, train loss: 3.885965085029602, val loss: 3.971018433570862, ETA in seconds: 1325881.605\n",
      "epoch: 213400, train loss: 3.8941207408905028, val loss: 3.9578110456466673, ETA in seconds: 1326299.409\n",
      "epoch: 213500, train loss: 3.8820428371429445, val loss: 3.9651733160018923, ETA in seconds: 1326711.937\n",
      "epoch: 213600, train loss: 3.8882548093795775, val loss: 3.968253803253174, ETA in seconds: 1327128.237\n",
      "epoch: 213700, train loss: 3.9005364656448362, val loss: 3.959034824371338, ETA in seconds: 1327543.582\n",
      "epoch: 213800, train loss: 3.897672200202942, val loss: 3.960549736022949, ETA in seconds: 1327961.969\n",
      "epoch: 213900, train loss: 3.8983685970306396, val loss: 3.968715786933899, ETA in seconds: 1328374.492\n",
      "epoch: 214000, train loss: 3.8891557693481444, val loss: 3.9631163597106935, ETA in seconds: 1328799.739\n",
      "epoch: 214100, train loss: 3.8885035037994387, val loss: 3.967760944366455, ETA in seconds: 1329209.523\n",
      "epoch: 214200, train loss: 3.887468409538269, val loss: 3.966740918159485, ETA in seconds: 1329619.561\n",
      "epoch: 214300, train loss: 3.8887827396392822, val loss: 3.966361403465271, ETA in seconds: 1330034.539\n",
      "epoch: 214400, train loss: 3.8954198598861693, val loss: 3.9656322717666628, ETA in seconds: 1330444.573\n",
      "epoch: 214500, train loss: 3.894279408454895, val loss: 3.971990132331848, ETA in seconds: 1330847.711\n",
      "epoch: 214600, train loss: 3.882359194755554, val loss: 3.9764951944351195, ETA in seconds: 1331287.897\n",
      "epoch: 214700, train loss: 3.882849907875061, val loss: 3.9556472301483154, ETA in seconds: 1331843.319\n",
      "epoch: 214800, train loss: 3.888852763175964, val loss: 3.9691174268722533, ETA in seconds: 1332372.715\n",
      "epoch: 214900, train loss: 3.9024234771728517, val loss: 3.9708741664886475, ETA in seconds: 1332795.883\n",
      "epoch: 215000, train loss: 3.8936014652252195, val loss: 3.9700050592422484, ETA in seconds: 1333223.371\n",
      "epoch: 215100, train loss: 3.8951025009155273, val loss: 3.955980825424194, ETA in seconds: 1333655.222\n",
      "epoch: 215200, train loss: 3.88280930519104, val loss: 3.9553144454956053, ETA in seconds: 1334057.267\n",
      "epoch: 215300, train loss: 3.8925784111022947, val loss: 3.9685614824295046, ETA in seconds: 1334457.757\n",
      "epoch: 215400, train loss: 3.890966606140137, val loss: 3.9610766172409058, ETA in seconds: 1334863.899\n",
      "epoch: 215500, train loss: 3.8945497274398804, val loss: 3.969684910774231, ETA in seconds: 1335275.653\n",
      "epoch: 215600, train loss: 3.8819379806518555, val loss: 3.9677725315093992, ETA in seconds: 1335698.625\n",
      "epoch: 215700, train loss: 3.887341785430908, val loss: 3.968016338348389, ETA in seconds: 1336118.301\n",
      "epoch: 215800, train loss: 3.899069976806641, val loss: 3.9623565673828125, ETA in seconds: 1336543.480\n",
      "epoch: 215900, train loss: 3.8873112916946413, val loss: 3.9720464944839478, ETA in seconds: 1336966.074\n",
      "epoch: 216000, train loss: 3.8945467710494994, val loss: 3.9597253084182737, ETA in seconds: 1337380.651\n",
      "epoch: 216100, train loss: 3.897120499610901, val loss: 3.965620827674866, ETA in seconds: 1337809.542\n",
      "epoch: 216200, train loss: 3.8987564563751222, val loss: 3.9686081647872924, ETA in seconds: 1338236.496\n",
      "epoch: 216300, train loss: 3.9011789321899415, val loss: 3.964657688140869, ETA in seconds: 1338662.772\n",
      "epoch: 216400, train loss: 3.8919760465621946, val loss: 3.9725157260894775, ETA in seconds: 1339143.512\n",
      "epoch: 216500, train loss: 3.8955544233322144, val loss: 3.9679457902908326, ETA in seconds: 1339583.279\n",
      "epoch: 216600, train loss: 3.897014045715332, val loss: 3.972996544837952, ETA in seconds: 1340015.462\n",
      "epoch: 216700, train loss: 3.8961517810821533, val loss: 3.969969606399536, ETA in seconds: 1340440.165\n",
      "epoch: 216800, train loss: 3.891706132888794, val loss: 3.9688750743865966, ETA in seconds: 1340874.721\n",
      "epoch: 216900, train loss: 3.89333918094635, val loss: 3.956406283378601, ETA in seconds: 1341301.454\n",
      "epoch: 217000, train loss: 3.8925463199615478, val loss: 3.9712440967559814, ETA in seconds: 1341726.941\n",
      "epoch: 217100, train loss: 3.8922451734542847, val loss: 3.9673088788986206, ETA in seconds: 1342154.933\n",
      "epoch: 217200, train loss: 3.899733877182007, val loss: 3.970788836479187, ETA in seconds: 1342582.587\n",
      "epoch: 217300, train loss: 3.8942930698394775, val loss: 3.9640369176864625, ETA in seconds: 1343057.377\n",
      "epoch: 217400, train loss: 3.902140164375305, val loss: 3.9630990266799926, ETA in seconds: 1343467.247\n",
      "epoch: 217500, train loss: 3.8852659940719603, val loss: 3.9765963792800902, ETA in seconds: 1343877.313\n",
      "epoch: 217600, train loss: 3.8903192281723022, val loss: 3.9719462156295777, ETA in seconds: 1344289.195\n",
      "epoch: 217700, train loss: 3.898392176628113, val loss: 3.9670994758605955, ETA in seconds: 1344709.777\n",
      "epoch: 217800, train loss: 3.8882472991943358, val loss: 3.9757839679718017, ETA in seconds: 1345143.410\n",
      "epoch: 217900, train loss: 3.8913044691085816, val loss: 3.977442908287048, ETA in seconds: 1345571.906\n",
      "epoch: 218000, train loss: 3.892665719985962, val loss: 3.9784602880477906, ETA in seconds: 1345996.674\n",
      "epoch: 218100, train loss: 3.8893716096878053, val loss: 3.9622081756591796, ETA in seconds: 1346430.632\n",
      "epoch: 218200, train loss: 3.8992738008499144, val loss: 3.9732276678085325, ETA in seconds: 1346977.345\n",
      "epoch: 218300, train loss: 3.8893645286560057, val loss: 3.964940047264099, ETA in seconds: 1347437.142\n",
      "epoch: 218400, train loss: 3.8985459327697756, val loss: 3.968265104293823, ETA in seconds: 1347832.581\n",
      "epoch: 218500, train loss: 3.895618176460266, val loss: 3.972535753250122, ETA in seconds: 1348214.146\n",
      "epoch: 218600, train loss: 3.8969606161117554, val loss: 3.967483901977539, ETA in seconds: 1348632.236\n",
      "epoch: 218700, train loss: 3.8908677101135254, val loss: 3.9629684925079345, ETA in seconds: 1349064.716\n",
      "epoch: 218800, train loss: 3.887589454650879, val loss: 3.970703935623169, ETA in seconds: 1349525.386\n",
      "epoch: 218900, train loss: 3.892028069496155, val loss: 3.967211055755615, ETA in seconds: 1350052.085\n",
      "epoch: 219000, train loss: 3.899847960472107, val loss: 3.9686918020248414, ETA in seconds: 1350571.320\n",
      "epoch: 219100, train loss: 3.894258713722229, val loss: 3.9665555715560914, ETA in seconds: 1351094.968\n",
      "epoch: 219200, train loss: 3.8915422916412354, val loss: 3.967056059837341, ETA in seconds: 1351614.749\n",
      "epoch: 219300, train loss: 3.9049115896224977, val loss: 3.96704216003418, ETA in seconds: 1352009.292\n",
      "epoch: 219400, train loss: 3.8977702617645265, val loss: 3.9653534650802613, ETA in seconds: 1352411.921\n",
      "epoch: 219500, train loss: 3.903521108627319, val loss: 3.965400767326355, ETA in seconds: 1352905.130\n",
      "epoch: 219600, train loss: 3.8923298358917235, val loss: 3.9717937231063845, ETA in seconds: 1353305.078\n",
      "epoch: 219700, train loss: 3.896214461326599, val loss: 3.963376355171204, ETA in seconds: 1353711.497\n",
      "epoch: 219800, train loss: 3.895986533164978, val loss: 3.9806374311447144, ETA in seconds: 1354129.751\n",
      "epoch: 219900, train loss: 3.8925281763076782, val loss: 3.9722574234008787, ETA in seconds: 1354546.019\n",
      "epoch: 220000, train loss: 3.8918909549713137, val loss: 3.9581693172454835, ETA in seconds: 1355005.400\n",
      "epoch: 220100, train loss: 3.8927544832229612, val loss: 3.972376012802124, ETA in seconds: 1355553.314\n",
      "epoch: 220200, train loss: 3.9043309450149537, val loss: 3.970452880859375, ETA in seconds: 1355999.248\n",
      "epoch: 220300, train loss: 3.8870614051818846, val loss: 3.9778337240219117, ETA in seconds: 1356432.407\n",
      "epoch: 220400, train loss: 3.906506586074829, val loss: 3.96545832157135, ETA in seconds: 1356838.522\n",
      "epoch: 220500, train loss: 3.894168972969055, val loss: 3.959147262573242, ETA in seconds: 1357241.101\n",
      "epoch: 220600, train loss: 3.893377447128296, val loss: 3.9792044401168822, ETA in seconds: 1357648.627\n",
      "epoch: 220700, train loss: 3.8929513454437257, val loss: 3.9785998582839968, ETA in seconds: 1358056.367\n",
      "epoch: 220800, train loss: 3.8865357398986817, val loss: 3.9693538188934325, ETA in seconds: 1358469.342\n",
      "epoch: 220900, train loss: 3.894678807258606, val loss: 3.9665770053863527, ETA in seconds: 1358893.198\n",
      "epoch: 221000, train loss: 3.89088134765625, val loss: 3.9606229782104494, ETA in seconds: 1359304.801\n",
      "epoch: 221100, train loss: 3.8825623273849486, val loss: 3.9724994897842407, ETA in seconds: 1359735.947\n",
      "epoch: 221200, train loss: 3.8983758211135866, val loss: 3.971005845069885, ETA in seconds: 1360164.341\n",
      "epoch: 221300, train loss: 3.8836178541183473, val loss: 3.9754804134368897, ETA in seconds: 1360590.934\n",
      "epoch: 221400, train loss: 3.8934008836746217, val loss: 3.964198040962219, ETA in seconds: 1361012.057\n",
      "epoch: 221500, train loss: 3.9003483057022095, val loss: 3.9715945959091186, ETA in seconds: 1361431.567\n",
      "epoch: 221600, train loss: 3.8874161720275877, val loss: 3.9587603330612184, ETA in seconds: 1361867.140\n",
      "epoch: 221700, train loss: 3.903359055519104, val loss: 3.980970335006714, ETA in seconds: 1362285.485\n",
      "epoch: 221800, train loss: 3.896268367767334, val loss: 3.974915862083435, ETA in seconds: 1362717.459\n",
      "epoch: 221900, train loss: 3.894807553291321, val loss: 3.9657185316085815, ETA in seconds: 1363139.965\n",
      "epoch: 222000, train loss: 3.898025965690613, val loss: 3.974209690093994, ETA in seconds: 1363559.095\n",
      "epoch: 222100, train loss: 3.895777177810669, val loss: 3.9608438491821287, ETA in seconds: 1363976.241\n",
      "epoch: 222200, train loss: 3.902802014350891, val loss: 3.9690225601196287, ETA in seconds: 1364395.544\n",
      "epoch: 222300, train loss: 3.891249418258667, val loss: 3.9708593845367433, ETA in seconds: 1364816.624\n",
      "epoch: 222400, train loss: 3.9022515296936033, val loss: 3.9693969249725343, ETA in seconds: 1365228.044\n",
      "epoch: 222500, train loss: 3.8940653562545777, val loss: 3.96152777671814, ETA in seconds: 1365651.084\n",
      "epoch: 222600, train loss: 3.8980355978012087, val loss: 3.9751668214797973, ETA in seconds: 1366057.906\n",
      "epoch: 222700, train loss: 3.9002932786941527, val loss: 3.990429902076721, ETA in seconds: 1366460.911\n",
      "epoch: 222800, train loss: 3.8937636375427247, val loss: 3.977989602088928, ETA in seconds: 1366933.895\n",
      "epoch: 222900, train loss: 3.889464020729065, val loss: 3.970129156112671, ETA in seconds: 1367341.378\n",
      "epoch: 223000, train loss: 3.899497938156128, val loss: 3.96294162273407, ETA in seconds: 1367756.069\n",
      "epoch: 223100, train loss: 3.89401261806488, val loss: 3.969348359107971, ETA in seconds: 1368183.452\n",
      "epoch: 223200, train loss: 3.8830963373184204, val loss: 3.9622946977615356, ETA in seconds: 1368643.682\n",
      "epoch: 223300, train loss: 3.889405059814453, val loss: 3.975041937828064, ETA in seconds: 1369026.718\n",
      "epoch: 223400, train loss: 3.892428493499756, val loss: 3.9577075481414794, ETA in seconds: 1369442.201\n",
      "epoch: 223500, train loss: 3.892078399658203, val loss: 3.960174632072449, ETA in seconds: 1369868.738\n",
      "epoch: 223600, train loss: 3.8978615045547484, val loss: 3.9762837648391725, ETA in seconds: 1370407.961\n",
      "epoch: 223700, train loss: 3.8924649715423585, val loss: 3.970169925689697, ETA in seconds: 1370976.342\n",
      "epoch: 223800, train loss: 3.89179413318634, val loss: 3.9758922815322877, ETA in seconds: 1371506.094\n",
      "epoch: 223900, train loss: 3.8955036401748657, val loss: 3.9626935720443726, ETA in seconds: 1372042.613\n",
      "epoch: 224000, train loss: 3.894320249557495, val loss: 3.9582266092300413, ETA in seconds: 1372530.884\n",
      "epoch: 224100, train loss: 3.891060137748718, val loss: 3.967912268638611, ETA in seconds: 1372989.434\n",
      "epoch: 224200, train loss: 3.886939811706543, val loss: 3.9790723085403443, ETA in seconds: 1373396.407\n",
      "epoch: 224300, train loss: 3.8935156583786013, val loss: 3.963001918792725, ETA in seconds: 1373798.571\n",
      "epoch: 224400, train loss: 3.8910953998565674, val loss: 3.968350315093994, ETA in seconds: 1374199.905\n",
      "epoch: 224500, train loss: 3.889304542541504, val loss: 3.959799075126648, ETA in seconds: 1374609.238\n",
      "epoch: 224600, train loss: 3.8909226417541505, val loss: 3.9703594923019407, ETA in seconds: 1375017.635\n",
      "epoch: 224700, train loss: 3.8894248723983766, val loss: 3.9793787240982055, ETA in seconds: 1375427.784\n",
      "epoch: 224800, train loss: 3.8942059755325316, val loss: 3.9691148519515993, ETA in seconds: 1375837.966\n",
      "epoch: 224900, train loss: 3.891530680656433, val loss: 3.976788949966431, ETA in seconds: 1376236.451\n",
      "epoch: 225000, train loss: 3.900989055633545, val loss: 3.973435401916504, ETA in seconds: 1376620.426\n",
      "epoch: 225100, train loss: 3.89066686630249, val loss: 3.964038062095642, ETA in seconds: 1377047.837\n",
      "epoch: 225200, train loss: 3.8897512912750245, val loss: 3.971092963218689, ETA in seconds: 1377564.834\n",
      "epoch: 225300, train loss: 3.895407295227051, val loss: 3.972447156906128, ETA in seconds: 1378082.188\n",
      "epoch: 225400, train loss: 3.8968734979629516, val loss: 3.979547691345215, ETA in seconds: 1378607.807\n",
      "epoch: 225500, train loss: 3.9008570194244383, val loss: 3.969407343864441, ETA in seconds: 1379128.390\n",
      "epoch: 225600, train loss: 3.8919266939163206, val loss: 3.9615029811859133, ETA in seconds: 1379706.158\n",
      "epoch: 225700, train loss: 3.8926278829574583, val loss: 3.9568428993225098, ETA in seconds: 1380210.118\n",
      "epoch: 225800, train loss: 3.898886489868164, val loss: 3.9549644947052003, ETA in seconds: 1380776.247\n",
      "epoch: 225900, train loss: 3.894117832183838, val loss: 3.9700327634811403, ETA in seconds: 1381188.790\n",
      "epoch: 226000, train loss: 3.8957311153411864, val loss: 3.972943329811096, ETA in seconds: 1381624.499\n",
      "epoch: 226100, train loss: 3.904930901527405, val loss: 3.963771033287048, ETA in seconds: 1382097.431\n",
      "epoch: 226200, train loss: 3.897853398323059, val loss: 3.9686745166778565, ETA in seconds: 1382553.719\n",
      "epoch: 226300, train loss: 3.8963496923446654, val loss: 3.972549891471863, ETA in seconds: 1382945.334\n",
      "epoch: 226400, train loss: 3.8944125175476074, val loss: 3.9755146503448486, ETA in seconds: 1383336.264\n",
      "epoch: 226500, train loss: 3.89817361831665, val loss: 3.9612301349639893, ETA in seconds: 1383741.564\n",
      "epoch: 226600, train loss: 3.8885590076446532, val loss: 3.965553379058838, ETA in seconds: 1384145.070\n",
      "epoch: 226700, train loss: 3.8996647357940675, val loss: 3.96945538520813, ETA in seconds: 1384547.209\n",
      "epoch: 226800, train loss: 3.8940643787384035, val loss: 3.9734452962875366, ETA in seconds: 1384949.774\n",
      "epoch: 226900, train loss: 3.8821322679519654, val loss: 3.9717722177505492, ETA in seconds: 1385335.917\n",
      "epoch: 227000, train loss: 3.884108233451843, val loss: 3.9910410404205323, ETA in seconds: 1385711.991\n",
      "epoch: 227100, train loss: 3.878405547142029, val loss: 3.972251844406128, ETA in seconds: 1386162.658\n",
      "epoch: 227200, train loss: 3.899640369415283, val loss: 3.9786622762680053, ETA in seconds: 1386542.107\n",
      "epoch: 227300, train loss: 3.894712543487549, val loss: 3.980014133453369, ETA in seconds: 1386920.844\n",
      "epoch: 227400, train loss: 3.888424587249756, val loss: 3.964666247367859, ETA in seconds: 1387306.838\n",
      "epoch: 227500, train loss: 3.8963250160217284, val loss: 3.9668622255325316, ETA in seconds: 1387698.968\n",
      "epoch: 227600, train loss: 3.8919013261795046, val loss: 3.973331642150879, ETA in seconds: 1388091.513\n",
      "epoch: 227700, train loss: 3.901505398750305, val loss: 3.9599546432495116, ETA in seconds: 1388486.801\n",
      "epoch: 227800, train loss: 3.900888752937317, val loss: 3.9675670623779298, ETA in seconds: 1388920.533\n",
      "epoch: 227900, train loss: 3.8890852451324465, val loss: 3.96951744556427, ETA in seconds: 1389412.662\n",
      "epoch: 228000, train loss: 3.903497838973999, val loss: 3.9638309478759766, ETA in seconds: 1389824.440\n",
      "epoch: 228100, train loss: 3.888957977294922, val loss: 3.9737751722335815, ETA in seconds: 1390221.890\n",
      "epoch: 228200, train loss: 3.892864441871643, val loss: 3.968421530723572, ETA in seconds: 1390614.694\n",
      "epoch: 228300, train loss: 3.8951505422592163, val loss: 3.9724089384078978, ETA in seconds: 1391016.798\n",
      "epoch: 228400, train loss: 3.8921555042266847, val loss: 3.96830837726593, ETA in seconds: 1391420.727\n",
      "epoch: 228500, train loss: 3.892204451560974, val loss: 3.9678489208221435, ETA in seconds: 1391803.972\n",
      "epoch: 228600, train loss: 3.8980311155319214, val loss: 3.9617586374282836, ETA in seconds: 1392193.958\n",
      "epoch: 228700, train loss: 3.8900665760040285, val loss: 3.97419331073761, ETA in seconds: 1392594.649\n",
      "epoch: 228800, train loss: 3.9000919818878175, val loss: 3.9628167867660524, ETA in seconds: 1393003.337\n",
      "epoch: 228900, train loss: 3.8921531677246093, val loss: 3.968096661567688, ETA in seconds: 1393414.188\n",
      "epoch: 229000, train loss: 3.902677607536316, val loss: 3.9673768043518067, ETA in seconds: 1393835.649\n",
      "epoch: 229100, train loss: 3.88322057723999, val loss: 3.958839845657349, ETA in seconds: 1394340.901\n",
      "epoch: 229200, train loss: 3.8973228931427, val loss: 3.9699130773544313, ETA in seconds: 1394850.872\n",
      "epoch: 229300, train loss: 3.8874614238739014, val loss: 3.9764792680740357, ETA in seconds: 1395357.353\n",
      "epoch: 229400, train loss: 3.891584539413452, val loss: 3.9795076131820677, ETA in seconds: 1395860.637\n",
      "epoch: 229500, train loss: 3.887048530578613, val loss: 3.9864468574523926, ETA in seconds: 1396363.927\n",
      "epoch: 229600, train loss: 3.8861945152282713, val loss: 3.972914481163025, ETA in seconds: 1396869.319\n",
      "epoch: 229700, train loss: 3.892609643936157, val loss: 3.9722501039505005, ETA in seconds: 1397372.582\n",
      "epoch: 229800, train loss: 3.892674374580383, val loss: 3.9635608196258545, ETA in seconds: 1397879.798\n",
      "epoch: 229900, train loss: 3.885031223297119, val loss: 3.975193190574646, ETA in seconds: 1398367.850\n",
      "epoch: 230000, train loss: 3.8890815496444704, val loss: 3.973969912528992, ETA in seconds: 1398765.779\n",
      "epoch: 230100, train loss: 3.89111864566803, val loss: 3.9711225032806396, ETA in seconds: 1399162.167\n",
      "epoch: 230200, train loss: 3.892425298690796, val loss: 3.9847922801971434, ETA in seconds: 1399551.453\n",
      "epoch: 230300, train loss: 3.890041399002075, val loss: 3.9803388357162475, ETA in seconds: 1399943.513\n",
      "epoch: 230400, train loss: 3.8892394304275513, val loss: 3.9646078824996946, ETA in seconds: 1400321.443\n",
      "epoch: 230500, train loss: 3.9001335859298707, val loss: 3.9757893562316893, ETA in seconds: 1400723.499\n",
      "epoch: 230600, train loss: 3.890862560272217, val loss: 3.96578586101532, ETA in seconds: 1401118.658\n",
      "epoch: 230700, train loss: 3.901461386680603, val loss: 3.9691261053085327, ETA in seconds: 1401529.669\n",
      "epoch: 230800, train loss: 3.8879636764526366, val loss: 3.9791232109069825, ETA in seconds: 1401919.830\n",
      "epoch: 230900, train loss: 3.895320916175842, val loss: 3.983965277671814, ETA in seconds: 1402298.117\n",
      "epoch: 231000, train loss: 3.8851980924606324, val loss: 3.9689045190811156, ETA in seconds: 1402682.633\n",
      "epoch: 231100, train loss: 3.895180034637451, val loss: 3.968970274925232, ETA in seconds: 1403076.570\n",
      "epoch: 231200, train loss: 3.8978959560394286, val loss: 3.9666514158248902, ETA in seconds: 1403469.684\n",
      "epoch: 231300, train loss: 3.899076056480408, val loss: 3.9814239263534548, ETA in seconds: 1403845.292\n",
      "epoch: 231400, train loss: 3.884548568725586, val loss: 3.970986342430115, ETA in seconds: 1404256.393\n",
      "epoch: 231500, train loss: 3.8938331842422484, val loss: 3.9735318183898927, ETA in seconds: 1404643.328\n",
      "epoch: 231600, train loss: 3.8936497449874876, val loss: 3.9759716033935546, ETA in seconds: 1405033.068\n",
      "epoch: 231700, train loss: 3.8970627307891847, val loss: 3.973709321022034, ETA in seconds: 1405422.144\n",
      "epoch: 231800, train loss: 3.8942570209503176, val loss: 3.9769971132278443, ETA in seconds: 1405813.824\n",
      "epoch: 231900, train loss: 3.8918100118637087, val loss: 3.972767686843872, ETA in seconds: 1406223.794\n",
      "epoch: 232000, train loss: 3.899924159049988, val loss: 3.9626663446426393, ETA in seconds: 1406640.679\n",
      "epoch: 232100, train loss: 3.899372029304504, val loss: 3.962735724449158, ETA in seconds: 1407057.970\n",
      "epoch: 232200, train loss: 3.8862107753753663, val loss: 3.9504121065139772, ETA in seconds: 1407469.827\n",
      "epoch: 232300, train loss: 3.8864187479019163, val loss: 3.9711719512939454, ETA in seconds: 1407886.435\n",
      "epoch: 232400, train loss: 3.8989954471588133, val loss: 3.9658584117889406, ETA in seconds: 1408296.416\n",
      "epoch: 232500, train loss: 3.8984108686447145, val loss: 3.9731829881668093, ETA in seconds: 1408707.303\n",
      "epoch: 232600, train loss: 3.8877707719802856, val loss: 3.974792242050171, ETA in seconds: 1409145.038\n",
      "epoch: 232700, train loss: 3.8892483949661254, val loss: 3.9673015117645263, ETA in seconds: 1409567.767\n",
      "epoch: 232800, train loss: 3.8888186693191527, val loss: 3.960811710357666, ETA in seconds: 1409979.748\n",
      "epoch: 232900, train loss: 3.8880186080932617, val loss: 3.9615901470184327, ETA in seconds: 1410408.175\n",
      "epoch: 233000, train loss: 3.8941099882125854, val loss: 3.970829629898071, ETA in seconds: 1410819.288\n",
      "epoch: 233100, train loss: 3.898675060272217, val loss: 3.965687561035156, ETA in seconds: 1411229.380\n",
      "epoch: 233200, train loss: 3.892470908164978, val loss: 3.971298265457153, ETA in seconds: 1411670.799\n",
      "epoch: 233300, train loss: 3.8989747762680054, val loss: 3.9677616119384767, ETA in seconds: 1412094.426\n",
      "epoch: 233400, train loss: 3.893759846687317, val loss: 3.9657196760177613, ETA in seconds: 1412443.979\n",
      "epoch: 233500, train loss: 3.897595834732056, val loss: 3.9606791496276856, ETA in seconds: 1412796.369\n",
      "epoch: 233600, train loss: 3.896307873725891, val loss: 3.960931992530823, ETA in seconds: 1413164.157\n",
      "epoch: 233700, train loss: 3.899857258796692, val loss: 3.9629220247268675, ETA in seconds: 1413528.523\n",
      "epoch: 233800, train loss: 3.8857832670211794, val loss: 3.9683231592178343, ETA in seconds: 1413894.305\n",
      "epoch: 233900, train loss: 3.895216536521912, val loss: 3.9655091524124146, ETA in seconds: 1414275.181\n",
      "epoch: 234000, train loss: 3.8895071506500245, val loss: 3.972933459281921, ETA in seconds: 1414662.549\n",
      "epoch: 234100, train loss: 3.898258924484253, val loss: 3.9640883207321167, ETA in seconds: 1415033.371\n",
      "epoch: 234200, train loss: 3.8987879753112793, val loss: 3.972976493835449, ETA in seconds: 1415401.298\n",
      "epoch: 234300, train loss: 3.8944921493530273, val loss: 3.9664998054504395, ETA in seconds: 1415784.268\n",
      "epoch: 234400, train loss: 3.8965802669525145, val loss: 3.9771664142608643, ETA in seconds: 1416149.835\n",
      "epoch: 234500, train loss: 3.899372911453247, val loss: 3.9793828964233398, ETA in seconds: 1416506.143\n",
      "epoch: 234600, train loss: 3.8976881742477416, val loss: 3.969183397293091, ETA in seconds: 1416877.176\n",
      "epoch: 234700, train loss: 3.895232892036438, val loss: 3.972141075134277, ETA in seconds: 1417235.089\n",
      "epoch: 234800, train loss: 3.8865177392959596, val loss: 3.982966923713684, ETA in seconds: 1417613.731\n",
      "epoch: 234900, train loss: 3.8925948619842528, val loss: 3.973527765274048, ETA in seconds: 1417977.268\n",
      "epoch: 235000, train loss: 3.8902376890182495, val loss: 3.9750965356826784, ETA in seconds: 1418347.092\n",
      "epoch: 235100, train loss: 3.883745217323303, val loss: 3.976473093032837, ETA in seconds: 1418717.253\n",
      "epoch: 235200, train loss: 3.887523579597473, val loss: 3.9841368913650514, ETA in seconds: 1419095.562\n",
      "epoch: 235300, train loss: 3.8869165897369387, val loss: 3.968924403190613, ETA in seconds: 1419563.060\n",
      "epoch: 235400, train loss: 3.896051454544067, val loss: 3.976479744911194, ETA in seconds: 1420080.714\n",
      "epoch: 235500, train loss: 3.8952512741088867, val loss: 3.984014391899109, ETA in seconds: 1420519.342\n",
      "epoch: 235600, train loss: 3.8946489095687866, val loss: 3.976591372489929, ETA in seconds: 1420881.286\n",
      "epoch: 235700, train loss: 3.891041469573975, val loss: 3.978583478927612, ETA in seconds: 1421408.247\n",
      "epoch: 235800, train loss: 3.890604019165039, val loss: 3.974586582183838, ETA in seconds: 1421838.188\n",
      "epoch: 235900, train loss: 3.8900492668151854, val loss: 3.9736607313156127, ETA in seconds: 1422310.998\n",
      "epoch: 236000, train loss: 3.902103853225708, val loss: 3.9748257637023925, ETA in seconds: 1422812.248\n",
      "epoch: 236100, train loss: 3.8926575422286986, val loss: 3.980987882614136, ETA in seconds: 1423367.080\n",
      "epoch: 236200, train loss: 3.898469924926758, val loss: 3.9692259311676024, ETA in seconds: 1423893.728\n",
      "epoch: 236300, train loss: 3.895654559135437, val loss: 3.9796481132507324, ETA in seconds: 1424402.847\n",
      "epoch: 236400, train loss: 3.888718271255493, val loss: 3.9795295715332033, ETA in seconds: 1424912.647\n",
      "epoch: 236500, train loss: 3.895292139053345, val loss: 3.9801758527755737, ETA in seconds: 1425421.216\n",
      "epoch: 236600, train loss: 3.8998807191848757, val loss: 3.9739978313446045, ETA in seconds: 1425928.405\n",
      "epoch: 236700, train loss: 3.889507699012756, val loss: 3.9732962608337403, ETA in seconds: 1426439.752\n",
      "epoch: 236800, train loss: 3.892952036857605, val loss: 3.97034547328949, ETA in seconds: 1426951.068\n",
      "epoch: 236900, train loss: 3.901529145240784, val loss: 3.9690665721893312, ETA in seconds: 1427460.050\n",
      "epoch: 237000, train loss: 3.9005829095840454, val loss: 3.9728219270706178, ETA in seconds: 1427968.202\n",
      "epoch: 237100, train loss: 3.8940568923950196, val loss: 3.9740381479263305, ETA in seconds: 1428479.470\n",
      "epoch: 237200, train loss: 3.892932105064392, val loss: 3.966974878311157, ETA in seconds: 1428988.570\n",
      "epoch: 237300, train loss: 3.890908646583557, val loss: 3.9784005880355835, ETA in seconds: 1429431.127\n",
      "epoch: 237400, train loss: 3.893773889541626, val loss: 3.97274112701416, ETA in seconds: 1429869.476\n",
      "epoch: 237500, train loss: 3.879230332374573, val loss: 3.9696550369262695, ETA in seconds: 1430362.782\n",
      "epoch: 237600, train loss: 3.8899561882019045, val loss: 3.9757716178894045, ETA in seconds: 1430858.044\n",
      "epoch: 237700, train loss: 3.881676363945007, val loss: 3.9721256494522095, ETA in seconds: 1431261.851\n",
      "epoch: 237800, train loss: 3.8910898208618163, val loss: 3.965752601623535, ETA in seconds: 1431622.120\n",
      "epoch: 237900, train loss: 3.8945027351379395, val loss: 3.9755850315093992, ETA in seconds: 1432072.824\n",
      "epoch: 238000, train loss: 3.8908003330230714, val loss: 3.968190813064575, ETA in seconds: 1432446.170\n",
      "epoch: 238100, train loss: 3.892865514755249, val loss: 3.9743301391601564, ETA in seconds: 1432823.220\n",
      "epoch: 238200, train loss: 3.891847085952759, val loss: 3.9648433446884157, ETA in seconds: 1433181.072\n",
      "epoch: 238300, train loss: 3.8987391233444213, val loss: 3.976930594444275, ETA in seconds: 1433547.994\n",
      "epoch: 238400, train loss: 3.8963506698608397, val loss: 3.9615193367004395, ETA in seconds: 1433928.733\n",
      "epoch: 238500, train loss: 3.8882709980010985, val loss: 3.9700372219085693, ETA in seconds: 1434314.632\n",
      "epoch: 238600, train loss: 3.8828692197799684, val loss: 3.9820686101913454, ETA in seconds: 1434694.019\n",
      "epoch: 238700, train loss: 3.8981707096099854, val loss: 3.9733988761901857, ETA in seconds: 1435075.240\n",
      "epoch: 238800, train loss: 3.8870439767837524, val loss: 3.9753783702850343, ETA in seconds: 1435448.190\n",
      "epoch: 238900, train loss: 3.8961068391799927, val loss: 3.9801241397857665, ETA in seconds: 1435824.489\n",
      "epoch: 239000, train loss: 3.8955023527145385, val loss: 3.96471483707428, ETA in seconds: 1436203.924\n",
      "epoch: 239100, train loss: 3.8845046758651733, val loss: 3.968649959564209, ETA in seconds: 1436584.031\n",
      "epoch: 239200, train loss: 3.893953227996826, val loss: 3.972509241104126, ETA in seconds: 1436965.246\n",
      "epoch: 239300, train loss: 3.901816892623901, val loss: 3.9760907649993897, ETA in seconds: 1437424.566\n",
      "epoch: 239400, train loss: 3.8910149574279784, val loss: 3.9694242238998414, ETA in seconds: 1437815.121\n",
      "epoch: 239500, train loss: 3.895967125892639, val loss: 3.9736708402633667, ETA in seconds: 1438196.256\n",
      "epoch: 239600, train loss: 3.892551612854004, val loss: 3.978741002082825, ETA in seconds: 1438577.192\n",
      "epoch: 239700, train loss: 3.887917995452881, val loss: 3.9674827337265013, ETA in seconds: 1438936.616\n",
      "epoch: 239800, train loss: 3.8922497272491454, val loss: 3.975208044052124, ETA in seconds: 1439314.439\n",
      "epoch: 239900, train loss: 3.895923948287964, val loss: 3.9733524560928344, ETA in seconds: 1439707.252\n",
      "epoch: 240000, train loss: 3.895231080055237, val loss: 3.9707996606826783, ETA in seconds: 1440096.045\n",
      "epoch: 240100, train loss: 3.8924872159957884, val loss: 3.966577386856079, ETA in seconds: 1440547.809\n",
      "epoch: 240200, train loss: 3.8965835094451906, val loss: 3.967078614234924, ETA in seconds: 1440936.229\n",
      "epoch: 240300, train loss: 3.8977067947387694, val loss: 3.95921106338501, ETA in seconds: 1441308.730\n",
      "epoch: 240400, train loss: 3.8888720750808714, val loss: 3.9668652534484865, ETA in seconds: 1441704.408\n",
      "epoch: 240500, train loss: 3.8954607009887696, val loss: 3.9788232803344727, ETA in seconds: 1442201.202\n",
      "epoch: 240600, train loss: 3.8931305408477783, val loss: 3.9749186038970947, ETA in seconds: 1442698.097\n",
      "epoch: 240700, train loss: 3.8875486135482786, val loss: 3.976825976371765, ETA in seconds: 1443207.197\n",
      "epoch: 240800, train loss: 3.884615993499756, val loss: 3.9786925077438355, ETA in seconds: 1443707.207\n",
      "epoch: 240900, train loss: 3.8939140319824217, val loss: 3.983882474899292, ETA in seconds: 1444202.238\n",
      "epoch: 241000, train loss: 3.8918627977371214, val loss: 3.9861948490142822, ETA in seconds: 1444695.820\n",
      "epoch: 241100, train loss: 3.8913098096847536, val loss: 3.9846367835998535, ETA in seconds: 1445176.469\n",
      "epoch: 241200, train loss: 3.891040086746216, val loss: 3.9868166208267213, ETA in seconds: 1445562.126\n",
      "epoch: 241300, train loss: 3.886775994300842, val loss: 3.983960580825806, ETA in seconds: 1445957.189\n",
      "epoch: 241400, train loss: 3.893894624710083, val loss: 3.9776140213012696, ETA in seconds: 1446372.510\n",
      "epoch: 241500, train loss: 3.898881220817566, val loss: 3.9668940544128417, ETA in seconds: 1446753.842\n",
      "epoch: 241600, train loss: 3.8857625484466554, val loss: 3.9633158683776855, ETA in seconds: 1447132.657\n",
      "epoch: 241700, train loss: 3.881129574775696, val loss: 3.965895915031433, ETA in seconds: 1447525.193\n",
      "epoch: 241800, train loss: 3.887682390213013, val loss: 3.973900890350342, ETA in seconds: 1447908.011\n",
      "epoch: 241900, train loss: 3.8900724172592165, val loss: 3.966788411140442, ETA in seconds: 1448271.632\n",
      "epoch: 242000, train loss: 3.898923921585083, val loss: 3.9768311262130736, ETA in seconds: 1448645.742\n",
      "epoch: 242100, train loss: 3.8898725748062133, val loss: 3.9713286399841308, ETA in seconds: 1449022.825\n",
      "epoch: 242200, train loss: 3.896726679801941, val loss: 3.9653936862945556, ETA in seconds: 1449428.359\n",
      "epoch: 242300, train loss: 3.8924809217453005, val loss: 3.967703342437744, ETA in seconds: 1449915.998\n",
      "epoch: 242400, train loss: 3.8924068212509155, val loss: 3.979184293746948, ETA in seconds: 1450408.655\n",
      "epoch: 242500, train loss: 3.90257306098938, val loss: 3.9755149364471434, ETA in seconds: 1450896.988\n",
      "epoch: 242600, train loss: 3.9026220083236693, val loss: 3.972015643119812, ETA in seconds: 1451382.996\n",
      "epoch: 242700, train loss: 3.888372707366943, val loss: 3.9746067762374877, ETA in seconds: 1451868.742\n",
      "epoch: 242800, train loss: 3.8931875944137575, val loss: 3.982079291343689, ETA in seconds: 1452364.485\n",
      "epoch: 242900, train loss: 3.8828839302062987, val loss: 3.970210409164429, ETA in seconds: 1452863.995\n",
      "epoch: 243000, train loss: 3.8948774099349976, val loss: 3.9681665182113646, ETA in seconds: 1453360.017\n",
      "epoch: 243100, train loss: 3.8885426998138426, val loss: 3.968273639678955, ETA in seconds: 1453850.351\n",
      "epoch: 243200, train loss: 3.8940529584884644, val loss: 3.9678346633911135, ETA in seconds: 1454335.589\n",
      "epoch: 243300, train loss: 3.894731116294861, val loss: 3.978308892250061, ETA in seconds: 1454827.162\n",
      "epoch: 243400, train loss: 3.9023950338363647, val loss: 3.972997856140137, ETA in seconds: 1455315.799\n",
      "epoch: 243500, train loss: 3.90242395401001, val loss: 3.963823652267456, ETA in seconds: 1455803.590\n",
      "epoch: 243600, train loss: 3.900673174858093, val loss: 3.9829195499420167, ETA in seconds: 1456299.369\n",
      "epoch: 243700, train loss: 3.892955994606018, val loss: 3.9816794633865356, ETA in seconds: 1456786.406\n",
      "epoch: 243800, train loss: 3.8898884057998657, val loss: 3.9759466648101807, ETA in seconds: 1457183.603\n",
      "epoch: 243900, train loss: 3.8900854349136353, val loss: 3.9634501934051514, ETA in seconds: 1457548.417\n",
      "epoch: 244000, train loss: 3.8912288188934325, val loss: 3.976005482673645, ETA in seconds: 1457902.696\n",
      "epoch: 244100, train loss: 3.891547918319702, val loss: 3.981197547912598, ETA in seconds: 1458276.980\n",
      "epoch: 244200, train loss: 3.883009171485901, val loss: 3.969808077812195, ETA in seconds: 1458658.164\n",
      "epoch: 244300, train loss: 3.890490460395813, val loss: 3.972724461555481, ETA in seconds: 1459035.901\n",
      "epoch: 244400, train loss: 3.896145558357239, val loss: 3.9675994634628298, ETA in seconds: 1459408.158\n",
      "epoch: 244500, train loss: 3.87982759475708, val loss: 3.968843960762024, ETA in seconds: 1459773.210\n",
      "epoch: 244600, train loss: 3.8918912172317506, val loss: 3.97319118976593, ETA in seconds: 1460137.714\n",
      "epoch: 244700, train loss: 3.8943776845932008, val loss: 3.9787447929382322, ETA in seconds: 1460522.313\n",
      "epoch: 244800, train loss: 3.896302676200867, val loss: 3.97206072807312, ETA in seconds: 1460904.060\n",
      "epoch: 244900, train loss: 3.8994954109191893, val loss: 3.9613731145858764, ETA in seconds: 1461319.430\n",
      "epoch: 245000, train loss: 3.889803647994995, val loss: 3.9628432273864744, ETA in seconds: 1461715.925\n",
      "epoch: 245100, train loss: 3.8793156147003174, val loss: 3.9662960529327393, ETA in seconds: 1462200.187\n",
      "epoch: 245200, train loss: 3.8869678020477294, val loss: 3.981620955467224, ETA in seconds: 1462607.976\n",
      "epoch: 245300, train loss: 3.890739893913269, val loss: 3.9798223257064818, ETA in seconds: 1462981.753\n",
      "epoch: 245400, train loss: 3.8909682035446167, val loss: 3.9579821825027466, ETA in seconds: 1463348.552\n",
      "epoch: 245500, train loss: 3.8925456047058105, val loss: 3.9641170263290406, ETA in seconds: 1463732.646\n",
      "epoch: 245600, train loss: 3.8954830169677734, val loss: 3.968466877937317, ETA in seconds: 1464120.314\n",
      "epoch: 245700, train loss: 3.8954802989959716, val loss: 3.961055779457092, ETA in seconds: 1464554.767\n",
      "epoch: 245800, train loss: 3.8905640840530396, val loss: 3.9656761646270753, ETA in seconds: 1464967.673\n",
      "epoch: 245900, train loss: 3.8854459285736085, val loss: 3.9685734510421753, ETA in seconds: 1465348.867\n",
      "epoch: 246000, train loss: 3.8920469284057617, val loss: 3.961451840400696, ETA in seconds: 1465731.666\n",
      "epoch: 246100, train loss: 3.8855902194976806, val loss: 3.971812534332275, ETA in seconds: 1466111.969\n",
      "epoch: 246200, train loss: 3.8921764850616456, val loss: 3.9683521509170534, ETA in seconds: 1466508.919\n",
      "epoch: 246300, train loss: 3.893390154838562, val loss: 3.965499997138977, ETA in seconds: 1466891.782\n",
      "epoch: 246400, train loss: 3.890341353416443, val loss: 3.9544153928756716, ETA in seconds: 1467262.557\n",
      "epoch: 246500, train loss: 3.898105573654175, val loss: 3.971209764480591, ETA in seconds: 1467631.841\n",
      "epoch: 246600, train loss: 3.893703889846802, val loss: 3.9707533359527587, ETA in seconds: 1468004.059\n",
      "epoch: 246700, train loss: 3.891929006576538, val loss: 3.96371705532074, ETA in seconds: 1468381.885\n",
      "epoch: 246800, train loss: 3.8972791910171507, val loss: 3.981285881996155, ETA in seconds: 1468770.542\n",
      "epoch: 246900, train loss: 3.899872303009033, val loss: 3.970037007331848, ETA in seconds: 1469154.049\n",
      "epoch: 247000, train loss: 3.90195255279541, val loss: 3.9608679533004763, ETA in seconds: 1469556.641\n",
      "epoch: 247100, train loss: 3.8892739057540893, val loss: 3.9678632736206056, ETA in seconds: 1469939.676\n",
      "epoch: 247200, train loss: 3.8973040342330934, val loss: 3.9646931886672974, ETA in seconds: 1470319.236\n",
      "epoch: 247300, train loss: 3.8936622381210326, val loss: 3.96951961517334, ETA in seconds: 1470701.862\n",
      "epoch: 247400, train loss: 3.9014737129211428, val loss: 3.9684567213058473, ETA in seconds: 1471084.271\n",
      "epoch: 247500, train loss: 3.8902139186859133, val loss: 3.9608107566833497, ETA in seconds: 1471464.930\n",
      "epoch: 247600, train loss: 3.8964688777923584, val loss: 3.9671422481536864, ETA in seconds: 1471848.293\n",
      "epoch: 247700, train loss: 3.884192872047424, val loss: 3.9713263511657715, ETA in seconds: 1472230.569\n",
      "epoch: 247800, train loss: 3.8978700160980226, val loss: 3.9600502490997314, ETA in seconds: 1472601.733\n",
      "epoch: 247900, train loss: 3.8894146919250487, val loss: 3.9615942001342774, ETA in seconds: 1472981.557\n",
      "epoch: 248000, train loss: 3.8936113834381105, val loss: 3.966548728942871, ETA in seconds: 1473341.000\n",
      "epoch: 248100, train loss: 3.89028160572052, val loss: 3.9628998756408693, ETA in seconds: 1473721.976\n",
      "epoch: 248200, train loss: 3.896440029144287, val loss: 3.9774914264678953, ETA in seconds: 1474107.715\n",
      "epoch: 248300, train loss: 3.8950436115264893, val loss: 3.9611630916595457, ETA in seconds: 1474486.265\n",
      "epoch: 248400, train loss: 3.886299967765808, val loss: 3.97249550819397, ETA in seconds: 1474870.327\n",
      "epoch: 248500, train loss: 3.8930213928222654, val loss: 3.9693857431411743, ETA in seconds: 1475244.949\n",
      "epoch: 248600, train loss: 3.892794704437256, val loss: 3.975216436386108, ETA in seconds: 1475629.921\n",
      "epoch: 248700, train loss: 3.90234751701355, val loss: 3.9649091005325316, ETA in seconds: 1476011.683\n",
      "epoch: 248800, train loss: 3.894169807434082, val loss: 3.965584135055542, ETA in seconds: 1476399.978\n",
      "epoch: 248900, train loss: 3.8873334884643556, val loss: 3.963629388809204, ETA in seconds: 1476782.115\n",
      "epoch: 249000, train loss: 3.890768599510193, val loss: 3.9801960945129395, ETA in seconds: 1477156.769\n",
      "epoch: 249100, train loss: 3.8900988340377807, val loss: 3.9752530336380003, ETA in seconds: 1477539.676\n",
      "epoch: 249200, train loss: 3.8919753789901734, val loss: 3.9710570096969606, ETA in seconds: 1477990.093\n",
      "epoch: 249300, train loss: 3.8915661573410034, val loss: 3.977379631996155, ETA in seconds: 1478475.468\n",
      "epoch: 249400, train loss: 3.8968053579330446, val loss: 3.977545976638794, ETA in seconds: 1478867.618\n",
      "epoch: 249500, train loss: 3.8980733156204224, val loss: 3.9713098764419557, ETA in seconds: 1479247.545\n",
      "epoch: 249600, train loss: 3.892940807342529, val loss: 3.9691145420074463, ETA in seconds: 1479630.199\n",
      "epoch: 249700, train loss: 3.8948482990264894, val loss: 3.968854475021362, ETA in seconds: 1480013.580\n",
      "epoch: 249800, train loss: 3.891271877288818, val loss: 3.9754223108291624, ETA in seconds: 1480406.441\n",
      "epoch: 249900, train loss: 3.8965336322784423, val loss: 3.9665968656539916, ETA in seconds: 1480809.269\n",
      "epoch: 250000, train loss: 3.887075662612915, val loss: 3.9686957120895388, ETA in seconds: 1481324.825\n",
      "epoch: 250100, train loss: 3.8906502723693848, val loss: 3.9677584886550905, ETA in seconds: 1481820.670\n",
      "epoch: 250200, train loss: 3.8909479856491087, val loss: 3.9666532278060913, ETA in seconds: 1482257.130\n",
      "epoch: 250300, train loss: 3.9011475324630736, val loss: 3.970594954490662, ETA in seconds: 1482654.308\n",
      "epoch: 250400, train loss: 3.8830618381500246, val loss: 3.973953866958618, ETA in seconds: 1483113.981\n",
      "epoch: 250500, train loss: 3.8898822546005247, val loss: 3.9655070066452027, ETA in seconds: 1483504.730\n",
      "epoch: 250600, train loss: 3.890530323982239, val loss: 3.9750059604644776, ETA in seconds: 1483894.673\n",
      "epoch: 250700, train loss: 3.894335627555847, val loss: 3.9721143007278443, ETA in seconds: 1484273.038\n",
      "epoch: 250800, train loss: 3.8976826906204223, val loss: 3.961434006690979, ETA in seconds: 1484650.022\n",
      "epoch: 250900, train loss: 3.889024496078491, val loss: 3.970023441314697, ETA in seconds: 1485032.409\n",
      "epoch: 251000, train loss: 3.8891116619110107, val loss: 3.952407121658325, ETA in seconds: 1485408.881\n",
      "epoch: 251100, train loss: 3.904778814315796, val loss: 3.9774736642837523, ETA in seconds: 1485820.879\n",
      "epoch: 251200, train loss: 3.9031802654266357, val loss: 3.9714053869247437, ETA in seconds: 1486206.231\n",
      "epoch: 251300, train loss: 3.895793104171753, val loss: 3.9697840213775635, ETA in seconds: 1486646.136\n",
      "epoch: 251400, train loss: 3.891010785102844, val loss: 3.976061964035034, ETA in seconds: 1487121.201\n",
      "epoch: 251500, train loss: 3.8998023748397825, val loss: 3.9701751947402952, ETA in seconds: 1487597.717\n",
      "epoch: 251600, train loss: 3.9005758285522463, val loss: 3.9667526483535767, ETA in seconds: 1488055.111\n",
      "epoch: 251700, train loss: 3.890920329093933, val loss: 3.9590287446975707, ETA in seconds: 1488488.069\n",
      "epoch: 251800, train loss: 3.8992297887802123, val loss: 3.9638070106506347, ETA in seconds: 1488903.651\n",
      "epoch: 251900, train loss: 3.8966943502426146, val loss: 3.975536894798279, ETA in seconds: 1489374.723\n",
      "epoch: 252000, train loss: 3.898635649681091, val loss: 3.9686771869659423, ETA in seconds: 1489761.335\n",
      "epoch: 252100, train loss: 3.892948126792908, val loss: 3.966131830215454, ETA in seconds: 1490164.875\n",
      "epoch: 252200, train loss: 3.887833762168884, val loss: 3.9617944717407227, ETA in seconds: 1490579.195\n",
      "epoch: 252300, train loss: 3.8903319120407103, val loss: 3.9756684064865113, ETA in seconds: 1490928.481\n",
      "epoch: 252400, train loss: 3.9012787342071533, val loss: 3.96505663394928, ETA in seconds: 1491366.462\n",
      "epoch: 252500, train loss: 3.8937154531478884, val loss: 3.973742890357971, ETA in seconds: 1491737.836\n",
      "epoch: 252600, train loss: 3.8966588258743284, val loss: 3.9694018840789793, ETA in seconds: 1492076.666\n",
      "epoch: 252700, train loss: 3.8971906185150145, val loss: 3.9743234872817994, ETA in seconds: 1492486.876\n",
      "epoch: 252800, train loss: 3.8935861110687258, val loss: 3.9687987089157106, ETA in seconds: 1492954.797\n",
      "epoch: 252900, train loss: 3.8944718837738037, val loss: 3.9619882345199584, ETA in seconds: 1493418.199\n",
      "epoch: 253000, train loss: 3.901648831367493, val loss: 3.9742236137390137, ETA in seconds: 1493887.470\n",
      "epoch: 253100, train loss: 3.9020607471466064, val loss: 3.977288031578064, ETA in seconds: 1494350.789\n",
      "epoch: 253200, train loss: 3.892686295509338, val loss: 3.967932033538818, ETA in seconds: 1494769.866\n",
      "epoch: 253300, train loss: 3.8937288761138915, val loss: 3.967215657234192, ETA in seconds: 1495110.540\n",
      "epoch: 253400, train loss: 3.898545813560486, val loss: 3.962269639968872, ETA in seconds: 1495442.959\n",
      "epoch: 253500, train loss: 3.8954735517501833, val loss: 3.9758046150207518, ETA in seconds: 1495781.413\n",
      "epoch: 253600, train loss: 3.8934760808944704, val loss: 3.9679224729537963, ETA in seconds: 1496152.083\n",
      "epoch: 253700, train loss: 3.9025752782821654, val loss: 3.9707494497299196, ETA in seconds: 1496593.353\n",
      "epoch: 253800, train loss: 3.8911972761154177, val loss: 3.980658221244812, ETA in seconds: 1496953.940\n",
      "epoch: 253900, train loss: 3.8933324098587034, val loss: 3.9810611724853517, ETA in seconds: 1497352.240\n",
      "epoch: 254000, train loss: 3.8877333641052245, val loss: 3.969181990623474, ETA in seconds: 1497749.886\n",
      "epoch: 254100, train loss: 3.9067538499832155, val loss: 3.9716418743133546, ETA in seconds: 1498099.672\n",
      "epoch: 254200, train loss: 3.899511456489563, val loss: 3.970282196998596, ETA in seconds: 1498491.078\n",
      "epoch: 254300, train loss: 3.9029202699661254, val loss: 3.9717400550842283, ETA in seconds: 1498912.211\n",
      "epoch: 254400, train loss: 3.8958839893341066, val loss: 3.9585869789123533, ETA in seconds: 1499266.610\n",
      "epoch: 254500, train loss: 3.8967625856399537, val loss: 3.9611212253570556, ETA in seconds: 1499654.484\n",
      "epoch: 254600, train loss: 3.905639147758484, val loss: 3.9686286211013795, ETA in seconds: 1500120.429\n",
      "epoch: 254700, train loss: 3.893580412864685, val loss: 3.965966248512268, ETA in seconds: 1500588.446\n",
      "epoch: 254800, train loss: 3.8934066534042358, val loss: 3.9660791873931887, ETA in seconds: 1500962.567\n",
      "epoch: 254900, train loss: 3.891163945198059, val loss: 3.971674942970276, ETA in seconds: 1501309.750\n",
      "epoch: 255000, train loss: 3.896807837486267, val loss: 3.968478488922119, ETA in seconds: 1501644.446\n",
      "epoch: 255100, train loss: 3.898362469673157, val loss: 3.9666947603225706, ETA in seconds: 1501994.250\n",
      "epoch: 255200, train loss: 3.8988686561584474, val loss: 3.9767751455307008, ETA in seconds: 1502414.389\n",
      "epoch: 255300, train loss: 3.88534791469574, val loss: 3.9628702878952025, ETA in seconds: 1502918.848\n",
      "epoch: 255400, train loss: 3.8911940336227415, val loss: 3.971346950531006, ETA in seconds: 1503422.138\n",
      "epoch: 255500, train loss: 3.8913095474243162, val loss: 3.9776032686233522, ETA in seconds: 1503737.838\n",
      "epoch: 255600, train loss: 3.8862653493881227, val loss: 3.9668890476226806, ETA in seconds: 1504056.980\n",
      "epoch: 255700, train loss: 3.89194118976593, val loss: 3.9697257041931153, ETA in seconds: 1504427.895\n",
      "epoch: 255800, train loss: 3.902017021179199, val loss: 3.965194058418274, ETA in seconds: 1504935.257\n",
      "epoch: 255900, train loss: 3.905551290512085, val loss: 3.9769031047821044, ETA in seconds: 1505347.340\n",
      "epoch: 256000, train loss: 3.9015678644180296, val loss: 3.953328084945679, ETA in seconds: 1505697.879\n",
      "epoch: 256100, train loss: 3.8983697652816773, val loss: 3.972906708717346, ETA in seconds: 1506045.782\n",
      "epoch: 256200, train loss: 3.8915597200393677, val loss: 3.9802319288253782, ETA in seconds: 1506390.057\n",
      "epoch: 256300, train loss: 3.902498745918274, val loss: 3.960677671432495, ETA in seconds: 1506738.995\n",
      "epoch: 256400, train loss: 3.8989935159683227, val loss: 3.9704729557037353, ETA in seconds: 1507104.403\n",
      "epoch: 256500, train loss: 3.887158513069153, val loss: 3.9727059841156005, ETA in seconds: 1507461.082\n",
      "epoch: 256600, train loss: 3.901924562454224, val loss: 3.9686273336410522, ETA in seconds: 1507817.183\n",
      "epoch: 256700, train loss: 3.8896129846572878, val loss: 3.977246141433716, ETA in seconds: 1508163.201\n",
      "epoch: 256800, train loss: 3.8990772008895873, val loss: 3.9769198179244993, ETA in seconds: 1508497.078\n",
      "epoch: 256900, train loss: 3.904750394821167, val loss: 3.974067759513855, ETA in seconds: 1508827.932\n",
      "epoch: 257000, train loss: 3.889790415763855, val loss: 3.9749800682067873, ETA in seconds: 1509167.488\n",
      "epoch: 257100, train loss: 3.8976629257202147, val loss: 3.97709743976593, ETA in seconds: 1509585.619\n",
      "epoch: 257200, train loss: 3.8903554677963257, val loss: 3.967961001396179, ETA in seconds: 1509914.550\n",
      "epoch: 257300, train loss: 3.8957213163375854, val loss: 3.973904538154602, ETA in seconds: 1510275.291\n",
      "epoch: 257400, train loss: 3.8932112216949464, val loss: 3.966518688201904, ETA in seconds: 1510729.835\n",
      "epoch: 257500, train loss: 3.8972477436065676, val loss: 3.9664698600769044, ETA in seconds: 1511185.544\n",
      "epoch: 257600, train loss: 3.8986541986465455, val loss: 3.9632530212402344, ETA in seconds: 1511643.100\n",
      "epoch: 257700, train loss: 3.897341823577881, val loss: 3.966414475440979, ETA in seconds: 1512003.510\n",
      "epoch: 257800, train loss: 3.8942034006118775, val loss: 3.9607792615890505, ETA in seconds: 1512515.554\n",
      "epoch: 257900, train loss: 3.892058253288269, val loss: 3.9727413177490236, ETA in seconds: 1513021.328\n",
      "epoch: 258000, train loss: 3.895786499977112, val loss: 3.9714781522750853, ETA in seconds: 1513466.447\n",
      "epoch: 258100, train loss: 3.895752692222595, val loss: 3.9645634651184083, ETA in seconds: 1513949.032\n",
      "epoch: 258200, train loss: 3.89772515296936, val loss: 3.973482942581177, ETA in seconds: 1514272.358\n",
      "epoch: 258300, train loss: 3.894147801399231, val loss: 3.9813970565795898, ETA in seconds: 1514586.670\n",
      "epoch: 258400, train loss: 3.8914734601974486, val loss: 3.9806772232055665, ETA in seconds: 1514916.644\n",
      "epoch: 258500, train loss: 3.882210612297058, val loss: 3.9703870534896852, ETA in seconds: 1515253.591\n",
      "epoch: 258600, train loss: 3.8898253440856934, val loss: 3.9689918041229246, ETA in seconds: 1515594.323\n",
      "epoch: 258700, train loss: 3.8874524354934694, val loss: 3.9679046630859376, ETA in seconds: 1515952.820\n",
      "epoch: 258800, train loss: 3.8982091426849363, val loss: 3.9716999769210815, ETA in seconds: 1516318.446\n",
      "epoch: 258900, train loss: 3.895062470436096, val loss: 3.9753501415252686, ETA in seconds: 1516679.134\n",
      "epoch: 259000, train loss: 3.896108865737915, val loss: 3.9762642860412596, ETA in seconds: 1517064.856\n",
      "epoch: 259100, train loss: 3.9001362323760986, val loss: 3.96714768409729, ETA in seconds: 1517535.673\n",
      "epoch: 259200, train loss: 3.8899303674697876, val loss: 3.967223811149597, ETA in seconds: 1517998.956\n",
      "epoch: 259300, train loss: 3.898797297477722, val loss: 3.96818151473999, ETA in seconds: 1518496.828\n",
      "epoch: 259400, train loss: 3.893424320220947, val loss: 3.970121955871582, ETA in seconds: 1519017.601\n",
      "epoch: 259500, train loss: 3.9038966417312624, val loss: 3.974381136894226, ETA in seconds: 1519535.887\n",
      "epoch: 259600, train loss: 3.892777705192566, val loss: 3.9712093830108643, ETA in seconds: 1520058.181\n",
      "epoch: 259700, train loss: 3.8907583236694334, val loss: 3.9727397918701173, ETA in seconds: 1520583.335\n",
      "epoch: 259800, train loss: 3.8974080801010134, val loss: 3.9709533929824827, ETA in seconds: 1521086.717\n",
      "epoch: 259900, train loss: 3.8994649171829225, val loss: 3.9679982900619506, ETA in seconds: 1521559.931\n",
      "epoch: 260000, train loss: 3.890434217453003, val loss: 3.9644375801086427, ETA in seconds: 1521893.480\n",
      "epoch: 260100, train loss: 3.894650387763977, val loss: 3.9679962396621704, ETA in seconds: 1522224.587\n",
      "epoch: 260200, train loss: 3.89826078414917, val loss: 3.96760950088501, ETA in seconds: 1522552.022\n",
      "epoch: 260300, train loss: 3.899937081336975, val loss: 3.9741096973419188, ETA in seconds: 1522897.509\n",
      "epoch: 260400, train loss: 3.8914292097091674, val loss: 3.9733928203582765, ETA in seconds: 1523237.006\n",
      "epoch: 260500, train loss: 3.8894695043563843, val loss: 3.963059139251709, ETA in seconds: 1523604.638\n",
      "epoch: 260600, train loss: 3.886145758628845, val loss: 3.9651270866394044, ETA in seconds: 1523998.273\n",
      "epoch: 260700, train loss: 3.8947492122650145, val loss: 3.9694897651672365, ETA in seconds: 1524368.284\n",
      "epoch: 260800, train loss: 3.8909737825393678, val loss: 3.976781797409058, ETA in seconds: 1524737.072\n",
      "epoch: 260900, train loss: 3.898068976402283, val loss: 3.971798801422119, ETA in seconds: 1525088.443\n",
      "epoch: 261000, train loss: 3.8993860483169556, val loss: 3.9715605974197388, ETA in seconds: 1525517.572\n",
      "epoch: 261100, train loss: 3.8981507539749147, val loss: 3.970996069908142, ETA in seconds: 1525867.847\n",
      "epoch: 261200, train loss: 3.896031355857849, val loss: 3.9704968214035032, ETA in seconds: 1526226.803\n",
      "epoch: 261300, train loss: 3.8991309642791747, val loss: 3.9719229459762575, ETA in seconds: 1526582.741\n",
      "epoch: 261400, train loss: 3.8864559650421144, val loss: 3.973156547546387, ETA in seconds: 1526970.807\n",
      "epoch: 261500, train loss: 3.8889006853103636, val loss: 3.9641024351119993, ETA in seconds: 1527470.114\n",
      "epoch: 261600, train loss: 3.89742488861084, val loss: 3.9728611946105956, ETA in seconds: 1527960.389\n",
      "epoch: 261700, train loss: 3.894252824783325, val loss: 3.9661381959915163, ETA in seconds: 1528432.876\n",
      "epoch: 261800, train loss: 3.89559919834137, val loss: 3.975242495536804, ETA in seconds: 1528902.143\n",
      "epoch: 261900, train loss: 3.8914123296737673, val loss: 3.9565954923629763, ETA in seconds: 1529373.361\n",
      "epoch: 262000, train loss: 3.8954092025756837, val loss: 3.9693700551986693, ETA in seconds: 1529834.715\n",
      "epoch: 262100, train loss: 3.896713638305664, val loss: 3.9697460889816285, ETA in seconds: 1530252.997\n",
      "epoch: 262200, train loss: 3.890843057632446, val loss: 3.9699792146682737, ETA in seconds: 1530597.593\n",
      "epoch: 262300, train loss: 3.899956154823303, val loss: 3.9643084049224853, ETA in seconds: 1530991.674\n",
      "epoch: 262400, train loss: 3.9056335210800173, val loss: 3.9657304525375365, ETA in seconds: 1531383.102\n",
      "epoch: 262500, train loss: 3.896609926223755, val loss: 3.9716882705688477, ETA in seconds: 1531729.257\n",
      "epoch: 262600, train loss: 3.9028945207595824, val loss: 3.967377519607544, ETA in seconds: 1532061.533\n",
      "epoch: 262700, train loss: 3.8854235649108886, val loss: 3.9597252130508425, ETA in seconds: 1532452.676\n",
      "epoch: 262800, train loss: 3.890826916694641, val loss: 3.9743741750717163, ETA in seconds: 1532908.529\n",
      "epoch: 262900, train loss: 3.895029640197754, val loss: 3.9798167943954468, ETA in seconds: 1533372.561\n",
      "epoch: 263000, train loss: 3.8883140087127686, val loss: 3.9750073194503783, ETA in seconds: 1533828.451\n",
      "epoch: 263100, train loss: 3.8949719667434692, val loss: 3.9698109149932863, ETA in seconds: 1534283.646\n",
      "epoch: 263200, train loss: 3.894583535194397, val loss: 3.974122977256775, ETA in seconds: 1534738.762\n",
      "epoch: 263300, train loss: 3.8929505586624145, val loss: 3.98158278465271, ETA in seconds: 1535194.458\n",
      "epoch: 263400, train loss: 3.900844764709473, val loss: 3.96471107006073, ETA in seconds: 1535647.906\n",
      "epoch: 263500, train loss: 3.892016386985779, val loss: 3.9675902366638183, ETA in seconds: 1536108.495\n",
      "epoch: 263600, train loss: 3.8937584400177, val loss: 3.9569668054580687, ETA in seconds: 1536563.508\n",
      "epoch: 263700, train loss: 3.897141194343567, val loss: 3.9730227470397947, ETA in seconds: 1537018.215\n",
      "epoch: 263800, train loss: 3.8879483461380007, val loss: 3.9639997482299805, ETA in seconds: 1537478.045\n",
      "epoch: 263900, train loss: 3.893388271331787, val loss: 3.967530703544617, ETA in seconds: 1537932.311\n",
      "epoch: 264000, train loss: 3.8881526470184324, val loss: 3.9774582862854, ETA in seconds: 1538384.426\n",
      "epoch: 264100, train loss: 3.889939284324646, val loss: 3.9646388053894044, ETA in seconds: 1538850.953\n",
      "epoch: 264200, train loss: 3.886863088607788, val loss: 3.9744486093521116, ETA in seconds: 1539302.991\n",
      "epoch: 264300, train loss: 3.897500228881836, val loss: 3.976751923561096, ETA in seconds: 1539753.576\n",
      "epoch: 264400, train loss: 3.894052028656006, val loss: 3.9796016931533815, ETA in seconds: 1540205.766\n",
      "epoch: 264500, train loss: 3.8949918031692503, val loss: 3.9676733016967773, ETA in seconds: 1540656.606\n",
      "epoch: 264600, train loss: 3.8882714033126833, val loss: 3.9770140647888184, ETA in seconds: 1541107.157\n",
      "epoch: 264700, train loss: 3.899093437194824, val loss: 3.9778235912323, ETA in seconds: 1541511.216\n",
      "epoch: 264800, train loss: 3.8886738061904906, val loss: 3.9675153255462647, ETA in seconds: 1541839.677\n",
      "epoch: 264900, train loss: 3.8972341299057005, val loss: 3.971162700653076, ETA in seconds: 1542162.807\n",
      "epoch: 265000, train loss: 3.8999057531356813, val loss: 3.9707140207290648, ETA in seconds: 1542487.692\n",
      "epoch: 265100, train loss: 3.894747090339661, val loss: 3.976841425895691, ETA in seconds: 1542822.703\n",
      "epoch: 265200, train loss: 3.8905508518218994, val loss: 3.967090368270874, ETA in seconds: 1543161.381\n",
      "epoch: 265300, train loss: 3.8938745975494387, val loss: 3.969840717315674, ETA in seconds: 1543511.481\n",
      "epoch: 265400, train loss: 3.891704821586609, val loss: 3.9749818563461305, ETA in seconds: 1543918.954\n",
      "epoch: 265500, train loss: 3.8932435274124146, val loss: 3.97157027721405, ETA in seconds: 1544244.794\n",
      "epoch: 265600, train loss: 3.8981368064880373, val loss: 3.982373046875, ETA in seconds: 1544579.609\n",
      "epoch: 265700, train loss: 3.8973371744155885, val loss: 3.976242184638977, ETA in seconds: 1544902.727\n",
      "epoch: 265800, train loss: 3.8919135093688966, val loss: 3.97668616771698, ETA in seconds: 1545226.054\n",
      "epoch: 265900, train loss: 3.8894537925720214, val loss: 3.967125916481018, ETA in seconds: 1545585.348\n",
      "epoch: 266000, train loss: 3.890552449226379, val loss: 3.9644065141677856, ETA in seconds: 1545946.193\n",
      "epoch: 266100, train loss: 3.897205686569214, val loss: 3.9678914308547975, ETA in seconds: 1546303.224\n",
      "epoch: 266200, train loss: 3.889922332763672, val loss: 3.973963975906372, ETA in seconds: 1546650.094\n",
      "epoch: 266300, train loss: 3.8906973600387573, val loss: 3.9626788854599, ETA in seconds: 1547009.869\n",
      "epoch: 266400, train loss: 3.8827735424041747, val loss: 3.9693627834320067, ETA in seconds: 1547367.028\n",
      "epoch: 266500, train loss: 3.8941192388534547, val loss: 3.973527431488037, ETA in seconds: 1547725.906\n",
      "epoch: 266600, train loss: 3.892782378196716, val loss: 3.9635024547576903, ETA in seconds: 1548085.339\n",
      "epoch: 266700, train loss: 3.8849915266036987, val loss: 3.972083020210266, ETA in seconds: 1548443.389\n",
      "epoch: 266800, train loss: 3.881095123291016, val loss: 3.9798902750015257, ETA in seconds: 1548838.407\n",
      "epoch: 266900, train loss: 3.895098090171814, val loss: 3.9680704832077027, ETA in seconds: 1549199.123\n",
      "epoch: 267000, train loss: 3.8853780746459963, val loss: 3.974598932266235, ETA in seconds: 1549543.504\n",
      "epoch: 267100, train loss: 3.8957711458206177, val loss: 3.9744055032730103, ETA in seconds: 1549898.445\n",
      "epoch: 267200, train loss: 3.8967501878738404, val loss: 3.9694125652313232, ETA in seconds: 1550359.834\n",
      "epoch: 267300, train loss: 3.896309971809387, val loss: 3.960635018348694, ETA in seconds: 1550835.666\n",
      "epoch: 267400, train loss: 3.8883519172668457, val loss: 3.9655635833740233, ETA in seconds: 1551320.830\n",
      "epoch: 267500, train loss: 3.883650469779968, val loss: 3.9675513029098513, ETA in seconds: 1551795.755\n",
      "epoch: 267600, train loss: 3.900610065460205, val loss: 3.9627342939376833, ETA in seconds: 1552275.939\n",
      "epoch: 267700, train loss: 3.899061155319214, val loss: 3.968934106826782, ETA in seconds: 1552761.171\n",
      "epoch: 267800, train loss: 3.9010410070419312, val loss: 3.9715797662734986, ETA in seconds: 1553247.357\n",
      "epoch: 267900, train loss: 3.8955877304077147, val loss: 3.971270537376404, ETA in seconds: 1553690.342\n",
      "epoch: 268000, train loss: 3.895715069770813, val loss: 3.962929058074951, ETA in seconds: 1554116.503\n",
      "epoch: 268100, train loss: 3.8916441917419435, val loss: 3.969397020339966, ETA in seconds: 1554558.091\n",
      "epoch: 268200, train loss: 3.8911204814910887, val loss: 3.9569828033447267, ETA in seconds: 1554999.168\n",
      "epoch: 268300, train loss: 3.8833297967910765, val loss: 3.9593986749649046, ETA in seconds: 1555357.080\n",
      "epoch: 268400, train loss: 3.889477753639221, val loss: 3.9818269491195677, ETA in seconds: 1555697.515\n",
      "epoch: 268500, train loss: 3.892037296295166, val loss: 3.962523913383484, ETA in seconds: 1556013.451\n",
      "epoch: 268600, train loss: 3.8954946756362916, val loss: 3.9734148740768434, ETA in seconds: 1556339.751\n",
      "epoch: 268700, train loss: 3.89200758934021, val loss: 3.963636374473572, ETA in seconds: 1556680.219\n",
      "epoch: 268800, train loss: 3.903526043891907, val loss: 3.965361213684082, ETA in seconds: 1557000.370\n",
      "epoch: 268900, train loss: 3.9000573635101317, val loss: 3.965072226524353, ETA in seconds: 1557324.651\n",
      "epoch: 269000, train loss: 3.895129179954529, val loss: 3.9692988634109496, ETA in seconds: 1557640.013\n",
      "epoch: 269100, train loss: 3.8851316213607787, val loss: 3.9688879251480103, ETA in seconds: 1557963.881\n",
      "epoch: 269200, train loss: 3.8973589181900024, val loss: 3.9672032833099364, ETA in seconds: 1558287.339\n",
      "epoch: 269300, train loss: 3.9036076068878174, val loss: 3.963617539405823, ETA in seconds: 1558609.294\n",
      "epoch: 269400, train loss: 3.8989401340484617, val loss: 3.9699265241622923, ETA in seconds: 1558939.457\n",
      "epoch: 269500, train loss: 3.8933633089065554, val loss: 3.989948201179504, ETA in seconds: 1559265.626\n",
      "epoch: 269600, train loss: 3.893515205383301, val loss: 3.9766356706619264, ETA in seconds: 1559581.270\n",
      "epoch: 269700, train loss: 3.89257755279541, val loss: 3.966538333892822, ETA in seconds: 1559907.516\n",
      "epoch: 269800, train loss: 3.886732292175293, val loss: 3.969025444984436, ETA in seconds: 1560237.028\n",
      "epoch: 269900, train loss: 3.8812798976898195, val loss: 3.9693753242492678, ETA in seconds: 1560557.344\n",
      "epoch: 270000, train loss: 3.8971194982528687, val loss: 3.9715813398361206, ETA in seconds: 1560874.278\n",
      "epoch: 270100, train loss: 3.8838847875595093, val loss: 3.9713229894638062, ETA in seconds: 1561212.808\n",
      "epoch: 270200, train loss: 3.913301134109497, val loss: 3.9718041896820067, ETA in seconds: 1561549.970\n",
      "epoch: 270300, train loss: 3.899327802658081, val loss: 3.9818824529647827, ETA in seconds: 1561875.903\n",
      "epoch: 270400, train loss: 3.889268946647644, val loss: 3.9764321088790893, ETA in seconds: 1562183.618\n",
      "epoch: 270500, train loss: 3.89461350440979, val loss: 3.9772871255874636, ETA in seconds: 1562496.833\n",
      "epoch: 270600, train loss: 3.9020167112350466, val loss: 3.9572615385055543, ETA in seconds: 1562824.257\n",
      "epoch: 270700, train loss: 3.898296666145325, val loss: 3.975862717628479, ETA in seconds: 1563153.336\n",
      "epoch: 270800, train loss: 3.8914257049560548, val loss: 3.974204754829407, ETA in seconds: 1563493.828\n",
      "epoch: 270900, train loss: 3.8946781396865844, val loss: 3.9797713279724123, ETA in seconds: 1563829.403\n",
      "epoch: 271000, train loss: 3.87974956035614, val loss: 3.970737171173096, ETA in seconds: 1564147.125\n",
      "epoch: 271100, train loss: 3.8939019680023192, val loss: 3.9729159593582155, ETA in seconds: 1564475.695\n",
      "epoch: 271200, train loss: 3.898329329490662, val loss: 3.9765865087509153, ETA in seconds: 1564789.111\n",
      "epoch: 271300, train loss: 3.8957439661026, val loss: 3.975568699836731, ETA in seconds: 1565100.746\n",
      "epoch: 271400, train loss: 3.900084376335144, val loss: 3.9762088537216185, ETA in seconds: 1565496.813\n",
      "epoch: 271500, train loss: 3.8912776470184327, val loss: 3.968432092666626, ETA in seconds: 1565817.350\n",
      "epoch: 271600, train loss: 3.8809850215911865, val loss: 3.9728328943252564, ETA in seconds: 1566157.262\n",
      "epoch: 271700, train loss: 3.8939931869506834, val loss: 3.9725274085998534, ETA in seconds: 1566484.059\n",
      "epoch: 271800, train loss: 3.9047914028167723, val loss: 3.9785773754119873, ETA in seconds: 1566792.384\n",
      "epoch: 271900, train loss: 3.9013826131820677, val loss: 3.9702381134033202, ETA in seconds: 1567108.085\n",
      "epoch: 272000, train loss: 3.8939551591873167, val loss: 3.9760457277297974, ETA in seconds: 1567426.857\n",
      "epoch: 272100, train loss: 3.8870765447616575, val loss: 3.979914689064026, ETA in seconds: 1567743.689\n",
      "epoch: 272200, train loss: 3.889668917655945, val loss: 3.973441410064697, ETA in seconds: 1568057.283\n",
      "epoch: 272300, train loss: 3.8961375951766968, val loss: 3.9878907680511473, ETA in seconds: 1568375.957\n",
      "epoch: 272400, train loss: 3.9058826684951784, val loss: 3.9800071001052855, ETA in seconds: 1568808.422\n",
      "epoch: 272500, train loss: 3.8901660680770873, val loss: 3.9806517362594604, ETA in seconds: 1569145.640\n",
      "epoch: 272600, train loss: 3.8791209936141966, val loss: 3.978697896003723, ETA in seconds: 1569462.001\n",
      "epoch: 272700, train loss: 3.895905303955078, val loss: 3.9758761405944822, ETA in seconds: 1569776.258\n",
      "epoch: 272800, train loss: 3.901180601119995, val loss: 3.9711551904678344, ETA in seconds: 1570090.150\n",
      "epoch: 272900, train loss: 3.8920063018798827, val loss: 3.9807825088500977, ETA in seconds: 1570407.945\n",
      "epoch: 273000, train loss: 3.8860068321228027, val loss: 3.9825933456420897, ETA in seconds: 1570734.412\n",
      "epoch: 273100, train loss: 3.8922834396362305, val loss: 3.979932498931885, ETA in seconds: 1571074.286\n",
      "epoch: 273200, train loss: 3.884351968765259, val loss: 3.9814237117767335, ETA in seconds: 1571439.609\n",
      "epoch: 273300, train loss: 3.8885345220565797, val loss: 3.977298402786255, ETA in seconds: 1571740.303\n",
      "epoch: 273400, train loss: 3.894693660736084, val loss: 3.9722777605056763, ETA in seconds: 1572062.822\n",
      "epoch: 273500, train loss: 3.892994594573975, val loss: 3.9766204357147217, ETA in seconds: 1572381.833\n",
      "epoch: 273600, train loss: 3.8877028226852417, val loss: 3.9758336544036865, ETA in seconds: 1572708.187\n",
      "epoch: 273700, train loss: 3.8901962518692015, val loss: 3.9847179412841798, ETA in seconds: 1573039.574\n",
      "epoch: 273800, train loss: 3.899608850479126, val loss: 3.970477032661438, ETA in seconds: 1573350.149\n",
      "epoch: 273900, train loss: 3.892235016822815, val loss: 3.969011902809143, ETA in seconds: 1573651.904\n",
      "epoch: 274000, train loss: 3.8887064456939697, val loss: 3.9709972858428957, ETA in seconds: 1573962.111\n",
      "epoch: 274100, train loss: 3.8965376138687136, val loss: 3.979119968414307, ETA in seconds: 1574281.622\n",
      "epoch: 274200, train loss: 3.894493079185486, val loss: 3.9605278730392457, ETA in seconds: 1574592.425\n",
      "epoch: 274300, train loss: 3.8880405187606812, val loss: 3.9674113035202025, ETA in seconds: 1574908.524\n",
      "epoch: 274400, train loss: 3.90000319480896, val loss: 3.9637956857681274, ETA in seconds: 1575227.527\n",
      "epoch: 274500, train loss: 3.892158055305481, val loss: 3.9822229623794554, ETA in seconds: 1575533.990\n",
      "epoch: 274600, train loss: 3.890983295440674, val loss: 3.9709193468093873, ETA in seconds: 1575841.382\n",
      "epoch: 274700, train loss: 3.895412564277649, val loss: 3.9735756874084474, ETA in seconds: 1576148.549\n",
      "epoch: 274800, train loss: 3.895208144187927, val loss: 3.965836000442505, ETA in seconds: 1576453.973\n",
      "epoch: 274900, train loss: 3.8866629362106324, val loss: 3.9728898286819456, ETA in seconds: 1576764.384\n",
      "epoch: 275000, train loss: 3.8973610639572143, val loss: 3.9642669439315794, ETA in seconds: 1577074.433\n",
      "epoch: 275100, train loss: 3.89336359500885, val loss: 3.961488199234009, ETA in seconds: 1577382.501\n",
      "epoch: 275200, train loss: 3.8898486375808714, val loss: 3.9713064432144165, ETA in seconds: 1577700.756\n",
      "epoch: 275300, train loss: 3.884778642654419, val loss: 3.973235082626343, ETA in seconds: 1578007.108\n",
      "epoch: 275400, train loss: 3.8939281702041626, val loss: 3.9669981241226195, ETA in seconds: 1578320.606\n",
      "epoch: 275500, train loss: 3.8939155101776124, val loss: 3.97434344291687, ETA in seconds: 1578679.956\n",
      "epoch: 275600, train loss: 3.891525888442993, val loss: 3.9719783782958986, ETA in seconds: 1579113.089\n",
      "epoch: 275700, train loss: 3.892463707923889, val loss: 3.9604214906692503, ETA in seconds: 1579512.815\n",
      "epoch: 275800, train loss: 3.8961853981018066, val loss: 3.9650388240814207, ETA in seconds: 1579822.978\n",
      "epoch: 275900, train loss: 3.8988077640533447, val loss: 3.963028812408447, ETA in seconds: 1580133.458\n",
      "epoch: 276000, train loss: 3.8926310777664184, val loss: 3.954877519607544, ETA in seconds: 1580435.681\n",
      "epoch: 276100, train loss: 3.896790385246277, val loss: 3.9593928813934327, ETA in seconds: 1580729.653\n",
      "epoch: 276200, train loss: 3.892115592956543, val loss: 3.973220133781433, ETA in seconds: 1581015.585\n",
      "epoch: 276300, train loss: 3.8870391130447386, val loss: 3.9615545511245727, ETA in seconds: 1581303.081\n",
      "epoch: 276400, train loss: 3.891293096542358, val loss: 3.9703400135040283, ETA in seconds: 1581605.422\n",
      "epoch: 276500, train loss: 3.899880528450012, val loss: 3.9667090654373167, ETA in seconds: 1581924.152\n",
      "epoch: 276600, train loss: 3.8872875452041624, val loss: 3.9684901237487793, ETA in seconds: 1582326.292\n",
      "epoch: 276700, train loss: 3.891250967979431, val loss: 3.963252902030945, ETA in seconds: 1582631.189\n",
      "epoch: 276800, train loss: 3.903430151939392, val loss: 3.963205075263977, ETA in seconds: 1582920.686\n",
      "epoch: 276900, train loss: 3.89301974773407, val loss: 3.972720003128052, ETA in seconds: 1583224.107\n",
      "epoch: 277000, train loss: 3.8914742946624754, val loss: 3.972370481491089, ETA in seconds: 1583529.785\n",
      "epoch: 277100, train loss: 3.8883241176605225, val loss: 3.972623014450073, ETA in seconds: 1583818.091\n",
      "epoch: 277200, train loss: 3.899692106246948, val loss: 3.967984199523926, ETA in seconds: 1584109.179\n",
      "epoch: 277300, train loss: 3.8975422620773315, val loss: 3.957680082321167, ETA in seconds: 1584409.823\n",
      "epoch: 277400, train loss: 3.8914041757583617, val loss: 3.978825402259827, ETA in seconds: 1584714.246\n",
      "epoch: 277500, train loss: 3.8953917980194093, val loss: 3.965445566177368, ETA in seconds: 1585008.484\n",
      "epoch: 277600, train loss: 3.8918516397476197, val loss: 3.9749382019042967, ETA in seconds: 1585307.968\n",
      "epoch: 277700, train loss: 3.894149827957153, val loss: 3.966528224945068, ETA in seconds: 1585608.290\n",
      "epoch: 277800, train loss: 3.8901774883270264, val loss: 3.973177742958069, ETA in seconds: 1585912.234\n",
      "epoch: 277900, train loss: 3.9002286911010744, val loss: 3.9701475620269777, ETA in seconds: 1586224.319\n",
      "epoch: 278000, train loss: 3.897346019744873, val loss: 3.9656800508499144, ETA in seconds: 1586529.085\n",
      "epoch: 278100, train loss: 3.8924546003341676, val loss: 3.9598626613616945, ETA in seconds: 1586844.172\n",
      "epoch: 278200, train loss: 3.900885891914368, val loss: 3.9754408836364745, ETA in seconds: 1587147.690\n",
      "epoch: 278300, train loss: 3.8931918144226074, val loss: 3.964837837219238, ETA in seconds: 1587448.567\n",
      "epoch: 278400, train loss: 3.8913237333297728, val loss: 3.9701130628585815, ETA in seconds: 1587746.599\n",
      "epoch: 278500, train loss: 3.8913331031799316, val loss: 3.9712867736816406, ETA in seconds: 1588047.528\n",
      "epoch: 278600, train loss: 3.8878658771514893, val loss: 3.9652297735214233, ETA in seconds: 1588360.569\n",
      "epoch: 278700, train loss: 3.8913717031478883, val loss: 3.970229697227478, ETA in seconds: 1588671.424\n",
      "epoch: 278800, train loss: 3.8965551137924193, val loss: 3.9704009532928466, ETA in seconds: 1589139.245\n",
      "epoch: 278900, train loss: 3.8976505994796753, val loss: 3.980250883102417, ETA in seconds: 1589467.977\n",
      "epoch: 279000, train loss: 3.8984607458114624, val loss: 3.9791950941085816, ETA in seconds: 1589790.586\n",
      "epoch: 279100, train loss: 3.8991767168045044, val loss: 3.9682560205459594, ETA in seconds: 1590106.960\n",
      "epoch: 279200, train loss: 3.900379943847656, val loss: 3.965561318397522, ETA in seconds: 1590417.423\n",
      "epoch: 279300, train loss: 3.8979231834411623, val loss: 3.9677743911743164, ETA in seconds: 1590724.478\n",
      "epoch: 279400, train loss: 3.8957265853881835, val loss: 3.9791433334350588, ETA in seconds: 1591045.517\n",
      "epoch: 279500, train loss: 3.889260697364807, val loss: 3.9670645952224732, ETA in seconds: 1591385.115\n",
      "epoch: 279600, train loss: 3.890509605407715, val loss: 3.9752235651016234, ETA in seconds: 1591695.641\n",
      "epoch: 279700, train loss: 3.889716124534607, val loss: 3.9750524759292603, ETA in seconds: 1592013.160\n",
      "epoch: 279800, train loss: 3.8879972457885743, val loss: 3.974348258972168, ETA in seconds: 1592341.809\n",
      "epoch: 279900, train loss: 3.8862038373947145, val loss: 3.9705621242523192, ETA in seconds: 1592652.762\n",
      "epoch: 280000, train loss: 3.8914868593215943, val loss: 3.9826451778411864, ETA in seconds: 1592956.783\n",
      "epoch: 280100, train loss: 3.8946155071258546, val loss: 3.9664386987686155, ETA in seconds: 1593257.334\n",
      "epoch: 280200, train loss: 3.8854104042053224, val loss: 3.9691343545913695, ETA in seconds: 1593612.874\n",
      "epoch: 280300, train loss: 3.8931812763214113, val loss: 3.969140100479126, ETA in seconds: 1593943.512\n",
      "epoch: 280400, train loss: 3.8910130739212034, val loss: 3.982356095314026, ETA in seconds: 1594249.460\n",
      "epoch: 280500, train loss: 3.8937850475311278, val loss: 3.97623987197876, ETA in seconds: 1594572.045\n",
      "epoch: 280600, train loss: 3.890075373649597, val loss: 3.9727919578552244, ETA in seconds: 1594907.452\n",
      "epoch: 280700, train loss: 3.8804306983947754, val loss: 3.974695014953613, ETA in seconds: 1595199.113\n",
      "epoch: 280800, train loss: 3.8967533826828005, val loss: 3.98222815990448, ETA in seconds: 1595508.217\n",
      "epoch: 280900, train loss: 3.8925698518753054, val loss: 3.9735328912734986, ETA in seconds: 1595835.075\n",
      "epoch: 281000, train loss: 3.8941969156265257, val loss: 3.9692895889282225, ETA in seconds: 1596141.814\n",
      "epoch: 281100, train loss: 3.89213604927063, val loss: 3.956149697303772, ETA in seconds: 1596453.117\n",
      "epoch: 281200, train loss: 3.887169098854065, val loss: 3.9810481548309324, ETA in seconds: 1596777.065\n",
      "epoch: 281300, train loss: 3.889378786087036, val loss: 3.977353405952454, ETA in seconds: 1597110.079\n",
      "epoch: 281400, train loss: 3.895456314086914, val loss: 3.971514916419983, ETA in seconds: 1597438.939\n",
      "epoch: 281500, train loss: 3.888517713546753, val loss: 3.970510387420654, ETA in seconds: 1597767.215\n",
      "epoch: 281600, train loss: 3.896203804016113, val loss: 3.980111241340637, ETA in seconds: 1598091.084\n",
      "epoch: 281700, train loss: 3.8947970867156982, val loss: 3.971806764602661, ETA in seconds: 1598416.421\n",
      "epoch: 281800, train loss: 3.8981151580810547, val loss: 3.959753727912903, ETA in seconds: 1598751.862\n",
      "epoch: 281900, train loss: 3.8939864158630373, val loss: 3.9586811304092406, ETA in seconds: 1599073.810\n",
      "epoch: 282000, train loss: 3.901137900352478, val loss: 3.9722869873046873, ETA in seconds: 1599393.681\n",
      "epoch: 282100, train loss: 3.8923743724823, val loss: 3.978711986541748, ETA in seconds: 1599706.507\n",
      "epoch: 282200, train loss: 3.8875619411468505, val loss: 3.974634575843811, ETA in seconds: 1600014.708\n",
      "epoch: 282300, train loss: 3.8931603908538817, val loss: 3.962157726287842, ETA in seconds: 1600340.518\n",
      "epoch: 282400, train loss: 3.896939826011658, val loss: 3.97994487285614, ETA in seconds: 1600659.244\n",
      "epoch: 282500, train loss: 3.896093225479126, val loss: 3.9676374197006226, ETA in seconds: 1600967.943\n",
      "epoch: 282600, train loss: 3.880828857421875, val loss: 3.972190761566162, ETA in seconds: 1601271.643\n",
      "epoch: 282700, train loss: 3.889505887031555, val loss: 3.9728145837783813, ETA in seconds: 1601576.731\n",
      "epoch: 282800, train loss: 3.8967131614685058, val loss: 3.9708874225616455, ETA in seconds: 1601884.986\n",
      "epoch: 282900, train loss: 3.8930882453918456, val loss: 3.9666433095932008, ETA in seconds: 1602184.262\n",
      "epoch: 283000, train loss: 3.8946592092514036, val loss: 3.9668267011642455, ETA in seconds: 1602491.706\n",
      "epoch: 283100, train loss: 3.894003987312317, val loss: 3.9676902294158936, ETA in seconds: 1602835.243\n",
      "epoch: 283200, train loss: 3.901709246635437, val loss: 3.960417366027832, ETA in seconds: 1603158.334\n",
      "epoch: 283300, train loss: 3.888379621505737, val loss: 3.972746729850769, ETA in seconds: 1603489.487\n",
      "epoch: 283400, train loss: 3.891255521774292, val loss: 3.971811366081238, ETA in seconds: 1603798.341\n",
      "epoch: 283500, train loss: 3.896695137023926, val loss: 3.9663094758987425, ETA in seconds: 1604100.611\n",
      "epoch: 283600, train loss: 3.896550440788269, val loss: 3.9672332763671876, ETA in seconds: 1604405.151\n",
      "epoch: 283700, train loss: 3.8888946056365965, val loss: 3.9707377672195436, ETA in seconds: 1604714.133\n",
      "epoch: 283800, train loss: 3.892706108093262, val loss: 3.9742849349975584, ETA in seconds: 1605021.886\n",
      "epoch: 283900, train loss: 3.8949570178985597, val loss: 3.970356488227844, ETA in seconds: 1605345.260\n",
      "epoch: 284000, train loss: 3.9008108139038087, val loss: 3.960581731796265, ETA in seconds: 1605674.918\n",
      "epoch: 284100, train loss: 3.8979252099990847, val loss: 3.982891821861267, ETA in seconds: 1606002.184\n",
      "epoch: 284200, train loss: 3.893326830863953, val loss: 3.971497082710266, ETA in seconds: 1606331.813\n",
      "epoch: 284300, train loss: 3.9004024505615233, val loss: 3.95980224609375, ETA in seconds: 1606648.087\n",
      "epoch: 284400, train loss: 3.8931122541427614, val loss: 3.962408113479614, ETA in seconds: 1606973.251\n",
      "epoch: 284500, train loss: 3.887799382209778, val loss: 3.9637085437774657, ETA in seconds: 1607343.561\n",
      "epoch: 284600, train loss: 3.896190118789673, val loss: 3.9705161094665526, ETA in seconds: 1607656.816\n",
      "epoch: 284700, train loss: 3.888735270500183, val loss: 3.9740182161331177, ETA in seconds: 1607965.655\n",
      "epoch: 284800, train loss: 3.892179775238037, val loss: 3.9624191522598267, ETA in seconds: 1608289.346\n",
      "epoch: 284900, train loss: 3.889632749557495, val loss: 3.9752020835876465, ETA in seconds: 1608722.399\n",
      "epoch: 285000, train loss: 3.881934332847595, val loss: 3.988814949989319, ETA in seconds: 1609156.845\n",
      "epoch: 285100, train loss: 3.8827301263809204, val loss: 3.968602776527405, ETA in seconds: 1609547.691\n",
      "epoch: 285200, train loss: 3.8953700065612793, val loss: 3.9786178827285767, ETA in seconds: 1609849.281\n",
      "epoch: 285300, train loss: 3.8927366971969604, val loss: 3.9666447162628176, ETA in seconds: 1610130.121\n",
      "epoch: 285400, train loss: 3.886014699935913, val loss: 3.967931389808655, ETA in seconds: 1610421.556\n",
      "epoch: 285500, train loss: 3.890987730026245, val loss: 3.9561134576797485, ETA in seconds: 1610718.855\n",
      "epoch: 285600, train loss: 3.8820937871932983, val loss: 3.9612298250198363, ETA in seconds: 1611011.141\n",
      "epoch: 285700, train loss: 3.892213988304138, val loss: 3.9649837255477904, ETA in seconds: 1611308.326\n",
      "epoch: 285800, train loss: 3.906742811203003, val loss: 3.9681927442550657, ETA in seconds: 1611600.154\n",
      "epoch: 285900, train loss: 3.892332410812378, val loss: 3.9821078062057493, ETA in seconds: 1611905.856\n",
      "epoch: 286000, train loss: 3.8949158906936647, val loss: 3.972920775413513, ETA in seconds: 1612212.214\n",
      "epoch: 286100, train loss: 3.889730525016785, val loss: 3.9717992305755616, ETA in seconds: 1612500.235\n",
      "epoch: 286200, train loss: 3.9020013093948362, val loss: 3.982172441482544, ETA in seconds: 1612792.431\n",
      "epoch: 286300, train loss: 3.8979616165161133, val loss: 3.9711306571960447, ETA in seconds: 1613088.512\n",
      "epoch: 286400, train loss: 3.885949659347534, val loss: 3.9772090196609495, ETA in seconds: 1613388.597\n",
      "epoch: 286500, train loss: 3.9055745601654053, val loss: 3.9566184759140013, ETA in seconds: 1613702.838\n",
      "epoch: 286600, train loss: 3.895120358467102, val loss: 3.9626389265060427, ETA in seconds: 1614004.166\n",
      "epoch: 286700, train loss: 3.8921324968338014, val loss: 3.971280813217163, ETA in seconds: 1614297.033\n",
      "epoch: 286800, train loss: 3.890522074699402, val loss: 3.9653069972991943, ETA in seconds: 1614587.755\n",
      "epoch: 286900, train loss: 3.9007737398147584, val loss: 3.9687172889709474, ETA in seconds: 1614871.873\n",
      "epoch: 287000, train loss: 3.891409158706665, val loss: 3.97692551612854, ETA in seconds: 1615167.641\n",
      "epoch: 287100, train loss: 3.8887927532196045, val loss: 3.9687870502471925, ETA in seconds: 1615464.474\n",
      "epoch: 287200, train loss: 3.888405919075012, val loss: 3.9715140581130983, ETA in seconds: 1615765.491\n",
      "epoch: 287300, train loss: 3.897672653198242, val loss: 3.9746148347854615, ETA in seconds: 1616076.125\n",
      "epoch: 287400, train loss: 3.895398497581482, val loss: 3.9700594186782836, ETA in seconds: 1616378.267\n",
      "epoch: 287500, train loss: 3.890388011932373, val loss: 3.975352239608765, ETA in seconds: 1616674.460\n",
      "epoch: 287600, train loss: 3.9010961294174193, val loss: 3.9699248313903808, ETA in seconds: 1616983.457\n",
      "epoch: 287700, train loss: 3.8889101266860964, val loss: 3.9694501161575317, ETA in seconds: 1617264.104\n",
      "epoch: 287800, train loss: 3.8949236392974855, val loss: 3.9626608371734617, ETA in seconds: 1617572.466\n",
      "epoch: 287900, train loss: 3.8923162460327148, val loss: 3.958607625961304, ETA in seconds: 1617928.305\n",
      "epoch: 288000, train loss: 3.8865726947784425, val loss: 3.9703165769577025, ETA in seconds: 1618214.939\n",
      "epoch: 288100, train loss: 3.9011361598968506, val loss: 3.955467414855957, ETA in seconds: 1618504.755\n",
      "epoch: 288200, train loss: 3.896964693069458, val loss: 3.9784559488296507, ETA in seconds: 1618793.877\n",
      "epoch: 288300, train loss: 3.899513864517212, val loss: 3.9640634775161745, ETA in seconds: 1619105.520\n",
      "epoch: 288400, train loss: 3.8852237939834593, val loss: 3.968554401397705, ETA in seconds: 1619407.189\n",
      "epoch: 288500, train loss: 3.891923761367798, val loss: 3.964424395561218, ETA in seconds: 1619858.838\n",
      "epoch: 288600, train loss: 3.9025402784347536, val loss: 3.964595675468445, ETA in seconds: 1620306.671\n",
      "epoch: 288700, train loss: 3.8909510135650636, val loss: 3.9624401330947876, ETA in seconds: 1620680.292\n",
      "epoch: 288800, train loss: 3.890450644493103, val loss: 3.976526880264282, ETA in seconds: 1620995.741\n",
      "epoch: 288900, train loss: 3.893761086463928, val loss: 3.9764685153961183, ETA in seconds: 1621312.556\n",
      "epoch: 289000, train loss: 3.8840343236923216, val loss: 3.957099509239197, ETA in seconds: 1621618.032\n",
      "epoch: 289100, train loss: 3.8906247854232787, val loss: 3.9670949220657348, ETA in seconds: 1621984.542\n",
      "epoch: 289200, train loss: 3.9000659227371215, val loss: 3.971260166168213, ETA in seconds: 1622449.471\n",
      "epoch: 289300, train loss: 3.893276262283325, val loss: 3.959619951248169, ETA in seconds: 1622870.497\n",
      "epoch: 289400, train loss: 3.883600187301636, val loss: 3.974556303024292, ETA in seconds: 1623286.764\n",
      "epoch: 289500, train loss: 3.8864434957504272, val loss: 3.9715911626815794, ETA in seconds: 1623701.884\n",
      "epoch: 289600, train loss: 3.890553832054138, val loss: 3.964879608154297, ETA in seconds: 1624124.225\n",
      "epoch: 289700, train loss: 3.886461925506592, val loss: 3.95833044052124, ETA in seconds: 1624524.814\n",
      "epoch: 289800, train loss: 3.8896415710449217, val loss: 3.954084849357605, ETA in seconds: 1624814.949\n",
      "epoch: 289900, train loss: 3.89098060131073, val loss: 3.9723506450653074, ETA in seconds: 1625124.821\n",
      "epoch: 290000, train loss: 3.8850645065307616, val loss: 3.987135171890259, ETA in seconds: 1625472.208\n",
      "epoch: 290100, train loss: 3.899043869972229, val loss: 3.966140031814575, ETA in seconds: 1625928.179\n",
      "epoch: 290200, train loss: 3.9012230396270753, val loss: 3.9557642459869387, ETA in seconds: 1626344.428\n",
      "epoch: 290300, train loss: 3.8933876752853394, val loss: 3.966748332977295, ETA in seconds: 1626695.413\n",
      "epoch: 290400, train loss: 3.8944833040237428, val loss: 3.963713812828064, ETA in seconds: 1627017.231\n",
      "epoch: 290500, train loss: 3.8987956285476684, val loss: 3.9657004356384276, ETA in seconds: 1627320.890\n",
      "epoch: 290600, train loss: 3.9012404918670653, val loss: 3.9657342433929443, ETA in seconds: 1627630.476\n",
      "epoch: 290700, train loss: 3.899867844581604, val loss: 3.9741615772247316, ETA in seconds: 1627944.004\n",
      "epoch: 290800, train loss: 3.8985076189041137, val loss: 3.966139578819275, ETA in seconds: 1628276.442\n",
      "epoch: 290900, train loss: 3.8961239576339723, val loss: 3.9763622999191286, ETA in seconds: 1628585.925\n",
      "epoch: 291000, train loss: 3.8946522951126097, val loss: 3.961695098876953, ETA in seconds: 1628882.852\n",
      "epoch: 291100, train loss: 3.8891846179962157, val loss: 3.965236282348633, ETA in seconds: 1629182.091\n",
      "epoch: 291200, train loss: 3.910841631889343, val loss: 3.9673248052597048, ETA in seconds: 1629497.117\n",
      "epoch: 291300, train loss: 3.8951874732971192, val loss: 3.9665304899215696, ETA in seconds: 1629809.211\n",
      "epoch: 291400, train loss: 3.891334128379822, val loss: 3.974321985244751, ETA in seconds: 1630110.658\n",
      "epoch: 291500, train loss: 3.891697096824646, val loss: 3.96602201461792, ETA in seconds: 1630456.351\n",
      "epoch: 291600, train loss: 3.8908276081085207, val loss: 3.9721107959747313, ETA in seconds: 1630773.228\n",
      "epoch: 291700, train loss: 3.8940713167190553, val loss: 3.964729642868042, ETA in seconds: 1631064.945\n",
      "epoch: 291800, train loss: 3.898044753074646, val loss: 3.9663142919540406, ETA in seconds: 1631363.395\n",
      "epoch: 291900, train loss: 3.8953976154327394, val loss: 3.9726091861724853, ETA in seconds: 1631640.341\n",
      "epoch: 292000, train loss: 3.888189363479614, val loss: 3.9666258096694946, ETA in seconds: 1632015.185\n",
      "epoch: 292100, train loss: 3.891930079460144, val loss: 3.9664530754089355, ETA in seconds: 1632329.892\n",
      "epoch: 292200, train loss: 3.88524067401886, val loss: 3.970409893989563, ETA in seconds: 1632658.357\n",
      "epoch: 292300, train loss: 3.8927114248275756, val loss: 3.9557165145874023, ETA in seconds: 1632977.014\n",
      "epoch: 292400, train loss: 3.8916775941848756, val loss: 3.957996678352356, ETA in seconds: 1633287.440\n",
      "epoch: 292500, train loss: 3.8949042320251466, val loss: 3.9602011680603026, ETA in seconds: 1633580.114\n",
      "epoch: 292600, train loss: 3.901350474357605, val loss: 3.9545902967453004, ETA in seconds: 1633872.254\n",
      "epoch: 292700, train loss: 3.8921590566635134, val loss: 3.9697012901306152, ETA in seconds: 1634176.055\n",
      "epoch: 292800, train loss: 3.8834670066833494, val loss: 3.966592884063721, ETA in seconds: 1634463.416\n",
      "epoch: 292900, train loss: 3.892347502708435, val loss: 3.9696247816085815, ETA in seconds: 1634764.721\n",
      "epoch: 293000, train loss: 3.8895272731781008, val loss: 3.968308520317078, ETA in seconds: 1635074.529\n",
      "epoch: 293100, train loss: 3.889881801605225, val loss: 3.9729047298431395, ETA in seconds: 1635372.183\n",
      "epoch: 293200, train loss: 3.8962554693222047, val loss: 3.971005988121033, ETA in seconds: 1635677.223\n",
      "epoch: 293300, train loss: 3.890951132774353, val loss: 3.9643734216690065, ETA in seconds: 1635966.757\n",
      "epoch: 293400, train loss: 3.885033702850342, val loss: 3.961346459388733, ETA in seconds: 1636270.825\n",
      "epoch: 293500, train loss: 3.8941691160202025, val loss: 3.966353249549866, ETA in seconds: 1636568.110\n",
      "epoch: 293600, train loss: 3.892476034164429, val loss: 3.9632933855056764, ETA in seconds: 1636896.980\n",
      "epoch: 293700, train loss: 3.8866909742355347, val loss: 3.9696367979049683, ETA in seconds: 1637206.421\n",
      "epoch: 293800, train loss: 3.894841718673706, val loss: 3.972904992103577, ETA in seconds: 1637537.197\n",
      "epoch: 293900, train loss: 3.896845483779907, val loss: 3.9741489410400392, ETA in seconds: 1637853.530\n",
      "epoch: 294000, train loss: 3.894152069091797, val loss: 3.9647592544555663, ETA in seconds: 1638122.172\n",
      "epoch: 294100, train loss: 3.897292137145996, val loss: 3.9656745195388794, ETA in seconds: 1638435.350\n",
      "epoch: 294200, train loss: 3.8880508184432983, val loss: 3.978182005882263, ETA in seconds: 1638741.644\n",
      "epoch: 294300, train loss: 3.8859214544296266, val loss: 3.968925642967224, ETA in seconds: 1639044.439\n",
      "epoch: 294400, train loss: 3.8928605556488036, val loss: 3.9670501232147215, ETA in seconds: 1639374.227\n",
      "epoch: 294500, train loss: 3.8941329956054687, val loss: 3.963047218322754, ETA in seconds: 1639774.265\n",
      "epoch: 294600, train loss: 3.889959931373596, val loss: 3.9654354095458983, ETA in seconds: 1640170.464\n",
      "epoch: 294700, train loss: 3.8985323667526246, val loss: 3.9722636699676515, ETA in seconds: 1640572.383\n",
      "epoch: 294800, train loss: 3.899989438056946, val loss: 3.980594110488892, ETA in seconds: 1640971.095\n",
      "epoch: 294900, train loss: 3.8874992609024046, val loss: 3.975103497505188, ETA in seconds: 1641364.682\n",
      "epoch: 295000, train loss: 3.8953103542327883, val loss: 3.9656291961669923, ETA in seconds: 1641709.157\n",
      "epoch: 295100, train loss: 3.888410711288452, val loss: 3.9620281219482423, ETA in seconds: 1642095.805\n",
      "epoch: 295200, train loss: 3.8951708316802978, val loss: 3.994421696662903, ETA in seconds: 1642496.274\n",
      "epoch: 295300, train loss: 3.903886413574219, val loss: 3.967188763618469, ETA in seconds: 1642898.151\n",
      "epoch: 295400, train loss: 3.9039376258850096, val loss: 3.970494270324707, ETA in seconds: 1643311.025\n",
      "epoch: 295500, train loss: 3.895672106742859, val loss: 3.971863317489624, ETA in seconds: 1643714.238\n",
      "epoch: 295600, train loss: 3.8803627490997314, val loss: 3.9697431564331054, ETA in seconds: 1644087.344\n",
      "epoch: 295700, train loss: 3.8913657426834107, val loss: 3.9586652517318726, ETA in seconds: 1644374.004\n",
      "epoch: 295800, train loss: 3.902946710586548, val loss: 3.9644970417022707, ETA in seconds: 1644660.391\n",
      "epoch: 295900, train loss: 3.8900817394256593, val loss: 3.9642853498458863, ETA in seconds: 1644956.952\n",
      "epoch: 296000, train loss: 3.894185781478882, val loss: 3.9693444967269897, ETA in seconds: 1645335.051\n",
      "epoch: 296100, train loss: 3.9000680446624756, val loss: 3.975652885437012, ETA in seconds: 1645649.166\n",
      "epoch: 296200, train loss: 3.8881789445877075, val loss: 3.9619998931884766, ETA in seconds: 1645929.144\n",
      "epoch: 296300, train loss: 3.909312295913696, val loss: 3.97148802280426, ETA in seconds: 1646203.952\n",
      "epoch: 296400, train loss: 3.8916146516799928, val loss: 3.964106798171997, ETA in seconds: 1646482.681\n",
      "epoch: 296500, train loss: 3.9058975696563722, val loss: 3.966861629486084, ETA in seconds: 1646754.960\n",
      "epoch: 296600, train loss: 3.89691104888916, val loss: 3.9683281660079954, ETA in seconds: 1647046.568\n",
      "epoch: 296700, train loss: 3.8944733142852783, val loss: 3.9677002668380736, ETA in seconds: 1647339.500\n",
      "epoch: 296800, train loss: 3.8924545764923097, val loss: 3.969281244277954, ETA in seconds: 1647647.150\n",
      "epoch: 296900, train loss: 3.8977227926254274, val loss: 3.966226673126221, ETA in seconds: 1647938.112\n",
      "epoch: 297000, train loss: 3.888933801651001, val loss: 3.9776085138320925, ETA in seconds: 1648246.960\n",
      "epoch: 297100, train loss: 3.895576000213623, val loss: 3.972716474533081, ETA in seconds: 1648562.284\n",
      "epoch: 297200, train loss: 3.9017537117004393, val loss: 3.9646260023117064, ETA in seconds: 1648874.810\n",
      "epoch: 297300, train loss: 3.8885677337646483, val loss: 3.9609121322631835, ETA in seconds: 1649181.667\n",
      "epoch: 297400, train loss: 3.8911767482757567, val loss: 3.9627484560012816, ETA in seconds: 1649560.668\n",
      "epoch: 297500, train loss: 3.8917809247970583, val loss: 3.9709184169769287, ETA in seconds: 1649894.184\n",
      "epoch: 297600, train loss: 3.8920767545700072, val loss: 3.9693484544754027, ETA in seconds: 1650265.163\n",
      "epoch: 297700, train loss: 3.8796961307525635, val loss: 3.968013572692871, ETA in seconds: 1650650.176\n",
      "epoch: 297800, train loss: 3.902391457557678, val loss: 3.9618158102035523, ETA in seconds: 1650936.022\n",
      "epoch: 297900, train loss: 3.894432020187378, val loss: 3.975188684463501, ETA in seconds: 1651201.931\n",
      "epoch: 298000, train loss: 3.9044103622436523, val loss: 3.9813132762908934, ETA in seconds: 1651489.013\n",
      "epoch: 298100, train loss: 3.8991124391555787, val loss: 3.9805680990219114, ETA in seconds: 1651850.733\n",
      "epoch: 298200, train loss: 3.902187633514404, val loss: 3.964510607719421, ETA in seconds: 1652128.597\n",
      "epoch: 298300, train loss: 3.8944520473480226, val loss: 3.9808210849761965, ETA in seconds: 1652408.127\n",
      "epoch: 298400, train loss: 3.8858832120895386, val loss: 3.980332350730896, ETA in seconds: 1652677.687\n",
      "epoch: 298500, train loss: 3.891227698326111, val loss: 3.9564043521881103, ETA in seconds: 1653060.003\n",
      "epoch: 298600, train loss: 3.893296551704407, val loss: 3.972458839416504, ETA in seconds: 1653451.647\n",
      "epoch: 298700, train loss: 3.889542031288147, val loss: 3.9729807138442994, ETA in seconds: 1653843.735\n",
      "epoch: 298800, train loss: 3.885325717926025, val loss: 3.9664152383804323, ETA in seconds: 1654239.248\n",
      "epoch: 298900, train loss: 3.894910740852356, val loss: 3.975213360786438, ETA in seconds: 1654633.628\n",
      "epoch: 299000, train loss: 3.887167501449585, val loss: 3.980772376060486, ETA in seconds: 1655042.602\n",
      "epoch: 299100, train loss: 3.8982510805130004, val loss: 3.97257513999939, ETA in seconds: 1655331.596\n",
      "epoch: 299200, train loss: 3.892978572845459, val loss: 3.97942955493927, ETA in seconds: 1655618.418\n",
      "epoch: 299300, train loss: 3.894270825386047, val loss: 3.97492253780365, ETA in seconds: 1655913.211\n",
      "epoch: 299400, train loss: 3.9031065464019776, val loss: 3.970195245742798, ETA in seconds: 1656209.868\n",
      "epoch: 299500, train loss: 3.893374276161194, val loss: 3.9741551637649537, ETA in seconds: 1656500.188\n",
      "epoch: 299600, train loss: 3.892639183998108, val loss: 3.9583186149597167, ETA in seconds: 1656787.279\n",
      "epoch: 299700, train loss: 3.891630434989929, val loss: 3.9664411783218383, ETA in seconds: 1657074.633\n",
      "epoch: 299800, train loss: 3.8897207260131834, val loss: 3.9668663501739503, ETA in seconds: 1657365.224\n",
      "epoch: 299900, train loss: 3.890760350227356, val loss: 3.9666799306869507, ETA in seconds: 1657688.810\n",
      "epoch: 300000, train loss: 3.8896845102310182, val loss: 3.9791409254074095, ETA in seconds: 1657966.614\n",
      "epoch: 300100, train loss: 3.894292688369751, val loss: 3.9701550006866455, ETA in seconds: 1658255.698\n",
      "epoch: 300200, train loss: 3.8960726261138916, val loss: 3.9715417861938476, ETA in seconds: 1658563.315\n",
      "epoch: 300300, train loss: 3.9036599159240724, val loss: 3.9740553379058836, ETA in seconds: 1658965.806\n",
      "epoch: 300400, train loss: 3.89777250289917, val loss: 3.977755546569824, ETA in seconds: 1659310.281\n",
      "epoch: 300500, train loss: 3.8921826601028444, val loss: 3.9670687437057497, ETA in seconds: 1659667.420\n",
      "epoch: 300600, train loss: 3.8917534351348877, val loss: 3.9676407098770143, ETA in seconds: 1660128.536\n",
      "epoch: 300700, train loss: 3.892001509666443, val loss: 3.975277900695801, ETA in seconds: 1660507.361\n",
      "epoch: 300800, train loss: 3.904298949241638, val loss: 3.9726690530776976, ETA in seconds: 1660834.815\n",
      "epoch: 300900, train loss: 3.899075174331665, val loss: 3.9785113096237184, ETA in seconds: 1661153.702\n",
      "epoch: 301000, train loss: 3.8991863250732424, val loss: 3.9803977489471434, ETA in seconds: 1661441.075\n",
      "epoch: 301100, train loss: 3.89592444896698, val loss: 3.972554326057434, ETA in seconds: 1661755.502\n",
      "epoch: 301200, train loss: 3.8852325201034548, val loss: 3.973989486694336, ETA in seconds: 1662079.337\n",
      "epoch: 301300, train loss: 3.8923017740249635, val loss: 3.9776856899261475, ETA in seconds: 1662400.644\n",
      "epoch: 301400, train loss: 3.8929850101470946, val loss: 3.9621138095855715, ETA in seconds: 1662726.414\n",
      "epoch: 301500, train loss: 3.9023481130599977, val loss: 3.963453245162964, ETA in seconds: 1663044.211\n",
      "epoch: 301600, train loss: 3.891191029548645, val loss: 3.975056004524231, ETA in seconds: 1663355.586\n",
      "epoch: 301700, train loss: 3.8883169889450073, val loss: 3.9737932205200197, ETA in seconds: 1663674.702\n",
      "epoch: 301800, train loss: 3.886714482307434, val loss: 3.971734046936035, ETA in seconds: 1663996.673\n",
      "epoch: 301900, train loss: 3.8958377599716187, val loss: 3.968105983734131, ETA in seconds: 1664321.253\n",
      "epoch: 302000, train loss: 3.896142053604126, val loss: 3.9723809003829955, ETA in seconds: 1664632.746\n",
      "epoch: 302100, train loss: 3.9016125202178955, val loss: 3.9869912385940554, ETA in seconds: 1664946.520\n",
      "epoch: 302200, train loss: 3.894177484512329, val loss: 3.964252233505249, ETA in seconds: 1665283.247\n",
      "epoch: 302300, train loss: 3.8920833587646486, val loss: 3.9747996091842652, ETA in seconds: 1665668.876\n",
      "epoch: 302400, train loss: 3.9018533706665037, val loss: 3.970174264907837, ETA in seconds: 1666054.745\n",
      "epoch: 302500, train loss: 3.898807096481323, val loss: 3.9599159479141237, ETA in seconds: 1666449.037\n",
      "epoch: 302600, train loss: 3.891299104690552, val loss: 3.967936062812805, ETA in seconds: 1666819.667\n",
      "epoch: 302700, train loss: 3.8908487796783446, val loss: 3.974114513397217, ETA in seconds: 1667141.619\n",
      "epoch: 302800, train loss: 3.892716073989868, val loss: 3.9783239126205445, ETA in seconds: 1667473.114\n",
      "epoch: 302900, train loss: 3.8883586645126345, val loss: 3.981452536582947, ETA in seconds: 1667841.213\n",
      "epoch: 303000, train loss: 3.8900046348571777, val loss: 3.964299511909485, ETA in seconds: 1668150.184\n",
      "epoch: 303100, train loss: 3.9003002882003783, val loss: 3.975373554229736, ETA in seconds: 1668505.114\n",
      "epoch: 303200, train loss: 3.894286775588989, val loss: 3.9773589611053466, ETA in seconds: 1668808.982\n",
      "epoch: 303300, train loss: 3.894681715965271, val loss: 3.9640607118606566, ETA in seconds: 1669083.950\n",
      "epoch: 303400, train loss: 3.8928484439849855, val loss: 3.966775059700012, ETA in seconds: 1669390.599\n",
      "epoch: 303500, train loss: 3.8885690450668333, val loss: 3.9573099613189697, ETA in seconds: 1669660.531\n",
      "epoch: 303600, train loss: 3.890456509590149, val loss: 3.958680248260498, ETA in seconds: 1669949.172\n",
      "epoch: 303700, train loss: 3.9024695634841917, val loss: 3.9698516368865966, ETA in seconds: 1670251.977\n",
      "epoch: 303800, train loss: 3.8902270078659056, val loss: 3.9776835441589355, ETA in seconds: 1670591.111\n",
      "epoch: 303900, train loss: 3.8972883462905883, val loss: 3.969248342514038, ETA in seconds: 1671003.488\n",
      "epoch: 304000, train loss: 3.892031264305115, val loss: 3.9635979413986204, ETA in seconds: 1671416.091\n",
      "epoch: 304100, train loss: 3.891845440864563, val loss: 3.970240354537964, ETA in seconds: 1671798.724\n",
      "epoch: 304200, train loss: 3.8873031377792358, val loss: 3.962494373321533, ETA in seconds: 1672112.205\n",
      "epoch: 304300, train loss: 3.8940824031829835, val loss: 3.974164533615112, ETA in seconds: 1672477.926\n",
      "epoch: 304400, train loss: 3.8848489999771116, val loss: 3.968550705909729, ETA in seconds: 1672766.682\n",
      "epoch: 304500, train loss: 3.8906697988510133, val loss: 3.9737929105758667, ETA in seconds: 1673057.759\n",
      "epoch: 304600, train loss: 3.887918472290039, val loss: 3.9684846878051756, ETA in seconds: 1673367.731\n",
      "epoch: 304700, train loss: 3.89347562789917, val loss: 3.970154809951782, ETA in seconds: 1673790.773\n",
      "epoch: 304800, train loss: 3.8911730289459228, val loss: 3.9745606422424316, ETA in seconds: 1674218.432\n",
      "epoch: 304900, train loss: 3.898699736595154, val loss: 3.9783896923065187, ETA in seconds: 1674489.714\n",
      "epoch: 305000, train loss: 3.89796142578125, val loss: 3.970694422721863, ETA in seconds: 1674752.120\n",
      "epoch: 305100, train loss: 3.889013481140137, val loss: 3.9633605241775514, ETA in seconds: 1675014.482\n",
      "epoch: 305200, train loss: 3.900181293487549, val loss: 3.96544189453125, ETA in seconds: 1675285.780\n",
      "epoch: 305300, train loss: 3.890098237991333, val loss: 3.964736223220825, ETA in seconds: 1675575.369\n",
      "epoch: 305400, train loss: 3.8920029163360597, val loss: 3.972917675971985, ETA in seconds: 1675865.363\n",
      "epoch: 305500, train loss: 3.8920626878738402, val loss: 3.9702105045318605, ETA in seconds: 1676171.539\n",
      "epoch: 305600, train loss: 3.897801399230957, val loss: 3.9739535570144655, ETA in seconds: 1676531.778\n",
      "epoch: 305700, train loss: 3.895281171798706, val loss: 3.97421612739563, ETA in seconds: 1676888.347\n",
      "epoch: 305800, train loss: 3.894159364700317, val loss: 3.976548194885254, ETA in seconds: 1677170.877\n",
      "epoch: 305900, train loss: 3.89874849319458, val loss: 3.958335614204407, ETA in seconds: 1677529.427\n",
      "epoch: 306000, train loss: 3.88922119140625, val loss: 3.9749181985855104, ETA in seconds: 1677842.135\n",
      "epoch: 306100, train loss: 3.8862202405929565, val loss: 3.9704024314880373, ETA in seconds: 1678171.682\n",
      "epoch: 306200, train loss: 3.890712022781372, val loss: 3.9716626405715942, ETA in seconds: 1678602.156\n",
      "epoch: 306300, train loss: 3.892077112197876, val loss: 3.9610252141952516, ETA in seconds: 1678969.215\n",
      "epoch: 306400, train loss: 3.8891315937042235, val loss: 3.963748741149902, ETA in seconds: 1679359.971\n",
      "epoch: 306500, train loss: 3.8967735290527346, val loss: 3.9538671493530275, ETA in seconds: 1679650.962\n",
      "epoch: 306600, train loss: 3.8999969720840455, val loss: 3.96126766204834, ETA in seconds: 1679937.254\n",
      "epoch: 306700, train loss: 3.882976460456848, val loss: 3.97046754360199, ETA in seconds: 1680215.254\n",
      "epoch: 306800, train loss: 3.8907675981521606, val loss: 3.959144449234009, ETA in seconds: 1680519.726\n",
      "epoch: 306900, train loss: 3.893091416358948, val loss: 3.9665385723114013, ETA in seconds: 1680802.395\n",
      "epoch: 307000, train loss: 3.898586797714233, val loss: 3.9726125955581666, ETA in seconds: 1681090.792\n",
      "epoch: 307100, train loss: 3.885564398765564, val loss: 3.9693384170532227, ETA in seconds: 1681378.463\n",
      "epoch: 307200, train loss: 3.8897965431213377, val loss: 3.9648154735565186, ETA in seconds: 1681660.390\n",
      "epoch: 307300, train loss: 3.898990035057068, val loss: 3.9677422046661377, ETA in seconds: 1681954.529\n",
      "epoch: 307400, train loss: 3.895227837562561, val loss: 3.972710204124451, ETA in seconds: 1682252.252\n",
      "epoch: 307500, train loss: 3.8964986562728883, val loss: 3.9687490224838258, ETA in seconds: 1682546.031\n",
      "epoch: 307600, train loss: 3.9055070161819456, val loss: 3.9666424036026, ETA in seconds: 1682845.631\n",
      "epoch: 307700, train loss: 3.89554443359375, val loss: 3.9775466918945312, ETA in seconds: 1683142.450\n",
      "epoch: 307800, train loss: 3.893485999107361, val loss: 3.9685383558273317, ETA in seconds: 1683432.716\n",
      "epoch: 307900, train loss: 3.9057639122009276, val loss: 3.9691703796386717, ETA in seconds: 1683717.115\n",
      "epoch: 308000, train loss: 3.8869094133377073, val loss: 3.976766896247864, ETA in seconds: 1683996.097\n",
      "epoch: 308100, train loss: 3.8850643157958986, val loss: 3.975620150566101, ETA in seconds: 1684270.379\n",
      "epoch: 308200, train loss: 3.890088939666748, val loss: 3.9788679122924804, ETA in seconds: 1684575.618\n",
      "epoch: 308300, train loss: 3.889006018638611, val loss: 3.9916711807250977, ETA in seconds: 1684831.624\n",
      "epoch: 308400, train loss: 3.8963058948516847, val loss: 3.9732766151428223, ETA in seconds: 1685121.665\n",
      "epoch: 308500, train loss: 3.899118185043335, val loss: 3.9829107522964478, ETA in seconds: 1685396.272\n",
      "epoch: 308600, train loss: 3.8905876874923706, val loss: 3.974405574798584, ETA in seconds: 1685679.708\n",
      "epoch: 308700, train loss: 3.888081908226013, val loss: 3.9808998346328734, ETA in seconds: 1685959.775\n",
      "epoch: 308800, train loss: 3.8935598850250246, val loss: 3.9838149785995483, ETA in seconds: 1686237.058\n",
      "epoch: 308900, train loss: 3.8900176286697388, val loss: 3.978372645378113, ETA in seconds: 1686502.162\n",
      "epoch: 309000, train loss: 3.900170016288757, val loss: 3.9765325784683228, ETA in seconds: 1686768.965\n",
      "epoch: 309100, train loss: 3.8878885984420775, val loss: 3.971097707748413, ETA in seconds: 1687139.222\n",
      "epoch: 309200, train loss: 3.901182675361633, val loss: 3.9690403938293457, ETA in seconds: 1687571.732\n",
      "epoch: 309300, train loss: 3.894164037704468, val loss: 3.9774590969085692, ETA in seconds: 1688002.515\n",
      "epoch: 309400, train loss: 3.896030569076538, val loss: 3.966797137260437, ETA in seconds: 1688441.968\n",
      "epoch: 309500, train loss: 3.8948989629745485, val loss: 3.9748047828674316, ETA in seconds: 1688823.108\n",
      "epoch: 309600, train loss: 3.890697479248047, val loss: 3.968628478050232, ETA in seconds: 1689254.793\n",
      "epoch: 309700, train loss: 3.891064977645874, val loss: 3.9616303205490113, ETA in seconds: 1689536.534\n",
      "epoch: 309800, train loss: 3.892839193344116, val loss: 3.9722480297088625, ETA in seconds: 1689810.520\n",
      "epoch: 309900, train loss: 3.8950733661651613, val loss: 3.974454164505005, ETA in seconds: 1690103.490\n",
      "epoch: 310000, train loss: 3.8962258815765383, val loss: 3.9691004276275637, ETA in seconds: 1690412.353\n",
      "epoch: 310100, train loss: 3.8980875492095945, val loss: 3.9682601928710937, ETA in seconds: 1690670.647\n",
      "epoch: 310200, train loss: 3.89552218914032, val loss: 3.975919246673584, ETA in seconds: 1690928.075\n",
      "epoch: 310300, train loss: 3.9017212629318236, val loss: 3.9728358030319213, ETA in seconds: 1691181.655\n",
      "epoch: 310400, train loss: 3.9017791748046875, val loss: 3.9742952585220337, ETA in seconds: 1691434.136\n",
      "epoch: 310500, train loss: 3.8891164541244505, val loss: 3.9830466508865356, ETA in seconds: 1691691.535\n",
      "epoch: 310600, train loss: 3.896606779098511, val loss: 3.98169481754303, ETA in seconds: 1691964.332\n",
      "epoch: 310700, train loss: 3.8860440015792848, val loss: 3.981304717063904, ETA in seconds: 1692242.047\n",
      "epoch: 310800, train loss: 3.8898608684539795, val loss: 3.964898467063904, ETA in seconds: 1692503.794\n",
      "epoch: 310900, train loss: 3.8886961460113527, val loss: 3.978069019317627, ETA in seconds: 1692771.354\n",
      "epoch: 311000, train loss: 3.884363317489624, val loss: 3.9632260322570803, ETA in seconds: 1693036.320\n",
      "epoch: 311100, train loss: 3.8974044561386108, val loss: 3.965592098236084, ETA in seconds: 1693293.527\n",
      "epoch: 311200, train loss: 3.8981483459472654, val loss: 3.963384580612183, ETA in seconds: 1693555.799\n",
      "epoch: 311300, train loss: 3.890917110443115, val loss: 3.968874979019165, ETA in seconds: 1693825.860\n",
      "epoch: 311400, train loss: 3.8863210439682008, val loss: 3.9718818426132203, ETA in seconds: 1694097.741\n",
      "epoch: 311500, train loss: 3.887013006210327, val loss: 3.970664310455322, ETA in seconds: 1694372.092\n",
      "epoch: 311600, train loss: 3.8898929595947265, val loss: 3.960271382331848, ETA in seconds: 1694654.357\n",
      "epoch: 311700, train loss: 3.895616865158081, val loss: 3.9623590469360352, ETA in seconds: 1694918.480\n",
      "epoch: 311800, train loss: 3.8900470495224, val loss: 3.9808043003082276, ETA in seconds: 1695221.965\n",
      "epoch: 311900, train loss: 3.902389168739319, val loss: 3.971812629699707, ETA in seconds: 1695489.368\n",
      "epoch: 312000, train loss: 3.8886595726013184, val loss: 3.9772968769073485, ETA in seconds: 1695749.791\n",
      "epoch: 312100, train loss: 3.8886598110198975, val loss: 3.9716776847839355, ETA in seconds: 1696039.192\n",
      "epoch: 312200, train loss: 3.9001070499420165, val loss: 3.974169659614563, ETA in seconds: 1696443.701\n",
      "epoch: 312300, train loss: 3.8773657083511353, val loss: 3.968660569190979, ETA in seconds: 1696844.994\n",
      "epoch: 312400, train loss: 3.9032242774963377, val loss: 3.9731944799423218, ETA in seconds: 1697215.030\n",
      "epoch: 312500, train loss: 3.8962090969085694, val loss: 3.955767607688904, ETA in seconds: 1697551.818\n",
      "epoch: 312600, train loss: 3.8905882596969605, val loss: 3.977593755722046, ETA in seconds: 1697833.501\n",
      "epoch: 312700, train loss: 3.8971065998077394, val loss: 3.961365842819214, ETA in seconds: 1698130.580\n",
      "epoch: 312800, train loss: 3.8852869987487795, val loss: 3.978892517089844, ETA in seconds: 1698446.170\n",
      "epoch: 312900, train loss: 3.889741826057434, val loss: 3.960615396499634, ETA in seconds: 1698753.952\n",
      "epoch: 313000, train loss: 3.898256230354309, val loss: 3.965641236305237, ETA in seconds: 1699024.268\n",
      "epoch: 313100, train loss: 3.881999707221985, val loss: 3.9775465726852417, ETA in seconds: 1699309.481\n",
      "epoch: 313200, train loss: 3.898240828514099, val loss: 3.9587738037109377, ETA in seconds: 1699556.143\n",
      "epoch: 313300, train loss: 3.9019793272018433, val loss: 3.9670804023742674, ETA in seconds: 1699812.513\n",
      "epoch: 313400, train loss: 3.885668134689331, val loss: 3.9747456073760987, ETA in seconds: 1700134.410\n",
      "epoch: 313500, train loss: 3.902627182006836, val loss: 3.9711377143859865, ETA in seconds: 1700392.867\n",
      "epoch: 313600, train loss: 3.895545172691345, val loss: 3.9759193897247314, ETA in seconds: 1700660.838\n",
      "epoch: 313700, train loss: 3.887831974029541, val loss: 3.9707149028778077, ETA in seconds: 1700907.147\n",
      "epoch: 313800, train loss: 3.8960199117660523, val loss: 3.9774625062942506, ETA in seconds: 1701163.816\n",
      "epoch: 313900, train loss: 3.8939898014068604, val loss: 3.962198758125305, ETA in seconds: 1701437.317\n",
      "epoch: 314000, train loss: 3.8889095306396486, val loss: 3.967141580581665, ETA in seconds: 1701694.751\n",
      "epoch: 314100, train loss: 3.88234658241272, val loss: 3.9687976598739625, ETA in seconds: 1701950.449\n",
      "epoch: 314200, train loss: 3.884949970245361, val loss: 3.9707141637802126, ETA in seconds: 1702239.519\n",
      "epoch: 314300, train loss: 3.893136978149414, val loss: 3.9762650966644286, ETA in seconds: 1702491.617\n",
      "epoch: 314400, train loss: 3.9014867305755616, val loss: 3.9655843496322634, ETA in seconds: 1702733.623\n",
      "epoch: 314500, train loss: 3.893686032295227, val loss: 3.9694865703582765, ETA in seconds: 1702979.435\n",
      "epoch: 314600, train loss: 3.901516056060791, val loss: 3.9730486631393434, ETA in seconds: 1703227.089\n",
      "epoch: 314700, train loss: 3.8956841230392456, val loss: 3.970454549789429, ETA in seconds: 1703481.141\n",
      "epoch: 314800, train loss: 3.896588349342346, val loss: 3.9696348667144776, ETA in seconds: 1703754.671\n",
      "epoch: 314900, train loss: 3.8774847030639648, val loss: 3.9791677713394167, ETA in seconds: 1704121.552\n",
      "epoch: 315000, train loss: 3.905941915512085, val loss: 3.976710891723633, ETA in seconds: 1704422.724\n",
      "epoch: 315100, train loss: 3.8836097717285156, val loss: 3.96747362613678, ETA in seconds: 1704710.072\n",
      "epoch: 315200, train loss: 3.9009610652923583, val loss: 3.975874590873718, ETA in seconds: 1704969.026\n",
      "epoch: 315300, train loss: 3.8952669858932496, val loss: 3.9806628465652465, ETA in seconds: 1705237.646\n",
      "epoch: 315400, train loss: 3.895166349411011, val loss: 3.9765849828720095, ETA in seconds: 1705493.838\n",
      "epoch: 315500, train loss: 3.887773299217224, val loss: 3.971795439720154, ETA in seconds: 1705746.381\n",
      "epoch: 315600, train loss: 3.8943386554718016, val loss: 3.976328444480896, ETA in seconds: 1706092.387\n",
      "epoch: 315700, train loss: 3.89004762172699, val loss: 3.9704464435577393, ETA in seconds: 1706353.567\n",
      "epoch: 315800, train loss: 3.9008352279663088, val loss: 3.984989356994629, ETA in seconds: 1706619.782\n",
      "epoch: 315900, train loss: 3.8992744207382204, val loss: 3.9724476814270018, ETA in seconds: 1706920.146\n",
      "epoch: 316000, train loss: 3.8863246202468873, val loss: 3.981044363975525, ETA in seconds: 1707207.993\n",
      "epoch: 316100, train loss: 3.8847519397735595, val loss: 3.9778390645980837, ETA in seconds: 1707478.619\n",
      "epoch: 316200, train loss: 3.894972729682922, val loss: 3.9725327253341676, ETA in seconds: 1707739.183\n",
      "epoch: 316300, train loss: 3.887969708442688, val loss: 3.973075771331787, ETA in seconds: 1708031.927\n",
      "epoch: 316400, train loss: 3.8972635507583617, val loss: 3.9837689876556395, ETA in seconds: 1708299.649\n",
      "epoch: 316500, train loss: 3.889739465713501, val loss: 3.9797234773635863, ETA in seconds: 1708539.923\n",
      "epoch: 316600, train loss: 3.90098557472229, val loss: 3.974014091491699, ETA in seconds: 1708785.176\n",
      "epoch: 316700, train loss: 3.883685827255249, val loss: 3.9863356590270995, ETA in seconds: 1709024.533\n",
      "epoch: 316800, train loss: 3.892383503913879, val loss: 3.9677442789077757, ETA in seconds: 1709277.579\n",
      "epoch: 316900, train loss: 3.896903228759766, val loss: 3.979226326942444, ETA in seconds: 1709515.550\n",
      "epoch: 317000, train loss: 3.8872994422912597, val loss: 3.9772111415863036, ETA in seconds: 1709778.006\n",
      "epoch: 317100, train loss: 3.897506284713745, val loss: 3.9788716793060304, ETA in seconds: 1710023.947\n",
      "epoch: 317200, train loss: 3.8980643272399904, val loss: 3.9804069995880127, ETA in seconds: 1710261.441\n",
      "epoch: 317300, train loss: 3.903881144523621, val loss: 3.988118386268616, ETA in seconds: 1710502.892\n",
      "epoch: 317400, train loss: 3.8927802562713625, val loss: 3.9805041551589966, ETA in seconds: 1710741.176\n",
      "epoch: 317500, train loss: 3.890305590629578, val loss: 3.973914313316345, ETA in seconds: 1710985.004\n",
      "epoch: 317600, train loss: 3.888827896118164, val loss: 3.9766290187835693, ETA in seconds: 1711277.775\n",
      "epoch: 317700, train loss: 3.8949874877929687, val loss: 3.970751142501831, ETA in seconds: 1711537.264\n",
      "epoch: 317800, train loss: 3.894251990318298, val loss: 3.9667999029159544, ETA in seconds: 1711804.862\n",
      "epoch: 317900, train loss: 3.8967278718948366, val loss: 3.974986171722412, ETA in seconds: 1712057.478\n",
      "epoch: 318000, train loss: 3.8859654664993286, val loss: 3.976625394821167, ETA in seconds: 1712326.406\n",
      "epoch: 318100, train loss: 3.8888177156448362, val loss: 3.969464421272278, ETA in seconds: 1712579.701\n",
      "epoch: 318200, train loss: 3.8907104969024657, val loss: 3.9706421375274656, ETA in seconds: 1712830.709\n",
      "epoch: 318300, train loss: 3.886502814292908, val loss: 3.9738273859024047, ETA in seconds: 1713081.484\n",
      "epoch: 318400, train loss: 3.89525420665741, val loss: 3.9758228302001952, ETA in seconds: 1713342.720\n",
      "epoch: 318500, train loss: 3.896457290649414, val loss: 3.974928069114685, ETA in seconds: 1713592.603\n",
      "epoch: 318600, train loss: 3.8886169672012327, val loss: 3.9774604082107543, ETA in seconds: 1713841.261\n",
      "epoch: 318700, train loss: 3.8881059169769285, val loss: 3.9700877189636232, ETA in seconds: 1714096.229\n",
      "epoch: 318800, train loss: 3.8828068733215333, val loss: 3.9869965076446534, ETA in seconds: 1714346.985\n",
      "epoch: 318900, train loss: 3.8911781549453734, val loss: 3.977493333816528, ETA in seconds: 1714594.005\n",
      "epoch: 319000, train loss: 3.885321259498596, val loss: 3.9681845188140867, ETA in seconds: 1714857.059\n",
      "epoch: 319100, train loss: 3.9059289693832397, val loss: 3.9795961141586305, ETA in seconds: 1715125.818\n",
      "epoch: 319200, train loss: 3.9041759967803955, val loss: 3.971606397628784, ETA in seconds: 1715375.968\n",
      "epoch: 319300, train loss: 3.895159387588501, val loss: 3.9788103103637695, ETA in seconds: 1715627.268\n",
      "epoch: 319400, train loss: 3.8934831619262695, val loss: 3.9711238145828247, ETA in seconds: 1715887.682\n",
      "epoch: 319500, train loss: 3.882835602760315, val loss: 3.9861064910888673, ETA in seconds: 1716131.675\n",
      "epoch: 319600, train loss: 3.8905479669570924, val loss: 3.963962268829346, ETA in seconds: 1716397.201\n",
      "epoch: 319700, train loss: 3.8885610580444334, val loss: 3.971779489517212, ETA in seconds: 1716716.734\n",
      "epoch: 319800, train loss: 3.8962926864624023, val loss: 3.9812753200531006, ETA in seconds: 1716984.430\n",
      "epoch: 319900, train loss: 3.895381212234497, val loss: 3.9762798309326173, ETA in seconds: 1717233.490\n",
      "epoch: 320000, train loss: 3.8915296316146852, val loss: 3.9690353870391846, ETA in seconds: 1717479.409\n",
      "epoch: 320100, train loss: 3.8928328275680544, val loss: 3.9726185321807863, ETA in seconds: 1717724.653\n",
      "epoch: 320200, train loss: 3.89608747959137, val loss: 3.973261594772339, ETA in seconds: 1717971.364\n",
      "epoch: 320300, train loss: 3.892636036872864, val loss: 3.98340380191803, ETA in seconds: 1718217.154\n",
      "epoch: 320400, train loss: 3.8966256618499755, val loss: 3.9760172605514525, ETA in seconds: 1718442.652\n",
      "epoch: 320500, train loss: 3.8879987001419067, val loss: 3.973826050758362, ETA in seconds: 1718668.862\n",
      "epoch: 320600, train loss: 3.901867151260376, val loss: 3.971159291267395, ETA in seconds: 1718911.498\n",
      "epoch: 320700, train loss: 3.879331183433533, val loss: 3.9739139080047607, ETA in seconds: 1719151.597\n",
      "epoch: 320800, train loss: 3.898409032821655, val loss: 3.9640338897705076, ETA in seconds: 1719402.373\n",
      "epoch: 320900, train loss: 3.8992522239685057, val loss: 3.977753925323486, ETA in seconds: 1719645.900\n",
      "epoch: 321000, train loss: 3.901441478729248, val loss: 3.968000602722168, ETA in seconds: 1719889.009\n",
      "epoch: 321100, train loss: 3.892949438095093, val loss: 3.968069338798523, ETA in seconds: 1720154.662\n",
      "epoch: 321200, train loss: 3.8926664352416993, val loss: 3.9809253215789795, ETA in seconds: 1720415.174\n",
      "epoch: 321300, train loss: 3.896909761428833, val loss: 3.9816768884658815, ETA in seconds: 1720687.200\n",
      "epoch: 321400, train loss: 3.893432545661926, val loss: 3.981605863571167, ETA in seconds: 1720979.675\n",
      "epoch: 321500, train loss: 3.895059871673584, val loss: 3.971359467506409, ETA in seconds: 1721244.520\n",
      "epoch: 321600, train loss: 3.890545439720154, val loss: 3.9693809270858766, ETA in seconds: 1721486.540\n",
      "epoch: 321700, train loss: 3.883170223236084, val loss: 3.9727193117141724, ETA in seconds: 1721724.198\n",
      "epoch: 321800, train loss: 3.8886508464813234, val loss: 3.9703660726547243, ETA in seconds: 1721971.806\n",
      "epoch: 321900, train loss: 3.8887290000915526, val loss: 3.9684740781784056, ETA in seconds: 1722218.622\n",
      "epoch: 322000, train loss: 3.8843969345092773, val loss: 3.9660117387771607, ETA in seconds: 1722490.430\n",
      "epoch: 322100, train loss: 3.8930010318756105, val loss: 3.9589920043945312, ETA in seconds: 1722779.363\n",
      "epoch: 322200, train loss: 3.8963624000549317, val loss: 3.968988037109375, ETA in seconds: 1723035.146\n",
      "epoch: 322300, train loss: 3.8941320419311523, val loss: 3.970594334602356, ETA in seconds: 1723304.929\n",
      "epoch: 322400, train loss: 3.9022953748703, val loss: 3.9648605585098267, ETA in seconds: 1723559.704\n",
      "epoch: 322500, train loss: 3.880491542816162, val loss: 3.9708426237106322, ETA in seconds: 1723787.147\n",
      "epoch: 322600, train loss: 3.8825791597366335, val loss: 3.9594493865966798, ETA in seconds: 1724028.882\n",
      "epoch: 322700, train loss: 3.8941736698150633, val loss: 3.961916923522949, ETA in seconds: 1724351.143\n",
      "epoch: 322800, train loss: 3.899551200866699, val loss: 3.9696116924285887, ETA in seconds: 1724603.889\n",
      "epoch: 322900, train loss: 3.898152709007263, val loss: 3.9631277322769165, ETA in seconds: 1724893.374\n",
      "epoch: 323000, train loss: 3.893946647644043, val loss: 3.9713125467300414, ETA in seconds: 1725221.418\n",
      "epoch: 323100, train loss: 3.8908166170120237, val loss: 3.9746721982955933, ETA in seconds: 1725464.509\n",
      "epoch: 323200, train loss: 3.8865350246429444, val loss: 3.9705628871917726, ETA in seconds: 1725698.228\n",
      "epoch: 323300, train loss: 3.897926187515259, val loss: 3.9629721879959106, ETA in seconds: 1725956.035\n",
      "epoch: 323400, train loss: 3.893135333061218, val loss: 3.96303653717041, ETA in seconds: 1726305.918\n",
      "epoch: 323500, train loss: 3.893888306617737, val loss: 3.9711312770843508, ETA in seconds: 1726558.478\n",
      "epoch: 323600, train loss: 3.8889527320861816, val loss: 3.968445301055908, ETA in seconds: 1726794.042\n",
      "epoch: 323700, train loss: 3.8939618110656737, val loss: 3.9622308015823364, ETA in seconds: 1727069.767\n",
      "epoch: 323800, train loss: 3.9097211837768553, val loss: 3.9758273124694825, ETA in seconds: 1727452.079\n",
      "epoch: 323900, train loss: 3.895645356178284, val loss: 3.9643073320388793, ETA in seconds: 1727733.938\n",
      "epoch: 324000, train loss: 3.8815231800079344, val loss: 3.966853475570679, ETA in seconds: 1727981.132\n",
      "epoch: 324100, train loss: 3.9056228160858155, val loss: 3.964252257347107, ETA in seconds: 1728287.268\n",
      "epoch: 324200, train loss: 3.8935094833374024, val loss: 3.964025855064392, ETA in seconds: 1728521.997\n",
      "epoch: 324300, train loss: 3.8898980617523193, val loss: 3.960026264190674, ETA in seconds: 1728766.792\n",
      "epoch: 324400, train loss: 3.887720799446106, val loss: 3.959273838996887, ETA in seconds: 1729006.546\n",
      "epoch: 324500, train loss: 3.888375687599182, val loss: 3.9758585929870605, ETA in seconds: 1729254.080\n",
      "epoch: 324600, train loss: 3.893681502342224, val loss: 3.9715604543685914, ETA in seconds: 1729512.526\n",
      "epoch: 324700, train loss: 3.8963707447052003, val loss: 3.96709303855896, ETA in seconds: 1729767.025\n",
      "epoch: 324800, train loss: 3.898028039932251, val loss: 3.9681161880493163, ETA in seconds: 1730030.177\n",
      "epoch: 324900, train loss: 3.9038052558898926, val loss: 3.9724907875061035, ETA in seconds: 1730279.117\n",
      "epoch: 325000, train loss: 3.895983123779297, val loss: 3.9643551826477053, ETA in seconds: 1730527.289\n",
      "epoch: 325100, train loss: 3.892505145072937, val loss: 3.9663252353668215, ETA in seconds: 1730764.890\n",
      "epoch: 325200, train loss: 3.9010059595108033, val loss: 3.9678600788116456, ETA in seconds: 1730995.409\n",
      "epoch: 325300, train loss: 3.889808988571167, val loss: 3.969793105125427, ETA in seconds: 1731241.639\n",
      "epoch: 325400, train loss: 3.898389291763306, val loss: 3.9672953367233275, ETA in seconds: 1731499.270\n",
      "epoch: 325500, train loss: 3.8985039472579954, val loss: 3.9769091844558715, ETA in seconds: 1731818.231\n",
      "epoch: 325600, train loss: 3.896256613731384, val loss: 3.9651615619659424, ETA in seconds: 1732127.909\n",
      "epoch: 325700, train loss: 3.892721390724182, val loss: 3.9729609727859496, ETA in seconds: 1732531.222\n",
      "epoch: 325800, train loss: 3.8936704635620116, val loss: 3.9715144872665404, ETA in seconds: 1732925.587\n",
      "epoch: 325900, train loss: 3.887838888168335, val loss: 3.9720777034759522, ETA in seconds: 1733196.900\n",
      "epoch: 326000, train loss: 3.888022208213806, val loss: 3.963913917541504, ETA in seconds: 1733473.244\n",
      "epoch: 326100, train loss: 3.893182373046875, val loss: 3.975082206726074, ETA in seconds: 1733721.487\n",
      "epoch: 326200, train loss: 3.9012203454971313, val loss: 3.975408148765564, ETA in seconds: 1733973.320\n",
      "epoch: 326300, train loss: 3.8976696968078612, val loss: 3.9706841945648192, ETA in seconds: 1734213.027\n",
      "epoch: 326400, train loss: 3.890887212753296, val loss: 3.9628293037414553, ETA in seconds: 1734451.792\n",
      "epoch: 326500, train loss: 3.89984929561615, val loss: 3.959554123878479, ETA in seconds: 1734687.815\n",
      "epoch: 326600, train loss: 3.8910894870758055, val loss: 3.968932938575745, ETA in seconds: 1734930.912\n",
      "epoch: 326700, train loss: 3.894932985305786, val loss: 3.9743572473526, ETA in seconds: 1735170.673\n",
      "epoch: 326800, train loss: 3.900680136680603, val loss: 3.966839075088501, ETA in seconds: 1735456.723\n",
      "epoch: 326900, train loss: 3.8999737024307253, val loss: 3.9642619848251344, ETA in seconds: 1735703.755\n",
      "epoch: 327000, train loss: 3.8997249841690063, val loss: 3.955357551574707, ETA in seconds: 1735955.705\n",
      "epoch: 327100, train loss: 3.8929755210876467, val loss: 3.9642323732376097, ETA in seconds: 1736265.962\n",
      "epoch: 327200, train loss: 3.886502718925476, val loss: 3.974912095069885, ETA in seconds: 1736618.685\n",
      "epoch: 327300, train loss: 3.889864611625671, val loss: 3.9777785539627075, ETA in seconds: 1736957.713\n",
      "epoch: 327400, train loss: 3.8925558805465696, val loss: 3.95883367061615, ETA in seconds: 1737277.090\n",
      "epoch: 327500, train loss: 3.8881218433380127, val loss: 3.9788158178329467, ETA in seconds: 1737538.140\n",
      "epoch: 327600, train loss: 3.8937064170837403, val loss: 3.9730198621749877, ETA in seconds: 1737795.870\n",
      "epoch: 327700, train loss: 3.8857407093048097, val loss: 3.9804966688156127, ETA in seconds: 1738056.674\n",
      "epoch: 327800, train loss: 3.8896162033081056, val loss: 3.963456153869629, ETA in seconds: 1738317.198\n",
      "epoch: 327900, train loss: 3.89316246509552, val loss: 3.981542158126831, ETA in seconds: 1738574.896\n",
      "epoch: 328000, train loss: 3.8898099660873413, val loss: 3.9811553239822386, ETA in seconds: 1738866.652\n",
      "epoch: 328100, train loss: 3.8901190042495726, val loss: 3.9652732372283936, ETA in seconds: 1739113.743\n",
      "epoch: 328200, train loss: 3.889078068733215, val loss: 3.980147433280945, ETA in seconds: 1739370.936\n",
      "epoch: 328300, train loss: 3.8989268064498903, val loss: 3.9652540922164916, ETA in seconds: 1739642.311\n",
      "epoch: 328400, train loss: 3.886815667152405, val loss: 3.970712900161743, ETA in seconds: 1739988.206\n",
      "epoch: 328500, train loss: 3.8947897434234617, val loss: 3.961891031265259, ETA in seconds: 1740336.853\n",
      "epoch: 328600, train loss: 3.8948943853378295, val loss: 3.9719103813171386, ETA in seconds: 1740603.143\n",
      "epoch: 328700, train loss: 3.8921873807907104, val loss: 3.9594191551208495, ETA in seconds: 1740850.571\n",
      "epoch: 328800, train loss: 3.9003353118896484, val loss: 3.9592258453369142, ETA in seconds: 1741117.219\n",
      "epoch: 328900, train loss: 3.8907768964767455, val loss: 3.9658331632614137, ETA in seconds: 1741372.951\n",
      "epoch: 329000, train loss: 3.8973105907440186, val loss: 3.9649823665618897, ETA in seconds: 1741623.012\n",
      "epoch: 329100, train loss: 3.909421443939209, val loss: 3.9629770517349243, ETA in seconds: 1741852.610\n",
      "epoch: 329200, train loss: 3.8902131795883177, val loss: 3.972019577026367, ETA in seconds: 1742076.761\n",
      "epoch: 329300, train loss: 3.8876793384552, val loss: 3.9604575395584107, ETA in seconds: 1742307.107\n",
      "epoch: 329400, train loss: 3.893955445289612, val loss: 3.966762065887451, ETA in seconds: 1742565.178\n",
      "epoch: 329500, train loss: 3.89646692276001, val loss: 3.9644773960113526, ETA in seconds: 1742888.930\n",
      "epoch: 329600, train loss: 3.8949612379074097, val loss: 3.9723013162612917, ETA in seconds: 1743132.420\n",
      "epoch: 329700, train loss: 3.890880084037781, val loss: 3.965856504440308, ETA in seconds: 1743383.840\n",
      "epoch: 329800, train loss: 3.8972014427185058, val loss: 3.970128798484802, ETA in seconds: 1743627.039\n",
      "epoch: 329900, train loss: 3.89327232837677, val loss: 3.9628511905670165, ETA in seconds: 1743869.804\n",
      "epoch: 330000, train loss: 3.897812008857727, val loss: 3.9718007326126097, ETA in seconds: 1744134.268\n",
      "epoch: 330100, train loss: 3.8959762811660767, val loss: 3.980112838745117, ETA in seconds: 1744477.940\n",
      "epoch: 330200, train loss: 3.887814497947693, val loss: 3.9768458366394044, ETA in seconds: 1744822.437\n",
      "epoch: 330300, train loss: 3.8920073986053465, val loss: 3.9622930526733398, ETA in seconds: 1745061.031\n",
      "epoch: 330400, train loss: 3.89436514377594, val loss: 3.9662392377853393, ETA in seconds: 1745283.047\n",
      "epoch: 330500, train loss: 3.8920073747634887, val loss: 3.978074884414673, ETA in seconds: 1745505.859\n",
      "epoch: 330600, train loss: 3.8918970346450807, val loss: 3.9685285806655886, ETA in seconds: 1745735.400\n",
      "epoch: 330700, train loss: 3.8977787733078, val loss: 3.972062849998474, ETA in seconds: 1745982.335\n",
      "epoch: 330800, train loss: 3.890107750892639, val loss: 3.967116284370422, ETA in seconds: 1746225.730\n",
      "epoch: 330900, train loss: 3.8925993919372557, val loss: 3.9631094217300413, ETA in seconds: 1746533.796\n",
      "epoch: 331000, train loss: 3.892737793922424, val loss: 3.9653874397277833, ETA in seconds: 1746870.207\n",
      "epoch: 331100, train loss: 3.8921430110931396, val loss: 3.9678197860717774, ETA in seconds: 1747154.559\n",
      "epoch: 331200, train loss: 3.8905401706695555, val loss: 3.972873330116272, ETA in seconds: 1747392.811\n",
      "epoch: 331300, train loss: 3.8853538990020753, val loss: 3.9736451387405394, ETA in seconds: 1747635.931\n",
      "epoch: 331400, train loss: 3.8868818283081055, val loss: 3.9670992612838747, ETA in seconds: 1747860.650\n",
      "epoch: 331500, train loss: 3.8857551336288454, val loss: 3.978769826889038, ETA in seconds: 1748070.339\n",
      "epoch: 331600, train loss: 3.9035131216049193, val loss: 3.9731148958206175, ETA in seconds: 1748331.147\n",
      "epoch: 331700, train loss: 3.8904154300689697, val loss: 3.9879643440246584, ETA in seconds: 1748592.314\n",
      "epoch: 331800, train loss: 3.896891450881958, val loss: 3.972925591468811, ETA in seconds: 1748822.755\n",
      "epoch: 331900, train loss: 3.8964456796646116, val loss: 3.9670217990875245, ETA in seconds: 1749054.532\n",
      "epoch: 332000, train loss: 3.892348623275757, val loss: 3.971298837661743, ETA in seconds: 1749298.424\n",
      "epoch: 332100, train loss: 3.89771773815155, val loss: 3.962517762184143, ETA in seconds: 1749532.358\n",
      "epoch: 332200, train loss: 3.8849509716033936, val loss: 3.9701098680496214, ETA in seconds: 1749749.640\n",
      "epoch: 332300, train loss: 3.89333553314209, val loss: 3.977283501625061, ETA in seconds: 1749994.188\n",
      "epoch: 332400, train loss: 3.887350058555603, val loss: 3.96388463973999, ETA in seconds: 1750234.179\n",
      "epoch: 332500, train loss: 3.8869852304458616, val loss: 3.984310269355774, ETA in seconds: 1750476.109\n",
      "epoch: 332600, train loss: 3.8872926235198975, val loss: 3.9713952779769897, ETA in seconds: 1750712.105\n",
      "epoch: 332700, train loss: 3.8826260328292848, val loss: 3.9697797775268553, ETA in seconds: 1750949.074\n",
      "epoch: 332800, train loss: 3.894692587852478, val loss: 3.9577052354812623, ETA in seconds: 1751197.530\n",
      "epoch: 332900, train loss: 3.8957208395004272, val loss: 3.9733735084533692, ETA in seconds: 1751440.632\n",
      "epoch: 333000, train loss: 3.8943883657455443, val loss: 3.9647865533828734, ETA in seconds: 1751686.339\n",
      "epoch: 333100, train loss: 3.8923137187957764, val loss: 3.971968913078308, ETA in seconds: 1751932.664\n",
      "epoch: 333200, train loss: 3.895103621482849, val loss: 3.974662256240845, ETA in seconds: 1752222.774\n",
      "epoch: 333300, train loss: 3.895252299308777, val loss: 3.9690696001052856, ETA in seconds: 1752490.913\n",
      "epoch: 333400, train loss: 3.890581822395325, val loss: 3.9778810024261473, ETA in seconds: 1752772.503\n",
      "epoch: 333500, train loss: 3.891449046134949, val loss: 3.9768662452697754, ETA in seconds: 1753018.724\n",
      "epoch: 333600, train loss: 3.8915783643722532, val loss: 3.9707286596298217, ETA in seconds: 1753276.904\n",
      "epoch: 333700, train loss: 3.896193432807922, val loss: 3.96132915019989, ETA in seconds: 1753570.464\n",
      "epoch: 333800, train loss: 3.8966596126556396, val loss: 3.9621060132980346, ETA in seconds: 1753844.146\n",
      "epoch: 333900, train loss: 3.9114508628845215, val loss: 3.962963581085205, ETA in seconds: 1754078.642\n",
      "epoch: 334000, train loss: 3.895594000816345, val loss: 3.9604184150695803, ETA in seconds: 1754325.307\n",
      "epoch: 334100, train loss: 3.894024872779846, val loss: 3.9562913179397583, ETA in seconds: 1754662.311\n",
      "epoch: 334200, train loss: 3.892668104171753, val loss: 3.9859328508377074, ETA in seconds: 1754994.798\n",
      "epoch: 334300, train loss: 3.8976564168930055, val loss: 3.9636717557907106, ETA in seconds: 1755329.702\n",
      "epoch: 334400, train loss: 3.8960315227508544, val loss: 3.959081602096558, ETA in seconds: 1755665.569\n",
      "epoch: 334500, train loss: 3.8896963834762572, val loss: 3.9633856773376466, ETA in seconds: 1755998.041\n",
      "epoch: 334600, train loss: 3.881433868408203, val loss: 3.9657866239547728, ETA in seconds: 1756337.302\n",
      "epoch: 334700, train loss: 3.895802640914917, val loss: 3.9721564292907714, ETA in seconds: 1756674.492\n",
      "epoch: 334800, train loss: 3.889014458656311, val loss: 3.9735615968704225, ETA in seconds: 1757013.757\n",
      "epoch: 334900, train loss: 3.8896713495254516, val loss: 3.978572702407837, ETA in seconds: 1757360.947\n",
      "epoch: 335000, train loss: 3.897221326828003, val loss: 3.9646506786346434, ETA in seconds: 1757694.741\n",
      "epoch: 335100, train loss: 3.8912264585494993, val loss: 3.9740462064743043, ETA in seconds: 1758027.502\n",
      "epoch: 335200, train loss: 3.888372540473938, val loss: 3.95768723487854, ETA in seconds: 1758365.194\n",
      "epoch: 335300, train loss: 3.8990846633911134, val loss: 3.982637548446655, ETA in seconds: 1758728.935\n",
      "epoch: 335400, train loss: 3.8962566614151, val loss: 3.96186683177948, ETA in seconds: 1758959.850\n",
      "epoch: 335500, train loss: 3.891095018386841, val loss: 3.9674648523330687, ETA in seconds: 1759198.602\n",
      "epoch: 335600, train loss: 3.890310549736023, val loss: 3.9685092210769652, ETA in seconds: 1759437.620\n",
      "epoch: 335700, train loss: 3.892356204986572, val loss: 3.9606996297836305, ETA in seconds: 1759667.475\n",
      "epoch: 335800, train loss: 3.901017761230469, val loss: 3.9501714944839477, ETA in seconds: 1759903.865\n",
      "epoch: 335900, train loss: 3.88677442073822, val loss: 3.9497366428375242, ETA in seconds: 1760156.448\n",
      "epoch: 336000, train loss: 3.893177103996277, val loss: 3.966744899749756, ETA in seconds: 1760397.010\n",
      "epoch: 336100, train loss: 3.898229956626892, val loss: 3.9753960609436034, ETA in seconds: 1760637.473\n",
      "epoch: 336200, train loss: 3.8980920553207397, val loss: 3.9803575754165648, ETA in seconds: 1760967.232\n",
      "epoch: 336300, train loss: 3.902290630340576, val loss: 3.964703416824341, ETA in seconds: 1761296.912\n",
      "epoch: 336400, train loss: 3.885317063331604, val loss: 3.9588669300079347, ETA in seconds: 1761541.584\n",
      "epoch: 336500, train loss: 3.8946622133255007, val loss: 3.966380572319031, ETA in seconds: 1761775.794\n",
      "epoch: 336600, train loss: 3.8923951148986817, val loss: 3.9752381563186647, ETA in seconds: 1762004.411\n",
      "epoch: 336700, train loss: 3.8842076301574706, val loss: 3.9718754529953, ETA in seconds: 1762236.084\n",
      "epoch: 336800, train loss: 3.8886544942855834, val loss: 3.962151789665222, ETA in seconds: 1762473.230\n",
      "epoch: 336900, train loss: 3.903646373748779, val loss: 3.9776073694229126, ETA in seconds: 1762711.163\n",
      "epoch: 337000, train loss: 3.8998174905776977, val loss: 3.9742270946502685, ETA in seconds: 1763032.399\n",
      "epoch: 337100, train loss: 3.8890902280807493, val loss: 3.9648704528808594, ETA in seconds: 1763353.109\n",
      "epoch: 337200, train loss: 3.8870046377182006, val loss: 3.9715611934661865, ETA in seconds: 1763608.597\n",
      "epoch: 337300, train loss: 3.8987481117248537, val loss: 3.9758902549743653, ETA in seconds: 1763822.062\n",
      "epoch: 337400, train loss: 3.8888381481170655, val loss: 3.982135605812073, ETA in seconds: 1764052.312\n",
      "epoch: 337500, train loss: 3.885760283470154, val loss: 3.9794734001159666, ETA in seconds: 1764267.771\n",
      "epoch: 337600, train loss: 3.8842081308364866, val loss: 3.9772382259368895, ETA in seconds: 1764507.605\n",
      "epoch: 337700, train loss: 3.8868004083633423, val loss: 3.97111120223999, ETA in seconds: 1764739.526\n",
      "epoch: 337800, train loss: 3.8939220905303955, val loss: 3.968948721885681, ETA in seconds: 1764970.293\n",
      "epoch: 337900, train loss: 3.895076298713684, val loss: 3.964516830444336, ETA in seconds: 1765218.823\n",
      "epoch: 338000, train loss: 3.8858174085617065, val loss: 3.9719082832336428, ETA in seconds: 1765464.482\n",
      "epoch: 338100, train loss: 3.8976150274276735, val loss: 3.9747922897338865, ETA in seconds: 1765711.245\n",
      "epoch: 338200, train loss: 3.9024800062179565, val loss: 3.9568801403045653, ETA in seconds: 1766003.815\n",
      "epoch: 338300, train loss: 3.888372230529785, val loss: 3.9683481454849243, ETA in seconds: 1766238.492\n",
      "epoch: 338400, train loss: 3.893060541152954, val loss: 3.9644721508026124, ETA in seconds: 1766485.352\n",
      "epoch: 338500, train loss: 3.8920757293701174, val loss: 3.9668981790542603, ETA in seconds: 1766760.508\n",
      "epoch: 338600, train loss: 3.895666313171387, val loss: 3.972314715385437, ETA in seconds: 1767098.643\n",
      "epoch: 338700, train loss: 3.9007649183273316, val loss: 3.978163242340088, ETA in seconds: 1767361.193\n",
      "epoch: 338800, train loss: 3.8875434398651123, val loss: 3.9722655534744264, ETA in seconds: 1767608.646\n",
      "epoch: 338900, train loss: 3.8999747037887573, val loss: 3.975852203369141, ETA in seconds: 1767933.071\n",
      "epoch: 339000, train loss: 3.886598324775696, val loss: 3.9693547248840333, ETA in seconds: 1768248.228\n",
      "epoch: 339100, train loss: 3.89219491481781, val loss: 3.9720792293548586, ETA in seconds: 1768483.518\n",
      "epoch: 339200, train loss: 3.8960259914398194, val loss: 3.9646281480789183, ETA in seconds: 1768726.279\n",
      "epoch: 339300, train loss: 3.9007761478424072, val loss: 3.9685251712799072, ETA in seconds: 1768961.710\n",
      "epoch: 339400, train loss: 3.8997620344161987, val loss: 3.971094489097595, ETA in seconds: 1769200.435\n",
      "epoch: 339500, train loss: 3.890612840652466, val loss: 3.970162034034729, ETA in seconds: 1769516.848\n",
      "epoch: 339600, train loss: 3.891267943382263, val loss: 3.9652578830718994, ETA in seconds: 1769882.617\n",
      "epoch: 339700, train loss: 3.901255226135254, val loss: 3.981309008598328, ETA in seconds: 1770103.529\n",
      "epoch: 339800, train loss: 3.887047266960144, val loss: 3.978685212135315, ETA in seconds: 1770339.463\n",
      "epoch: 339900, train loss: 3.900617170333862, val loss: 3.968920946121216, ETA in seconds: 1770594.678\n",
      "epoch: 340000, train loss: 3.89013090133667, val loss: 3.9621583223342896, ETA in seconds: 1770827.570\n",
      "epoch: 340100, train loss: 3.896947479248047, val loss: 3.972977304458618, ETA in seconds: 1771064.259\n",
      "epoch: 340200, train loss: 3.8919342279434206, val loss: 3.960641956329346, ETA in seconds: 1771299.502\n",
      "epoch: 340300, train loss: 3.8945152521133424, val loss: 3.9623839139938353, ETA in seconds: 1771544.210\n",
      "epoch: 340400, train loss: 3.8885263442993163, val loss: 3.9674410581588746, ETA in seconds: 1771775.145\n",
      "epoch: 340500, train loss: 3.9012035369873046, val loss: 3.9721062660217283, ETA in seconds: 1772018.282\n",
      "epoch: 340600, train loss: 3.8858420848846436, val loss: 3.966785740852356, ETA in seconds: 1772288.135\n",
      "epoch: 340700, train loss: 3.9047094583511353, val loss: 3.9666988134384153, ETA in seconds: 1772523.257\n",
      "epoch: 340800, train loss: 3.891628456115723, val loss: 3.9538928747177122, ETA in seconds: 1772757.538\n",
      "epoch: 340900, train loss: 3.896856999397278, val loss: 3.9700143337249756, ETA in seconds: 1772997.188\n",
      "epoch: 341000, train loss: 3.8827752590179445, val loss: 3.974207949638367, ETA in seconds: 1773242.058\n",
      "epoch: 341100, train loss: 3.894493007659912, val loss: 3.9673293113708494, ETA in seconds: 1773478.857\n",
      "epoch: 341200, train loss: 3.8889615058898928, val loss: 3.975111413002014, ETA in seconds: 1773736.014\n",
      "epoch: 341300, train loss: 3.894420313835144, val loss: 3.97569797039032, ETA in seconds: 1773965.599\n",
      "epoch: 341400, train loss: 3.8932996511459352, val loss: 3.9542083024978636, ETA in seconds: 1774194.104\n",
      "epoch: 341500, train loss: 3.8873210668563845, val loss: 3.9532134771347045, ETA in seconds: 1774432.788\n",
      "epoch: 341600, train loss: 3.8842626333236696, val loss: 3.969281029701233, ETA in seconds: 1774671.537\n",
      "epoch: 341700, train loss: 3.8990015745162965, val loss: 3.972229743003845, ETA in seconds: 1774972.473\n",
      "epoch: 341800, train loss: 3.901001763343811, val loss: 3.9580724000930787, ETA in seconds: 1775232.917\n",
      "epoch: 341900, train loss: 3.88512761592865, val loss: 3.977446174621582, ETA in seconds: 1775452.927\n",
      "epoch: 342000, train loss: 3.8939080238342285, val loss: 3.9708001613616943, ETA in seconds: 1775679.330\n",
      "epoch: 342100, train loss: 3.8911596059799196, val loss: 3.967510485649109, ETA in seconds: 1775927.924\n",
      "epoch: 342200, train loss: 3.8951598405838013, val loss: 3.9762990713119506, ETA in seconds: 1776249.026\n",
      "epoch: 342300, train loss: 3.8908802270889282, val loss: 3.9609056234359743, ETA in seconds: 1776457.007\n",
      "epoch: 342400, train loss: 3.900197243690491, val loss: 3.972497749328613, ETA in seconds: 1776692.127\n",
      "epoch: 342500, train loss: 3.8956474542617796, val loss: 3.9663909912109374, ETA in seconds: 1777053.819\n",
      "epoch: 342600, train loss: 3.889596152305603, val loss: 3.9852323055267336, ETA in seconds: 1777385.519\n",
      "epoch: 342700, train loss: 3.893029713630676, val loss: 3.9718250751495363, ETA in seconds: 1777748.800\n",
      "epoch: 342800, train loss: 3.8865609645843504, val loss: 3.976484680175781, ETA in seconds: 1778075.326\n",
      "epoch: 342900, train loss: 3.8765570402145384, val loss: 3.97037410736084, ETA in seconds: 1778325.560\n",
      "epoch: 343000, train loss: 3.8883687973022463, val loss: 3.97780601978302, ETA in seconds: 1778570.247\n",
      "epoch: 343100, train loss: 3.887992763519287, val loss: 3.9723294734954835, ETA in seconds: 1778812.346\n",
      "epoch: 343200, train loss: 3.8991920948028564, val loss: 3.9630887031555178, ETA in seconds: 1779038.115\n",
      "epoch: 343300, train loss: 3.8926177263259887, val loss: 3.982190704345703, ETA in seconds: 1779291.252\n",
      "epoch: 343400, train loss: 3.886747884750366, val loss: 3.9704326391220093, ETA in seconds: 1779654.073\n",
      "epoch: 343500, train loss: 3.901149129867554, val loss: 3.979747200012207, ETA in seconds: 1779890.052\n",
      "epoch: 343600, train loss: 3.8941869258880617, val loss: 3.9671475172042845, ETA in seconds: 1780154.969\n",
      "epoch: 343700, train loss: 3.881202960014343, val loss: 3.975999927520752, ETA in seconds: 1780393.703\n",
      "epoch: 343800, train loss: 3.8986400604248046, val loss: 3.9773901224136354, ETA in seconds: 1780617.747\n",
      "epoch: 343900, train loss: 3.8992055892944335, val loss: 3.9755322694778443, ETA in seconds: 1780853.141\n",
      "epoch: 344000, train loss: 3.8962284088134767, val loss: 3.9721328735351564, ETA in seconds: 1781197.960\n",
      "epoch: 344100, train loss: 3.8896092414855956, val loss: 3.9613478660583494, ETA in seconds: 1781486.061\n",
      "epoch: 344200, train loss: 3.8936137437820433, val loss: 3.9606313705444336, ETA in seconds: 1781708.449\n",
      "epoch: 344300, train loss: 3.898031759262085, val loss: 3.9675639390945436, ETA in seconds: 1781935.293\n",
      "epoch: 344400, train loss: 3.8975774526596068, val loss: 3.972817850112915, ETA in seconds: 1782211.659\n",
      "epoch: 344500, train loss: 3.882546138763428, val loss: 3.9797482252120973, ETA in seconds: 1782527.819\n",
      "epoch: 344600, train loss: 3.895529866218567, val loss: 3.9681041717529295, ETA in seconds: 1782839.798\n",
      "epoch: 344700, train loss: 3.8869089603424074, val loss: 3.9684164047241213, ETA in seconds: 1783177.227\n",
      "epoch: 344800, train loss: 3.8889084815979005, val loss: 3.9623737573623656, ETA in seconds: 1783396.224\n",
      "epoch: 344900, train loss: 3.888124918937683, val loss: 3.9672345876693726, ETA in seconds: 1783629.723\n",
      "epoch: 345000, train loss: 3.8933597803115845, val loss: 3.965971279144287, ETA in seconds: 1783849.450\n",
      "epoch: 345100, train loss: 3.8899667501449584, val loss: 3.970840883255005, ETA in seconds: 1784082.211\n",
      "epoch: 345200, train loss: 3.8970306634902956, val loss: 3.9758448600769043, ETA in seconds: 1784316.858\n",
      "epoch: 345300, train loss: 3.895761013031006, val loss: 3.960031008720398, ETA in seconds: 1784509.115\n",
      "epoch: 345400, train loss: 3.894180965423584, val loss: 3.966784620285034, ETA in seconds: 1784835.883\n",
      "epoch: 345500, train loss: 3.8868226289749144, val loss: 3.972493886947632, ETA in seconds: 1785149.320\n",
      "epoch: 345600, train loss: 3.892838716506958, val loss: 3.9656955003738403, ETA in seconds: 1785409.828\n",
      "epoch: 345700, train loss: 3.8909734964370726, val loss: 3.9810237884521484, ETA in seconds: 1785609.080\n",
      "epoch: 345800, train loss: 3.8965538263320925, val loss: 3.969355511665344, ETA in seconds: 1785801.074\n",
      "epoch: 345900, train loss: 3.8985735893249513, val loss: 3.9723613262176514, ETA in seconds: 1785997.740\n",
      "epoch: 346000, train loss: 3.8959092617034914, val loss: 3.9719443321228027, ETA in seconds: 1786206.125\n",
      "epoch: 346100, train loss: 3.8967944860458372, val loss: 3.9686816692352296, ETA in seconds: 1786404.902\n",
      "epoch: 346200, train loss: 3.88468599319458, val loss: 3.970426154136658, ETA in seconds: 1786607.425\n",
      "epoch: 346300, train loss: 3.904545760154724, val loss: 3.9639758348464964, ETA in seconds: 1786824.748\n",
      "epoch: 346400, train loss: 3.887613391876221, val loss: 3.970378065109253, ETA in seconds: 1787038.026\n",
      "epoch: 346500, train loss: 3.8887943029403687, val loss: 3.9712661266326905, ETA in seconds: 1787233.418\n",
      "epoch: 346600, train loss: 3.8831227302551268, val loss: 3.9766350746154786, ETA in seconds: 1787461.383\n",
      "epoch: 346700, train loss: 3.896838402748108, val loss: 3.972621703147888, ETA in seconds: 1787673.107\n",
      "epoch: 346800, train loss: 3.896651864051819, val loss: 3.977738857269287, ETA in seconds: 1787869.559\n",
      "epoch: 346900, train loss: 3.894589829444885, val loss: 3.976668429374695, ETA in seconds: 1788070.026\n",
      "epoch: 347000, train loss: 3.8921743631362915, val loss: 3.968421697616577, ETA in seconds: 1788283.340\n",
      "epoch: 347100, train loss: 3.899199366569519, val loss: 3.9678545951843263, ETA in seconds: 1788547.614\n",
      "epoch: 347200, train loss: 3.878873872756958, val loss: 3.9794967412948608, ETA in seconds: 1788781.612\n",
      "epoch: 347300, train loss: 3.899526762962341, val loss: 3.97072491645813, ETA in seconds: 1789095.498\n",
      "epoch: 347400, train loss: 3.89663245677948, val loss: 3.9709208965301515, ETA in seconds: 1789380.786\n",
      "epoch: 347500, train loss: 3.8947749376296996, val loss: 3.966842460632324, ETA in seconds: 1789635.938\n",
      "epoch: 347600, train loss: 3.894342637062073, val loss: 3.967074656486511, ETA in seconds: 1789839.134\n",
      "epoch: 347700, train loss: 3.881970834732056, val loss: 3.965392231941223, ETA in seconds: 1790056.797\n",
      "epoch: 347800, train loss: 3.8891579627990724, val loss: 3.960718011856079, ETA in seconds: 1790284.924\n",
      "epoch: 347900, train loss: 3.8965929985046386, val loss: 3.960908365249634, ETA in seconds: 1790501.155\n",
      "epoch: 348000, train loss: 3.896611046791077, val loss: 3.963146424293518, ETA in seconds: 1790713.017\n",
      "epoch: 348100, train loss: 3.901115369796753, val loss: 3.9624683380126955, ETA in seconds: 1790961.734\n",
      "epoch: 348200, train loss: 3.888128137588501, val loss: 3.969556999206543, ETA in seconds: 1791217.713\n",
      "epoch: 348300, train loss: 3.888186812400818, val loss: 3.9797420263290406, ETA in seconds: 1791444.375\n",
      "epoch: 348400, train loss: 3.889434552192688, val loss: 3.963861608505249, ETA in seconds: 1791693.765\n",
      "epoch: 348500, train loss: 3.8982690572738647, val loss: 3.970893049240112, ETA in seconds: 1791920.847\n",
      "epoch: 348600, train loss: 3.89326548576355, val loss: 3.966523623466492, ETA in seconds: 1792145.444\n",
      "epoch: 348700, train loss: 3.89204683303833, val loss: 3.963817024230957, ETA in seconds: 1792391.501\n",
      "epoch: 348800, train loss: 3.8876920461654665, val loss: 3.9650074005126954, ETA in seconds: 1792702.695\n",
      "epoch: 348900, train loss: 3.898554539680481, val loss: 3.9762641191482544, ETA in seconds: 1793012.711\n",
      "epoch: 349000, train loss: 3.9019470691680906, val loss: 3.9775908470153807, ETA in seconds: 1793326.121\n",
      "epoch: 349100, train loss: 3.886851954460144, val loss: 3.9679460763931274, ETA in seconds: 1793621.413\n",
      "epoch: 349200, train loss: 3.893934941291809, val loss: 3.9683887243270872, ETA in seconds: 1793839.952\n",
      "epoch: 349300, train loss: 3.896564412117004, val loss: 3.9651174545288086, ETA in seconds: 1794062.790\n",
      "epoch: 349400, train loss: 3.8927257299423217, val loss: 3.979308915138245, ETA in seconds: 1794297.416\n",
      "epoch: 349500, train loss: 3.8922593355178834, val loss: 3.968002271652222, ETA in seconds: 1794507.920\n",
      "epoch: 349600, train loss: 3.886698079109192, val loss: 3.9757508039474487, ETA in seconds: 1794781.517\n",
      "epoch: 349700, train loss: 3.899663496017456, val loss: 3.9600770235061646, ETA in seconds: 1794999.194\n",
      "epoch: 349800, train loss: 3.8908058166503907, val loss: 3.973457145690918, ETA in seconds: 1795209.237\n",
      "epoch: 349900, train loss: 3.8952915191650392, val loss: 3.96818745136261, ETA in seconds: 1795423.221\n",
      "epoch: 350000, train loss: 3.893249344825745, val loss: 3.9482521057128905, ETA in seconds: 1795634.035\n",
      "epoch: 350100, train loss: 3.890207886695862, val loss: 3.981909990310669, ETA in seconds: 1795833.863\n",
      "epoch: 350200, train loss: 3.8986149549484255, val loss: 3.980306887626648, ETA in seconds: 1796041.837\n",
      "epoch: 350300, train loss: 3.8982256650924683, val loss: 3.9709432125091553, ETA in seconds: 1796297.037\n",
      "epoch: 350400, train loss: 3.8892060279846192, val loss: 3.965907168388367, ETA in seconds: 1796519.325\n",
      "epoch: 350500, train loss: 3.8982568264007567, val loss: 3.9637380838394165, ETA in seconds: 1796821.499\n",
      "epoch: 350600, train loss: 3.8961049556732177, val loss: 3.963316798210144, ETA in seconds: 1797117.975\n",
      "epoch: 350700, train loss: 3.8913022756576536, val loss: 3.9696258544921874, ETA in seconds: 1797368.240\n",
      "epoch: 350800, train loss: 3.892017865180969, val loss: 3.9824043035507204, ETA in seconds: 1797614.363\n",
      "epoch: 350900, train loss: 3.884142208099365, val loss: 3.9645586729049684, ETA in seconds: 1797822.190\n",
      "epoch: 351000, train loss: 3.8946879148483275, val loss: 3.978053879737854, ETA in seconds: 1798002.306\n",
      "epoch: 351100, train loss: 3.8913998365402223, val loss: 3.9668463468551636, ETA in seconds: 1798306.307\n",
      "epoch: 351200, train loss: 3.8862571477890016, val loss: 3.973095417022705, ETA in seconds: 1798539.662\n",
      "epoch: 351300, train loss: 3.8901344776153564, val loss: 3.969593858718872, ETA in seconds: 1798738.330\n",
      "epoch: 351400, train loss: 3.897412395477295, val loss: 3.9678647041320803, ETA in seconds: 1798943.523\n",
      "epoch: 351500, train loss: 3.9000681161880495, val loss: 3.9699820280075073, ETA in seconds: 1799139.744\n",
      "epoch: 351600, train loss: 3.8944567918777464, val loss: 3.958145332336426, ETA in seconds: 1799335.894\n",
      "epoch: 351700, train loss: 3.895022463798523, val loss: 3.9795565843582152, ETA in seconds: 1799541.927\n",
      "epoch: 351800, train loss: 3.896554732322693, val loss: 3.9717543840408327, ETA in seconds: 1799760.079\n",
      "epoch: 351900, train loss: 3.8876534700393677, val loss: 3.966616463661194, ETA in seconds: 1799973.490\n",
      "epoch: 352000, train loss: 3.8896506071090697, val loss: 3.9609708070755003, ETA in seconds: 1800180.980\n",
      "epoch: 352100, train loss: 3.8906333208084107, val loss: 3.975456976890564, ETA in seconds: 1800380.333\n",
      "epoch: 352200, train loss: 3.891134166717529, val loss: 3.981453800201416, ETA in seconds: 1800562.819\n",
      "epoch: 352300, train loss: 3.8909467220306397, val loss: 3.9675416707992555, ETA in seconds: 1800754.671\n",
      "epoch: 352400, train loss: 3.8932225465774537, val loss: 3.9687649965286256, ETA in seconds: 1800946.365\n",
      "epoch: 352500, train loss: 3.8976078271865844, val loss: 3.9715190649032595, ETA in seconds: 1801145.622\n",
      "epoch: 352600, train loss: 3.8957542181015015, val loss: 3.96898775100708, ETA in seconds: 1801341.161\n",
      "epoch: 352700, train loss: 3.897366261482239, val loss: 3.983034610748291, ETA in seconds: 1801599.137\n",
      "epoch: 352800, train loss: 3.8906146049499513, val loss: 3.9707326889038086, ETA in seconds: 1801785.030\n",
      "epoch: 352900, train loss: 3.8861766338348387, val loss: 3.9687737941741945, ETA in seconds: 1801976.769\n",
      "epoch: 353000, train loss: 3.8988036155700683, val loss: 3.971379017829895, ETA in seconds: 1802163.040\n",
      "epoch: 353100, train loss: 3.892674994468689, val loss: 3.9764594793319703, ETA in seconds: 1802348.817\n",
      "epoch: 353200, train loss: 3.8986244678497313, val loss: 3.9769652605056764, ETA in seconds: 1802544.693\n",
      "epoch: 353300, train loss: 3.8886558771133424, val loss: 3.960266447067261, ETA in seconds: 1802750.441\n",
      "epoch: 353400, train loss: 3.8948428630828857, val loss: 3.970136308670044, ETA in seconds: 1802938.687\n",
      "epoch: 353500, train loss: 3.8926289796829225, val loss: 3.9694945096969603, ETA in seconds: 1803115.025\n",
      "epoch: 353600, train loss: 3.8897861003875733, val loss: 3.9611584901809693, ETA in seconds: 1803294.256\n",
      "epoch: 353700, train loss: 3.8943732738494874, val loss: 3.974373960494995, ETA in seconds: 1803479.861\n",
      "epoch: 353800, train loss: 3.8887035846710205, val loss: 3.9816266536712646, ETA in seconds: 1803660.945\n",
      "epoch: 353900, train loss: 3.886456251144409, val loss: 3.965174651145935, ETA in seconds: 1803842.351\n",
      "epoch: 354000, train loss: 3.891577696800232, val loss: 3.9724711418151855, ETA in seconds: 1804046.170\n",
      "epoch: 354100, train loss: 3.897571873664856, val loss: 3.9679147005081177, ETA in seconds: 1804229.761\n",
      "epoch: 354200, train loss: 3.9025161504745483, val loss: 3.977773833274841, ETA in seconds: 1804418.504\n",
      "epoch: 354300, train loss: 3.892708158493042, val loss: 3.9702077150344848, ETA in seconds: 1804601.515\n",
      "epoch: 354400, train loss: 3.898152780532837, val loss: 3.9670296192169188, ETA in seconds: 1804786.829\n",
      "epoch: 354500, train loss: 3.8888746738433837, val loss: 3.970922756195068, ETA in seconds: 1804974.329\n",
      "epoch: 354600, train loss: 3.8927692651748655, val loss: 3.9661556243896485, ETA in seconds: 1805157.358\n",
      "epoch: 354700, train loss: 3.8959516525268554, val loss: 3.960311698913574, ETA in seconds: 1805359.622\n",
      "epoch: 354800, train loss: 3.8943592309951782, val loss: 3.9604862451553347, ETA in seconds: 1805544.437\n",
      "epoch: 354900, train loss: 3.8973658800125124, val loss: 3.972631740570068, ETA in seconds: 1805725.671\n",
      "epoch: 355000, train loss: 3.8849204540252686, val loss: 3.967888069152832, ETA in seconds: 1805911.338\n",
      "epoch: 355100, train loss: 3.894451856613159, val loss: 3.967984938621521, ETA in seconds: 1806096.958\n",
      "epoch: 355200, train loss: 3.894561743736267, val loss: 3.96907114982605, ETA in seconds: 1806278.333\n",
      "epoch: 355300, train loss: 3.900575351715088, val loss: 3.9718793869018554, ETA in seconds: 1806462.204\n",
      "epoch: 355400, train loss: 3.8975051403045655, val loss: 3.9683700799942017, ETA in seconds: 1806684.832\n",
      "epoch: 355500, train loss: 3.8927807092666624, val loss: 3.957696533203125, ETA in seconds: 1806906.336\n",
      "epoch: 355600, train loss: 3.8963234424591064, val loss: 3.9792743921279907, ETA in seconds: 1807093.984\n",
      "epoch: 355700, train loss: 3.900551414489746, val loss: 3.9804558515548707, ETA in seconds: 1807276.811\n",
      "epoch: 355800, train loss: 3.894693374633789, val loss: 3.9667898654937743, ETA in seconds: 1807457.458\n",
      "epoch: 355900, train loss: 3.8908565044403076, val loss: 3.9688541412353517, ETA in seconds: 1807640.609\n",
      "epoch: 356000, train loss: 3.886877250671387, val loss: 3.968340516090393, ETA in seconds: 1807823.677\n",
      "epoch: 356100, train loss: 3.8960144996643065, val loss: 3.9661935329437257, ETA in seconds: 1808004.102\n",
      "epoch: 356200, train loss: 3.8860123634338377, val loss: 3.967665934562683, ETA in seconds: 1808201.487\n",
      "epoch: 356300, train loss: 3.8993517637252806, val loss: 3.9642009019851683, ETA in seconds: 1808386.887\n",
      "epoch: 356400, train loss: 3.902986979484558, val loss: 3.96734824180603, ETA in seconds: 1808567.760\n",
      "epoch: 356500, train loss: 3.8955697059631347, val loss: 3.961422920227051, ETA in seconds: 1808748.724\n",
      "epoch: 356600, train loss: 3.894683051109314, val loss: 3.9710158348083495, ETA in seconds: 1808936.814\n",
      "epoch: 356700, train loss: 3.8984256029129027, val loss: 3.9759830474853515, ETA in seconds: 1809122.868\n",
      "epoch: 356800, train loss: 3.8919836044311524, val loss: 3.968243646621704, ETA in seconds: 1809306.416\n",
      "epoch: 356900, train loss: 3.8923914432525635, val loss: 3.958841609954834, ETA in seconds: 1809500.932\n",
      "epoch: 357000, train loss: 3.895144534111023, val loss: 3.9686975479125977, ETA in seconds: 1809682.345\n",
      "epoch: 357100, train loss: 3.88948233127594, val loss: 3.96892306804657, ETA in seconds: 1809884.077\n",
      "epoch: 357200, train loss: 3.894504690170288, val loss: 3.968427062034607, ETA in seconds: 1810150.883\n",
      "epoch: 357300, train loss: 3.8952154159545898, val loss: 3.971235227584839, ETA in seconds: 1810328.728\n",
      "epoch: 357400, train loss: 3.8902778148651125, val loss: 3.963175964355469, ETA in seconds: 1810594.974\n",
      "epoch: 357500, train loss: 3.8924601078033447, val loss: 3.974752640724182, ETA in seconds: 1810780.585\n",
      "epoch: 357600, train loss: 3.887661075592041, val loss: 3.9678704500198365, ETA in seconds: 1811011.154\n",
      "epoch: 357700, train loss: 3.8889459371566772, val loss: 3.9688582181930543, ETA in seconds: 1811289.752\n",
      "epoch: 357800, train loss: 3.8968693256378173, val loss: 3.9699371337890623, ETA in seconds: 1811473.853\n",
      "epoch: 357900, train loss: 3.8930384159088134, val loss: 3.988732409477234, ETA in seconds: 1811655.525\n",
      "epoch: 358000, train loss: 3.897055506706238, val loss: 3.9688734292984007, ETA in seconds: 1811848.013\n",
      "epoch: 358100, train loss: 3.8839690685272217, val loss: 3.9759713888168333, ETA in seconds: 1812041.298\n",
      "epoch: 358200, train loss: 3.8896644115448, val loss: 3.9687060356140136, ETA in seconds: 1812230.691\n",
      "epoch: 358300, train loss: 3.891888403892517, val loss: 3.958015704154968, ETA in seconds: 1812434.112\n",
      "epoch: 358400, train loss: 3.8929975032806396, val loss: 3.975878930091858, ETA in seconds: 1812629.589\n",
      "epoch: 358500, train loss: 3.8949734449386595, val loss: 3.9696312665939333, ETA in seconds: 1812819.282\n",
      "epoch: 358600, train loss: 3.8883117198944093, val loss: 3.961849403381348, ETA in seconds: 1813012.389\n",
      "epoch: 358700, train loss: 3.8960922241210936, val loss: 3.9676599979400633, ETA in seconds: 1813283.433\n",
      "epoch: 358800, train loss: 3.888854241371155, val loss: 3.9699817657470704, ETA in seconds: 1813562.008\n",
      "epoch: 358900, train loss: 3.8931804418563845, val loss: 3.9633350372314453, ETA in seconds: 1813775.006\n",
      "epoch: 359000, train loss: 3.885284686088562, val loss: 3.9658294916152954, ETA in seconds: 1813991.378\n",
      "epoch: 359100, train loss: 3.889471483230591, val loss: 3.965938758850098, ETA in seconds: 1814203.106\n",
      "epoch: 359200, train loss: 3.902875304222107, val loss: 3.965220046043396, ETA in seconds: 1814429.950\n",
      "epoch: 359300, train loss: 3.8822413206100466, val loss: 3.967399835586548, ETA in seconds: 1814720.272\n",
      "epoch: 359400, train loss: 3.89650182723999, val loss: 3.9743200302124024, ETA in seconds: 1814978.084\n",
      "epoch: 359500, train loss: 3.8840650796890257, val loss: 3.9643107414245606, ETA in seconds: 1815181.008\n",
      "epoch: 359600, train loss: 3.891698145866394, val loss: 3.9758689165115357, ETA in seconds: 1815378.865\n",
      "epoch: 359700, train loss: 3.8963509798049927, val loss: 3.9641727447509765, ETA in seconds: 1815638.870\n",
      "epoch: 359800, train loss: 3.8875826358795167, val loss: 3.96348876953125, ETA in seconds: 1815886.345\n",
      "epoch: 359900, train loss: 3.8914777517318724, val loss: 3.9697376489639282, ETA in seconds: 1816103.001\n",
      "epoch: 360000, train loss: 3.8886337995529177, val loss: 3.9649396419525145, ETA in seconds: 1816305.835\n",
      "epoch: 360100, train loss: 3.885973834991455, val loss: 3.965943932533264, ETA in seconds: 1816545.169\n",
      "epoch: 360200, train loss: 3.8909651041030884, val loss: 3.9705958366394043, ETA in seconds: 1816823.394\n",
      "epoch: 360300, train loss: 3.888051915168762, val loss: 3.943566370010376, ETA in seconds: 1817032.181\n",
      "epoch: 360400, train loss: 3.8821433067321776, val loss: 3.9693984746932984, ETA in seconds: 1817240.966\n",
      "epoch: 360500, train loss: 3.898476314544678, val loss: 3.9701143980026243, ETA in seconds: 1817446.980\n",
      "epoch: 360600, train loss: 3.8924744606018065, val loss: 3.972285509109497, ETA in seconds: 1817645.894\n",
      "epoch: 360700, train loss: 3.880915904045105, val loss: 3.9666023015975953, ETA in seconds: 1817868.256\n",
      "epoch: 360800, train loss: 3.896209955215454, val loss: 3.9727020025253297, ETA in seconds: 1818116.254\n",
      "epoch: 360900, train loss: 3.8933456182479858, val loss: 3.9659818172454835, ETA in seconds: 1818308.859\n",
      "epoch: 361000, train loss: 3.888581132888794, val loss: 3.9714381456375123, ETA in seconds: 1818536.174\n",
      "epoch: 361100, train loss: 3.895916152000427, val loss: 3.972146987915039, ETA in seconds: 1818787.865\n",
      "epoch: 361200, train loss: 3.8866384029388428, val loss: 3.9767773151397705, ETA in seconds: 1819027.204\n",
      "epoch: 361300, train loss: 3.8999687671661376, val loss: 3.973591113090515, ETA in seconds: 1819220.837\n",
      "epoch: 361400, train loss: 3.8961238861083984, val loss: 3.96892511844635, ETA in seconds: 1819445.967\n",
      "epoch: 361500, train loss: 3.8958053588867188, val loss: 3.975402331352234, ETA in seconds: 1819734.342\n",
      "epoch: 361600, train loss: 3.8855231523513796, val loss: 3.9672627449035645, ETA in seconds: 1820023.231\n",
      "epoch: 361700, train loss: 3.8931103706359864, val loss: 3.97758948802948, ETA in seconds: 1820260.352\n",
      "epoch: 361800, train loss: 3.8903022050857543, val loss: 3.97477388381958, ETA in seconds: 1820456.002\n",
      "epoch: 361900, train loss: 3.895222020149231, val loss: 3.961074686050415, ETA in seconds: 1820648.856\n",
      "epoch: 362000, train loss: 3.8973946809768676, val loss: 3.9657936811447145, ETA in seconds: 1820861.921\n",
      "epoch: 362100, train loss: 3.8831326961517334, val loss: 3.964476227760315, ETA in seconds: 1821093.869\n",
      "epoch: 362200, train loss: 3.8802590370178223, val loss: 3.975059700012207, ETA in seconds: 1821303.757\n",
      "epoch: 362300, train loss: 3.8810151338577272, val loss: 3.966904354095459, ETA in seconds: 1821534.357\n",
      "epoch: 362400, train loss: 3.8906818628311157, val loss: 3.9682188987731934, ETA in seconds: 1821816.129\n",
      "epoch: 362500, train loss: 3.8974721908569334, val loss: 3.9648812055587768, ETA in seconds: 1822057.496\n",
      "epoch: 362600, train loss: 3.8969770193099977, val loss: 3.959994339942932, ETA in seconds: 1822251.297\n",
      "epoch: 362700, train loss: 3.893000364303589, val loss: 3.9587939977645874, ETA in seconds: 1822444.456\n",
      "epoch: 362800, train loss: 3.8966378450393675, val loss: 3.980851745605469, ETA in seconds: 1822647.565\n",
      "epoch: 362900, train loss: 3.8865853548049927, val loss: 3.977171611785889, ETA in seconds: 1822879.486\n",
      "epoch: 363000, train loss: 3.8890368223190306, val loss: 3.970148491859436, ETA in seconds: 1823087.161\n",
      "epoch: 363100, train loss: 3.8922924757003785, val loss: 3.9727277517318726, ETA in seconds: 1823272.696\n",
      "epoch: 363200, train loss: 3.900185561180115, val loss: 3.9652485132217405, ETA in seconds: 1823562.843\n",
      "epoch: 363300, train loss: 3.8881991863250733, val loss: 3.970529341697693, ETA in seconds: 1823901.317\n",
      "epoch: 363400, train loss: 3.900478553771973, val loss: 3.9697878122329713, ETA in seconds: 1824123.246\n",
      "epoch: 363500, train loss: 3.889147996902466, val loss: 3.9715609550476074, ETA in seconds: 1824305.469\n",
      "epoch: 363600, train loss: 3.8917905569076536, val loss: 3.969683885574341, ETA in seconds: 1824489.032\n",
      "epoch: 363700, train loss: 3.8925976514816285, val loss: 3.965420699119568, ETA in seconds: 1824684.030\n",
      "epoch: 363800, train loss: 3.8966245889663695, val loss: 3.9688002109527587, ETA in seconds: 1824874.279\n",
      "epoch: 363900, train loss: 3.888190221786499, val loss: 3.97575364112854, ETA in seconds: 1825046.517\n",
      "epoch: 364000, train loss: 3.8959359407424925, val loss: 3.9672842741012575, ETA in seconds: 1825255.913\n",
      "epoch: 364100, train loss: 3.8932005882263185, val loss: 3.9707716941833495, ETA in seconds: 1825449.680\n",
      "epoch: 364200, train loss: 3.893995523452759, val loss: 3.974285101890564, ETA in seconds: 1825629.602\n",
      "epoch: 364300, train loss: 3.8867213249206545, val loss: 3.962002158164978, ETA in seconds: 1825814.293\n",
      "epoch: 364400, train loss: 3.889524817466736, val loss: 3.9654839754104616, ETA in seconds: 1826003.724\n",
      "epoch: 364500, train loss: 3.8921476125717165, val loss: 3.96936981678009, ETA in seconds: 1826185.171\n",
      "epoch: 364600, train loss: 3.8977812051773073, val loss: 3.9636224031448366, ETA in seconds: 1826387.919\n",
      "epoch: 364700, train loss: 3.89840886592865, val loss: 3.9699517488479614, ETA in seconds: 1826588.971\n",
      "epoch: 364800, train loss: 3.892824387550354, val loss: 3.9732707738876343, ETA in seconds: 1826748.692\n",
      "epoch: 364900, train loss: 3.8984223127365114, val loss: 3.9759835243225097, ETA in seconds: 1826913.767\n",
      "epoch: 365000, train loss: 3.8859164237976076, val loss: 3.983346939086914, ETA in seconds: 1827099.992\n",
      "epoch: 365100, train loss: 3.88832483291626, val loss: 3.9783449172973633, ETA in seconds: 1827302.829\n",
      "epoch: 365200, train loss: 3.8911784887313843, val loss: 3.9708213090896605, ETA in seconds: 1827499.534\n",
      "epoch: 365300, train loss: 3.894139266014099, val loss: 3.973607325553894, ETA in seconds: 1827666.920\n",
      "epoch: 365400, train loss: 3.8924395561218263, val loss: 3.975194001197815, ETA in seconds: 1827835.266\n",
      "epoch: 365500, train loss: 3.8926865577697756, val loss: 3.974616003036499, ETA in seconds: 1828021.543\n",
      "epoch: 365600, train loss: 3.881907081604004, val loss: 3.9758163928985595, ETA in seconds: 1828211.218\n",
      "epoch: 365700, train loss: 3.8943634986877442, val loss: 3.9748006582260134, ETA in seconds: 1828387.212\n",
      "epoch: 365800, train loss: 3.8875981092453005, val loss: 3.980404186248779, ETA in seconds: 1828631.389\n",
      "epoch: 365900, train loss: 3.8963408708572387, val loss: 3.968937540054321, ETA in seconds: 1828827.723\n",
      "epoch: 366000, train loss: 3.90152530670166, val loss: 3.97072696685791, ETA in seconds: 1829023.356\n",
      "epoch: 366100, train loss: 3.888633608818054, val loss: 3.9584954261779783, ETA in seconds: 1829223.403\n",
      "epoch: 366200, train loss: 3.9001566648483275, val loss: 3.970406723022461, ETA in seconds: 1829422.238\n",
      "epoch: 366300, train loss: 3.8924331665039062, val loss: 3.9660319089889526, ETA in seconds: 1829616.034\n",
      "epoch: 366400, train loss: 3.900501561164856, val loss: 3.9701212882995605, ETA in seconds: 1829807.136\n",
      "epoch: 366500, train loss: 3.885293889045715, val loss: 3.964042091369629, ETA in seconds: 1829969.771\n",
      "epoch: 366600, train loss: 3.9006727933883667, val loss: 3.9654301404953003, ETA in seconds: 1830151.241\n",
      "epoch: 366700, train loss: 3.8966280698776243, val loss: 3.9619894504547117, ETA in seconds: 1830353.667\n",
      "epoch: 366800, train loss: 3.9030935764312744, val loss: 3.9638278007507326, ETA in seconds: 1830536.815\n",
      "epoch: 366900, train loss: 3.893092727661133, val loss: 3.967169904708862, ETA in seconds: 1830708.911\n",
      "epoch: 367000, train loss: 3.8934935092926026, val loss: 3.978837823867798, ETA in seconds: 1830892.140\n",
      "epoch: 367100, train loss: 3.8933694839477537, val loss: 3.9664958000183104, ETA in seconds: 1831093.731\n",
      "epoch: 367200, train loss: 3.8895199060440064, val loss: 3.962559151649475, ETA in seconds: 1831323.119\n",
      "epoch: 367300, train loss: 3.8833399057388305, val loss: 3.967936635017395, ETA in seconds: 1831576.678\n",
      "epoch: 367400, train loss: 3.8918063402175904, val loss: 3.9729005813598635, ETA in seconds: 1831876.805\n",
      "epoch: 367500, train loss: 3.8890205144882204, val loss: 3.9795026540756226, ETA in seconds: 1832176.440\n",
      "epoch: 367600, train loss: 3.8950194120407104, val loss: 3.97023720741272, ETA in seconds: 1832411.994\n",
      "epoch: 367700, train loss: 3.8901215314865114, val loss: 3.979971718788147, ETA in seconds: 1832609.482\n",
      "epoch: 367800, train loss: 3.896929478645325, val loss: 3.9714077949523925, ETA in seconds: 1832778.340\n",
      "epoch: 367900, train loss: 3.893808054924011, val loss: 3.969594120979309, ETA in seconds: 1832983.101\n",
      "epoch: 368000, train loss: 3.898085618019104, val loss: 3.9702208042144775, ETA in seconds: 1833265.925\n",
      "epoch: 368100, train loss: 3.8995355129241944, val loss: 3.9853795766830444, ETA in seconds: 1833468.624\n",
      "epoch: 368200, train loss: 3.8934322357177735, val loss: 3.9670629262924195, ETA in seconds: 1833638.995\n",
      "epoch: 368300, train loss: 3.8910797595977784, val loss: 3.967758798599243, ETA in seconds: 1833822.410\n",
      "epoch: 368400, train loss: 3.89025194644928, val loss: 3.9750202894210815, ETA in seconds: 1833987.889\n",
      "epoch: 368500, train loss: 3.891528916358948, val loss: 3.9709161520004272, ETA in seconds: 1834200.624\n",
      "epoch: 368600, train loss: 3.892625617980957, val loss: 3.971767783164978, ETA in seconds: 1834476.883\n",
      "epoch: 368700, train loss: 3.886550450325012, val loss: 3.968487858772278, ETA in seconds: 1834645.165\n",
      "epoch: 368800, train loss: 3.8920359134674074, val loss: 3.9632836818695067, ETA in seconds: 1834870.022\n",
      "epoch: 368900, train loss: 3.8920017957687376, val loss: 3.9715677738189696, ETA in seconds: 1835148.573\n",
      "epoch: 369000, train loss: 3.8886348724365236, val loss: 3.971267557144165, ETA in seconds: 1835426.240\n",
      "epoch: 369100, train loss: 3.887998104095459, val loss: 3.961066222190857, ETA in seconds: 1835706.647\n",
      "epoch: 369200, train loss: 3.899480438232422, val loss: 3.9724006175994875, ETA in seconds: 1835989.229\n",
      "epoch: 369300, train loss: 3.8923073530197145, val loss: 3.961443877220154, ETA in seconds: 1836231.248\n",
      "epoch: 369400, train loss: 3.891107678413391, val loss: 3.982106065750122, ETA in seconds: 1836419.443\n",
      "epoch: 369500, train loss: 3.893143343925476, val loss: 3.9724231004714965, ETA in seconds: 1836590.395\n",
      "epoch: 369600, train loss: 3.8970549583435057, val loss: 3.9676884412765503, ETA in seconds: 1836785.443\n",
      "epoch: 369700, train loss: 3.8924099922180178, val loss: 3.971313738822937, ETA in seconds: 1837059.099\n",
      "epoch: 369800, train loss: 3.894249606132507, val loss: 3.965131902694702, ETA in seconds: 1837274.359\n",
      "epoch: 369900, train loss: 3.8940754890441895, val loss: 3.9750779628753663, ETA in seconds: 1837455.447\n",
      "epoch: 370000, train loss: 3.89311146736145, val loss: 3.9855164527893066, ETA in seconds: 1837644.007\n",
      "epoch: 370100, train loss: 3.884007143974304, val loss: 3.9757332801818848, ETA in seconds: 1837832.614\n",
      "epoch: 370200, train loss: 3.8892497301101683, val loss: 3.976464319229126, ETA in seconds: 1838019.464\n",
      "epoch: 370300, train loss: 3.8942845106124877, val loss: 3.9800094604492187, ETA in seconds: 1838269.972\n",
      "epoch: 370400, train loss: 3.8978844165802, val loss: 3.9705222606658936, ETA in seconds: 1838538.116\n",
      "epoch: 370500, train loss: 3.896177363395691, val loss: 3.9667683124542235, ETA in seconds: 1838812.036\n",
      "epoch: 370600, train loss: 3.8980151176452638, val loss: 3.9786136627197264, ETA in seconds: 1839046.525\n",
      "epoch: 370700, train loss: 3.888607048988342, val loss: 3.961431097984314, ETA in seconds: 1839207.223\n",
      "epoch: 370800, train loss: 3.8977075815200806, val loss: 3.968749189376831, ETA in seconds: 1839376.948\n",
      "epoch: 370900, train loss: 3.8824389219284057, val loss: 3.981186580657959, ETA in seconds: 1839559.794\n",
      "epoch: 371000, train loss: 3.890939474105835, val loss: 3.980796790122986, ETA in seconds: 1839753.114\n",
      "epoch: 371100, train loss: 3.8933319091796874, val loss: 3.9671849727630617, ETA in seconds: 1839940.078\n",
      "epoch: 371200, train loss: 3.8933976888656616, val loss: 3.971945858001709, ETA in seconds: 1840102.558\n",
      "epoch: 371300, train loss: 3.909367799758911, val loss: 3.968566560745239, ETA in seconds: 1840284.917\n",
      "epoch: 371400, train loss: 3.8917632818222048, val loss: 3.970060443878174, ETA in seconds: 1840436.187\n",
      "epoch: 371500, train loss: 3.910227084159851, val loss: 3.9741158962249754, ETA in seconds: 1840605.759\n",
      "epoch: 371600, train loss: 3.88704137802124, val loss: 3.9753803253173827, ETA in seconds: 1840777.213\n",
      "epoch: 371700, train loss: 3.8861502408981323, val loss: 3.9735596656799315, ETA in seconds: 1840942.827\n",
      "epoch: 371800, train loss: 3.9031100511550902, val loss: 3.982073426246643, ETA in seconds: 1841165.434\n",
      "epoch: 371900, train loss: 3.8966185569763185, val loss: 3.9677467346191406, ETA in seconds: 1841358.768\n",
      "epoch: 372000, train loss: 3.886220645904541, val loss: 3.9684019327163695, ETA in seconds: 1841526.041\n",
      "epoch: 372100, train loss: 3.888234090805054, val loss: 3.971236538887024, ETA in seconds: 1841704.991\n",
      "epoch: 372200, train loss: 3.894561195373535, val loss: 3.99193696975708, ETA in seconds: 1841866.462\n",
      "epoch: 372300, train loss: 3.898153877258301, val loss: 3.974797773361206, ETA in seconds: 1842016.523\n",
      "epoch: 372400, train loss: 3.8896345615386965, val loss: 3.95691339969635, ETA in seconds: 1842176.085\n",
      "epoch: 372500, train loss: 3.8897757053375246, val loss: 3.9668366432189943, ETA in seconds: 1842335.258\n",
      "epoch: 372600, train loss: 3.897957420349121, val loss: 3.974748229980469, ETA in seconds: 1842578.855\n",
      "epoch: 372700, train loss: 3.9050158977508547, val loss: 3.9687024354934692, ETA in seconds: 1842801.457\n",
      "epoch: 372800, train loss: 3.889103150367737, val loss: 3.968173289299011, ETA in seconds: 1843062.888\n",
      "epoch: 372900, train loss: 3.889466381072998, val loss: 3.968067741394043, ETA in seconds: 1843235.564\n",
      "epoch: 373000, train loss: 3.8863849878311156, val loss: 3.96006383895874, ETA in seconds: 1843421.011\n",
      "epoch: 373100, train loss: 3.895472598075867, val loss: 3.9726683616638185, ETA in seconds: 1843603.315\n",
      "epoch: 373200, train loss: 3.8866871118545534, val loss: 3.9688122987747194, ETA in seconds: 1843779.413\n",
      "epoch: 373300, train loss: 3.8887245655059814, val loss: 3.9694799900054933, ETA in seconds: 1844021.758\n",
      "epoch: 373400, train loss: 3.888333034515381, val loss: 3.9768192529678346, ETA in seconds: 1844196.360\n",
      "epoch: 373500, train loss: 3.8955662727355955, val loss: 3.9761274814605714, ETA in seconds: 1844385.263\n",
      "epoch: 373600, train loss: 3.8875973224639893, val loss: 3.974300575256348, ETA in seconds: 1844564.868\n",
      "epoch: 373700, train loss: 3.8895621061325074, val loss: 3.9627835512161256, ETA in seconds: 1844742.191\n",
      "epoch: 373800, train loss: 3.8953208923339844, val loss: 3.9743292808532713, ETA in seconds: 1844921.674\n",
      "epoch: 373900, train loss: 3.8930413484573365, val loss: 3.9675915241241455, ETA in seconds: 1845135.215\n",
      "epoch: 374000, train loss: 3.887881064414978, val loss: 3.9793785333633425, ETA in seconds: 1845412.357\n",
      "epoch: 374100, train loss: 3.8952057361602783, val loss: 3.9689300298690795, ETA in seconds: 1845683.291\n",
      "epoch: 374200, train loss: 3.90251567363739, val loss: 3.975911545753479, ETA in seconds: 1845956.092\n",
      "epoch: 374300, train loss: 3.8949460983276367, val loss: 3.9730358600616453, ETA in seconds: 1846190.263\n",
      "epoch: 374400, train loss: 3.8954344749450684, val loss: 3.970931625366211, ETA in seconds: 1846394.509\n",
      "epoch: 374500, train loss: 3.898170232772827, val loss: 3.985330367088318, ETA in seconds: 1846661.949\n",
      "epoch: 374600, train loss: 3.8922712802886963, val loss: 3.9748889684677122, ETA in seconds: 1846932.291\n",
      "epoch: 374700, train loss: 3.904793548583984, val loss: 3.97414026260376, ETA in seconds: 1847193.662\n",
      "epoch: 374800, train loss: 3.8974482297897337, val loss: 3.9798675537109376, ETA in seconds: 1847413.387\n",
      "epoch: 374900, train loss: 3.891524004936218, val loss: 3.9644603967666625, ETA in seconds: 1847617.147\n",
      "epoch: 375000, train loss: 3.88196177482605, val loss: 3.9709349632263184, ETA in seconds: 1847843.738\n",
      "epoch: 375100, train loss: 3.902744269371033, val loss: 3.9768900156021116, ETA in seconds: 1848102.101\n",
      "epoch: 375200, train loss: 3.8826111555099487, val loss: 3.968423581123352, ETA in seconds: 1848268.569\n",
      "epoch: 375300, train loss: 3.8854439735412596, val loss: 3.966049838066101, ETA in seconds: 1848434.047\n",
      "epoch: 375400, train loss: 3.887048053741455, val loss: 3.9882447957992553, ETA in seconds: 1848611.741\n",
      "epoch: 375500, train loss: 3.8954745054244997, val loss: 3.9780150175094606, ETA in seconds: 1848793.142\n",
      "epoch: 375600, train loss: 3.8998071432113646, val loss: 3.968567419052124, ETA in seconds: 1849025.142\n",
      "epoch: 375700, train loss: 3.8911440849304197, val loss: 3.9790732145309446, ETA in seconds: 1849293.714\n",
      "epoch: 375800, train loss: 3.898745584487915, val loss: 3.963728594779968, ETA in seconds: 1849499.702\n",
      "epoch: 375900, train loss: 3.8916208267211916, val loss: 3.9704389333724976, ETA in seconds: 1849667.929\n",
      "epoch: 376000, train loss: 3.8948652744293213, val loss: 3.978482484817505, ETA in seconds: 1849835.233\n",
      "epoch: 376100, train loss: 3.8920323133468626, val loss: 3.971972918510437, ETA in seconds: 1850012.090\n",
      "epoch: 376200, train loss: 3.880633807182312, val loss: 3.9766006231307984, ETA in seconds: 1850200.391\n",
      "epoch: 376300, train loss: 3.888972854614258, val loss: 3.990902471542358, ETA in seconds: 1850494.350\n",
      "epoch: 376400, train loss: 3.8995867013931274, val loss: 3.9738576889038084, ETA in seconds: 1850778.855\n",
      "epoch: 376500, train loss: 3.8947275161743162, val loss: 3.9725064277648925, ETA in seconds: 1850964.843\n",
      "epoch: 376600, train loss: 3.8838077545166017, val loss: 3.9815438032150268, ETA in seconds: 1851152.796\n",
      "epoch: 376700, train loss: 3.8900697946548464, val loss: 3.979613995552063, ETA in seconds: 1851334.964\n",
      "epoch: 376800, train loss: 3.880612587928772, val loss: 3.9771466970443727, ETA in seconds: 1851549.176\n",
      "epoch: 376900, train loss: 3.8997635364532472, val loss: 3.9722776651382445, ETA in seconds: 1851737.864\n",
      "epoch: 377000, train loss: 3.893152093887329, val loss: 3.9810296297073364, ETA in seconds: 1851906.563\n",
      "epoch: 377100, train loss: 3.894192123413086, val loss: 3.9737838745117187, ETA in seconds: 1852071.498\n",
      "epoch: 377200, train loss: 3.898216891288757, val loss: 3.9713786363601686, ETA in seconds: 1852249.148\n",
      "epoch: 377300, train loss: 3.905449652671814, val loss: 3.978150200843811, ETA in seconds: 1852426.675\n",
      "epoch: 377400, train loss: 3.8881539583206175, val loss: 3.9711002349853515, ETA in seconds: 1852585.917\n",
      "epoch: 377500, train loss: 3.886467218399048, val loss: 3.989435315132141, ETA in seconds: 1852728.638\n",
      "epoch: 377600, train loss: 3.893918204307556, val loss: 3.969695472717285, ETA in seconds: 1852881.251\n",
      "epoch: 377700, train loss: 3.8825738191604615, val loss: 3.9823366165161134, ETA in seconds: 1853042.002\n",
      "epoch: 377800, train loss: 3.8961005926132204, val loss: 3.9811458587646484, ETA in seconds: 1853312.373\n",
      "epoch: 377900, train loss: 3.884866404533386, val loss: 3.976851725578308, ETA in seconds: 1853503.384\n",
      "epoch: 378000, train loss: 3.8901771783828734, val loss: 3.9718857049942016, ETA in seconds: 1853684.593\n",
      "epoch: 378100, train loss: 3.900775909423828, val loss: 3.9648037910461427, ETA in seconds: 1853869.289\n",
      "epoch: 378200, train loss: 3.895316743850708, val loss: 3.9757880210876464, ETA in seconds: 1854142.060\n",
      "epoch: 378300, train loss: 3.8942277431488037, val loss: 3.972913312911987, ETA in seconds: 1854452.383\n",
      "epoch: 378400, train loss: 3.892380785942078, val loss: 3.9782880544662476, ETA in seconds: 1854765.022\n",
      "epoch: 378500, train loss: 3.8939496278762817, val loss: 3.980996108055115, ETA in seconds: 1855071.587\n",
      "epoch: 378600, train loss: 3.8912102937698365, val loss: 3.992464709281921, ETA in seconds: 1855385.361\n",
      "epoch: 378700, train loss: 3.88585479259491, val loss: 3.969814324378967, ETA in seconds: 1855682.170\n",
      "epoch: 378800, train loss: 3.8899606227874757, val loss: 3.9831399440765383, ETA in seconds: 1855913.992\n",
      "epoch: 378900, train loss: 3.8973991870880127, val loss: 3.9780930519104003, ETA in seconds: 1856069.504\n",
      "epoch: 379000, train loss: 3.8890686988830567, val loss: 3.9720831871032716, ETA in seconds: 1856226.785\n",
      "epoch: 379100, train loss: 3.889946174621582, val loss: 3.974475049972534, ETA in seconds: 1856389.428\n",
      "epoch: 379200, train loss: 3.892328643798828, val loss: 3.974828863143921, ETA in seconds: 1856563.370\n",
      "epoch: 379300, train loss: 3.887555980682373, val loss: 3.9827282190322877, ETA in seconds: 1856777.342\n",
      "epoch: 379400, train loss: 3.884058713912964, val loss: 3.9750747680664062, ETA in seconds: 1856958.601\n",
      "epoch: 379500, train loss: 3.8934849739074706, val loss: 3.971434760093689, ETA in seconds: 1857128.982\n",
      "epoch: 379600, train loss: 3.9010857820510862, val loss: 3.9756413459777833, ETA in seconds: 1857315.119\n",
      "epoch: 379700, train loss: 3.897724747657776, val loss: 3.9726585149765015, ETA in seconds: 1857496.336\n",
      "epoch: 379800, train loss: 3.886802816390991, val loss: 3.981978917121887, ETA in seconds: 1857667.746\n",
      "epoch: 379900, train loss: 3.8963049173355104, val loss: 3.97659547328949, ETA in seconds: 1857843.970\n",
      "epoch: 380000, train loss: 3.889288091659546, val loss: 3.9793522357940674, ETA in seconds: 1857999.693\n",
      "epoch: 380100, train loss: 3.9008219957351686, val loss: 3.9681744813919066, ETA in seconds: 1858167.029\n",
      "epoch: 380200, train loss: 3.887056994438171, val loss: 3.9753671407699587, ETA in seconds: 1858319.399\n",
      "epoch: 380300, train loss: 3.8938387393951417, val loss: 3.972446417808533, ETA in seconds: 1858479.389\n",
      "epoch: 380400, train loss: 3.8972407817840575, val loss: 3.965285539627075, ETA in seconds: 1858641.103\n",
      "epoch: 380500, train loss: 3.885559582710266, val loss: 3.9716107845306396, ETA in seconds: 1858804.294\n",
      "epoch: 380600, train loss: 3.8931689500808715, val loss: 3.9717782974243163, ETA in seconds: 1858971.178\n",
      "epoch: 380700, train loss: 3.8888591289520265, val loss: 3.975641965866089, ETA in seconds: 1859129.434\n",
      "epoch: 380800, train loss: 3.893932819366455, val loss: 3.9803972244262695, ETA in seconds: 1859290.916\n",
      "epoch: 380900, train loss: 3.8829185247421263, val loss: 3.9742112159729004, ETA in seconds: 1859456.958\n",
      "epoch: 381000, train loss: 3.892960953712463, val loss: 3.964225673675537, ETA in seconds: 1859626.700\n",
      "epoch: 381100, train loss: 3.889094042778015, val loss: 3.97085702419281, ETA in seconds: 1859787.380\n",
      "epoch: 381200, train loss: 3.9011932373046876, val loss: 3.976825165748596, ETA in seconds: 1859946.407\n",
      "epoch: 381300, train loss: 3.903101944923401, val loss: 3.986565899848938, ETA in seconds: 1860105.420\n",
      "epoch: 381400, train loss: 3.8888520002365112, val loss: 3.9774591445922853, ETA in seconds: 1860279.213\n",
      "epoch: 381500, train loss: 3.8974684476852417, val loss: 3.962831473350525, ETA in seconds: 1860450.309\n",
      "epoch: 381600, train loss: 3.891476345062256, val loss: 3.9726014137268066, ETA in seconds: 1860609.974\n",
      "epoch: 381700, train loss: 3.8969529628753663, val loss: 3.9788212299346926, ETA in seconds: 1860773.566\n",
      "epoch: 381800, train loss: 3.884806513786316, val loss: 3.9747397184371946, ETA in seconds: 1860943.255\n",
      "epoch: 381900, train loss: 3.8916812181472777, val loss: 3.9715404748916625, ETA in seconds: 1861099.256\n",
      "epoch: 382000, train loss: 3.893370175361633, val loss: 3.9702650547027587, ETA in seconds: 1861264.732\n",
      "epoch: 382100, train loss: 3.889883780479431, val loss: 3.9718723058700562, ETA in seconds: 1861440.026\n",
      "epoch: 382200, train loss: 3.8943506717681884, val loss: 3.976511836051941, ETA in seconds: 1861597.954\n",
      "epoch: 382300, train loss: 3.8922692060470583, val loss: 3.9785879850387573, ETA in seconds: 1861754.637\n",
      "epoch: 382400, train loss: 3.892021131515503, val loss: 3.975171995162964, ETA in seconds: 1861948.975\n",
      "epoch: 382500, train loss: 3.895194149017334, val loss: 3.9695598602294924, ETA in seconds: 1862248.862\n",
      "epoch: 382600, train loss: 3.895762085914612, val loss: 3.9760696649551392, ETA in seconds: 1862464.052\n",
      "epoch: 382700, train loss: 3.888488435745239, val loss: 3.971482515335083, ETA in seconds: 1862671.347\n",
      "epoch: 382800, train loss: 3.8873035669326783, val loss: 3.96485059261322, ETA in seconds: 1862846.932\n",
      "epoch: 382900, train loss: 3.893760061264038, val loss: 3.975569415092468, ETA in seconds: 1863023.300\n",
      "epoch: 383000, train loss: 3.901063418388367, val loss: 3.975689148902893, ETA in seconds: 1863193.256\n",
      "epoch: 383100, train loss: 3.9015440940856934, val loss: 3.9683586835861204, ETA in seconds: 1863495.464\n",
      "epoch: 383200, train loss: 3.893255877494812, val loss: 3.974360489845276, ETA in seconds: 1863660.564\n",
      "epoch: 383300, train loss: 3.8912362813949586, val loss: 3.9751282215118406, ETA in seconds: 1863814.155\n",
      "epoch: 383400, train loss: 3.8848375558853148, val loss: 3.9673215627670286, ETA in seconds: 1864028.016\n",
      "epoch: 383500, train loss: 3.898689842224121, val loss: 3.980823040008545, ETA in seconds: 1864209.188\n",
      "epoch: 383600, train loss: 3.888956356048584, val loss: 3.9712181091308594, ETA in seconds: 1864374.393\n",
      "epoch: 383700, train loss: 3.899279069900513, val loss: 3.9775357484817504, ETA in seconds: 1864519.792\n",
      "epoch: 383800, train loss: 3.890171027183533, val loss: 3.9652589321136475, ETA in seconds: 1864656.638\n",
      "epoch: 383900, train loss: 3.8884771347045897, val loss: 3.977332663536072, ETA in seconds: 1864798.492\n",
      "epoch: 384000, train loss: 3.898796224594116, val loss: 3.9841510534286497, ETA in seconds: 1864943.338\n",
      "epoch: 384100, train loss: 3.896527624130249, val loss: 3.9746243000030517, ETA in seconds: 1865118.532\n",
      "epoch: 384200, train loss: 3.889973998069763, val loss: 3.982331728935242, ETA in seconds: 1865417.757\n",
      "epoch: 384300, train loss: 3.8844929695129395, val loss: 3.977123260498047, ETA in seconds: 1865646.198\n",
      "epoch: 384400, train loss: 3.8898889064788817, val loss: 3.977528786659241, ETA in seconds: 1865789.462\n",
      "epoch: 384500, train loss: 3.8912777423858644, val loss: 3.9780146360397337, ETA in seconds: 1865967.530\n",
      "epoch: 384600, train loss: 3.8888931274414062, val loss: 3.9763566732406614, ETA in seconds: 1866129.661\n",
      "epoch: 384700, train loss: 3.890869927406311, val loss: 3.9688967943191527, ETA in seconds: 1866299.537\n",
      "epoch: 384800, train loss: 3.8846854686737062, val loss: 3.9717706203460694, ETA in seconds: 1866465.527\n",
      "epoch: 384900, train loss: 3.8933966636657713, val loss: 3.9717605352401733, ETA in seconds: 1866625.017\n",
      "epoch: 385000, train loss: 3.893138885498047, val loss: 3.9666570663452148, ETA in seconds: 1866824.393\n",
      "epoch: 385100, train loss: 3.8911699056625366, val loss: 3.9855721473693846, ETA in seconds: 1867001.938\n",
      "epoch: 385200, train loss: 3.89082350730896, val loss: 3.972111463546753, ETA in seconds: 1867154.440\n",
      "epoch: 385300, train loss: 3.889821910858154, val loss: 3.977586770057678, ETA in seconds: 1867325.562\n",
      "epoch: 385400, train loss: 3.8920885562896728, val loss: 3.9729551792144777, ETA in seconds: 1867559.852\n",
      "epoch: 385500, train loss: 3.9002203941345215, val loss: 3.972671556472778, ETA in seconds: 1867774.954\n",
      "epoch: 385600, train loss: 3.898318123817444, val loss: 3.9668540239334105, ETA in seconds: 1867997.647\n",
      "epoch: 385700, train loss: 3.8947270631790163, val loss: 3.974062919616699, ETA in seconds: 1868146.646\n",
      "epoch: 385800, train loss: 3.8868175745010376, val loss: 3.9810353994369505, ETA in seconds: 1868294.909\n",
      "epoch: 385900, train loss: 3.886538934707642, val loss: 3.9799752712249754, ETA in seconds: 1868473.101\n",
      "epoch: 386000, train loss: 3.886028456687927, val loss: 3.977334499359131, ETA in seconds: 1868735.335\n",
      "epoch: 386100, train loss: 3.8931981563568114, val loss: 3.9837751626968383, ETA in seconds: 1868904.153\n",
      "epoch: 386200, train loss: 3.891920018196106, val loss: 3.975447487831116, ETA in seconds: 1869048.379\n",
      "epoch: 386300, train loss: 3.8901007890701296, val loss: 3.9792699813842773, ETA in seconds: 1869194.986\n",
      "epoch: 386400, train loss: 3.891821551322937, val loss: 3.973161244392395, ETA in seconds: 1869370.456\n",
      "epoch: 386500, train loss: 3.892303204536438, val loss: 3.97115535736084, ETA in seconds: 1869526.652\n",
      "epoch: 386600, train loss: 3.8894151926040648, val loss: 3.9703032970428467, ETA in seconds: 1869675.655\n",
      "epoch: 386700, train loss: 3.8905433416366577, val loss: 3.97628493309021, ETA in seconds: 1869829.877\n",
      "epoch: 386800, train loss: 3.890497589111328, val loss: 3.9834137678146364, ETA in seconds: 1869993.198\n",
      "epoch: 386900, train loss: 3.900843787193298, val loss: 3.975516510009766, ETA in seconds: 1870172.782\n",
      "epoch: 387000, train loss: 3.888325905799866, val loss: 3.9726735830307005, ETA in seconds: 1870340.828\n",
      "epoch: 387100, train loss: 3.888093662261963, val loss: 3.9712281942367555, ETA in seconds: 1870511.371\n",
      "epoch: 387200, train loss: 3.895518255233765, val loss: 3.9856204748153687, ETA in seconds: 1870677.793\n",
      "epoch: 387300, train loss: 3.8977758646011353, val loss: 3.979525256156921, ETA in seconds: 1870837.794\n",
      "epoch: 387400, train loss: 3.8953893184661865, val loss: 3.973345422744751, ETA in seconds: 1871001.128\n",
      "epoch: 387500, train loss: 3.895643377304077, val loss: 3.967330288887024, ETA in seconds: 1871181.429\n",
      "epoch: 387600, train loss: 3.899737763404846, val loss: 3.979833173751831, ETA in seconds: 1871351.950\n",
      "epoch: 387700, train loss: 3.8994523525238036, val loss: 3.979574275016785, ETA in seconds: 1871499.043\n",
      "epoch: 387800, train loss: 3.901533818244934, val loss: 3.9715566396713258, ETA in seconds: 1871657.549\n",
      "epoch: 387900, train loss: 3.8993412971496584, val loss: 3.9761043548583985, ETA in seconds: 1871804.510\n",
      "epoch: 388000, train loss: 3.8945449113845827, val loss: 3.989889645576477, ETA in seconds: 1871963.718\n",
      "epoch: 388100, train loss: 3.8944315195083616, val loss: 3.9786452054977417, ETA in seconds: 1872178.777\n",
      "epoch: 388200, train loss: 3.892199158668518, val loss: 3.9784533023834228, ETA in seconds: 1872343.869\n",
      "epoch: 388300, train loss: 3.899528741836548, val loss: 3.9762003421783447, ETA in seconds: 1872508.261\n",
      "epoch: 388400, train loss: 3.8987029075622557, val loss: 3.9873847723007203, ETA in seconds: 1872751.994\n",
      "epoch: 388500, train loss: 3.893242621421814, val loss: 3.9758565187454225, ETA in seconds: 1872995.860\n",
      "epoch: 388600, train loss: 3.904408168792725, val loss: 3.983206582069397, ETA in seconds: 1873242.766\n",
      "epoch: 388700, train loss: 3.8901440143585204, val loss: 3.9638890266418456, ETA in seconds: 1873490.285\n",
      "epoch: 388800, train loss: 3.8935672521591185, val loss: 3.9681692361831664, ETA in seconds: 1873739.376\n",
      "epoch: 388900, train loss: 3.8939544439315794, val loss: 3.9754276275634766, ETA in seconds: 1873988.706\n",
      "epoch: 389000, train loss: 3.894116735458374, val loss: 3.9660908222198485, ETA in seconds: 1874234.399\n",
      "epoch: 389100, train loss: 3.894731569290161, val loss: 3.968116044998169, ETA in seconds: 1874369.857\n",
      "epoch: 389200, train loss: 3.8861155986785887, val loss: 3.964267611503601, ETA in seconds: 1874538.308\n",
      "epoch: 389300, train loss: 3.8895750999450684, val loss: 3.9833271503448486, ETA in seconds: 1874751.061\n",
      "epoch: 389400, train loss: 3.8835953950881956, val loss: 3.9808504819869994, ETA in seconds: 1874899.421\n",
      "epoch: 389500, train loss: 3.890235161781311, val loss: 3.9693971157073973, ETA in seconds: 1875068.451\n",
      "epoch: 389600, train loss: 3.8948894500732423, val loss: 3.981819987297058, ETA in seconds: 1875221.408\n",
      "epoch: 389700, train loss: 3.895201325416565, val loss: 3.961715745925903, ETA in seconds: 1875389.829\n",
      "epoch: 389800, train loss: 3.8865712165832518, val loss: 3.966442918777466, ETA in seconds: 1875571.555\n",
      "epoch: 389900, train loss: 3.8918689727783202, val loss: 3.974703001976013, ETA in seconds: 1875805.067\n",
      "epoch: 390000, train loss: 3.8937927722930907, val loss: 3.9724239349365233, ETA in seconds: 1876065.912\n",
      "epoch: 390100, train loss: 3.8866920471191406, val loss: 3.967277193069458, ETA in seconds: 1876236.912\n",
      "epoch: 390200, train loss: 3.8935327768325805, val loss: 3.962210774421692, ETA in seconds: 1876399.281\n",
      "epoch: 390300, train loss: 3.905095672607422, val loss: 3.967851400375366, ETA in seconds: 1876595.640\n",
      "epoch: 390400, train loss: 3.8878141403198243, val loss: 3.9686734437942506, ETA in seconds: 1876750.402\n",
      "epoch: 390500, train loss: 3.883444666862488, val loss: 3.964635801315308, ETA in seconds: 1876895.954\n",
      "epoch: 390600, train loss: 3.888528037071228, val loss: 3.967336416244507, ETA in seconds: 1877038.831\n",
      "epoch: 390700, train loss: 3.8902551412582396, val loss: 3.960002303123474, ETA in seconds: 1877190.862\n",
      "epoch: 390800, train loss: 3.880519247055054, val loss: 3.9820507764816284, ETA in seconds: 1877354.432\n",
      "epoch: 390900, train loss: 3.8961997032165527, val loss: 3.9696144819259644, ETA in seconds: 1877499.351\n",
      "epoch: 391000, train loss: 3.8973003149032595, val loss: 3.979918599128723, ETA in seconds: 1877644.877\n",
      "epoch: 391100, train loss: 3.8979365825653076, val loss: 3.9637088060379027, ETA in seconds: 1877787.129\n",
      "epoch: 391200, train loss: 3.8935147523880005, val loss: 3.9774338960647584, ETA in seconds: 1877926.074\n",
      "epoch: 391300, train loss: 3.892644834518433, val loss: 3.964183211326599, ETA in seconds: 1878070.812\n",
      "epoch: 391400, train loss: 3.8938937187194824, val loss: 3.96562922000885, ETA in seconds: 1878236.270\n",
      "epoch: 391500, train loss: 3.8924928665161134, val loss: 3.954728031158447, ETA in seconds: 1878399.650\n",
      "epoch: 391600, train loss: 3.893407678604126, val loss: 3.959820032119751, ETA in seconds: 1878540.705\n",
      "epoch: 391700, train loss: 3.900435137748718, val loss: 3.9614314317703245, ETA in seconds: 1878683.000\n",
      "epoch: 391800, train loss: 3.8900184869766234, val loss: 3.968148708343506, ETA in seconds: 1878821.163\n",
      "epoch: 391900, train loss: 3.8849211931228638, val loss: 3.974652576446533, ETA in seconds: 1878961.692\n",
      "epoch: 392000, train loss: 3.8963974475860597, val loss: 3.9712748765945434, ETA in seconds: 1879101.632\n",
      "epoch: 392100, train loss: 3.8898077249526977, val loss: 3.965216636657715, ETA in seconds: 1879241.005\n",
      "epoch: 392200, train loss: 3.886486530303955, val loss: 3.97836058139801, ETA in seconds: 1879406.649\n",
      "epoch: 392300, train loss: 3.9005650520324706, val loss: 3.975109601020813, ETA in seconds: 1879554.466\n",
      "epoch: 392400, train loss: 3.8943142175674437, val loss: 3.9785207509994507, ETA in seconds: 1879695.223\n",
      "epoch: 392500, train loss: 3.9012834548950197, val loss: 3.970920729637146, ETA in seconds: 1879828.845\n",
      "epoch: 392600, train loss: 3.8904836416244506, val loss: 3.984364891052246, ETA in seconds: 1879976.362\n",
      "epoch: 392700, train loss: 3.8957693815231322, val loss: 3.9666988611221314, ETA in seconds: 1880111.515\n",
      "epoch: 392800, train loss: 3.8987420082092283, val loss: 3.9798418045043946, ETA in seconds: 1880250.239\n",
      "epoch: 392900, train loss: 3.90286238193512, val loss: 3.9804808139801025, ETA in seconds: 1880389.352\n",
      "epoch: 393000, train loss: 3.8942672967910767, val loss: 3.977947306632996, ETA in seconds: 1880534.018\n",
      "epoch: 393100, train loss: 3.8917486667633057, val loss: 3.9794824600219725, ETA in seconds: 1880690.946\n",
      "epoch: 393200, train loss: 3.891279435157776, val loss: 3.970523738861084, ETA in seconds: 1880872.935\n",
      "epoch: 393300, train loss: 3.890768027305603, val loss: 3.973320198059082, ETA in seconds: 1881008.731\n",
      "epoch: 393400, train loss: 3.8924405574798584, val loss: 3.9713269233703614, ETA in seconds: 1881140.895\n",
      "epoch: 393500, train loss: 3.8937263011932375, val loss: 3.9793757438659667, ETA in seconds: 1881268.522\n",
      "epoch: 393600, train loss: 3.897682046890259, val loss: 3.975329279899597, ETA in seconds: 1881415.981\n",
      "epoch: 393700, train loss: 3.8943574905395506, val loss: 3.9787667751312257, ETA in seconds: 1881656.236\n",
      "epoch: 393800, train loss: 3.8901031970977784, val loss: 3.9701011896133425, ETA in seconds: 1881811.947\n",
      "epoch: 393900, train loss: 3.896256375312805, val loss: 3.981756067276001, ETA in seconds: 1881945.920\n",
      "epoch: 394000, train loss: 3.8932356595993043, val loss: 3.975659799575806, ETA in seconds: 1882079.325\n",
      "epoch: 394100, train loss: 3.8902918100357056, val loss: 3.9660815238952636, ETA in seconds: 1882225.567\n",
      "epoch: 394200, train loss: 3.8969564199447633, val loss: 3.979263496398926, ETA in seconds: 1882362.676\n",
      "epoch: 394300, train loss: 3.8865281105041505, val loss: 3.9696370124816895, ETA in seconds: 1882495.872\n",
      "epoch: 394400, train loss: 3.8920061111450197, val loss: 3.9716159820556642, ETA in seconds: 1882637.873\n",
      "epoch: 394500, train loss: 3.896578145027161, val loss: 3.9644896268844603, ETA in seconds: 1882769.770\n",
      "epoch: 394600, train loss: 3.900607967376709, val loss: 3.964192843437195, ETA in seconds: 1882907.271\n",
      "epoch: 394700, train loss: 3.8942451238632203, val loss: 3.9708765268325807, ETA in seconds: 1883039.143\n",
      "epoch: 394800, train loss: 3.901815128326416, val loss: 3.9604807138442992, ETA in seconds: 1883169.754\n",
      "epoch: 394900, train loss: 3.8956217527389527, val loss: 3.963830494880676, ETA in seconds: 1883301.882\n",
      "epoch: 395000, train loss: 3.896310329437256, val loss: 3.958222270011902, ETA in seconds: 1883434.404\n",
      "epoch: 395100, train loss: 3.8863406419754027, val loss: 3.967003417015076, ETA in seconds: 1883574.209\n",
      "epoch: 395200, train loss: 3.896364164352417, val loss: 3.970577764511108, ETA in seconds: 1883709.175\n",
      "epoch: 395300, train loss: 3.880894088745117, val loss: 3.976260256767273, ETA in seconds: 1883840.593\n",
      "epoch: 395400, train loss: 3.89079954624176, val loss: 3.9637309074401856, ETA in seconds: 1883974.363\n",
      "epoch: 395500, train loss: 3.8975839138031008, val loss: 3.9644283294677733, ETA in seconds: 1884117.495\n",
      "epoch: 395600, train loss: 3.897801399230957, val loss: 3.9675363540649413, ETA in seconds: 1884250.052\n",
      "epoch: 395700, train loss: 3.8961031913757322, val loss: 3.9650560855865478, ETA in seconds: 1884383.131\n",
      "epoch: 395800, train loss: 3.8895404815673826, val loss: 3.965408778190613, ETA in seconds: 1884520.515\n",
      "epoch: 395900, train loss: 3.901375389099121, val loss: 3.9787979125976562, ETA in seconds: 1884651.478\n",
      "epoch: 396000, train loss: 3.8925761461257933, val loss: 3.9778117656707765, ETA in seconds: 1884781.143\n",
      "epoch: 396100, train loss: 3.895619583129883, val loss: 3.970660614967346, ETA in seconds: 1884914.190\n",
      "epoch: 396200, train loss: 3.901823329925537, val loss: 3.969814395904541, ETA in seconds: 1885044.100\n",
      "epoch: 396300, train loss: 3.900428056716919, val loss: 3.970233368873596, ETA in seconds: 1885183.931\n",
      "epoch: 396400, train loss: 3.8914599657058715, val loss: 3.9689714193344114, ETA in seconds: 1885315.076\n",
      "epoch: 396500, train loss: 3.8868073225021362, val loss: 3.973781681060791, ETA in seconds: 1885450.398\n",
      "epoch: 396600, train loss: 3.885658860206604, val loss: 3.971553158760071, ETA in seconds: 1885587.397\n",
      "epoch: 396700, train loss: 3.893049192428589, val loss: 3.970820426940918, ETA in seconds: 1885733.473\n",
      "epoch: 396800, train loss: 3.899283027648926, val loss: 3.9671677350997925, ETA in seconds: 1885866.312\n",
      "epoch: 396900, train loss: 3.892166352272034, val loss: 3.966974472999573, ETA in seconds: 1886005.279\n",
      "epoch: 397000, train loss: 3.897436261177063, val loss: 3.9649007320404053, ETA in seconds: 1886136.948\n",
      "epoch: 397100, train loss: 3.892842650413513, val loss: 3.9639188766479494, ETA in seconds: 1886268.031\n",
      "epoch: 397200, train loss: 3.891678977012634, val loss: 3.9709498643875123, ETA in seconds: 1886414.597\n",
      "epoch: 397300, train loss: 3.890040969848633, val loss: 3.96177761554718, ETA in seconds: 1886558.330\n",
      "epoch: 397400, train loss: 3.890246033668518, val loss: 3.9800673484802247, ETA in seconds: 1886780.360\n",
      "epoch: 397500, train loss: 3.888505482673645, val loss: 3.966456341743469, ETA in seconds: 1886999.922\n",
      "epoch: 397600, train loss: 3.8969066619873045, val loss: 3.9809611082077025, ETA in seconds: 1887224.120\n",
      "epoch: 397700, train loss: 3.894751763343811, val loss: 3.9799972772598267, ETA in seconds: 1887402.589\n",
      "epoch: 397800, train loss: 3.8973944425582885, val loss: 3.9762362957000734, ETA in seconds: 1887540.942\n",
      "epoch: 397900, train loss: 3.8939282178878782, val loss: 3.9760627508163453, ETA in seconds: 1887716.867\n",
      "epoch: 398000, train loss: 3.899579572677612, val loss: 3.976265573501587, ETA in seconds: 1887883.746\n",
      "epoch: 398100, train loss: 3.8930546760559084, val loss: 3.9787491083145143, ETA in seconds: 1888020.170\n",
      "epoch: 398200, train loss: 3.8886374473571776, val loss: 3.9631885051727296, ETA in seconds: 1888170.503\n",
      "epoch: 398300, train loss: 3.8970000743865967, val loss: 3.965305709838867, ETA in seconds: 1888303.322\n",
      "epoch: 398400, train loss: 3.8919920682907105, val loss: 3.96866660118103, ETA in seconds: 1888430.598\n",
      "epoch: 398500, train loss: 3.9015224456787108, val loss: 3.970628523826599, ETA in seconds: 1888565.220\n",
      "epoch: 398600, train loss: 3.8915682315826414, val loss: 3.9678396463394163, ETA in seconds: 1888705.221\n",
      "epoch: 398700, train loss: 3.892197513580322, val loss: 3.9687715768814087, ETA in seconds: 1888837.130\n",
      "epoch: 398800, train loss: 3.8879751682281496, val loss: 3.9739211082458494, ETA in seconds: 1889062.695\n",
      "epoch: 398900, train loss: 3.9057827949523927, val loss: 3.964805579185486, ETA in seconds: 1889281.012\n",
      "epoch: 399000, train loss: 3.899038481712341, val loss: 3.971634936332703, ETA in seconds: 1889483.810\n",
      "epoch: 399100, train loss: 3.893470549583435, val loss: 3.9731064796447755, ETA in seconds: 1889621.364\n",
      "epoch: 399200, train loss: 3.8940210342407227, val loss: 3.969439911842346, ETA in seconds: 1889762.857\n",
      "epoch: 399300, train loss: 3.897514486312866, val loss: 3.9778544902801514, ETA in seconds: 1889910.601\n",
      "epoch: 399400, train loss: 3.8938834190368654, val loss: 3.9752437114715575, ETA in seconds: 1890066.270\n",
      "epoch: 399500, train loss: 3.889379596710205, val loss: 3.971007513999939, ETA in seconds: 1890286.972\n",
      "epoch: 399600, train loss: 3.893317151069641, val loss: 3.9720218896865847, ETA in seconds: 1890452.897\n",
      "epoch: 399700, train loss: 3.8920631408691406, val loss: 3.968659496307373, ETA in seconds: 1890607.882\n",
      "epoch: 399800, train loss: 3.885319948196411, val loss: 3.974279546737671, ETA in seconds: 1890758.247\n",
      "epoch: 399900, train loss: 3.8964131832122804, val loss: 3.967815399169922, ETA in seconds: 1890908.505\n",
      "epoch: 400000, train loss: 3.893586444854736, val loss: 3.9635582447052, ETA in seconds: 1891056.237\n",
      "epoch: 400100, train loss: 3.894755148887634, val loss: 3.9716060876846315, ETA in seconds: 1891198.820\n",
      "epoch: 400200, train loss: 3.88937304019928, val loss: 3.9654422044754027, ETA in seconds: 1891353.648\n",
      "epoch: 400300, train loss: 3.89817898273468, val loss: 3.958047389984131, ETA in seconds: 1891484.720\n",
      "epoch: 400400, train loss: 3.890524411201477, val loss: 3.9780441761016845, ETA in seconds: 1891624.143\n",
      "epoch: 400500, train loss: 3.8921348810195924, val loss: 3.9701109886169434, ETA in seconds: 1891765.192\n",
      "epoch: 400600, train loss: 3.9000004529953003, val loss: 3.9746643781661986, ETA in seconds: 1891908.104\n",
      "epoch: 400700, train loss: 3.893062686920166, val loss: 3.96592059135437, ETA in seconds: 1892053.531\n",
      "epoch: 400800, train loss: 3.8920140504837035, val loss: 3.975072979927063, ETA in seconds: 1892188.991\n",
      "epoch: 400900, train loss: 3.895690155029297, val loss: 3.9868836641311645, ETA in seconds: 1892334.564\n",
      "epoch: 401000, train loss: 3.902851104736328, val loss: 3.9623116493225097, ETA in seconds: 1892508.025\n",
      "epoch: 401100, train loss: 3.9011934280395506, val loss: 3.969510889053345, ETA in seconds: 1892740.646\n",
      "epoch: 401200, train loss: 3.8932239055633544, val loss: 3.9704357624053954, ETA in seconds: 1892974.447\n",
      "epoch: 401300, train loss: 3.8932162284851075, val loss: 3.9756667137146, ETA in seconds: 1893203.389\n",
      "epoch: 401400, train loss: 3.8914560317993163, val loss: 3.9740966796875, ETA in seconds: 1893424.414\n",
      "epoch: 401500, train loss: 3.892046499252319, val loss: 3.9708661317825316, ETA in seconds: 1893615.987\n",
      "epoch: 401600, train loss: 3.8892339706420898, val loss: 3.9682836055755617, ETA in seconds: 1893851.210\n",
      "epoch: 401700, train loss: 3.9052322149276733, val loss: 3.966008257865906, ETA in seconds: 1893987.914\n",
      "epoch: 401800, train loss: 3.8913614988327025, val loss: 3.9808099985122682, ETA in seconds: 1894163.535\n",
      "epoch: 401900, train loss: 3.897224020957947, val loss: 3.965446424484253, ETA in seconds: 1894396.893\n",
      "epoch: 402000, train loss: 3.888381099700928, val loss: 3.9649635314941407, ETA in seconds: 1894534.869\n",
      "epoch: 402100, train loss: 3.8867228269577025, val loss: 3.967390513420105, ETA in seconds: 1894737.812\n",
      "epoch: 402200, train loss: 3.897408199310303, val loss: 3.9777786493301392, ETA in seconds: 1894968.610\n",
      "epoch: 402300, train loss: 3.899716281890869, val loss: 3.9652459144592287, ETA in seconds: 1895193.321\n",
      "epoch: 402400, train loss: 3.903794598579407, val loss: 3.959023690223694, ETA in seconds: 1895369.220\n",
      "epoch: 402500, train loss: 3.8905396938323973, val loss: 3.9724992990493773, ETA in seconds: 1895511.228\n",
      "epoch: 402600, train loss: 3.8900219678878782, val loss: 3.9754205465316774, ETA in seconds: 1895656.234\n",
      "epoch: 402700, train loss: 3.8907949447631838, val loss: 3.9788320541381834, ETA in seconds: 1895809.003\n",
      "epoch: 402800, train loss: 3.891155552864075, val loss: 3.9612344980239866, ETA in seconds: 1895943.955\n",
      "epoch: 402900, train loss: 3.892339491844177, val loss: 3.9640725374221804, ETA in seconds: 1896076.496\n",
      "epoch: 403000, train loss: 3.8830670595169066, val loss: 3.9639867544174194, ETA in seconds: 1896225.009\n",
      "epoch: 403100, train loss: 3.887449026107788, val loss: 3.980267286300659, ETA in seconds: 1896396.990\n",
      "epoch: 403200, train loss: 3.892890191078186, val loss: 3.972096824645996, ETA in seconds: 1896596.120\n",
      "epoch: 403300, train loss: 3.883640909194946, val loss: 3.969448137283325, ETA in seconds: 1896734.284\n",
      "epoch: 403400, train loss: 3.901754069328308, val loss: 3.971490573883057, ETA in seconds: 1896864.061\n",
      "epoch: 403500, train loss: 3.8892966985702513, val loss: 3.963294768333435, ETA in seconds: 1896975.165\n",
      "epoch: 403600, train loss: 3.8944815397262573, val loss: 3.9648365020751952, ETA in seconds: 1897100.239\n",
      "epoch: 403700, train loss: 3.8914700031280516, val loss: 3.967367911338806, ETA in seconds: 1897225.758\n",
      "epoch: 403800, train loss: 3.8912541389465334, val loss: 3.9689235925674438, ETA in seconds: 1897356.328\n",
      "epoch: 403900, train loss: 3.881237006187439, val loss: 3.9538629055023193, ETA in seconds: 1897484.922\n",
      "epoch: 404000, train loss: 3.890490436553955, val loss: 3.9659130573272705, ETA in seconds: 1897613.761\n",
      "epoch: 404100, train loss: 3.904253840446472, val loss: 3.9720073699951173, ETA in seconds: 1897751.292\n",
      "epoch: 404200, train loss: 3.8889484643936156, val loss: 3.975787091255188, ETA in seconds: 1897892.460\n",
      "epoch: 404300, train loss: 3.8976261138916017, val loss: 3.9615554571151734, ETA in seconds: 1898090.983\n",
      "epoch: 404400, train loss: 3.8859813451766967, val loss: 3.9712072372436524, ETA in seconds: 1898227.753\n",
      "epoch: 404500, train loss: 3.8763761043548586, val loss: 3.974300503730774, ETA in seconds: 1898363.916\n",
      "epoch: 404600, train loss: 3.9012009859085084, val loss: 3.9749725818634034, ETA in seconds: 1898585.852\n",
      "epoch: 404700, train loss: 3.8776364803314207, val loss: 3.97878475189209, ETA in seconds: 1898713.374\n",
      "epoch: 404800, train loss: 3.897480320930481, val loss: 3.9886714935302736, ETA in seconds: 1898867.372\n",
      "epoch: 404900, train loss: 3.8884732007980345, val loss: 3.953525757789612, ETA in seconds: 1899000.192\n",
      "epoch: 405000, train loss: 3.896889328956604, val loss: 3.9573506832122805, ETA in seconds: 1899147.478\n",
      "epoch: 405100, train loss: 3.889894461631775, val loss: 3.9669642210006715, ETA in seconds: 1899285.264\n",
      "epoch: 405200, train loss: 3.8888658046722413, val loss: 3.970934772491455, ETA in seconds: 1899504.900\n",
      "epoch: 405300, train loss: 3.896119475364685, val loss: 3.9763925790786745, ETA in seconds: 1899725.709\n",
      "epoch: 405400, train loss: 3.8906857490539553, val loss: 3.978464651107788, ETA in seconds: 1899941.386\n",
      "epoch: 405500, train loss: 3.8925023555755613, val loss: 3.9566542148590087, ETA in seconds: 1900151.662\n",
      "epoch: 405600, train loss: 3.9066161155700683, val loss: 3.972394347190857, ETA in seconds: 1900320.463\n",
      "epoch: 405700, train loss: 3.889916920661926, val loss: 3.964144968986511, ETA in seconds: 1900419.935\n",
      "epoch: 405800, train loss: 3.8960274696350097, val loss: 3.961035943031311, ETA in seconds: 1900522.868\n",
      "epoch: 405900, train loss: 3.8945397615432737, val loss: 3.974213123321533, ETA in seconds: 1900651.847\n",
      "epoch: 406000, train loss: 3.891907238960266, val loss: 3.969693493843079, ETA in seconds: 1900785.640\n",
      "epoch: 406100, train loss: 3.885172390937805, val loss: 3.964668035507202, ETA in seconds: 1900919.085\n",
      "epoch: 406200, train loss: 3.891326332092285, val loss: 3.95916862487793, ETA in seconds: 1901040.059\n",
      "epoch: 406300, train loss: 3.889148473739624, val loss: 3.96141996383667, ETA in seconds: 1901163.211\n",
      "epoch: 406400, train loss: 3.881733465194702, val loss: 3.9716336011886595, ETA in seconds: 1901294.935\n",
      "epoch: 406500, train loss: 3.87932710647583, val loss: 3.9684823513031007, ETA in seconds: 1901418.848\n",
      "epoch: 406600, train loss: 3.897315001487732, val loss: 3.9688703536987306, ETA in seconds: 1901538.747\n",
      "epoch: 406700, train loss: 3.8906963586807253, val loss: 3.968294048309326, ETA in seconds: 1901770.475\n",
      "epoch: 406800, train loss: 3.89973623752594, val loss: 3.966913437843323, ETA in seconds: 1901891.166\n",
      "epoch: 406900, train loss: 3.894953465461731, val loss: 3.9675670146942137, ETA in seconds: 1902014.963\n",
      "epoch: 407000, train loss: 3.8904061317443848, val loss: 3.9550204038619996, ETA in seconds: 1902260.769\n",
      "epoch: 407100, train loss: 3.8911075830459594, val loss: 3.9642459630966185, ETA in seconds: 1902517.691\n",
      "epoch: 407200, train loss: 3.901586079597473, val loss: 3.9653566598892214, ETA in seconds: 1902748.842\n",
      "epoch: 407300, train loss: 3.89837589263916, val loss: 3.9718632221221926, ETA in seconds: 1902987.558\n",
      "epoch: 407400, train loss: 3.8987282037734987, val loss: 3.9604976177215576, ETA in seconds: 1903235.890\n",
      "epoch: 407500, train loss: 3.898862433433533, val loss: 3.9659252405166625, ETA in seconds: 1903463.320\n",
      "epoch: 407600, train loss: 3.8955963373184206, val loss: 3.960017466545105, ETA in seconds: 1903699.265\n",
      "epoch: 407700, train loss: 3.904119634628296, val loss: 3.960375356674194, ETA in seconds: 1903860.185\n",
      "epoch: 407800, train loss: 3.8951035737991333, val loss: 3.9590370893478393, ETA in seconds: 1904047.287\n",
      "epoch: 407900, train loss: 3.9002898216247557, val loss: 3.965587115287781, ETA in seconds: 1904295.112\n",
      "epoch: 408000, train loss: 3.8945260524749754, val loss: 3.96337411403656, ETA in seconds: 1904487.690\n",
      "epoch: 408100, train loss: 3.892237734794617, val loss: 3.9565635442733766, ETA in seconds: 1904595.973\n",
      "epoch: 408200, train loss: 3.8971325635910032, val loss: 3.9688789367675783, ETA in seconds: 1904709.897\n",
      "epoch: 408300, train loss: 3.9064388036727906, val loss: 3.9540456533432007, ETA in seconds: 1904831.084\n",
      "epoch: 408400, train loss: 3.899576210975647, val loss: 3.9777342081069946, ETA in seconds: 1904957.213\n",
      "epoch: 408500, train loss: 3.896427845954895, val loss: 3.9705145835876463, ETA in seconds: 1905121.304\n",
      "epoch: 408600, train loss: 3.9018117666244505, val loss: 3.9785001277923584, ETA in seconds: 1905236.959\n",
      "epoch: 408700, train loss: 3.8821086168289183, val loss: 3.96920907497406, ETA in seconds: 1905364.960\n",
      "epoch: 408800, train loss: 3.8966007947921755, val loss: 3.9637805223464966, ETA in seconds: 1905565.874\n",
      "epoch: 408900, train loss: 3.8883208990097047, val loss: 3.973962831497192, ETA in seconds: 1905665.170\n",
      "epoch: 409000, train loss: 3.891214299201965, val loss: 3.979162168502808, ETA in seconds: 1905789.484\n",
      "epoch: 409100, train loss: 3.897623157501221, val loss: 3.97238142490387, ETA in seconds: 1905909.092\n",
      "epoch: 409200, train loss: 3.8913716793060305, val loss: 3.9637089014053344, ETA in seconds: 1906029.998\n",
      "epoch: 409300, train loss: 3.892840576171875, val loss: 3.965505838394165, ETA in seconds: 1906156.390\n",
      "epoch: 409400, train loss: 3.888758397102356, val loss: 3.9672842025756836, ETA in seconds: 1906311.789\n",
      "epoch: 409500, train loss: 3.8915997982025146, val loss: 3.978218102455139, ETA in seconds: 1906428.168\n",
      "epoch: 409600, train loss: 3.8946396827697756, val loss: 3.971841740608215, ETA in seconds: 1906567.312\n",
      "epoch: 409700, train loss: 3.894419527053833, val loss: 3.972583198547363, ETA in seconds: 1906720.006\n",
      "epoch: 409800, train loss: 3.8949812412261964, val loss: 3.960977554321289, ETA in seconds: 1906812.884\n",
      "epoch: 409900, train loss: 3.8902863264083862, val loss: 3.9653972864151, ETA in seconds: 1906914.401\n",
      "epoch: 410000, train loss: 3.8944589614868166, val loss: 3.961303472518921, ETA in seconds: 1907006.112\n",
      "epoch: 410100, train loss: 3.898445653915405, val loss: 3.9812000036239623, ETA in seconds: 1907117.128\n",
      "epoch: 410200, train loss: 3.8912625312805176, val loss: 3.9678721189498902, ETA in seconds: 1907234.880\n",
      "epoch: 410300, train loss: 3.896092486381531, val loss: 3.9622260332107544, ETA in seconds: 1907330.517\n",
      "epoch: 410400, train loss: 3.898641657829285, val loss: 3.9649085521698, ETA in seconds: 1907422.074\n",
      "epoch: 410500, train loss: 3.8984326362609862, val loss: 3.9613818407058714, ETA in seconds: 1907530.798\n",
      "epoch: 410600, train loss: 3.899876284599304, val loss: 3.9640119552612303, ETA in seconds: 1907640.557\n",
      "epoch: 410700, train loss: 3.9014872789382933, val loss: 3.968848705291748, ETA in seconds: 1907762.276\n",
      "epoch: 410800, train loss: 3.883442521095276, val loss: 3.96074492931366, ETA in seconds: 1907888.548\n",
      "epoch: 410900, train loss: 3.8937704801559447, val loss: 3.9727407455444337, ETA in seconds: 1908007.475\n",
      "epoch: 411000, train loss: 3.895221304893494, val loss: 3.9771427631378176, ETA in seconds: 1908216.163\n",
      "epoch: 411100, train loss: 3.8914346933364867, val loss: 3.977851152420044, ETA in seconds: 1908473.104\n",
      "epoch: 411200, train loss: 3.8915150642395018, val loss: 3.980950403213501, ETA in seconds: 1908724.710\n",
      "epoch: 411300, train loss: 3.8877809762954714, val loss: 3.9756988525390624, ETA in seconds: 1908972.598\n",
      "epoch: 411400, train loss: 3.8977582931518553, val loss: 3.979081702232361, ETA in seconds: 1909100.905\n",
      "epoch: 411500, train loss: 3.8918254375457764, val loss: 3.973628306388855, ETA in seconds: 1909227.320\n",
      "epoch: 411600, train loss: 3.893809127807617, val loss: 3.9731013059616087, ETA in seconds: 1909359.675\n",
      "epoch: 411700, train loss: 3.884001064300537, val loss: 3.9671613931655885, ETA in seconds: 1909547.145\n",
      "epoch: 411800, train loss: 3.892365002632141, val loss: 3.985486125946045, ETA in seconds: 1909673.174\n",
      "epoch: 411900, train loss: 3.885832166671753, val loss: 3.9719757556915285, ETA in seconds: 1909810.867\n",
      "epoch: 412000, train loss: 3.899189257621765, val loss: 3.975648522377014, ETA in seconds: 1910061.580\n",
      "epoch: 412100, train loss: 3.8842098474502564, val loss: 3.9752645254135133, ETA in seconds: 1910297.316\n",
      "epoch: 412200, train loss: 3.894714331626892, val loss: 3.9776472091674804, ETA in seconds: 1910507.691\n",
      "epoch: 412300, train loss: 3.895293617248535, val loss: 3.9683098554611207, ETA in seconds: 1910684.729\n",
      "epoch: 412400, train loss: 3.896526026725769, val loss: 3.9759669303894043, ETA in seconds: 1910882.425\n",
      "epoch: 412500, train loss: 3.8937314987182616, val loss: 3.9595356225967406, ETA in seconds: 1911020.036\n",
      "epoch: 412600, train loss: 3.893005919456482, val loss: 3.9716881275177003, ETA in seconds: 1911169.233\n",
      "epoch: 412700, train loss: 3.894033408164978, val loss: 3.9649866342544557, ETA in seconds: 1911366.656\n",
      "epoch: 412800, train loss: 3.8984215259552, val loss: 3.9593499183654783, ETA in seconds: 1911566.114\n",
      "epoch: 412900, train loss: 3.8857779502868652, val loss: 3.9693615198135377, ETA in seconds: 1911770.779\n",
      "epoch: 413000, train loss: 3.88320415019989, val loss: 3.9652589082717897, ETA in seconds: 1911929.179\n",
      "epoch: 413100, train loss: 3.8976869344711305, val loss: 3.9738452434539795, ETA in seconds: 1912026.633\n",
      "epoch: 413200, train loss: 3.897724914550781, val loss: 3.9715945959091186, ETA in seconds: 1912146.429\n",
      "epoch: 413300, train loss: 3.889033317565918, val loss: 3.967278480529785, ETA in seconds: 1912232.318\n",
      "epoch: 413400, train loss: 3.8941018342971803, val loss: 3.964459252357483, ETA in seconds: 1912425.867\n",
      "epoch: 413500, train loss: 3.898061513900757, val loss: 3.9724499940872193, ETA in seconds: 1912624.356\n",
      "epoch: 413600, train loss: 3.8961681127548218, val loss: 3.9776337862014772, ETA in seconds: 1912795.857\n",
      "epoch: 413700, train loss: 3.8980127811431884, val loss: 3.9754390001296995, ETA in seconds: 1912939.352\n",
      "epoch: 413800, train loss: 3.8899776458740236, val loss: 3.9616480827331544, ETA in seconds: 1913092.466\n",
      "epoch: 413900, train loss: 3.8954288244247435, val loss: 3.9701388835906983, ETA in seconds: 1913246.919\n",
      "epoch: 414000, train loss: 3.8866845607757567, val loss: 3.9685567378997804, ETA in seconds: 1913398.194\n",
      "epoch: 414100, train loss: 3.8919206857681274, val loss: 3.971139430999756, ETA in seconds: 1913540.258\n",
      "epoch: 414200, train loss: 3.8951441287994384, val loss: 3.9718732833862305, ETA in seconds: 1913728.076\n",
      "epoch: 414300, train loss: 3.9022483110427855, val loss: 3.9787528991699217, ETA in seconds: 1913948.864\n",
      "epoch: 414400, train loss: 3.886964726448059, val loss: 3.9673184633255003, ETA in seconds: 1914190.869\n",
      "epoch: 414500, train loss: 3.900100255012512, val loss: 3.973924994468689, ETA in seconds: 1914438.095\n",
      "epoch: 414600, train loss: 3.8891796112060546, val loss: 3.964479613304138, ETA in seconds: 1914661.643\n",
      "epoch: 414700, train loss: 3.898254704475403, val loss: 3.975187158584595, ETA in seconds: 1914858.093\n",
      "epoch: 414800, train loss: 3.9006705045700074, val loss: 3.9677238941192625, ETA in seconds: 1915019.423\n",
      "epoch: 414900, train loss: 3.891586518287659, val loss: 3.9678299903869627, ETA in seconds: 1915112.890\n",
      "epoch: 415000, train loss: 3.8948068857192992, val loss: 3.958889055252075, ETA in seconds: 1915235.997\n",
      "epoch: 415100, train loss: 3.8932391881942747, val loss: 3.9630254983901976, ETA in seconds: 1915355.823\n",
      "epoch: 415200, train loss: 3.895990991592407, val loss: 3.970993661880493, ETA in seconds: 1915470.131\n",
      "epoch: 415300, train loss: 3.8938790798187255, val loss: 3.9627628326416016, ETA in seconds: 1915622.483\n",
      "epoch: 415400, train loss: 3.8948819398880006, val loss: 3.970638608932495, ETA in seconds: 1915731.763\n",
      "epoch: 415500, train loss: 3.900512433052063, val loss: 3.9775176763534548, ETA in seconds: 1915846.959\n",
      "epoch: 415600, train loss: 3.8950682878494263, val loss: 3.9673889875411987, ETA in seconds: 1915964.790\n",
      "epoch: 415700, train loss: 3.9021422624588014, val loss: 3.9858471870422365, ETA in seconds: 1916069.050\n",
      "epoch: 415800, train loss: 3.8952527761459352, val loss: 3.9785194635391234, ETA in seconds: 1916167.392\n",
      "epoch: 415900, train loss: 3.894219493865967, val loss: 3.9799753427505493, ETA in seconds: 1916265.197\n",
      "epoch: 416000, train loss: 3.887332248687744, val loss: 3.974115228652954, ETA in seconds: 1916366.258\n",
      "epoch: 416100, train loss: 3.888378643989563, val loss: 3.965943694114685, ETA in seconds: 1916491.287\n",
      "epoch: 416200, train loss: 3.891264009475708, val loss: 3.9668885469436646, ETA in seconds: 1916620.749\n",
      "epoch: 416300, train loss: 3.890955591201782, val loss: 3.9726488828659057, ETA in seconds: 1916821.877\n",
      "epoch: 416400, train loss: 3.8978760719299315, val loss: 3.972416853904724, ETA in seconds: 1917016.634\n",
      "epoch: 416500, train loss: 3.8884335279464723, val loss: 3.9746031522750855, ETA in seconds: 1917216.632\n",
      "epoch: 416600, train loss: 3.888871598243713, val loss: 3.9653565645217896, ETA in seconds: 1917410.142\n",
      "epoch: 416700, train loss: 3.8954289674758913, val loss: 3.9773459434509277, ETA in seconds: 1917602.548\n",
      "epoch: 416800, train loss: 3.8957076549530028, val loss: 3.965540289878845, ETA in seconds: 1917792.727\n",
      "epoch: 416900, train loss: 3.882663369178772, val loss: 3.9740533351898195, ETA in seconds: 1918000.919\n",
      "epoch: 417000, train loss: 3.8913398027420043, val loss: 3.9861761331558228, ETA in seconds: 1918234.700\n",
      "epoch: 417100, train loss: 3.888078451156616, val loss: 3.9744505882263184, ETA in seconds: 1918439.065\n",
      "epoch: 417200, train loss: 3.8918915748596192, val loss: 3.9708717823028565, ETA in seconds: 1918633.139\n",
      "epoch: 417300, train loss: 3.8889115333557127, val loss: 3.971475601196289, ETA in seconds: 1918797.882\n",
      "epoch: 417400, train loss: 3.899014449119568, val loss: 3.971536731719971, ETA in seconds: 1918918.997\n",
      "epoch: 417500, train loss: 3.8905690908432007, val loss: 3.9618915557861327, ETA in seconds: 1919029.070\n",
      "epoch: 417600, train loss: 3.895228314399719, val loss: 3.968622255325317, ETA in seconds: 1919142.775\n",
      "epoch: 417700, train loss: 3.897310900688171, val loss: 3.964559864997864, ETA in seconds: 1919261.941\n",
      "epoch: 417800, train loss: 3.9008426666259766, val loss: 3.9763495445251467, ETA in seconds: 1919372.795\n",
      "epoch: 417900, train loss: 3.8917582511901854, val loss: 3.9688222646713256, ETA in seconds: 1919480.927\n",
      "epoch: 418000, train loss: 3.8960033416748048, val loss: 3.9611501693725586, ETA in seconds: 1919609.555\n",
      "epoch: 418100, train loss: 3.885098934173584, val loss: 3.9685964822769164, ETA in seconds: 1919780.143\n",
      "epoch: 418200, train loss: 3.9016305685043333, val loss: 3.9681116104125977, ETA in seconds: 1919980.493\n",
      "epoch: 418300, train loss: 3.8896264314651487, val loss: 3.973554587364197, ETA in seconds: 1920097.999\n",
      "epoch: 418400, train loss: 3.890359330177307, val loss: 3.975107479095459, ETA in seconds: 1920219.393\n",
      "epoch: 418500, train loss: 3.8901390075683593, val loss: 3.9616072177886963, ETA in seconds: 1920331.306\n",
      "epoch: 418600, train loss: 3.8914397954940796, val loss: 3.9701757431030273, ETA in seconds: 1920473.259\n",
      "epoch: 418700, train loss: 3.894497799873352, val loss: 3.96304931640625, ETA in seconds: 1920578.266\n",
      "epoch: 418800, train loss: 3.880569100379944, val loss: 3.960273003578186, ETA in seconds: 1920688.920\n",
      "epoch: 418900, train loss: 3.8995994329452515, val loss: 3.965288829803467, ETA in seconds: 1920806.412\n",
      "epoch: 419000, train loss: 3.8898953199386597, val loss: 3.965527296066284, ETA in seconds: 1920918.780\n",
      "epoch: 419100, train loss: 3.895328688621521, val loss: 3.9737364530563353, ETA in seconds: 1921032.723\n",
      "epoch: 419200, train loss: 3.8951475620269775, val loss: 3.9740320682525634, ETA in seconds: 1921145.481\n",
      "epoch: 419300, train loss: 3.8862325668334963, val loss: 3.978840208053589, ETA in seconds: 1921252.242\n",
      "epoch: 419400, train loss: 3.8975942134857178, val loss: 3.970273971557617, ETA in seconds: 1921422.800\n",
      "epoch: 419500, train loss: 3.895705533027649, val loss: 3.9604379177093505, ETA in seconds: 1921614.788\n",
      "epoch: 419600, train loss: 3.8903654336929323, val loss: 3.971579599380493, ETA in seconds: 1921761.710\n",
      "epoch: 419700, train loss: 3.8864641189575195, val loss: 3.9677579164505006, ETA in seconds: 1921868.229\n",
      "epoch: 419800, train loss: 3.9057766914367678, val loss: 3.9658193826675414, ETA in seconds: 1921972.526\n",
      "epoch: 419900, train loss: 3.9007049083709715, val loss: 3.965391445159912, ETA in seconds: 1922070.247\n",
      "epoch: 420000, train loss: 3.89929358959198, val loss: 3.972855806350708, ETA in seconds: 1922175.625\n",
      "epoch: 420100, train loss: 3.8828349351882934, val loss: 3.9701433897018434, ETA in seconds: 1922277.674\n",
      "epoch: 420200, train loss: 3.891467046737671, val loss: 3.97168869972229, ETA in seconds: 1922380.592\n",
      "epoch: 420300, train loss: 3.8930973052978515, val loss: 3.9684274435043334, ETA in seconds: 1922466.634\n",
      "epoch: 420400, train loss: 3.8995876550674438, val loss: 3.9716764211654665, ETA in seconds: 1922592.658\n",
      "epoch: 420500, train loss: 3.8915855646133424, val loss: 3.968379354476929, ETA in seconds: 1922742.184\n",
      "epoch: 420600, train loss: 3.8887815952301024, val loss: 3.9516674280166626, ETA in seconds: 1922841.224\n",
      "epoch: 420700, train loss: 3.905453062057495, val loss: 3.9698847055435182, ETA in seconds: 1922939.301\n",
      "epoch: 420800, train loss: 3.8965084314346314, val loss: 3.968413233757019, ETA in seconds: 1923033.823\n",
      "epoch: 420900, train loss: 3.8906813859939575, val loss: 3.961179733276367, ETA in seconds: 1923130.153\n",
      "epoch: 421000, train loss: 3.904842185974121, val loss: 3.959479832649231, ETA in seconds: 1923231.064\n",
      "epoch: 421100, train loss: 3.886265420913696, val loss: 3.969386076927185, ETA in seconds: 1923445.881\n",
      "epoch: 421200, train loss: 3.886589288711548, val loss: 3.9668790102005005, ETA in seconds: 1923572.772\n",
      "epoch: 421300, train loss: 3.898774743080139, val loss: 3.9735080003738403, ETA in seconds: 1923688.149\n",
      "epoch: 421400, train loss: 3.896446943283081, val loss: 3.9642820596694945, ETA in seconds: 1923782.516\n",
      "epoch: 421500, train loss: 3.8901057958602907, val loss: 3.96880624294281, ETA in seconds: 1923885.492\n",
      "epoch: 421600, train loss: 3.8971328258514406, val loss: 3.9625195980072023, ETA in seconds: 1923983.348\n",
      "epoch: 421700, train loss: 3.897007918357849, val loss: 3.958298897743225, ETA in seconds: 1924079.148\n",
      "epoch: 421800, train loss: 3.9000971794128416, val loss: 3.961118745803833, ETA in seconds: 1924187.166\n",
      "epoch: 421900, train loss: 3.8926324605941773, val loss: 3.9661136627197267, ETA in seconds: 1924306.436\n",
      "epoch: 422000, train loss: 3.896411895751953, val loss: 3.976753306388855, ETA in seconds: 1924413.359\n",
      "epoch: 422100, train loss: 3.889493703842163, val loss: 3.9730451107025146, ETA in seconds: 1924519.729\n",
      "epoch: 422200, train loss: 3.8873290538787844, val loss: 3.9626869916915894, ETA in seconds: 1924639.316\n",
      "epoch: 422300, train loss: 3.894396257400513, val loss: 3.966387939453125, ETA in seconds: 1924735.984\n",
      "epoch: 422400, train loss: 3.8867821216583254, val loss: 3.9841100692749025, ETA in seconds: 1924839.248\n",
      "epoch: 422500, train loss: 3.886659288406372, val loss: 3.9618868589401246, ETA in seconds: 1924947.615\n",
      "epoch: 422600, train loss: 3.8939423322677613, val loss: 3.9741881847381593, ETA in seconds: 1925055.024\n",
      "epoch: 422700, train loss: 3.895553207397461, val loss: 3.975253438949585, ETA in seconds: 1925161.997\n",
      "epoch: 422800, train loss: 3.891052079200745, val loss: 3.9685600996017456, ETA in seconds: 1925282.643\n",
      "epoch: 422900, train loss: 3.8916242122650146, val loss: 3.9648242950439454, ETA in seconds: 1925395.456\n",
      "epoch: 423000, train loss: 3.8975629091262816, val loss: 3.968171000480652, ETA in seconds: 1925514.298\n",
      "epoch: 423100, train loss: 3.894983100891113, val loss: 3.974834132194519, ETA in seconds: 1925602.314\n",
      "epoch: 423200, train loss: 3.8849596977233887, val loss: 3.964430809020996, ETA in seconds: 1925711.045\n",
      "epoch: 423300, train loss: 3.892111802101135, val loss: 3.973290300369263, ETA in seconds: 1925876.793\n",
      "epoch: 423400, train loss: 3.8901620626449587, val loss: 3.977306437492371, ETA in seconds: 1925962.461\n",
      "epoch: 423500, train loss: 3.8920037031173704, val loss: 3.9647231340408324, ETA in seconds: 1926046.338\n",
      "epoch: 423600, train loss: 3.893281888961792, val loss: 3.9666269540786745, ETA in seconds: 1926125.772\n",
      "epoch: 423700, train loss: 3.8966710567474365, val loss: 3.9812424182891846, ETA in seconds: 1926213.460\n",
      "epoch: 423800, train loss: 3.8867727518081665, val loss: 3.9713914155960084, ETA in seconds: 1926322.554\n",
      "epoch: 423900, train loss: 3.889206051826477, val loss: 3.9646180391311647, ETA in seconds: 1926429.179\n",
      "epoch: 424000, train loss: 3.8972845315933227, val loss: 3.9759541749954224, ETA in seconds: 1926527.846\n",
      "epoch: 424100, train loss: 3.8899845838546754, val loss: 3.969455122947693, ETA in seconds: 1926626.667\n",
      "epoch: 424200, train loss: 3.8886175870895388, val loss: 3.9648757219314574, ETA in seconds: 1926726.162\n",
      "epoch: 424300, train loss: 3.894859504699707, val loss: 3.9639255523681642, ETA in seconds: 1926866.430\n",
      "epoch: 424400, train loss: 3.8920959711074827, val loss: 3.9754266023635862, ETA in seconds: 1926977.482\n",
      "epoch: 424500, train loss: 3.8884041786193846, val loss: 3.9701283454895018, ETA in seconds: 1927214.642\n",
      "epoch: 424600, train loss: 3.8927505254745483, val loss: 3.969240093231201, ETA in seconds: 1927450.129\n",
      "epoch: 424700, train loss: 3.8971103191375733, val loss: 3.9753076076507567, ETA in seconds: 1927565.148\n",
      "epoch: 424800, train loss: 3.885821080207825, val loss: 3.9796571254730226, ETA in seconds: 1927662.866\n",
      "epoch: 424900, train loss: 3.895901656150818, val loss: 3.9598114252090455, ETA in seconds: 1927783.064\n",
      "epoch: 425000, train loss: 3.903275966644287, val loss: 3.978867840766907, ETA in seconds: 1927907.240\n",
      "epoch: 425100, train loss: 3.889661693572998, val loss: 3.9633555173873902, ETA in seconds: 1928031.242\n",
      "epoch: 425200, train loss: 3.889753723144531, val loss: 3.969507074356079, ETA in seconds: 1928151.897\n",
      "epoch: 425300, train loss: 3.894651174545288, val loss: 3.9798470973968505, ETA in seconds: 1928265.849\n",
      "epoch: 425400, train loss: 3.8912297964096068, val loss: 3.9729662895202638, ETA in seconds: 1928379.110\n",
      "epoch: 425500, train loss: 3.8985224962234497, val loss: 3.97377872467041, ETA in seconds: 1928500.416\n",
      "epoch: 425600, train loss: 3.8957379341125487, val loss: 3.97365984916687, ETA in seconds: 1928591.078\n",
      "epoch: 425700, train loss: 3.900712752342224, val loss: 3.9742939472198486, ETA in seconds: 1928691.998\n",
      "epoch: 425800, train loss: 3.890961742401123, val loss: 3.956104612350464, ETA in seconds: 1928824.058\n",
      "epoch: 425900, train loss: 3.896110010147095, val loss: 3.964362549781799, ETA in seconds: 1929024.334\n",
      "epoch: 426000, train loss: 3.894935417175293, val loss: 3.9771838665008543, ETA in seconds: 1929200.766\n",
      "epoch: 426100, train loss: 3.8969932079315184, val loss: 3.9697298049926757, ETA in seconds: 1929299.390\n",
      "epoch: 426200, train loss: 3.895124006271362, val loss: 3.9691550016403196, ETA in seconds: 1929387.732\n",
      "epoch: 426300, train loss: 3.896328401565552, val loss: 3.9666704416275023, ETA in seconds: 1929510.058\n",
      "epoch: 426400, train loss: 3.8906818628311157, val loss: 3.9746140241622925, ETA in seconds: 1929624.493\n",
      "epoch: 426500, train loss: 3.8920136213302614, val loss: 3.9690510988235475, ETA in seconds: 1929756.298\n",
      "epoch: 426600, train loss: 3.8921433687210083, val loss: 3.9665135383605956, ETA in seconds: 1929881.437\n",
      "epoch: 426700, train loss: 3.890446186065674, val loss: 3.9679672002792357, ETA in seconds: 1929998.840\n",
      "epoch: 426800, train loss: 3.8970147371292114, val loss: 3.973473596572876, ETA in seconds: 1930122.612\n",
      "epoch: 426900, train loss: 3.888099527359009, val loss: 3.966801905632019, ETA in seconds: 1930245.505\n",
      "epoch: 427000, train loss: 3.90446515083313, val loss: 3.9792301654815674, ETA in seconds: 1930386.520\n",
      "epoch: 427100, train loss: 3.8960182666778564, val loss: 3.9634958267211915, ETA in seconds: 1930470.755\n",
      "epoch: 427200, train loss: 3.891021490097046, val loss: 3.965264320373535, ETA in seconds: 1930619.473\n",
      "epoch: 427300, train loss: 3.9015592336654663, val loss: 3.9776852130889893, ETA in seconds: 1930748.008\n",
      "epoch: 427400, train loss: 3.895464873313904, val loss: 3.9705523014068604, ETA in seconds: 1930841.346\n",
      "epoch: 427500, train loss: 3.899228882789612, val loss: 3.9735194206237794, ETA in seconds: 1930938.918\n",
      "epoch: 427600, train loss: 3.9013238191604613, val loss: 3.9730975151062013, ETA in seconds: 1931052.828\n",
      "epoch: 427700, train loss: 3.897778058052063, val loss: 3.976039409637451, ETA in seconds: 1931151.878\n",
      "epoch: 427800, train loss: 3.892483448982239, val loss: 3.9723562479019163, ETA in seconds: 1931248.681\n",
      "epoch: 427900, train loss: 3.9016273975372315, val loss: 3.973199224472046, ETA in seconds: 1931348.220\n",
      "epoch: 428000, train loss: 3.8893691539764403, val loss: 3.975781774520874, ETA in seconds: 1931440.951\n",
      "epoch: 428100, train loss: 3.9008657932281494, val loss: 3.9702416181564333, ETA in seconds: 1931535.695\n",
      "epoch: 428200, train loss: 3.8872377634048463, val loss: 3.9630207300186155, ETA in seconds: 1931638.669\n",
      "epoch: 428300, train loss: 3.89266722202301, val loss: 3.9676531553268433, ETA in seconds: 1931743.818\n",
      "epoch: 428400, train loss: 3.880319023132324, val loss: 3.9761826276779173, ETA in seconds: 1931831.039\n",
      "epoch: 428500, train loss: 3.890595531463623, val loss: 3.972270464897156, ETA in seconds: 1931921.007\n",
      "epoch: 428600, train loss: 3.8902636528015138, val loss: 3.975467824935913, ETA in seconds: 1932059.203\n",
      "epoch: 428700, train loss: 3.8858503103256226, val loss: 3.982595992088318, ETA in seconds: 1932170.434\n",
      "epoch: 428800, train loss: 3.8938332557678224, val loss: 3.965997838973999, ETA in seconds: 1932254.645\n",
      "epoch: 428900, train loss: 3.8952462434768678, val loss: 3.9758153438568113, ETA in seconds: 1932358.214\n",
      "epoch: 429000, train loss: 3.8947271585464476, val loss: 3.9768463134765626, ETA in seconds: 1932459.225\n",
      "epoch: 429100, train loss: 3.8899000883102417, val loss: 3.963492751121521, ETA in seconds: 1932559.595\n",
      "epoch: 429200, train loss: 3.888009786605835, val loss: 3.987670564651489, ETA in seconds: 1932630.034\n",
      "epoch: 429300, train loss: 3.8958451986312865, val loss: 3.9667866945266725, ETA in seconds: 1932707.387\n",
      "epoch: 429400, train loss: 3.892162561416626, val loss: 3.969148278236389, ETA in seconds: 1932800.247\n",
      "epoch: 429500, train loss: 3.8822564363479612, val loss: 3.960898232460022, ETA in seconds: 1932886.316\n",
      "epoch: 429600, train loss: 3.8900704383850098, val loss: 3.973309636116028, ETA in seconds: 1932956.571\n",
      "epoch: 429700, train loss: 3.906934714317322, val loss: 3.9771849632263185, ETA in seconds: 1933028.214\n",
      "epoch: 429800, train loss: 3.8942257881164553, val loss: 3.964029383659363, ETA in seconds: 1933097.943\n",
      "epoch: 429900, train loss: 3.8876694202423097, val loss: 3.9677409648895265, ETA in seconds: 1933183.643\n",
      "epoch: 430000, train loss: 3.8901329040527344, val loss: 3.9671255350112915, ETA in seconds: 1933274.189\n",
      "epoch: 430100, train loss: 3.886612892150879, val loss: 3.973715376853943, ETA in seconds: 1933386.546\n",
      "epoch: 430200, train loss: 3.8923307418823243, val loss: 3.9819556951522825, ETA in seconds: 1933487.294\n",
      "epoch: 430300, train loss: 3.8863592386245727, val loss: 3.9730445623397825, ETA in seconds: 1933583.064\n",
      "epoch: 430400, train loss: 3.89211790561676, val loss: 3.9733125448226927, ETA in seconds: 1933664.541\n",
      "epoch: 430500, train loss: 3.8964275598526, val loss: 3.9697627782821656, ETA in seconds: 1933757.534\n",
      "epoch: 430600, train loss: 3.8889232635498048, val loss: 3.9822818517684935, ETA in seconds: 1933848.673\n",
      "epoch: 430700, train loss: 3.885501480102539, val loss: 3.956903839111328, ETA in seconds: 1933935.693\n",
      "epoch: 430800, train loss: 3.8854283809661867, val loss: 3.963020348548889, ETA in seconds: 1934037.504\n",
      "epoch: 430900, train loss: 3.894329857826233, val loss: 3.966975951194763, ETA in seconds: 1934128.933\n",
      "epoch: 431000, train loss: 3.8878862142562864, val loss: 3.9787625074386597, ETA in seconds: 1934209.322\n",
      "epoch: 431100, train loss: 3.8970643997192385, val loss: 3.973825192451477, ETA in seconds: 1934292.154\n",
      "epoch: 431200, train loss: 3.9012994050979612, val loss: 3.9805271863937377, ETA in seconds: 1934379.005\n",
      "epoch: 431300, train loss: 3.8960484981536867, val loss: 3.9702625036239625, ETA in seconds: 1934456.187\n",
      "epoch: 431400, train loss: 3.8963948011398317, val loss: 3.9813195705413817, ETA in seconds: 1934532.726\n",
      "epoch: 431500, train loss: 3.883820629119873, val loss: 3.9676653146743774, ETA in seconds: 1934613.838\n",
      "epoch: 431600, train loss: 3.890406918525696, val loss: 3.9756346225738524, ETA in seconds: 1934691.164\n",
      "epoch: 431700, train loss: 3.895281720161438, val loss: 3.97297842502594, ETA in seconds: 1934762.401\n",
      "epoch: 431800, train loss: 3.892642879486084, val loss: 3.9732361316680906, ETA in seconds: 1934835.155\n",
      "epoch: 431900, train loss: 3.897180414199829, val loss: 3.976711463928223, ETA in seconds: 1934906.687\n",
      "epoch: 432000, train loss: 3.885002684593201, val loss: 3.9754269123077393, ETA in seconds: 1934967.459\n",
      "epoch: 432100, train loss: 3.894578766822815, val loss: 3.971548390388489, ETA in seconds: 1935030.373\n",
      "epoch: 432200, train loss: 3.8944704294204713, val loss: 3.979360508918762, ETA in seconds: 1935162.574\n",
      "epoch: 432300, train loss: 3.8961140394210814, val loss: 3.9739983797073366, ETA in seconds: 1935232.688\n",
      "epoch: 432400, train loss: 3.886234712600708, val loss: 3.9653753280639648, ETA in seconds: 1935380.974\n",
      "epoch: 432500, train loss: 3.893983769416809, val loss: 3.970154905319214, ETA in seconds: 1935446.380\n",
      "epoch: 432600, train loss: 3.9078446865081786, val loss: 3.9862706899642943, ETA in seconds: 1935526.981\n",
      "epoch: 432700, train loss: 3.898006272315979, val loss: 3.970143270492554, ETA in seconds: 1935676.349\n",
      "epoch: 432800, train loss: 3.9005510330200197, val loss: 3.97338924407959, ETA in seconds: 1935735.964\n",
      "epoch: 432900, train loss: 3.8988346338272093, val loss: 3.971910071372986, ETA in seconds: 1935809.440\n",
      "epoch: 433000, train loss: 3.9030646085739136, val loss: 3.9708449125289915, ETA in seconds: 1935884.842\n",
      "epoch: 433100, train loss: 3.8973825454711912, val loss: 3.9689551830291747, ETA in seconds: 1935970.543\n",
      "epoch: 433200, train loss: 3.889225172996521, val loss: 3.9710841178894043, ETA in seconds: 1936040.091\n",
      "epoch: 433300, train loss: 3.8956916093826295, val loss: 3.978107452392578, ETA in seconds: 1936108.258\n",
      "epoch: 433400, train loss: 3.893691563606262, val loss: 3.970965933799744, ETA in seconds: 1936179.409\n",
      "epoch: 433500, train loss: 3.8895238637924194, val loss: 3.9674866914749147, ETA in seconds: 1936244.714\n",
      "epoch: 433600, train loss: 3.8945703983306883, val loss: 3.9640942335128786, ETA in seconds: 1936322.950\n",
      "epoch: 433700, train loss: 3.888841676712036, val loss: 3.960019755363464, ETA in seconds: 1936434.864\n",
      "epoch: 433800, train loss: 3.894039034843445, val loss: 3.966461968421936, ETA in seconds: 1936493.195\n",
      "epoch: 433900, train loss: 3.9016911029815673, val loss: 3.961963725090027, ETA in seconds: 1936590.017\n",
      "epoch: 434000, train loss: 3.8972227811813354, val loss: 3.9642271757125855, ETA in seconds: 1936753.102\n",
      "epoch: 434100, train loss: 3.8958167552948, val loss: 3.971453881263733, ETA in seconds: 1936840.261\n",
      "epoch: 434200, train loss: 3.889443302154541, val loss: 3.963742995262146, ETA in seconds: 1936912.392\n",
      "epoch: 434300, train loss: 3.890913224220276, val loss: 3.9730448484420777, ETA in seconds: 1936998.021\n",
      "epoch: 434400, train loss: 3.8900781869888306, val loss: 3.97374312877655, ETA in seconds: 1937065.076\n",
      "epoch: 434500, train loss: 3.883405661582947, val loss: 3.9670374155044557, ETA in seconds: 1937149.558\n",
      "epoch: 434600, train loss: 3.895248365402222, val loss: 3.965224862098694, ETA in seconds: 1937289.906\n",
      "epoch: 434700, train loss: 3.8985759735107424, val loss: 3.9747907876968385, ETA in seconds: 1937352.806\n",
      "epoch: 434800, train loss: 3.8932000398635864, val loss: 3.977711820602417, ETA in seconds: 1937446.794\n",
      "epoch: 434900, train loss: 3.8843434333801268, val loss: 3.9763330221176147, ETA in seconds: 1937607.476\n",
      "epoch: 435000, train loss: 3.8943628787994387, val loss: 3.9777085304260256, ETA in seconds: 1937771.750\n",
      "epoch: 435100, train loss: 3.8903001070022585, val loss: 3.974358654022217, ETA in seconds: 1937910.614\n",
      "epoch: 435200, train loss: 3.88964159488678, val loss: 3.977678632736206, ETA in seconds: 1937962.637\n",
      "epoch: 435300, train loss: 3.897367072105408, val loss: 3.971153140068054, ETA in seconds: 1938023.589\n",
      "epoch: 435400, train loss: 3.895846128463745, val loss: 3.9736818075180054, ETA in seconds: 1938078.341\n",
      "epoch: 435500, train loss: 3.8892640590667726, val loss: 3.9794148445129394, ETA in seconds: 1938136.586\n",
      "epoch: 435600, train loss: 3.8946364164352416, val loss: 3.970234441757202, ETA in seconds: 1938198.303\n",
      "epoch: 435700, train loss: 3.8928266763687134, val loss: 3.971359395980835, ETA in seconds: 1938261.668\n",
      "epoch: 435800, train loss: 3.890432834625244, val loss: 3.9764660835266112, ETA in seconds: 1938347.281\n",
      "epoch: 435900, train loss: 3.900825595855713, val loss: 3.9698257446289062, ETA in seconds: 1938409.707\n",
      "epoch: 436000, train loss: 3.9047821044921873, val loss: 3.965075063705444, ETA in seconds: 1938491.993\n",
      "epoch: 436100, train loss: 3.8894891023635862, val loss: 3.9747020959854127, ETA in seconds: 1938656.775\n",
      "epoch: 436200, train loss: 3.9010915517807008, val loss: 3.973484921455383, ETA in seconds: 1938739.553\n",
      "epoch: 436300, train loss: 3.8905906438827516, val loss: 3.976698327064514, ETA in seconds: 1938835.611\n",
      "epoch: 436400, train loss: 3.8894479751586912, val loss: 3.9586511135101317, ETA in seconds: 1938919.575\n",
      "epoch: 436500, train loss: 3.904993462562561, val loss: 3.970835828781128, ETA in seconds: 1939006.474\n",
      "epoch: 436600, train loss: 3.900355648994446, val loss: 3.9703054189682008, ETA in seconds: 1939085.043\n",
      "epoch: 436700, train loss: 3.886021852493286, val loss: 3.9665679216384886, ETA in seconds: 1939154.118\n",
      "epoch: 436800, train loss: 3.896431303024292, val loss: 3.9753552436828614, ETA in seconds: 1939221.185\n",
      "epoch: 436900, train loss: 3.894486498832703, val loss: 3.9639042139053347, ETA in seconds: 1939315.913\n",
      "epoch: 437000, train loss: 3.8966057777404783, val loss: 3.9745091915130617, ETA in seconds: 1939386.977\n",
      "epoch: 437100, train loss: 3.8927631855010985, val loss: 3.965934157371521, ETA in seconds: 1939450.198\n",
      "epoch: 437200, train loss: 3.899849271774292, val loss: 3.967466115951538, ETA in seconds: 1939524.352\n",
      "epoch: 437300, train loss: 3.8935965299606323, val loss: 3.969722318649292, ETA in seconds: 1939598.436\n",
      "epoch: 437400, train loss: 3.895658278465271, val loss: 3.964596748352051, ETA in seconds: 1939666.789\n",
      "epoch: 437500, train loss: 3.9006556034088136, val loss: 3.974398136138916, ETA in seconds: 1939748.972\n",
      "epoch: 437600, train loss: 3.902928948402405, val loss: 3.967289161682129, ETA in seconds: 1939910.085\n",
      "epoch: 437700, train loss: 3.8846558809280394, val loss: 3.9665068626403808, ETA in seconds: 1939979.253\n",
      "epoch: 437800, train loss: 3.886685276031494, val loss: 3.9760433197021485, ETA in seconds: 1940055.116\n",
      "epoch: 437900, train loss: 3.9004587650299074, val loss: 3.972511339187622, ETA in seconds: 1940146.849\n",
      "epoch: 438000, train loss: 3.8901704072952272, val loss: 3.96787052154541, ETA in seconds: 1940210.635\n",
      "epoch: 438100, train loss: 3.8964189291000366, val loss: 3.980646276473999, ETA in seconds: 1940276.993\n",
      "epoch: 438200, train loss: 3.8919795513153077, val loss: 3.9787269830703735, ETA in seconds: 1940369.161\n",
      "epoch: 438300, train loss: 3.890241026878357, val loss: 3.9634079933166504, ETA in seconds: 1940454.998\n",
      "epoch: 438400, train loss: 3.892253375053406, val loss: 3.9676584720611574, ETA in seconds: 1940525.274\n",
      "epoch: 438500, train loss: 3.894068717956543, val loss: 3.9655651092529296, ETA in seconds: 1940593.159\n",
      "epoch: 438600, train loss: 3.8928394079208375, val loss: 3.9635841608047486, ETA in seconds: 1940674.279\n",
      "epoch: 438700, train loss: 3.893957567214966, val loss: 3.9640361070632935, ETA in seconds: 1940763.038\n",
      "epoch: 438800, train loss: 3.9067479372024536, val loss: 3.9603288412094115, ETA in seconds: 1940835.729\n",
      "epoch: 438900, train loss: 3.888959002494812, val loss: 3.971254515647888, ETA in seconds: 1940915.857\n",
      "epoch: 439000, train loss: 3.8925342559814453, val loss: 3.978201079368591, ETA in seconds: 1940998.012\n",
      "epoch: 439100, train loss: 3.9093403816223145, val loss: 3.972506809234619, ETA in seconds: 1941087.743\n",
      "epoch: 439200, train loss: 3.892329525947571, val loss: 3.9699114322662354, ETA in seconds: 1941176.881\n",
      "epoch: 439300, train loss: 3.9046671867370604, val loss: 3.9715617179870604, ETA in seconds: 1941273.058\n",
      "epoch: 439400, train loss: 3.890385055541992, val loss: 3.9676355123519897, ETA in seconds: 1941365.147\n",
      "epoch: 439500, train loss: 3.8964795589447023, val loss: 3.9681161403656007, ETA in seconds: 1941454.666\n",
      "epoch: 439600, train loss: 3.9035929918289183, val loss: 3.9761440992355346, ETA in seconds: 1941547.046\n",
      "epoch: 439700, train loss: 3.9026317834854125, val loss: 3.970346689224243, ETA in seconds: 1941621.399\n",
      "epoch: 439800, train loss: 3.8841662406921387, val loss: 3.9827187299728393, ETA in seconds: 1941689.543\n",
      "epoch: 439900, train loss: 3.894132614135742, val loss: 3.9697927474975585, ETA in seconds: 1941767.248\n",
      "epoch: 440000, train loss: 3.897452640533447, val loss: 3.962535834312439, ETA in seconds: 1941906.597\n",
      "epoch: 440100, train loss: 3.8924136400222777, val loss: 3.975657105445862, ETA in seconds: 1941976.578\n",
      "epoch: 440200, train loss: 3.8845749378204344, val loss: 3.977575397491455, ETA in seconds: 1942081.345\n",
      "epoch: 440300, train loss: 3.894132375717163, val loss: 3.977432036399841, ETA in seconds: 1942244.728\n",
      "epoch: 440400, train loss: 3.8967987537384032, val loss: 3.9738680601119993, ETA in seconds: 1942411.087\n",
      "epoch: 440500, train loss: 3.8900042533874513, val loss: 3.971848177909851, ETA in seconds: 1942573.325\n",
      "epoch: 440600, train loss: 3.887555456161499, val loss: 3.9622164487838747, ETA in seconds: 1942738.378\n",
      "epoch: 440700, train loss: 3.8988893747329714, val loss: 3.9792428970336915, ETA in seconds: 1942900.629\n",
      "epoch: 440800, train loss: 3.887306809425354, val loss: 3.9784160375595095, ETA in seconds: 1943064.630\n",
      "epoch: 440900, train loss: 3.895200824737549, val loss: 3.9773795127868654, ETA in seconds: 1943153.476\n",
      "epoch: 441000, train loss: 3.8861049890518187, val loss: 3.97044517993927, ETA in seconds: 1943196.896\n",
      "epoch: 441100, train loss: 3.889360451698303, val loss: 3.975370740890503, ETA in seconds: 1943277.478\n",
      "epoch: 441200, train loss: 3.8967235803604128, val loss: 3.976585602760315, ETA in seconds: 1943385.699\n",
      "epoch: 441300, train loss: 3.8916080713272097, val loss: 3.9748157501220702, ETA in seconds: 1943527.833\n",
      "epoch: 441400, train loss: 3.8880189180374147, val loss: 3.985128664970398, ETA in seconds: 1943626.174\n",
      "epoch: 441500, train loss: 3.8892175197601317, val loss: 3.9736897945404053, ETA in seconds: 1943683.818\n",
      "epoch: 441600, train loss: 3.889227819442749, val loss: 3.9680135250091553, ETA in seconds: 1943864.357\n",
      "epoch: 441700, train loss: 3.8900527000427245, val loss: 3.9682254791259766, ETA in seconds: 1944052.406\n",
      "epoch: 441800, train loss: 3.8872304439544676, val loss: 3.9766503810882567, ETA in seconds: 1944207.603\n",
      "epoch: 441900, train loss: 3.8870697259902953, val loss: 3.9817261934280395, ETA in seconds: 1944364.412\n",
      "epoch: 442000, train loss: 3.902597999572754, val loss: 3.9713777780532835, ETA in seconds: 1944521.954\n",
      "epoch: 442100, train loss: 3.888551640510559, val loss: 3.9714563369750975, ETA in seconds: 1944670.601\n",
      "epoch: 442200, train loss: 3.9022700071334837, val loss: 3.970140480995178, ETA in seconds: 1944744.200\n",
      "epoch: 442300, train loss: 3.9015351057052614, val loss: 3.9737476825714113, ETA in seconds: 1944815.215\n",
      "epoch: 442400, train loss: 3.8904527902603148, val loss: 3.968929100036621, ETA in seconds: 1944891.403\n",
      "epoch: 442500, train loss: 3.8934744596481323, val loss: 3.9772091627120973, ETA in seconds: 1944962.722\n",
      "epoch: 442600, train loss: 3.8862918615341187, val loss: 3.966414523124695, ETA in seconds: 1945065.155\n",
      "epoch: 442700, train loss: 3.9025586605072022, val loss: 3.9742507934570312, ETA in seconds: 1945217.055\n",
      "epoch: 442800, train loss: 3.8940336227416994, val loss: 3.973089814186096, ETA in seconds: 1945292.081\n",
      "epoch: 442900, train loss: 3.900985026359558, val loss: 3.9740368366241454, ETA in seconds: 1945352.389\n",
      "epoch: 443000, train loss: 3.8952697992324827, val loss: 3.972406196594238, ETA in seconds: 1945404.809\n",
      "epoch: 443100, train loss: 3.888443922996521, val loss: 3.9707266330718993, ETA in seconds: 1945456.268\n",
      "epoch: 443200, train loss: 3.892255759239197, val loss: 3.970100736618042, ETA in seconds: 1945509.679\n",
      "epoch: 443300, train loss: 3.8942386150360107, val loss: 3.970922350883484, ETA in seconds: 1945580.526\n",
      "epoch: 443400, train loss: 3.899140501022339, val loss: 3.9763572216033936, ETA in seconds: 1945678.762\n",
      "epoch: 443500, train loss: 3.8941004514694213, val loss: 3.971618986129761, ETA in seconds: 1945744.069\n",
      "epoch: 443600, train loss: 3.8923363208770754, val loss: 3.9771889209747315, ETA in seconds: 1945793.841\n",
      "epoch: 443700, train loss: 3.8899292230606077, val loss: 3.980951428413391, ETA in seconds: 1945842.972\n",
      "epoch: 443800, train loss: 3.9016177892684936, val loss: 3.9785109996795653, ETA in seconds: 1945932.029\n",
      "epoch: 443900, train loss: 3.895462989807129, val loss: 3.9677175760269163, ETA in seconds: 1946067.725\n",
      "epoch: 444000, train loss: 3.890694761276245, val loss: 3.9785541772842405, ETA in seconds: 1946157.893\n",
      "epoch: 444100, train loss: 3.884584069252014, val loss: 3.9658644914627077, ETA in seconds: 1946236.417\n",
      "epoch: 444200, train loss: 3.896325206756592, val loss: 3.9601072788238527, ETA in seconds: 1946307.212\n",
      "epoch: 444300, train loss: 3.893494176864624, val loss: 3.9784219026565553, ETA in seconds: 1946372.905\n",
      "epoch: 444400, train loss: 3.890758514404297, val loss: 3.977277064323425, ETA in seconds: 1946438.417\n",
      "epoch: 444500, train loss: 3.894168496131897, val loss: 3.9559308528900146, ETA in seconds: 1946541.587\n",
      "epoch: 444600, train loss: 3.8985673904418947, val loss: 3.975531983375549, ETA in seconds: 1946689.446\n",
      "epoch: 444700, train loss: 3.88377525806427, val loss: 3.965449023246765, ETA in seconds: 1946851.988\n",
      "epoch: 444800, train loss: 3.8984912157058718, val loss: 3.9735207319259644, ETA in seconds: 1946961.840\n",
      "epoch: 444900, train loss: 3.887548303604126, val loss: 3.9802008867263794, ETA in seconds: 1947035.721\n",
      "epoch: 445000, train loss: 3.8912890672683718, val loss: 3.972642707824707, ETA in seconds: 1947148.611\n",
      "epoch: 445100, train loss: 3.888416600227356, val loss: 3.9683621168136596, ETA in seconds: 1947311.482\n",
      "epoch: 445200, train loss: 3.893061947822571, val loss: 3.974393343925476, ETA in seconds: 1947433.623\n",
      "epoch: 445300, train loss: 3.8954238653182984, val loss: 3.9787013292312623, ETA in seconds: 1947590.173\n",
      "epoch: 445400, train loss: 3.894619607925415, val loss: 3.9730735540390016, ETA in seconds: 1947686.680\n",
      "epoch: 445500, train loss: 3.8833781480789185, val loss: 3.966939187049866, ETA in seconds: 1947748.502\n",
      "epoch: 445600, train loss: 3.8994839191436768, val loss: 3.964654278755188, ETA in seconds: 1947812.111\n",
      "epoch: 445700, train loss: 3.892954397201538, val loss: 3.972295546531677, ETA in seconds: 1947884.411\n",
      "epoch: 445800, train loss: 3.8890605926513673, val loss: 3.9788961172103883, ETA in seconds: 1947949.771\n",
      "epoch: 445900, train loss: 3.901297950744629, val loss: 3.96522536277771, ETA in seconds: 1948019.205\n",
      "epoch: 446000, train loss: 3.888513422012329, val loss: 3.9767972946166994, ETA in seconds: 1948090.299\n",
      "epoch: 446100, train loss: 3.884954023361206, val loss: 3.9688074588775635, ETA in seconds: 1948158.480\n",
      "epoch: 446200, train loss: 3.8766924381256103, val loss: 3.9597781181335447, ETA in seconds: 1948224.586\n",
      "epoch: 446300, train loss: 3.897662401199341, val loss: 3.978340268135071, ETA in seconds: 1948289.577\n",
      "epoch: 446400, train loss: 3.8923419237136843, val loss: 3.9642053842544556, ETA in seconds: 1948352.029\n",
      "epoch: 446500, train loss: 3.901048755645752, val loss: 3.9670178413391115, ETA in seconds: 1948424.264\n",
      "epoch: 446600, train loss: 3.895230269432068, val loss: 3.9634997606277467, ETA in seconds: 1948503.962\n",
      "epoch: 446700, train loss: 3.8975085496902464, val loss: 3.971591925621033, ETA in seconds: 1948589.219\n",
      "epoch: 446800, train loss: 3.884881329536438, val loss: 3.971769833564758, ETA in seconds: 1948675.556\n",
      "epoch: 446900, train loss: 3.8862051010131835, val loss: 3.977819585800171, ETA in seconds: 1948752.768\n",
      "epoch: 447000, train loss: 3.8973405838012694, val loss: 3.9680935621261595, ETA in seconds: 1948827.966\n",
      "epoch: 447100, train loss: 3.890554356575012, val loss: 3.973057579994202, ETA in seconds: 1948905.369\n",
      "epoch: 447200, train loss: 3.885820007324219, val loss: 3.9792961359024046, ETA in seconds: 1948984.098\n",
      "epoch: 447300, train loss: 3.895551824569702, val loss: 3.9712009191513062, ETA in seconds: 1949070.237\n",
      "epoch: 447400, train loss: 3.880651926994324, val loss: 3.9671250343322755, ETA in seconds: 1949162.592\n",
      "epoch: 447500, train loss: 3.881970834732056, val loss: 3.9662946701049804, ETA in seconds: 1949244.938\n",
      "epoch: 447600, train loss: 3.8886157512664794, val loss: 3.978736662864685, ETA in seconds: 1949372.123\n",
      "epoch: 447700, train loss: 3.886263108253479, val loss: 3.966354012489319, ETA in seconds: 1949564.273\n",
      "epoch: 447800, train loss: 3.8915614604949953, val loss: 3.9547234773635864, ETA in seconds: 1949709.196\n",
      "epoch: 447900, train loss: 3.8898316383361817, val loss: 3.969377851486206, ETA in seconds: 1949858.754\n",
      "epoch: 448000, train loss: 3.9007382154464723, val loss: 3.967286705970764, ETA in seconds: 1949963.871\n",
      "epoch: 448100, train loss: 3.8874359130859375, val loss: 3.9748199939727784, ETA in seconds: 1950020.229\n",
      "epoch: 448200, train loss: 3.882220911979675, val loss: 3.9659356117248534, ETA in seconds: 1950071.958\n",
      "epoch: 448300, train loss: 3.901674199104309, val loss: 3.977690100669861, ETA in seconds: 1950129.243\n",
      "epoch: 448400, train loss: 3.8889840841293335, val loss: 3.970617341995239, ETA in seconds: 1950194.487\n",
      "epoch: 448500, train loss: 3.8906734704971315, val loss: 3.971401739120483, ETA in seconds: 1950250.034\n",
      "epoch: 448600, train loss: 3.8974265336990355, val loss: 3.9607507467269896, ETA in seconds: 1950329.101\n",
      "epoch: 448700, train loss: 3.886628818511963, val loss: 3.9782880783081054, ETA in seconds: 1950471.934\n",
      "epoch: 448800, train loss: 3.901664900779724, val loss: 3.9595652341842653, ETA in seconds: 1950608.700\n",
      "epoch: 448900, train loss: 3.8959528684616087, val loss: 3.9732477188110353, ETA in seconds: 1950747.333\n",
      "epoch: 449000, train loss: 3.8912496328353883, val loss: 3.9628196477890016, ETA in seconds: 1950884.338\n",
      "epoch: 449100, train loss: 3.885613131523132, val loss: 3.961770272254944, ETA in seconds: 1951020.532\n",
      "epoch: 449200, train loss: 3.8884173393249513, val loss: 3.959332513809204, ETA in seconds: 1951157.994\n",
      "epoch: 449300, train loss: 3.889963412284851, val loss: 3.9649469375610353, ETA in seconds: 1951226.734\n",
      "epoch: 449400, train loss: 3.8934836864471434, val loss: 3.968905735015869, ETA in seconds: 1951279.804\n",
      "epoch: 449500, train loss: 3.896582269668579, val loss: 3.9775691509246824, ETA in seconds: 1951358.800\n",
      "epoch: 449600, train loss: 3.889073443412781, val loss: 3.9660993576049806, ETA in seconds: 1951492.963\n",
      "epoch: 449700, train loss: 3.888172745704651, val loss: 3.965620756149292, ETA in seconds: 1951551.431\n",
      "epoch: 449800, train loss: 3.8918569564819334, val loss: 3.9647363424301147, ETA in seconds: 1951616.472\n",
      "epoch: 449900, train loss: 3.900003957748413, val loss: 3.973105549812317, ETA in seconds: 1951678.406\n",
      "epoch: 450000, train loss: 3.8953646421432495, val loss: 3.9689608335494997, ETA in seconds: 1951746.983\n",
      "epoch: 450100, train loss: 3.901165270805359, val loss: 3.969308376312256, ETA in seconds: 1951802.258\n",
      "epoch: 450200, train loss: 3.8960450410842897, val loss: 3.9743808031082155, ETA in seconds: 1951871.974\n",
      "epoch: 450300, train loss: 3.8893527746200562, val loss: 3.963141393661499, ETA in seconds: 1951984.652\n",
      "epoch: 450400, train loss: 3.8917328119277954, val loss: 3.966650700569153, ETA in seconds: 1952144.211\n",
      "epoch: 450500, train loss: 3.8905005931854246, val loss: 3.9628991365432737, ETA in seconds: 1952280.825\n",
      "epoch: 450600, train loss: 3.8896592378616335, val loss: 3.973571467399597, ETA in seconds: 1952422.286\n",
      "epoch: 450700, train loss: 3.8951959133148195, val loss: 3.958302927017212, ETA in seconds: 1952559.809\n",
      "epoch: 450800, train loss: 3.897299027442932, val loss: 3.975292611122131, ETA in seconds: 1952697.475\n",
      "epoch: 450900, train loss: 3.88799843788147, val loss: 3.9667572736740113, ETA in seconds: 1952833.454\n",
      "epoch: 451000, train loss: 3.892161989212036, val loss: 3.9695205211639406, ETA in seconds: 1952973.051\n",
      "epoch: 451100, train loss: 3.9060588836669923, val loss: 3.959287977218628, ETA in seconds: 1953115.181\n",
      "epoch: 451200, train loss: 3.894812822341919, val loss: 3.971594762802124, ETA in seconds: 1953179.780\n",
      "epoch: 451300, train loss: 3.8901970863342283, val loss: 3.9733898878097533, ETA in seconds: 1953238.872\n",
      "epoch: 451400, train loss: 3.899319124221802, val loss: 3.9705991983413695, ETA in seconds: 1953295.371\n",
      "epoch: 451500, train loss: 3.899959850311279, val loss: 3.9787805795669557, ETA in seconds: 1953360.970\n",
      "epoch: 451600, train loss: 3.887646222114563, val loss: 3.9623135328292847, ETA in seconds: 1953442.826\n",
      "epoch: 451700, train loss: 3.8849170923233034, val loss: 3.9667714118957518, ETA in seconds: 1953529.581\n",
      "epoch: 451800, train loss: 3.8921312570571898, val loss: 3.9649926900863646, ETA in seconds: 1953597.050\n",
      "epoch: 451900, train loss: 3.895288872718811, val loss: 3.957082915306091, ETA in seconds: 1953676.945\n",
      "epoch: 452000, train loss: 3.8894757270812987, val loss: 3.969847083091736, ETA in seconds: 1953739.676\n",
      "epoch: 452100, train loss: 3.891839528083801, val loss: 3.957541346549988, ETA in seconds: 1953800.223\n",
      "epoch: 452200, train loss: 3.889276695251465, val loss: 3.9722063064575197, ETA in seconds: 1953861.980\n",
      "epoch: 452300, train loss: 3.8977357864379885, val loss: 3.9747623443603515, ETA in seconds: 1953942.499\n",
      "epoch: 452400, train loss: 3.8904372692108153, val loss: 3.967405676841736, ETA in seconds: 1954029.129\n",
      "epoch: 452500, train loss: 3.8907293319702148, val loss: 3.9787017345428466, ETA in seconds: 1954118.113\n",
      "epoch: 452600, train loss: 3.8966423749923704, val loss: 3.9721450567245484, ETA in seconds: 1954208.824\n",
      "epoch: 452700, train loss: 3.9049322605133057, val loss: 3.9695424795150758, ETA in seconds: 1954294.992\n",
      "epoch: 452800, train loss: 3.8837613821029664, val loss: 3.9734742879867553, ETA in seconds: 1954376.481\n",
      "epoch: 452900, train loss: 3.894776940345764, val loss: 3.96421058177948, ETA in seconds: 1954461.934\n",
      "epoch: 453000, train loss: 3.892497253417969, val loss: 3.9604327201843263, ETA in seconds: 1954528.263\n",
      "epoch: 453100, train loss: 3.893737530708313, val loss: 3.966447186470032, ETA in seconds: 1954595.353\n",
      "epoch: 453200, train loss: 3.8899348258972166, val loss: 3.9684637546539308, ETA in seconds: 1954670.326\n",
      "epoch: 453300, train loss: 3.8810770511627197, val loss: 3.976328730583191, ETA in seconds: 1954724.408\n",
      "epoch: 453400, train loss: 3.8921881437301638, val loss: 3.977843451499939, ETA in seconds: 1954776.908\n",
      "epoch: 453500, train loss: 3.8944810390472413, val loss: 3.9677236318588256, ETA in seconds: 1954834.784\n",
      "epoch: 453600, train loss: 3.894050192832947, val loss: 3.961239051818848, ETA in seconds: 1954929.986\n",
      "epoch: 453700, train loss: 3.8852923631668093, val loss: 3.9748134136199953, ETA in seconds: 1955018.182\n",
      "epoch: 453800, train loss: 3.897114300727844, val loss: 3.9575074434280397, ETA in seconds: 1955066.402\n",
      "epoch: 453900, train loss: 3.89202139377594, val loss: 3.982159161567688, ETA in seconds: 1955116.784\n",
      "epoch: 454000, train loss: 3.8850022077560427, val loss: 3.957030415534973, ETA in seconds: 1955200.299\n",
      "epoch: 454100, train loss: 3.899221348762512, val loss: 3.9723649501800535, ETA in seconds: 1955284.093\n",
      "epoch: 454200, train loss: 3.895762395858765, val loss: 3.969742226600647, ETA in seconds: 1955363.337\n",
      "epoch: 454300, train loss: 3.8906163692474367, val loss: 3.9657116413116453, ETA in seconds: 1955444.353\n",
      "epoch: 454400, train loss: 3.899478530883789, val loss: 3.9713042736053468, ETA in seconds: 1955518.264\n",
      "epoch: 454500, train loss: 3.888654112815857, val loss: 3.9711836338043214, ETA in seconds: 1955575.935\n",
      "epoch: 454600, train loss: 3.9003023147583007, val loss: 3.97041916847229, ETA in seconds: 1955703.968\n",
      "epoch: 454700, train loss: 3.896230959892273, val loss: 3.9616132259368895, ETA in seconds: 1955845.332\n",
      "epoch: 454800, train loss: 3.8993234634399414, val loss: 3.9761510610580446, ETA in seconds: 1955898.290\n",
      "epoch: 454900, train loss: 3.8966145753860473, val loss: 3.959970164299011, ETA in seconds: 1955947.870\n",
      "epoch: 455000, train loss: 3.885056471824646, val loss: 3.968970012664795, ETA in seconds: 1955996.269\n",
      "epoch: 455100, train loss: 3.8989118576049804, val loss: 3.967123579978943, ETA in seconds: 1956030.558\n",
      "epoch: 455200, train loss: 3.894566464424133, val loss: 3.968371772766113, ETA in seconds: 1956055.741\n",
      "epoch: 455300, train loss: 3.8932723999023438, val loss: 3.9738821744918824, ETA in seconds: 1956102.737\n",
      "epoch: 455400, train loss: 3.894398736953735, val loss: 3.973860239982605, ETA in seconds: 1956145.100\n",
      "epoch: 455500, train loss: 3.8978371143341066, val loss: 3.9707521915435793, ETA in seconds: 1956183.220\n",
      "epoch: 455600, train loss: 3.8965158224105836, val loss: 3.9802977085113525, ETA in seconds: 1956216.973\n",
      "epoch: 455700, train loss: 3.8953847646713258, val loss: 3.9706376075744627, ETA in seconds: 1956250.601\n",
      "epoch: 455800, train loss: 3.8989140510559084, val loss: 3.9640920400619506, ETA in seconds: 1956309.901\n",
      "epoch: 455900, train loss: 3.8943301677703857, val loss: 3.9686150312423707, ETA in seconds: 1956436.992\n",
      "epoch: 456000, train loss: 3.8975421667098997, val loss: 3.9690528392791746, ETA in seconds: 1956565.909\n",
      "epoch: 456100, train loss: 3.891188907623291, val loss: 3.969025182723999, ETA in seconds: 1956697.000\n",
      "epoch: 456200, train loss: 3.883615326881409, val loss: 3.9716383457183837, ETA in seconds: 1956724.857\n",
      "epoch: 456300, train loss: 3.8928315162658693, val loss: 3.9716095685958863, ETA in seconds: 1956745.700\n",
      "epoch: 456400, train loss: 3.895005702972412, val loss: 3.9665037393569946, ETA in seconds: 1956794.259\n",
      "epoch: 456500, train loss: 3.892535138130188, val loss: 3.9631561994552613, ETA in seconds: 1956832.930\n",
      "epoch: 456600, train loss: 3.9002482652664185, val loss: 3.968260884284973, ETA in seconds: 1956871.241\n",
      "epoch: 456700, train loss: 3.8916802167892457, val loss: 3.966951084136963, ETA in seconds: 1956920.114\n",
      "epoch: 456800, train loss: 3.894890475273132, val loss: 3.9633880376815798, ETA in seconds: 1956955.546\n",
      "epoch: 456900, train loss: 3.897421956062317, val loss: 3.981102776527405, ETA in seconds: 1957007.744\n",
      "epoch: 457000, train loss: 3.891754150390625, val loss: 3.9603142738342285, ETA in seconds: 1957114.056\n",
      "epoch: 457100, train loss: 3.8949126958847047, val loss: 3.9690261363983153, ETA in seconds: 1957165.474\n",
      "epoch: 457200, train loss: 3.8870301961898805, val loss: 3.9633543491363525, ETA in seconds: 1957229.148\n",
      "epoch: 457300, train loss: 3.891599488258362, val loss: 3.9726001739501955, ETA in seconds: 1957289.526\n",
      "epoch: 457400, train loss: 3.9007822275161743, val loss: 3.974622869491577, ETA in seconds: 1957358.391\n",
      "epoch: 457500, train loss: 3.896458649635315, val loss: 3.9743485927581785, ETA in seconds: 1957431.120\n",
      "epoch: 457600, train loss: 3.8947779893875123, val loss: 3.9591547012329102, ETA in seconds: 1957520.019\n",
      "epoch: 457700, train loss: 3.8988943338394164, val loss: 3.969429039955139, ETA in seconds: 1957564.245\n",
      "epoch: 457800, train loss: 3.893723177909851, val loss: 3.9793097734451295, ETA in seconds: 1957618.437\n",
      "epoch: 457900, train loss: 3.8882630109786986, val loss: 3.9820617914199827, ETA in seconds: 1957675.163\n",
      "epoch: 458000, train loss: 3.892214608192444, val loss: 3.97057363986969, ETA in seconds: 1957723.890\n",
      "epoch: 458100, train loss: 3.8938533544540403, val loss: 3.9684798955917358, ETA in seconds: 1957772.435\n",
      "epoch: 458200, train loss: 3.8898605823516847, val loss: 3.9786136388778686, ETA in seconds: 1957828.713\n",
      "epoch: 458300, train loss: 3.8918356895446777, val loss: 3.9694178342819213, ETA in seconds: 1957874.174\n",
      "epoch: 458400, train loss: 3.8873587131500242, val loss: 3.9735326051712034, ETA in seconds: 1957957.613\n",
      "epoch: 458500, train loss: 3.8909010171890257, val loss: 3.964573860168457, ETA in seconds: 1958112.314\n",
      "epoch: 458600, train loss: 3.8954621315002442, val loss: 3.9673722267150877, ETA in seconds: 1958154.986\n",
      "epoch: 458700, train loss: 3.904722714424133, val loss: 3.9680872440338133, ETA in seconds: 1958212.201\n",
      "epoch: 458800, train loss: 3.902454710006714, val loss: 3.966969680786133, ETA in seconds: 1958265.900\n",
      "epoch: 458900, train loss: 3.8959734439849854, val loss: 3.9667752265930174, ETA in seconds: 1958320.585\n",
      "epoch: 459000, train loss: 3.8988434791564943, val loss: 3.9685423135757447, ETA in seconds: 1958379.142\n",
      "epoch: 459100, train loss: 3.893923282623291, val loss: 3.9758166551589964, ETA in seconds: 1958459.819\n",
      "epoch: 459200, train loss: 3.8930226802825927, val loss: 3.968700408935547, ETA in seconds: 1958585.557\n",
      "epoch: 459300, train loss: 3.890912103652954, val loss: 3.9662060022354124, ETA in seconds: 1958659.747\n",
      "epoch: 459400, train loss: 3.8921134233474732, val loss: 3.972827196121216, ETA in seconds: 1958727.991\n",
      "epoch: 459500, train loss: 3.8917192459106444, val loss: 3.96564359664917, ETA in seconds: 1958778.110\n",
      "epoch: 459600, train loss: 3.889476203918457, val loss: 3.9806759119033814, ETA in seconds: 1958822.332\n",
      "epoch: 459700, train loss: 3.884787178039551, val loss: 3.9777673482894897, ETA in seconds: 1958876.011\n",
      "epoch: 459800, train loss: 3.8928216218948366, val loss: 3.96979341506958, ETA in seconds: 1958934.468\n",
      "epoch: 459900, train loss: 3.891777992248535, val loss: 3.9736297369003295, ETA in seconds: 1958970.177\n",
      "epoch: 460000, train loss: 3.8899579286575316, val loss: 3.9763406038284304, ETA in seconds: 1959014.786\n",
      "epoch: 460100, train loss: 3.887910985946655, val loss: 3.9668344259262085, ETA in seconds: 1959076.269\n",
      "epoch: 460200, train loss: 3.8940096378326414, val loss: 3.9636670112609864, ETA in seconds: 1959135.041\n",
      "epoch: 460300, train loss: 3.8919029712677, val loss: 3.9715903997421265, ETA in seconds: 1959215.823\n",
      "epoch: 460400, train loss: 3.8968456268310545, val loss: 3.9647841930389403, ETA in seconds: 1959336.316\n",
      "epoch: 460500, train loss: 3.8956363201141357, val loss: 3.9809411764144897, ETA in seconds: 1959455.858\n",
      "epoch: 460600, train loss: 3.8889965057373046, val loss: 3.9728580713272095, ETA in seconds: 1959536.744\n",
      "epoch: 460700, train loss: 3.8968151569366456, val loss: 3.965434718132019, ETA in seconds: 1959592.747\n",
      "epoch: 460800, train loss: 3.8923008680343627, val loss: 3.9630537748336794, ETA in seconds: 1959641.224\n",
      "epoch: 460900, train loss: 3.890915608406067, val loss: 3.9822874546051024, ETA in seconds: 1959739.194\n",
      "epoch: 461000, train loss: 3.9025258302688597, val loss: 3.977577018737793, ETA in seconds: 1959836.980\n",
      "epoch: 461100, train loss: 3.8958694696426392, val loss: 3.9654387474060058, ETA in seconds: 1959956.753\n",
      "epoch: 461200, train loss: 3.8930339574813844, val loss: 3.958195352554321, ETA in seconds: 1960001.594\n",
      "epoch: 461300, train loss: 3.900071358680725, val loss: 3.9715045213699343, ETA in seconds: 1960047.599\n",
      "epoch: 461400, train loss: 3.8905670404434205, val loss: 3.9713335037231445, ETA in seconds: 1960208.434\n",
      "epoch: 461500, train loss: 3.894810438156128, val loss: 3.96881959438324, ETA in seconds: 1960345.519\n",
      "epoch: 461600, train loss: 3.9039414882659913, val loss: 3.9692779302597048, ETA in seconds: 1960433.306\n",
      "epoch: 461700, train loss: 3.8885422706604005, val loss: 3.963539743423462, ETA in seconds: 1960472.338\n",
      "epoch: 461800, train loss: 3.9017542600631714, val loss: 3.9789297103881838, ETA in seconds: 1960510.880\n",
      "epoch: 461900, train loss: 3.8793192863464356, val loss: 3.9679319143295286, ETA in seconds: 1960532.764\n",
      "epoch: 462000, train loss: 3.8921725749969482, val loss: 3.9778671741485594, ETA in seconds: 1960559.379\n",
      "epoch: 462100, train loss: 3.8905525684356688, val loss: 3.9741457223892214, ETA in seconds: 1960586.911\n",
      "epoch: 462200, train loss: 3.8902663946151734, val loss: 3.974388360977173, ETA in seconds: 1960629.356\n",
      "epoch: 462300, train loss: 3.9006742238998413, val loss: 3.965367865562439, ETA in seconds: 1960709.601\n",
      "epoch: 462400, train loss: 3.8884615659713746, val loss: 3.9633735179901124, ETA in seconds: 1960850.257\n",
      "epoch: 462500, train loss: 3.8976820707321167, val loss: 3.9764864683151244, ETA in seconds: 1961006.016\n",
      "epoch: 462600, train loss: 3.8911193370819093, val loss: 3.9745451688766478, ETA in seconds: 1961134.675\n",
      "epoch: 462700, train loss: 3.896640419960022, val loss: 3.9741676568984987, ETA in seconds: 1961259.558\n",
      "epoch: 462800, train loss: 3.8818795919418334, val loss: 3.973318076133728, ETA in seconds: 1961377.975\n",
      "epoch: 462900, train loss: 3.8981868982315064, val loss: 3.971922445297241, ETA in seconds: 1961498.080\n",
      "epoch: 463000, train loss: 3.8999980688095093, val loss: 3.9675678968429566, ETA in seconds: 1961542.815\n",
      "epoch: 463100, train loss: 3.8930920362472534, val loss: 3.967488479614258, ETA in seconds: 1961595.277\n",
      "epoch: 463200, train loss: 3.898509645462036, val loss: 3.9677783727645872, ETA in seconds: 1961644.325\n",
      "epoch: 463300, train loss: 3.897700619697571, val loss: 3.9690805435180665, ETA in seconds: 1961708.652\n",
      "epoch: 463400, train loss: 3.896514964103699, val loss: 3.9757521867752077, ETA in seconds: 1961756.994\n",
      "epoch: 463500, train loss: 3.8945989847183227, val loss: 3.97243869304657, ETA in seconds: 1961802.033\n",
      "epoch: 463600, train loss: 3.891108441352844, val loss: 3.9672429084777834, ETA in seconds: 1961845.350\n",
      "epoch: 463700, train loss: 3.897628092765808, val loss: 3.9847798347473145, ETA in seconds: 1961892.592\n",
      "epoch: 463800, train loss: 3.892931914329529, val loss: 3.9647108793258665, ETA in seconds: 1961941.867\n",
      "epoch: 463900, train loss: 3.90441517829895, val loss: 3.9708312511444093, ETA in seconds: 1961983.133\n",
      "epoch: 464000, train loss: 3.8901119232177734, val loss: 3.9553430318832397, ETA in seconds: 1962041.474\n",
      "epoch: 464100, train loss: 3.8951247215270994, val loss: 3.964252233505249, ETA in seconds: 1962080.986\n",
      "epoch: 464200, train loss: 3.8885193347930906, val loss: 3.9713643550872804, ETA in seconds: 1962120.510\n",
      "epoch: 464300, train loss: 3.9052186250686645, val loss: 3.9645599365234374, ETA in seconds: 1962160.609\n",
      "epoch: 464400, train loss: 3.894253206253052, val loss: 3.9706950187683105, ETA in seconds: 1962187.923\n",
      "epoch: 464500, train loss: 3.897737145423889, val loss: 3.9739516735076905, ETA in seconds: 1962237.351\n",
      "epoch: 464600, train loss: 3.8879886627197267, val loss: 3.950891542434692, ETA in seconds: 1962298.556\n",
      "epoch: 464700, train loss: 3.890144920349121, val loss: 3.973656916618347, ETA in seconds: 1962354.360\n",
      "epoch: 464800, train loss: 3.8992398023605346, val loss: 3.9736694574356077, ETA in seconds: 1962415.610\n",
      "epoch: 464900, train loss: 3.8995404481887816, val loss: 3.963353490829468, ETA in seconds: 1962467.653\n",
      "epoch: 465000, train loss: 3.8917287826538085, val loss: 3.980067229270935, ETA in seconds: 1962540.211\n",
      "epoch: 465100, train loss: 3.888578248023987, val loss: 3.9751460552215576, ETA in seconds: 1962619.192\n",
      "epoch: 465200, train loss: 3.887697720527649, val loss: 3.972162437438965, ETA in seconds: 1962671.813\n",
      "epoch: 465300, train loss: 3.904118871688843, val loss: 3.981036376953125, ETA in seconds: 1962729.382\n",
      "epoch: 465400, train loss: 3.8941741704940798, val loss: 3.971331310272217, ETA in seconds: 1962798.878\n",
      "epoch: 465500, train loss: 3.89373095035553, val loss: 3.9818492412567137, ETA in seconds: 1962858.558\n",
      "epoch: 465600, train loss: 3.8956549406051635, val loss: 3.975220513343811, ETA in seconds: 1962919.370\n",
      "epoch: 465700, train loss: 3.8991032361984255, val loss: 3.974415683746338, ETA in seconds: 1963000.644\n",
      "epoch: 465800, train loss: 3.899149465560913, val loss: 3.9692559480667113, ETA in seconds: 1963085.425\n",
      "epoch: 465900, train loss: 3.8996368646621704, val loss: 3.960298442840576, ETA in seconds: 1963125.226\n",
      "epoch: 466000, train loss: 3.8952388525009156, val loss: 3.9801002502441407, ETA in seconds: 1963169.972\n",
      "epoch: 466100, train loss: 3.885925626754761, val loss: 3.9673694610595702, ETA in seconds: 1963201.860\n",
      "epoch: 466200, train loss: 3.898853135108948, val loss: 3.971449589729309, ETA in seconds: 1963227.427\n",
      "epoch: 466300, train loss: 3.8966454744338987, val loss: 3.9748064279556274, ETA in seconds: 1963254.067\n",
      "epoch: 466400, train loss: 3.887902522087097, val loss: 3.981752872467041, ETA in seconds: 1963282.592\n",
      "epoch: 466500, train loss: 3.8875900983810423, val loss: 3.975510597229004, ETA in seconds: 1963301.532\n",
      "epoch: 466600, train loss: 3.8905776500701905, val loss: 3.9610378742218018, ETA in seconds: 1963323.110\n",
      "epoch: 466700, train loss: 3.8980520963668823, val loss: 3.9608662843704225, ETA in seconds: 1963342.453\n",
      "epoch: 466800, train loss: 3.8913624048233033, val loss: 3.9684303760528565, ETA in seconds: 1963375.466\n",
      "epoch: 466900, train loss: 3.897351121902466, val loss: 3.962895131111145, ETA in seconds: 1963402.386\n",
      "epoch: 467000, train loss: 3.8941540002822874, val loss: 3.9715636730194093, ETA in seconds: 1963425.713\n",
      "epoch: 467100, train loss: 3.8957249879837037, val loss: 3.960338592529297, ETA in seconds: 1963446.408\n",
      "epoch: 467200, train loss: 3.8877992391586305, val loss: 3.959336280822754, ETA in seconds: 1963461.241\n",
      "epoch: 467300, train loss: 3.8927074670791626, val loss: 3.973374438285828, ETA in seconds: 1963495.839\n",
      "epoch: 467400, train loss: 3.894858407974243, val loss: 3.974664640426636, ETA in seconds: 1963527.185\n",
      "epoch: 467500, train loss: 3.894235110282898, val loss: 3.9749140977859496, ETA in seconds: 1963567.439\n",
      "epoch: 467600, train loss: 3.8941121101379395, val loss: 3.9644365072250367, ETA in seconds: 1963585.951\n",
      "epoch: 467700, train loss: 3.892880439758301, val loss: 3.9569871187210084, ETA in seconds: 1963604.313\n",
      "epoch: 467800, train loss: 3.900612139701843, val loss: 3.9669411897659304, ETA in seconds: 1963626.207\n",
      "epoch: 467900, train loss: 3.8948864221572874, val loss: 3.9604374170303345, ETA in seconds: 1963650.167\n",
      "epoch: 468000, train loss: 3.8982232570648194, val loss: 3.9703872442245483, ETA in seconds: 1963675.311\n",
      "epoch: 468100, train loss: 3.8983259201049805, val loss: 3.9681703090667724, ETA in seconds: 1963699.042\n",
      "epoch: 468200, train loss: 3.890784978866577, val loss: 3.9741448402404784, ETA in seconds: 1963780.851\n",
      "epoch: 468300, train loss: 3.8943352937698363, val loss: 3.9700873374938963, ETA in seconds: 1963804.742\n",
      "epoch: 468400, train loss: 3.8967899322509765, val loss: 3.972899842262268, ETA in seconds: 1963835.448\n",
      "epoch: 468500, train loss: 3.8987375020980837, val loss: 3.978075408935547, ETA in seconds: 1963864.685\n",
      "epoch: 468600, train loss: 3.8929465293884276, val loss: 3.980665111541748, ETA in seconds: 1963878.256\n",
      "epoch: 468700, train loss: 3.880386400222778, val loss: 3.9729140996932983, ETA in seconds: 1963900.063\n",
      "epoch: 468800, train loss: 3.8967734813690185, val loss: 3.97724072933197, ETA in seconds: 1963927.731\n",
      "epoch: 468900, train loss: 3.9002623081207277, val loss: 3.9718606948852537, ETA in seconds: 1963965.032\n",
      "epoch: 469000, train loss: 3.887507104873657, val loss: 3.9692278146743774, ETA in seconds: 1963999.394\n",
      "epoch: 469100, train loss: 3.8932434558868407, val loss: 3.967558574676514, ETA in seconds: 1964077.060\n",
      "epoch: 469200, train loss: 3.8978671789169312, val loss: 3.9761834383010863, ETA in seconds: 1964151.063\n",
      "epoch: 469300, train loss: 3.8998239755630495, val loss: 3.964528036117554, ETA in seconds: 1964184.332\n",
      "epoch: 469400, train loss: 3.8909183740615845, val loss: 3.9619505405426025, ETA in seconds: 1964198.062\n",
      "epoch: 469500, train loss: 3.8825767993927003, val loss: 3.968985891342163, ETA in seconds: 1964225.831\n",
      "epoch: 469600, train loss: 3.8968105792999266, val loss: 3.9764570713043215, ETA in seconds: 1964278.378\n",
      "epoch: 469700, train loss: 3.8869171142578125, val loss: 3.9807671546936034, ETA in seconds: 1964384.973\n",
      "epoch: 469800, train loss: 3.903478717803955, val loss: 3.9670926094055177, ETA in seconds: 1964491.211\n",
      "epoch: 469900, train loss: 3.8946944952011107, val loss: 3.9727999448776243, ETA in seconds: 1964597.533\n",
      "epoch: 470000, train loss: 3.88810248374939, val loss: 3.9734858751296995, ETA in seconds: 1964707.141\n",
      "epoch: 470100, train loss: 3.8912529945373535, val loss: 3.976110887527466, ETA in seconds: 1964797.361\n",
      "epoch: 470200, train loss: 3.889937686920166, val loss: 3.95534508228302, ETA in seconds: 1964908.633\n",
      "epoch: 470300, train loss: 3.897837829589844, val loss: 3.96776077747345, ETA in seconds: 1964997.906\n",
      "epoch: 470400, train loss: 3.885229802131653, val loss: 3.9634303569793703, ETA in seconds: 1965003.789\n",
      "epoch: 470500, train loss: 3.89417405128479, val loss: 3.9720832824707033, ETA in seconds: 1965016.779\n",
      "epoch: 470600, train loss: 3.884032869338989, val loss: 3.964609408378601, ETA in seconds: 1965030.516\n",
      "epoch: 470700, train loss: 3.896569013595581, val loss: 3.9674237728118897, ETA in seconds: 1965047.272\n",
      "epoch: 470800, train loss: 3.89399733543396, val loss: 3.9668920755386354, ETA in seconds: 1965094.653\n",
      "epoch: 470900, train loss: 3.87640438079834, val loss: 3.978364276885986, ETA in seconds: 1965200.137\n",
      "epoch: 471000, train loss: 3.8991151809692384, val loss: 3.969349193572998, ETA in seconds: 1965254.871\n",
      "epoch: 471100, train loss: 3.886737084388733, val loss: 3.9644661426544188, ETA in seconds: 1965297.570\n",
      "epoch: 471200, train loss: 3.895415687561035, val loss: 3.9698849439620973, ETA in seconds: 1965360.512\n",
      "epoch: 471300, train loss: 3.887817549705505, val loss: 3.972520184516907, ETA in seconds: 1965398.824\n",
      "epoch: 471400, train loss: 3.8938899755477907, val loss: 3.9695200443267824, ETA in seconds: 1965441.852\n",
      "epoch: 471500, train loss: 3.895526313781738, val loss: 3.9766947269439696, ETA in seconds: 1965476.690\n",
      "epoch: 471600, train loss: 3.891423225402832, val loss: 3.9650049924850466, ETA in seconds: 1965514.452\n",
      "epoch: 471700, train loss: 3.8946368217468263, val loss: 3.9781086683273315, ETA in seconds: 1965544.245\n",
      "epoch: 471800, train loss: 3.89238440990448, val loss: 3.962357997894287, ETA in seconds: 1965582.294\n",
      "epoch: 471900, train loss: 3.8879454374313354, val loss: 3.972783398628235, ETA in seconds: 1965617.542\n",
      "epoch: 472000, train loss: 3.8940825700759887, val loss: 3.973124551773071, ETA in seconds: 1965657.281\n",
      "epoch: 472100, train loss: 3.8947548627853394, val loss: 3.9666120767593385, ETA in seconds: 1965687.225\n",
      "epoch: 472200, train loss: 3.886785078048706, val loss: 3.974354958534241, ETA in seconds: 1965708.014\n",
      "epoch: 472300, train loss: 3.8996586561203004, val loss: 3.9627546072006226, ETA in seconds: 1965742.786\n",
      "epoch: 472400, train loss: 3.8945056676864622, val loss: 3.971811628341675, ETA in seconds: 1965767.926\n",
      "epoch: 472500, train loss: 3.8922603368759154, val loss: 3.9810253143310548, ETA in seconds: 1965796.573\n",
      "epoch: 472600, train loss: 3.8942936420440675, val loss: 3.968876767158508, ETA in seconds: 1965869.574\n",
      "epoch: 472700, train loss: 3.8825467348098757, val loss: 3.9736785650253297, ETA in seconds: 1965978.567\n",
      "epoch: 472800, train loss: 3.8994323015213013, val loss: 3.985507035255432, ETA in seconds: 1966008.070\n",
      "epoch: 472900, train loss: 3.896648955345154, val loss: 3.9650768518447874, ETA in seconds: 1966038.031\n",
      "epoch: 473000, train loss: 3.9017848491668703, val loss: 3.9689136743545532, ETA in seconds: 1966065.718\n",
      "epoch: 473100, train loss: 3.8932749748229982, val loss: 3.9702239274978637, ETA in seconds: 1966084.238\n",
      "epoch: 473200, train loss: 3.898804020881653, val loss: 3.9619609832763674, ETA in seconds: 1966110.494\n",
      "epoch: 473300, train loss: 3.8946375370025637, val loss: 3.967030239105225, ETA in seconds: 1966242.723\n",
      "epoch: 473400, train loss: 3.9043865442276, val loss: 3.9688328981399534, ETA in seconds: 1966356.997\n",
      "epoch: 473500, train loss: 3.8945000648498533, val loss: 3.975740170478821, ETA in seconds: 1966461.004\n",
      "epoch: 473600, train loss: 3.8935834407806396, val loss: 3.964879107475281, ETA in seconds: 1966566.793\n",
      "epoch: 473700, train loss: 3.8954533100128175, val loss: 3.9708558320999146, ETA in seconds: 1966611.588\n",
      "epoch: 473800, train loss: 3.893413209915161, val loss: 3.9698732614517214, ETA in seconds: 1966644.079\n",
      "epoch: 473900, train loss: 3.8926512002944946, val loss: 3.9693925857543944, ETA in seconds: 1966668.327\n",
      "epoch: 474000, train loss: 3.8868951320648195, val loss: 3.975723481178284, ETA in seconds: 1966681.501\n",
      "epoch: 474100, train loss: 3.896383500099182, val loss: 3.9658060312271117, ETA in seconds: 1966702.765\n",
      "epoch: 474200, train loss: 3.9099513053894044, val loss: 3.971083474159241, ETA in seconds: 1966716.710\n",
      "epoch: 474300, train loss: 3.901984190940857, val loss: 3.9722575426101683, ETA in seconds: 1966745.584\n",
      "epoch: 474400, train loss: 3.899448370933533, val loss: 3.965285563468933, ETA in seconds: 1966791.843\n",
      "epoch: 474500, train loss: 3.8968236207962037, val loss: 3.9789619922637938, ETA in seconds: 1966826.277\n",
      "epoch: 474600, train loss: 3.8973716259002686, val loss: 3.9705486536026, ETA in seconds: 1966856.902\n",
      "epoch: 474700, train loss: 3.8845062017440797, val loss: 3.9851994037628176, ETA in seconds: 1966895.214\n",
      "epoch: 474800, train loss: 3.892027187347412, val loss: 3.972864007949829, ETA in seconds: 1966957.625\n",
      "epoch: 474900, train loss: 3.891125273704529, val loss: 3.9772234916687013, ETA in seconds: 1966995.540\n",
      "epoch: 475000, train loss: 3.899213743209839, val loss: 3.971043348312378, ETA in seconds: 1967033.879\n",
      "epoch: 475100, train loss: 3.8960912466049193, val loss: 3.9786706924438477, ETA in seconds: 1967046.990\n",
      "epoch: 475200, train loss: 3.8921306848526003, val loss: 3.9657230138778687, ETA in seconds: 1967066.083\n",
      "epoch: 475300, train loss: 3.8939176321029665, val loss: 3.969878625869751, ETA in seconds: 1967084.688\n",
      "epoch: 475400, train loss: 3.893172526359558, val loss: 3.968394422531128, ETA in seconds: 1967142.869\n",
      "epoch: 475500, train loss: 3.894005632400513, val loss: 3.9739519357681274, ETA in seconds: 1967167.147\n",
      "epoch: 475600, train loss: 3.8960687160491942, val loss: 3.9717398405075075, ETA in seconds: 1967188.335\n",
      "epoch: 475700, train loss: 3.8910711288452147, val loss: 3.9766289710998537, ETA in seconds: 1967232.320\n",
      "epoch: 475800, train loss: 3.8968361377716065, val loss: 3.971696162223816, ETA in seconds: 1967263.793\n",
      "epoch: 475900, train loss: 3.8953036308288573, val loss: 3.9834193706512453, ETA in seconds: 1967287.008\n",
      "epoch: 476000, train loss: 3.8950507164001467, val loss: 3.9744056224823, ETA in seconds: 1967301.821\n",
      "epoch: 476100, train loss: 3.901269221305847, val loss: 3.970365524291992, ETA in seconds: 1967320.276\n",
      "epoch: 476200, train loss: 3.899039649963379, val loss: 3.9781637668609617, ETA in seconds: 1967390.861\n",
      "epoch: 476300, train loss: 3.9014132738113405, val loss: 3.9790865421295165, ETA in seconds: 1967495.843\n",
      "epoch: 476400, train loss: 3.8952585458755493, val loss: 3.98305504322052, ETA in seconds: 1967597.829\n",
      "epoch: 476500, train loss: 3.8849167585372926, val loss: 3.976265811920166, ETA in seconds: 1967702.850\n",
      "epoch: 476600, train loss: 3.8891102075576782, val loss: 3.973844027519226, ETA in seconds: 1967741.375\n",
      "epoch: 476700, train loss: 3.8875896453857424, val loss: 3.9712673902511595, ETA in seconds: 1967763.057\n",
      "epoch: 476800, train loss: 3.8920342445373537, val loss: 3.975873255729675, ETA in seconds: 1967786.572\n",
      "epoch: 476900, train loss: 3.8924806833267214, val loss: 3.9763486623764037, ETA in seconds: 1967823.245\n",
      "epoch: 477000, train loss: 3.8904467821121216, val loss: 3.9689937353134157, ETA in seconds: 1967860.437\n",
      "epoch: 477100, train loss: 3.895465683937073, val loss: 3.9681788206100466, ETA in seconds: 1967899.049\n",
      "epoch: 477200, train loss: 3.8954649925231934, val loss: 3.969262957572937, ETA in seconds: 1967920.315\n",
      "epoch: 477300, train loss: 3.9031230211257935, val loss: 3.9795658111572267, ETA in seconds: 1967941.438\n",
      "epoch: 477400, train loss: 3.8925618886947633, val loss: 3.9639883995056153, ETA in seconds: 1967958.347\n",
      "epoch: 477500, train loss: 3.8863369703292845, val loss: 3.9673343658447267, ETA in seconds: 1967975.508\n",
      "epoch: 477600, train loss: 3.892116093635559, val loss: 3.974808859825134, ETA in seconds: 1967996.122\n",
      "epoch: 477700, train loss: 3.892936897277832, val loss: 3.9682078838348387, ETA in seconds: 1968047.301\n",
      "epoch: 477800, train loss: 3.8938291549682615, val loss: 3.966449522972107, ETA in seconds: 1968073.983\n",
      "epoch: 477900, train loss: 3.884237194061279, val loss: 3.978765773773193, ETA in seconds: 1968100.001\n",
      "epoch: 478000, train loss: 3.894063925743103, val loss: 3.968201446533203, ETA in seconds: 1968136.215\n",
      "epoch: 478100, train loss: 3.8915096044540407, val loss: 3.9800196170806883, ETA in seconds: 1968159.390\n",
      "epoch: 478200, train loss: 3.895840549468994, val loss: 3.9748804330825807, ETA in seconds: 1968162.156\n",
      "epoch: 478300, train loss: 3.8877673387527465, val loss: 3.9755265951156615, ETA in seconds: 1968168.879\n",
      "epoch: 478400, train loss: 3.8972208976745604, val loss: 3.9754221439361572, ETA in seconds: 1968174.289\n",
      "epoch: 478500, train loss: 3.8913498163223266, val loss: 3.964540934562683, ETA in seconds: 1968182.020\n",
      "epoch: 478600, train loss: 3.8868129730224608, val loss: 3.9707242012023927, ETA in seconds: 1968187.033\n",
      "epoch: 478700, train loss: 3.900778961181641, val loss: 3.9748499631881713, ETA in seconds: 1968260.641\n",
      "epoch: 478800, train loss: 3.890751504898071, val loss: 3.9765414237976073, ETA in seconds: 1968250.483\n",
      "epoch: 478900, train loss: 3.887810158729553, val loss: 3.965239071846008, ETA in seconds: 1968256.592\n",
      "epoch: 479000, train loss: 3.8873411417007446, val loss: 3.9677155733108522, ETA in seconds: 1968255.767\n",
      "epoch: 479100, train loss: 3.896540975570679, val loss: 3.972285008430481, ETA in seconds: 1968264.761\n",
      "epoch: 479200, train loss: 3.903784894943237, val loss: 3.9756399393081665, ETA in seconds: 1968265.321\n",
      "epoch: 479300, train loss: 3.898645615577698, val loss: 3.9810851573944093, ETA in seconds: 1968274.758\n",
      "epoch: 479400, train loss: 3.8926981925964355, val loss: 3.9658748865127564, ETA in seconds: 1968270.622\n",
      "epoch: 479500, train loss: 3.8888781309127807, val loss: 3.976991367340088, ETA in seconds: 1968261.257\n",
      "epoch: 479600, train loss: 3.884463930130005, val loss: 3.967391777038574, ETA in seconds: 1968258.442\n",
      "epoch: 479700, train loss: 3.8847837448120117, val loss: 3.9631821870803834, ETA in seconds: 1968268.389\n",
      "epoch: 479800, train loss: 3.895262932777405, val loss: 3.9767821073532104, ETA in seconds: 1968276.005\n",
      "epoch: 479900, train loss: 3.8903589487075805, val loss: 3.9650359630584715, ETA in seconds: 1968283.239\n",
      "epoch: 480000, train loss: 3.8910086393356322, val loss: 3.967768836021423, ETA in seconds: 1968289.729\n",
      "epoch: 480100, train loss: 3.899879884719849, val loss: 3.954697108268738, ETA in seconds: 1968296.055\n",
      "epoch: 480200, train loss: 3.8860504388809205, val loss: 3.9725966691970824, ETA in seconds: 1968293.526\n",
      "epoch: 480300, train loss: 3.891468071937561, val loss: 3.96597580909729, ETA in seconds: 1968293.168\n",
      "epoch: 480400, train loss: 3.893638181686401, val loss: 3.962458539009094, ETA in seconds: 1968297.289\n",
      "epoch: 480500, train loss: 3.8923995733261108, val loss: 3.978933596611023, ETA in seconds: 1968311.837\n",
      "epoch: 480600, train loss: 3.8907772064208985, val loss: 3.9659802436828615, ETA in seconds: 1968306.850\n",
      "epoch: 480700, train loss: 3.88742253780365, val loss: 3.9774161100387575, ETA in seconds: 1968303.294\n",
      "epoch: 480800, train loss: 3.884900975227356, val loss: 3.9752191066741944, ETA in seconds: 1968321.911\n",
      "epoch: 480900, train loss: 3.8986013174057006, val loss: 3.9619704723358153, ETA in seconds: 1968313.764\n",
      "epoch: 481000, train loss: 3.8982588768005373, val loss: 3.975312280654907, ETA in seconds: 1968323.759\n",
      "epoch: 481100, train loss: 3.895806884765625, val loss: 3.966527318954468, ETA in seconds: 1968410.167\n",
      "epoch: 481200, train loss: 3.893909525871277, val loss: 3.9689254999160766, ETA in seconds: 1968498.058\n",
      "epoch: 481300, train loss: 3.8956919431686403, val loss: 3.9782649278640747, ETA in seconds: 1968585.456\n",
      "epoch: 481400, train loss: 3.8990208387374876, val loss: 3.966387629508972, ETA in seconds: 1968656.128\n",
      "epoch: 481500, train loss: 3.886971044540405, val loss: 3.9653534650802613, ETA in seconds: 1968646.484\n",
      "epoch: 481600, train loss: 3.89641695022583, val loss: 3.9783106327056883, ETA in seconds: 1968639.131\n",
      "epoch: 481700, train loss: 3.893245983123779, val loss: 3.960109567642212, ETA in seconds: 1968636.312\n",
      "epoch: 481800, train loss: 3.897384023666382, val loss: 3.9825716495513914, ETA in seconds: 1968620.807\n",
      "epoch: 481900, train loss: 3.899374580383301, val loss: 3.9746891260147095, ETA in seconds: 1968614.975\n",
      "epoch: 482000, train loss: 3.902089548110962, val loss: 3.9649975299835205, ETA in seconds: 1968605.771\n",
      "epoch: 482100, train loss: 3.881643772125244, val loss: 3.96561541557312, ETA in seconds: 1968606.384\n",
      "epoch: 482200, train loss: 3.89161217212677, val loss: 3.9751681804656984, ETA in seconds: 1968591.040\n",
      "epoch: 482300, train loss: 3.90019428730011, val loss: 3.9618133783340452, ETA in seconds: 1968587.103\n",
      "epoch: 482400, train loss: 3.8943464517593385, val loss: 3.9658358573913572, ETA in seconds: 1968581.661\n",
      "epoch: 482500, train loss: 3.8939816474914553, val loss: 3.9658587694168093, ETA in seconds: 1968603.318\n",
      "epoch: 482600, train loss: 3.888233518600464, val loss: 3.9581192255020143, ETA in seconds: 1968693.438\n",
      "epoch: 482700, train loss: 3.8947381258010862, val loss: 3.9761536598205565, ETA in seconds: 1968742.103\n",
      "epoch: 482800, train loss: 3.8862317323684694, val loss: 3.97509880065918, ETA in seconds: 1968748.274\n",
      "epoch: 482900, train loss: 3.894120621681213, val loss: 3.963190031051636, ETA in seconds: 1968760.277\n",
      "epoch: 483000, train loss: 3.886245918273926, val loss: 3.964778447151184, ETA in seconds: 1968772.478\n",
      "epoch: 483100, train loss: 3.8924991607666017, val loss: 3.9832674503326415, ETA in seconds: 1968788.357\n",
      "epoch: 483200, train loss: 3.8984467029571532, val loss: 3.968994307518005, ETA in seconds: 1968800.944\n",
      "epoch: 483300, train loss: 3.8883190393447875, val loss: 3.967354726791382, ETA in seconds: 1968817.617\n",
      "epoch: 483400, train loss: 3.8953309774398805, val loss: 3.9630077362060545, ETA in seconds: 1968825.209\n",
      "epoch: 483500, train loss: 3.8999588012695314, val loss: 3.9798737287521364, ETA in seconds: 1968817.190\n",
      "epoch: 483600, train loss: 3.9025537252426146, val loss: 3.97140793800354, ETA in seconds: 1968799.459\n",
      "epoch: 483700, train loss: 3.9038222789764405, val loss: 3.9733078718185424, ETA in seconds: 1968782.841\n",
      "epoch: 483800, train loss: 3.9050886154174806, val loss: 3.977782726287842, ETA in seconds: 1968806.040\n",
      "epoch: 483900, train loss: 3.8967281579971313, val loss: 3.968235230445862, ETA in seconds: 1968832.986\n",
      "epoch: 484000, train loss: 3.893767523765564, val loss: 3.969003510475159, ETA in seconds: 1968909.645\n",
      "epoch: 484100, train loss: 3.8831550598144533, val loss: 3.9690116167068483, ETA in seconds: 1968925.964\n",
      "epoch: 484200, train loss: 3.8971944570541384, val loss: 3.983141040802002, ETA in seconds: 1968949.257\n",
      "epoch: 484300, train loss: 3.899837279319763, val loss: 3.970547103881836, ETA in seconds: 1968950.362\n",
      "epoch: 484400, train loss: 3.8992716550827025, val loss: 3.9704550743103026, ETA in seconds: 1968963.318\n",
      "epoch: 484500, train loss: 3.8946016788482667, val loss: 3.9647897481918335, ETA in seconds: 1968971.374\n",
      "epoch: 484600, train loss: 3.8983060836791994, val loss: 3.9755988121032715, ETA in seconds: 1968977.291\n",
      "epoch: 484700, train loss: 3.8894999504089354, val loss: 3.967518663406372, ETA in seconds: 1968997.275\n",
      "epoch: 484800, train loss: 3.903021287918091, val loss: 3.973504734039307, ETA in seconds: 1969011.423\n",
      "epoch: 484900, train loss: 3.8877403259277346, val loss: 3.9607179403305053, ETA in seconds: 1969026.137\n",
      "epoch: 485000, train loss: 3.890726399421692, val loss: 3.9699663639068605, ETA in seconds: 1969027.679\n",
      "epoch: 485100, train loss: 3.894064736366272, val loss: 3.973441410064697, ETA in seconds: 1969024.449\n",
      "epoch: 485200, train loss: 3.8840108156204223, val loss: 3.9817837476730347, ETA in seconds: 1969011.394\n",
      "epoch: 485300, train loss: 3.8934046745300295, val loss: 3.9635084867477417, ETA in seconds: 1969009.069\n",
      "epoch: 485400, train loss: 3.8992996215820312, val loss: 3.9776366710662843, ETA in seconds: 1969020.280\n",
      "epoch: 485500, train loss: 3.8845268964767454, val loss: 3.9851832389831543, ETA in seconds: 1969033.224\n",
      "epoch: 485600, train loss: 3.8994623899459837, val loss: 3.96656973361969, ETA in seconds: 1969045.569\n",
      "epoch: 485700, train loss: 3.892435836791992, val loss: 3.9777294158935548, ETA in seconds: 1969052.492\n",
      "epoch: 485800, train loss: 3.903700566291809, val loss: 3.9694941520690916, ETA in seconds: 1969059.531\n",
      "epoch: 485900, train loss: 3.8888088703155517, val loss: 3.9766216039657594, ETA in seconds: 1969052.488\n",
      "epoch: 486000, train loss: 3.887271285057068, val loss: 3.967329812049866, ETA in seconds: 1969059.574\n",
      "epoch: 486100, train loss: 3.8913195610046385, val loss: 3.980302858352661, ETA in seconds: 1969074.428\n",
      "epoch: 486200, train loss: 3.8915725708007813, val loss: 3.98484628200531, ETA in seconds: 1969089.189\n",
      "epoch: 486300, train loss: 3.884499764442444, val loss: 3.971821141242981, ETA in seconds: 1969101.018\n",
      "epoch: 486400, train loss: 3.8925294876098633, val loss: 3.9722003698349, ETA in seconds: 1969110.137\n",
      "epoch: 486500, train loss: 3.8910732507705688, val loss: 3.979112243652344, ETA in seconds: 1969107.323\n",
      "epoch: 486600, train loss: 3.8930181503295898, val loss: 3.9815649509429933, ETA in seconds: 1969104.470\n",
      "epoch: 486700, train loss: 3.897834229469299, val loss: 3.970663833618164, ETA in seconds: 1969111.914\n",
      "epoch: 486800, train loss: 3.8883572101593016, val loss: 3.9651999473571777, ETA in seconds: 1969120.518\n",
      "epoch: 486900, train loss: 3.886560082435608, val loss: 3.9719000101089477, ETA in seconds: 1969117.635\n",
      "epoch: 487000, train loss: 3.894497442245483, val loss: 3.9790246963500975, ETA in seconds: 1969151.732\n",
      "epoch: 487100, train loss: 3.8962438821792604, val loss: 3.979376435279846, ETA in seconds: 1969228.220\n",
      "epoch: 487200, train loss: 3.8934309244155885, val loss: 3.973552441596985, ETA in seconds: 1969303.664\n",
      "epoch: 487300, train loss: 3.891265034675598, val loss: 3.971931886672974, ETA in seconds: 1969380.808\n",
      "epoch: 487400, train loss: 3.894712233543396, val loss: 3.971400189399719, ETA in seconds: 1969456.147\n",
      "epoch: 487500, train loss: 3.8908978700637817, val loss: 3.9804618835449217, ETA in seconds: 1969535.809\n",
      "epoch: 487600, train loss: 3.8901225090026856, val loss: 3.9727452754974366, ETA in seconds: 1969590.405\n",
      "epoch: 487700, train loss: 3.8896735668182374, val loss: 3.977541971206665, ETA in seconds: 1969570.892\n",
      "epoch: 487800, train loss: 3.892189621925354, val loss: 3.9642658948898317, ETA in seconds: 1969563.710\n",
      "epoch: 487900, train loss: 3.896323227882385, val loss: 3.972618913650513, ETA in seconds: 1969560.867\n",
      "epoch: 488000, train loss: 3.893547773361206, val loss: 3.9695425510406492, ETA in seconds: 1969553.318\n",
      "epoch: 488100, train loss: 3.8910795927047728, val loss: 3.9733279943466187, ETA in seconds: 1969544.721\n",
      "epoch: 488200, train loss: 3.8991534948349, val loss: 3.9704649448394775, ETA in seconds: 1969559.289\n",
      "epoch: 488300, train loss: 3.8967027187347414, val loss: 3.9659099102020265, ETA in seconds: 1969550.924\n",
      "epoch: 488400, train loss: 3.886672043800354, val loss: 3.965963912010193, ETA in seconds: 1969536.389\n",
      "epoch: 488500, train loss: 3.8988072633743287, val loss: 3.9616787672042846, ETA in seconds: 1969520.727\n",
      "epoch: 488600, train loss: 3.889388823509216, val loss: 3.9691328048706054, ETA in seconds: 1969506.081\n",
      "epoch: 488700, train loss: 3.8977317810058594, val loss: 3.973323202133179, ETA in seconds: 1969509.612\n",
      "epoch: 488800, train loss: 3.894425392150879, val loss: 3.9654974937438965, ETA in seconds: 1969586.427\n",
      "epoch: 488900, train loss: 3.8974220991134643, val loss: 3.979678010940552, ETA in seconds: 1969635.964\n",
      "epoch: 489000, train loss: 3.8896288871765137, val loss: 3.9701167345046997, ETA in seconds: 1969629.934\n",
      "epoch: 489100, train loss: 3.8779855251312254, val loss: 3.964462327957153, ETA in seconds: 1969625.287\n",
      "epoch: 489200, train loss: 3.890289640426636, val loss: 3.969251346588135, ETA in seconds: 1969624.350\n",
      "epoch: 489300, train loss: 3.8886419773101806, val loss: 3.9782447099685667, ETA in seconds: 1969617.688\n",
      "epoch: 489400, train loss: 3.886247086524963, val loss: 3.971793365478516, ETA in seconds: 1969634.488\n",
      "epoch: 489500, train loss: 3.8887985229492186, val loss: 3.970058226585388, ETA in seconds: 1969687.843\n",
      "epoch: 489600, train loss: 3.895728421211243, val loss: 3.976995825767517, ETA in seconds: 1969699.110\n",
      "epoch: 489700, train loss: 3.9019760131835937, val loss: 3.9753136157989504, ETA in seconds: 1969705.514\n",
      "epoch: 489800, train loss: 3.891133999824524, val loss: 3.967754769325256, ETA in seconds: 1969710.540\n",
      "epoch: 489900, train loss: 3.890225148200989, val loss: 3.9748610258102417, ETA in seconds: 1969717.130\n",
      "epoch: 490000, train loss: 3.89081015586853, val loss: 3.9720547437667846, ETA in seconds: 1969720.665\n",
      "epoch: 490100, train loss: 3.88874032497406, val loss: 3.974348473548889, ETA in seconds: 1969716.287\n",
      "epoch: 490200, train loss: 3.894909119606018, val loss: 3.9665516138076784, ETA in seconds: 1969779.120\n",
      "epoch: 490300, train loss: 3.8897809267044066, val loss: 3.970179295539856, ETA in seconds: 1969806.218\n",
      "epoch: 490400, train loss: 3.8935354709625245, val loss: 3.966276502609253, ETA in seconds: 1969800.684\n",
      "epoch: 490500, train loss: 3.900423288345337, val loss: 3.980082368850708, ETA in seconds: 1969793.054\n",
      "epoch: 490600, train loss: 3.9012652397155763, val loss: 3.968657112121582, ETA in seconds: 1969787.607\n",
      "epoch: 490700, train loss: 3.8931827783584594, val loss: 3.970485305786133, ETA in seconds: 1969780.154\n",
      "epoch: 490800, train loss: 3.8892106294631956, val loss: 3.9722030401229858, ETA in seconds: 1969769.082\n",
      "epoch: 490900, train loss: 3.9007047414779663, val loss: 3.9723676681518554, ETA in seconds: 1969773.634\n",
      "epoch: 491000, train loss: 3.8871315002441404, val loss: 3.9726320028305055, ETA in seconds: 1969788.162\n",
      "epoch: 491100, train loss: 3.896492576599121, val loss: 3.975482773780823, ETA in seconds: 1969773.503\n",
      "epoch: 491200, train loss: 3.90358567237854, val loss: 3.976848912239075, ETA in seconds: 1969781.080\n",
      "epoch: 491300, train loss: 3.9001583576202394, val loss: 3.982536196708679, ETA in seconds: 1969852.640\n",
      "epoch: 491400, train loss: 3.8882861137390137, val loss: 3.9812626600265504, ETA in seconds: 1969888.349\n",
      "epoch: 491500, train loss: 3.893014073371887, val loss: 3.9772836208343505, ETA in seconds: 1969878.812\n",
      "epoch: 491600, train loss: 3.8869606971740724, val loss: 3.975195074081421, ETA in seconds: 1969873.287\n",
      "epoch: 491700, train loss: 3.8889171361923216, val loss: 3.9733920812606813, ETA in seconds: 1969869.901\n",
      "epoch: 491800, train loss: 3.896306109428406, val loss: 3.9674025535583497, ETA in seconds: 1969869.895\n",
      "epoch: 491900, train loss: 3.8904885768890383, val loss: 3.9736493825912476, ETA in seconds: 1969915.746\n",
      "epoch: 492000, train loss: 3.893765115737915, val loss: 3.981205677986145, ETA in seconds: 1969953.841\n",
      "epoch: 492100, train loss: 3.8884933471679686, val loss: 3.9699256896972654, ETA in seconds: 1969952.277\n",
      "epoch: 492200, train loss: 3.887952423095703, val loss: 3.983097553253174, ETA in seconds: 1969979.674\n",
      "epoch: 492300, train loss: 3.889181041717529, val loss: 3.970556640625, ETA in seconds: 1970050.138\n",
      "epoch: 492400, train loss: 3.899542212486267, val loss: 3.972534441947937, ETA in seconds: 1970079.406\n",
      "epoch: 492500, train loss: 3.891653633117676, val loss: 3.9760960817337034, ETA in seconds: 1970081.185\n",
      "epoch: 492600, train loss: 3.9082373857498167, val loss: 3.9708790302276613, ETA in seconds: 1970076.110\n",
      "epoch: 492700, train loss: 3.8874173164367676, val loss: 3.966916632652283, ETA in seconds: 1970093.034\n",
      "epoch: 492800, train loss: 3.892679524421692, val loss: 3.9711119413375853, ETA in seconds: 1970161.973\n",
      "epoch: 492900, train loss: 3.8871822118759156, val loss: 3.9805490493774416, ETA in seconds: 1970229.764\n",
      "epoch: 493000, train loss: 3.8887567281723023, val loss: 3.9746206283569334, ETA in seconds: 1970301.593\n",
      "epoch: 493100, train loss: 3.8872541427612304, val loss: 3.974767065048218, ETA in seconds: 1970371.725\n",
      "epoch: 493200, train loss: 3.891162371635437, val loss: 3.9813273668289186, ETA in seconds: 1970441.696\n",
      "epoch: 493300, train loss: 3.893217945098877, val loss: 3.969945192337036, ETA in seconds: 1970440.885\n",
      "epoch: 493400, train loss: 3.892106628417969, val loss: 3.9806670427322386, ETA in seconds: 1970455.265\n",
      "epoch: 493500, train loss: 3.8957022190093995, val loss: 3.970806121826172, ETA in seconds: 1970444.215\n",
      "epoch: 493600, train loss: 3.887293553352356, val loss: 3.9797807693481446, ETA in seconds: 1970440.936\n",
      "epoch: 493700, train loss: 3.903068447113037, val loss: 3.969312858581543, ETA in seconds: 1970440.746\n",
      "epoch: 493800, train loss: 3.8895908117294313, val loss: 3.960262084007263, ETA in seconds: 1970513.536\n",
      "epoch: 493900, train loss: 3.894267129898071, val loss: 3.96179838180542, ETA in seconds: 1970572.911\n",
      "epoch: 494000, train loss: 3.8968466997146605, val loss: 3.9717044115066527, ETA in seconds: 1970548.758\n",
      "epoch: 494100, train loss: 3.89126832485199, val loss: 3.966806411743164, ETA in seconds: 1970525.444\n",
      "epoch: 494200, train loss: 3.8981576442718504, val loss: 3.973546099662781, ETA in seconds: 1970523.078\n",
      "epoch: 494300, train loss: 3.903869152069092, val loss: 3.9742571353912353, ETA in seconds: 1970542.907\n",
      "epoch: 494400, train loss: 3.906870436668396, val loss: 3.9772778749465942, ETA in seconds: 1970517.440\n",
      "epoch: 494500, train loss: 3.8996963024139406, val loss: 3.973907160758972, ETA in seconds: 1970489.037\n",
      "epoch: 494600, train loss: 3.889695882797241, val loss: 3.9755136013031005, ETA in seconds: 1970464.468\n",
      "epoch: 494700, train loss: 3.8872580528259277, val loss: 3.9630645275115968, ETA in seconds: 1970433.417\n",
      "epoch: 494800, train loss: 3.898060750961304, val loss: 3.977413702011108, ETA in seconds: 1970410.580\n",
      "epoch: 494900, train loss: 3.9044238090515138, val loss: 3.9778798103332518, ETA in seconds: 1970390.698\n",
      "epoch: 495000, train loss: 3.8906386852264405, val loss: 3.9684542179107667, ETA in seconds: 1970380.846\n",
      "epoch: 495100, train loss: 3.904258131980896, val loss: 3.9771550416946413, ETA in seconds: 1970370.089\n",
      "epoch: 495200, train loss: 3.894334053993225, val loss: 3.9811403274536135, ETA in seconds: 1970360.522\n",
      "epoch: 495300, train loss: 3.890245246887207, val loss: 3.9849156856536867, ETA in seconds: 1970341.043\n",
      "epoch: 495400, train loss: 3.8882339000701904, val loss: 3.959232234954834, ETA in seconds: 1970309.850\n",
      "epoch: 495500, train loss: 3.891907572746277, val loss: 3.9765451431274412, ETA in seconds: 1970297.873\n",
      "epoch: 495600, train loss: 3.8854506492614744, val loss: 3.972758936882019, ETA in seconds: 1970282.148\n",
      "epoch: 495700, train loss: 3.8986955881118774, val loss: 3.9847251653671263, ETA in seconds: 1970300.841\n",
      "epoch: 495800, train loss: 3.8934964418411253, val loss: 3.964658522605896, ETA in seconds: 1970397.784\n",
      "epoch: 495900, train loss: 3.8974763631820677, val loss: 3.9712223529815676, ETA in seconds: 1970367.694\n",
      "epoch: 496000, train loss: 3.8921352863311767, val loss: 3.960820269584656, ETA in seconds: 1970345.229\n",
      "epoch: 496100, train loss: 3.8896845817565917, val loss: 3.962510824203491, ETA in seconds: 1970326.398\n",
      "epoch: 496200, train loss: 3.8981257677078247, val loss: 3.9763054609298707, ETA in seconds: 1970308.318\n",
      "epoch: 496300, train loss: 3.884579801559448, val loss: 3.9723202705383303, ETA in seconds: 1970307.723\n",
      "epoch: 496400, train loss: 3.892313742637634, val loss: 3.9736960411071776, ETA in seconds: 1970358.033\n",
      "epoch: 496500, train loss: 3.890013909339905, val loss: 3.96386661529541, ETA in seconds: 1970343.696\n",
      "epoch: 496600, train loss: 3.8891817808151243, val loss: 3.9672082901000976, ETA in seconds: 1970338.230\n",
      "epoch: 496700, train loss: 3.89494309425354, val loss: 3.973487114906311, ETA in seconds: 1970328.910\n",
      "epoch: 496800, train loss: 3.905499529838562, val loss: 3.984431266784668, ETA in seconds: 1970318.503\n",
      "epoch: 496900, train loss: 3.8930744409561155, val loss: 3.978732967376709, ETA in seconds: 1970312.150\n",
      "epoch: 497000, train loss: 3.8941768169403077, val loss: 3.978381323814392, ETA in seconds: 1970321.082\n",
      "epoch: 497100, train loss: 3.8940596103668215, val loss: 3.962694764137268, ETA in seconds: 1970318.498\n",
      "epoch: 497200, train loss: 3.8911691427230837, val loss: 3.9720582723617555, ETA in seconds: 1970370.091\n",
      "epoch: 497300, train loss: 3.8951809644699096, val loss: 3.9677786111831663, ETA in seconds: 1970406.030\n",
      "epoch: 497400, train loss: 3.903324818611145, val loss: 3.955810856819153, ETA in seconds: 1970396.321\n",
      "epoch: 497500, train loss: 3.894558000564575, val loss: 3.961634874343872, ETA in seconds: 1970403.195\n",
      "epoch: 497600, train loss: 3.895604968070984, val loss: 3.974239158630371, ETA in seconds: 1970422.809\n",
      "epoch: 497700, train loss: 3.8906875371932985, val loss: 3.9607671737670898, ETA in seconds: 1970420.029\n",
      "epoch: 497800, train loss: 3.8867626428604125, val loss: 3.965619516372681, ETA in seconds: 1970433.675\n",
      "epoch: 497900, train loss: 3.9000765323638915, val loss: 3.9604677677154543, ETA in seconds: 1970494.572\n",
      "epoch: 498000, train loss: 3.8874884366989138, val loss: 3.971008849143982, ETA in seconds: 1970551.754\n",
      "epoch: 498100, train loss: 3.889198398590088, val loss: 3.9686475515365602, ETA in seconds: 1970607.892\n",
      "epoch: 498200, train loss: 3.8870806455612184, val loss: 3.9513734102249147, ETA in seconds: 1970612.497\n",
      "epoch: 498300, train loss: 3.895131802558899, val loss: 3.96149218082428, ETA in seconds: 1970591.835\n",
      "epoch: 498400, train loss: 3.892431044578552, val loss: 3.9701961517333983, ETA in seconds: 1970589.010\n",
      "epoch: 498500, train loss: 3.897490882873535, val loss: 3.9641238689422607, ETA in seconds: 1970632.179\n",
      "epoch: 498600, train loss: 3.889247441291809, val loss: 3.9744120121002195, ETA in seconds: 1970696.415\n",
      "epoch: 498700, train loss: 3.9026839971542358, val loss: 3.9591468811035155, ETA in seconds: 1970756.513\n",
      "epoch: 498800, train loss: 3.897776746749878, val loss: 3.963889455795288, ETA in seconds: 1970805.957\n",
      "epoch: 498900, train loss: 3.8905802011489867, val loss: 3.9717464447021484, ETA in seconds: 1970887.963\n",
      "epoch: 499000, train loss: 3.8920925140380858, val loss: 3.961544942855835, ETA in seconds: 1970920.630\n",
      "epoch: 499100, train loss: 3.88883113861084, val loss: 3.9755512475967407, ETA in seconds: 1970896.804\n",
      "epoch: 499200, train loss: 3.893928647041321, val loss: 3.9658251762390138, ETA in seconds: 1970920.347\n",
      "epoch: 499300, train loss: 3.8833220481872557, val loss: 3.9726877927780153, ETA in seconds: 1970990.686\n",
      "epoch: 499400, train loss: 3.8925415515899657, val loss: 3.97321457862854, ETA in seconds: 1971055.306\n",
      "epoch: 499500, train loss: 3.8973774671554566, val loss: 3.970675230026245, ETA in seconds: 1971084.794\n",
      "epoch: 499600, train loss: 3.8867159366607664, val loss: 3.972654938697815, ETA in seconds: 1971130.482\n",
      "epoch: 499700, train loss: 3.8917431354522707, val loss: 3.9756768703460694, ETA in seconds: 1971120.063\n",
      "epoch: 499800, train loss: 3.8911958932876587, val loss: 3.973946952819824, ETA in seconds: 1971086.198\n",
      "epoch: 499900, train loss: 3.88992555141449, val loss: 3.9705183506011963, ETA in seconds: 1971072.038\n",
      "epoch: 500000, train loss: 3.891782784461975, val loss: 3.978406071662903, ETA in seconds: 1971094.371\n",
      "epoch: 500100, train loss: 3.8858265399932863, val loss: 3.973292660713196, ETA in seconds: 1971079.264\n",
      "epoch: 500200, train loss: 3.8925774097442627, val loss: 3.972855305671692, ETA in seconds: 1971068.272\n",
      "epoch: 500300, train loss: 3.8904639959335325, val loss: 3.9738470315933228, ETA in seconds: 1971052.638\n",
      "epoch: 500400, train loss: 3.892947769165039, val loss: 3.9679259300231933, ETA in seconds: 1971020.227\n",
      "epoch: 500500, train loss: 3.8917829990386963, val loss: 3.9701011419296264, ETA in seconds: 1971006.528\n",
      "epoch: 500600, train loss: 3.890820837020874, val loss: 3.98206684589386, ETA in seconds: 1970993.556\n",
      "epoch: 500700, train loss: 3.8863977670669554, val loss: 3.964501714706421, ETA in seconds: 1970979.806\n",
      "epoch: 500800, train loss: 3.891718363761902, val loss: 3.9726633071899413, ETA in seconds: 1970971.072\n",
      "epoch: 500900, train loss: 3.89895703792572, val loss: 3.9758622884750365, ETA in seconds: 1970957.568\n",
      "epoch: 501000, train loss: 3.896092081069946, val loss: 3.9784539222717283, ETA in seconds: 1970985.972\n",
      "epoch: 501100, train loss: 3.8990015268325804, val loss: 3.9685959815979004, ETA in seconds: 1971043.849\n",
      "epoch: 501200, train loss: 3.885035276412964, val loss: 3.968269968032837, ETA in seconds: 1971101.291\n",
      "epoch: 501300, train loss: 3.891336274147034, val loss: 3.9662171363830567, ETA in seconds: 1971094.932\n",
      "epoch: 501400, train loss: 3.88945677280426, val loss: 3.9769177198410035, ETA in seconds: 1971079.207\n",
      "epoch: 501500, train loss: 3.8928008556365965, val loss: 3.965791440010071, ETA in seconds: 1971052.561\n",
      "epoch: 501600, train loss: 3.8971501350402833, val loss: 3.9710030794143676, ETA in seconds: 1971031.537\n",
      "epoch: 501700, train loss: 3.8883729934692384, val loss: 3.963025379180908, ETA in seconds: 1971029.512\n",
      "epoch: 501800, train loss: 3.8938006162643433, val loss: 3.970199966430664, ETA in seconds: 1971094.627\n",
      "epoch: 501900, train loss: 3.8915364742279053, val loss: 3.9660419464111327, ETA in seconds: 1971111.178\n",
      "epoch: 502000, train loss: 3.884965991973877, val loss: 3.9619088172912598, ETA in seconds: 1971086.963\n",
      "epoch: 502100, train loss: 3.8928646564483644, val loss: 3.9756906747817995, ETA in seconds: 1971067.467\n",
      "epoch: 502200, train loss: 3.894531273841858, val loss: 3.9641686201095583, ETA in seconds: 1971032.679\n",
      "epoch: 502300, train loss: 3.8999199628829957, val loss: 3.9784327268600466, ETA in seconds: 1970996.763\n",
      "epoch: 502400, train loss: 3.8868788719177245, val loss: 3.9727829456329347, ETA in seconds: 1970971.869\n",
      "epoch: 502500, train loss: 3.8912104606628417, val loss: 3.970737266540527, ETA in seconds: 1970953.711\n",
      "epoch: 502600, train loss: 3.8977335929870605, val loss: 3.9646926164627074, ETA in seconds: 1970934.220\n",
      "epoch: 502700, train loss: 3.879506731033325, val loss: 3.9669507503509522, ETA in seconds: 1970913.906\n",
      "epoch: 502800, train loss: 3.901992917060852, val loss: 3.9724807024002073, ETA in seconds: 1970893.268\n",
      "epoch: 502900, train loss: 3.889331555366516, val loss: 3.9699284076690673, ETA in seconds: 1970871.834\n",
      "epoch: 503000, train loss: 3.8864526033401487, val loss: 3.966184878349304, ETA in seconds: 1970838.424\n",
      "epoch: 503100, train loss: 3.8971012353897097, val loss: 3.976559782028198, ETA in seconds: 1970885.440\n",
      "epoch: 503200, train loss: 3.9048020124435423, val loss: 3.9710865020751953, ETA in seconds: 1970910.200\n",
      "epoch: 503300, train loss: 3.8874550819396974, val loss: 3.972342872619629, ETA in seconds: 1970882.379\n",
      "epoch: 503400, train loss: 3.8911261320114137, val loss: 3.9632647752761843, ETA in seconds: 1970875.816\n",
      "epoch: 503500, train loss: 3.8934009075164795, val loss: 3.969918990135193, ETA in seconds: 1970858.623\n",
      "epoch: 503600, train loss: 3.8920600414276123, val loss: 3.9814449548721313, ETA in seconds: 1970839.031\n",
      "epoch: 503700, train loss: 3.895364832878113, val loss: 3.9704494953155516, ETA in seconds: 1970835.274\n",
      "epoch: 503800, train loss: 3.8955936193466187, val loss: 3.9659568071365356, ETA in seconds: 1970830.652\n",
      "epoch: 503900, train loss: 3.8838118076324464, val loss: 3.9667524099349976, ETA in seconds: 1970797.343\n",
      "epoch: 504000, train loss: 3.8919424057006835, val loss: 3.9631006479263307, ETA in seconds: 1970772.983\n",
      "epoch: 504100, train loss: 3.8961660146713255, val loss: 3.9643723011016845, ETA in seconds: 1970745.091\n",
      "epoch: 504200, train loss: 3.9038037061691284, val loss: 3.9733696937561036, ETA in seconds: 1970720.618\n",
      "epoch: 504300, train loss: 3.900471878051758, val loss: 3.9692517280578614, ETA in seconds: 1970695.063\n",
      "epoch: 504400, train loss: 3.9039947986602783, val loss: 3.9727306365966797, ETA in seconds: 1970711.436\n",
      "epoch: 504500, train loss: 3.8836715459823608, val loss: 3.971336770057678, ETA in seconds: 1970673.412\n",
      "epoch: 504600, train loss: 3.896498990058899, val loss: 3.9769122123718263, ETA in seconds: 1970637.777\n",
      "epoch: 504700, train loss: 3.8950762033462523, val loss: 3.9850656509399416, ETA in seconds: 1970640.020\n",
      "epoch: 504800, train loss: 3.887030005455017, val loss: 3.965619611740112, ETA in seconds: 1970618.324\n",
      "epoch: 504900, train loss: 3.8945703744888305, val loss: 3.968866157531738, ETA in seconds: 1970604.069\n",
      "epoch: 505000, train loss: 3.8843347072601317, val loss: 3.9691911935806274, ETA in seconds: 1970593.293\n",
      "epoch: 505100, train loss: 3.8975691556930543, val loss: 3.9730767011642456, ETA in seconds: 1970648.860\n",
      "epoch: 505200, train loss: 3.895385193824768, val loss: 3.979990005493164, ETA in seconds: 1970699.268\n",
      "epoch: 505300, train loss: 3.8948235511779785, val loss: 3.974491834640503, ETA in seconds: 1970721.910\n",
      "epoch: 505400, train loss: 3.8985134840011595, val loss: 3.9788118600845337, ETA in seconds: 1970698.135\n",
      "epoch: 505500, train loss: 3.9022361040115356, val loss: 3.985592770576477, ETA in seconds: 1970658.437\n",
      "epoch: 505600, train loss: 3.894928550720215, val loss: 3.968233823776245, ETA in seconds: 1970646.240\n",
      "epoch: 505700, train loss: 3.8968597650527954, val loss: 3.984050822257996, ETA in seconds: 1970622.251\n",
      "epoch: 505800, train loss: 3.897729921340942, val loss: 3.9835103511810304, ETA in seconds: 1970601.697\n",
      "epoch: 505900, train loss: 3.894055438041687, val loss: 3.9712356090545655, ETA in seconds: 1970589.762\n",
      "epoch: 506000, train loss: 3.8945080757141115, val loss: 3.972754406929016, ETA in seconds: 1970568.436\n",
      "epoch: 506100, train loss: 3.8963358640670775, val loss: 3.97206072807312, ETA in seconds: 1970539.248\n",
      "epoch: 506200, train loss: 3.8952136993408204, val loss: 3.9677565813064577, ETA in seconds: 1970534.463\n",
      "epoch: 506300, train loss: 3.893370842933655, val loss: 3.979137921333313, ETA in seconds: 1970597.759\n",
      "epoch: 506400, train loss: 3.894643187522888, val loss: 3.978988695144653, ETA in seconds: 1970602.635\n",
      "epoch: 506500, train loss: 3.889125180244446, val loss: 3.9783624172210694, ETA in seconds: 1970599.138\n",
      "epoch: 506600, train loss: 3.890006399154663, val loss: 3.965152049064636, ETA in seconds: 1970589.556\n",
      "epoch: 506700, train loss: 3.892330288887024, val loss: 3.9765259981155396, ETA in seconds: 1970574.876\n",
      "epoch: 506800, train loss: 3.9018259286880492, val loss: 3.965030813217163, ETA in seconds: 1970587.694\n",
      "epoch: 506900, train loss: 3.8918120861053467, val loss: 3.968164801597595, ETA in seconds: 1970629.863\n",
      "epoch: 507000, train loss: 3.8838356018066404, val loss: 3.979978108406067, ETA in seconds: 1970671.909\n",
      "epoch: 507100, train loss: 3.8924359560012816, val loss: 3.9824026584625245, ETA in seconds: 1970718.381\n",
      "epoch: 507200, train loss: 3.884195017814636, val loss: 3.9697113275527953, ETA in seconds: 1970761.521\n",
      "epoch: 507300, train loss: 3.886768651008606, val loss: 3.9794922828674317, ETA in seconds: 1970803.158\n",
      "epoch: 507400, train loss: 3.897570514678955, val loss: 3.969170022010803, ETA in seconds: 1970845.644\n",
      "epoch: 507500, train loss: 3.8860279321670532, val loss: 3.9812507390975953, ETA in seconds: 1970886.320\n",
      "epoch: 507600, train loss: 3.8884785413742065, val loss: 3.973764944076538, ETA in seconds: 1970924.306\n",
      "epoch: 507700, train loss: 3.886956286430359, val loss: 3.972557306289673, ETA in seconds: 1971000.321\n",
      "epoch: 507800, train loss: 3.8954843044281007, val loss: 3.970308709144592, ETA in seconds: 1971067.647\n",
      "epoch: 507900, train loss: 3.9004766225814818, val loss: 3.966871953010559, ETA in seconds: 1971108.761\n",
      "epoch: 508000, train loss: 3.8920277833938597, val loss: 3.9673654794692994, ETA in seconds: 1971082.157\n",
      "epoch: 508100, train loss: 3.886521100997925, val loss: 3.968071389198303, ETA in seconds: 1971064.460\n",
      "epoch: 508200, train loss: 3.8877256393432615, val loss: 3.964088773727417, ETA in seconds: 1971030.535\n",
      "epoch: 508300, train loss: 3.8876562118530273, val loss: 3.968982625007629, ETA in seconds: 1971008.506\n",
      "epoch: 508400, train loss: 3.8876675367355347, val loss: 3.96440269947052, ETA in seconds: 1970982.401\n",
      "epoch: 508500, train loss: 3.892495346069336, val loss: 3.970678472518921, ETA in seconds: 1970959.835\n",
      "epoch: 508600, train loss: 3.9006877660751345, val loss: 3.9670079946517944, ETA in seconds: 1970958.637\n",
      "epoch: 508700, train loss: 3.8876306295394896, val loss: 3.970349431037903, ETA in seconds: 1970947.476\n",
      "epoch: 508800, train loss: 3.896734833717346, val loss: 3.965642285346985, ETA in seconds: 1970931.836\n",
      "epoch: 508900, train loss: 3.8917723417282106, val loss: 3.966933488845825, ETA in seconds: 1970904.637\n",
      "epoch: 509000, train loss: 3.901589846611023, val loss: 3.9549498319625855, ETA in seconds: 1970895.171\n",
      "epoch: 509100, train loss: 3.900788736343384, val loss: 3.9820623874664305, ETA in seconds: 1970881.305\n",
      "epoch: 509200, train loss: 3.8937309980392456, val loss: 3.9715437650680543, ETA in seconds: 1970863.133\n",
      "epoch: 509300, train loss: 3.8891294479370115, val loss: 3.9548351049423216, ETA in seconds: 1970855.884\n",
      "epoch: 509400, train loss: 3.886112833023071, val loss: 3.9711346864700316, ETA in seconds: 1970840.479\n",
      "epoch: 509500, train loss: 3.8881662368774412, val loss: 3.972846841812134, ETA in seconds: 1970836.041\n",
      "epoch: 509600, train loss: 3.8929262161254883, val loss: 3.9781649589538572, ETA in seconds: 1970869.838\n",
      "epoch: 509700, train loss: 3.8924134492874147, val loss: 3.9743141174316405, ETA in seconds: 1970908.143\n",
      "epoch: 509800, train loss: 3.896165895462036, val loss: 3.970505142211914, ETA in seconds: 1970943.071\n",
      "epoch: 509900, train loss: 3.8879313230514527, val loss: 3.979529595375061, ETA in seconds: 1970928.141\n",
      "epoch: 510000, train loss: 3.8993865728378294, val loss: 3.966653084754944, ETA in seconds: 1970901.796\n",
      "epoch: 510100, train loss: 3.892879915237427, val loss: 3.971389985084534, ETA in seconds: 1970875.994\n",
      "epoch: 510200, train loss: 3.882039785385132, val loss: 3.970013070106506, ETA in seconds: 1970842.668\n",
      "epoch: 510300, train loss: 3.9007678031921387, val loss: 3.9600757122039796, ETA in seconds: 1970819.336\n",
      "epoch: 510400, train loss: 3.889446568489075, val loss: 3.9696168899536133, ETA in seconds: 1970793.714\n",
      "epoch: 510500, train loss: 3.890653872489929, val loss: 3.9773104190826416, ETA in seconds: 1970759.956\n",
      "epoch: 510600, train loss: 3.8932681322097777, val loss: 3.9752588748931883, ETA in seconds: 1970730.080\n",
      "epoch: 510700, train loss: 3.884091353416443, val loss: 3.9694056510925293, ETA in seconds: 1970731.236\n",
      "epoch: 510800, train loss: 3.8940813064575197, val loss: 3.9697667598724364, ETA in seconds: 1970789.888\n",
      "epoch: 510900, train loss: 3.8976055145263673, val loss: 3.9696046352386474, ETA in seconds: 1970769.693\n",
      "epoch: 511000, train loss: 3.89100341796875, val loss: 3.968578243255615, ETA in seconds: 1970766.055\n",
      "epoch: 511100, train loss: 3.8957229375839235, val loss: 3.9725148677825928, ETA in seconds: 1970806.759\n",
      "epoch: 511200, train loss: 3.898135209083557, val loss: 3.975155830383301, ETA in seconds: 1970776.672\n",
      "epoch: 511300, train loss: 3.896274471282959, val loss: 3.9679556369781492, ETA in seconds: 1970742.844\n",
      "epoch: 511400, train loss: 3.8896788597106933, val loss: 3.9691386938095095, ETA in seconds: 1970736.292\n",
      "epoch: 511500, train loss: 3.8960112571716308, val loss: 3.9604739904403687, ETA in seconds: 1970722.504\n",
      "epoch: 511600, train loss: 3.892937421798706, val loss: 3.967199969291687, ETA in seconds: 1970703.844\n",
      "epoch: 511700, train loss: 3.8872257471084595, val loss: 3.9732735395431518, ETA in seconds: 1970697.544\n",
      "epoch: 511800, train loss: 3.894897222518921, val loss: 3.9639108180999756, ETA in seconds: 1970678.092\n",
      "epoch: 511900, train loss: 3.8917171478271486, val loss: 3.9733086109161375, ETA in seconds: 1970668.957\n",
      "epoch: 512000, train loss: 3.897300696372986, val loss: 3.9849793195724486, ETA in seconds: 1970652.130\n",
      "epoch: 512100, train loss: 3.8911998510360717, val loss: 3.974603700637817, ETA in seconds: 1970622.306\n",
      "epoch: 512200, train loss: 3.898358130455017, val loss: 3.9721790313720704, ETA in seconds: 1970577.181\n",
      "epoch: 512300, train loss: 3.8920940160751343, val loss: 3.9671293973922728, ETA in seconds: 1970573.452\n",
      "epoch: 512400, train loss: 3.89215829372406, val loss: 3.967153239250183, ETA in seconds: 1970548.532\n",
      "epoch: 512500, train loss: 3.892106771469116, val loss: 3.9735527515411375, ETA in seconds: 1970513.928\n",
      "epoch: 512600, train loss: 3.8979278087615965, val loss: 3.9770097017288206, ETA in seconds: 1970485.994\n",
      "epoch: 512700, train loss: 3.9022682428359987, val loss: 3.967031788825989, ETA in seconds: 1970454.168\n",
      "epoch: 512800, train loss: 3.893503284454346, val loss: 3.9666092157363892, ETA in seconds: 1970447.142\n",
      "epoch: 512900, train loss: 3.8951108932495115, val loss: 3.96875479221344, ETA in seconds: 1970420.435\n",
      "epoch: 513000, train loss: 3.8843286752700807, val loss: 3.974189281463623, ETA in seconds: 1970388.150\n",
      "epoch: 513100, train loss: 3.886453366279602, val loss: 3.968481922149658, ETA in seconds: 1970380.770\n",
      "epoch: 513200, train loss: 3.897887372970581, val loss: 3.960354208946228, ETA in seconds: 1970352.423\n",
      "epoch: 513300, train loss: 3.8851248979568482, val loss: 3.9711665391921995, ETA in seconds: 1970331.953\n",
      "epoch: 513400, train loss: 3.887484312057495, val loss: 3.9653889417648314, ETA in seconds: 1970310.883\n",
      "epoch: 513500, train loss: 3.8905242681503296, val loss: 3.9831507444381713, ETA in seconds: 1970273.475\n",
      "epoch: 513600, train loss: 3.892769193649292, val loss: 3.973779821395874, ETA in seconds: 1970233.534\n",
      "epoch: 513700, train loss: 3.9032275676727295, val loss: 3.9739232063293457, ETA in seconds: 1970199.370\n",
      "epoch: 513800, train loss: 3.8951603889465334, val loss: 3.971052360534668, ETA in seconds: 1970155.549\n",
      "epoch: 513900, train loss: 3.8907402038574217, val loss: 3.970788860321045, ETA in seconds: 1970117.139\n",
      "epoch: 514000, train loss: 3.8955463409423827, val loss: 3.9664306640625, ETA in seconds: 1970078.199\n",
      "epoch: 514100, train loss: 3.8921147346496583, val loss: 3.974212718009949, ETA in seconds: 1970032.535\n",
      "epoch: 514200, train loss: 3.8985222578048706, val loss: 3.966680884361267, ETA in seconds: 1969997.567\n",
      "epoch: 514300, train loss: 3.8960127353668215, val loss: 3.9705281019210816, ETA in seconds: 1969977.764\n",
      "epoch: 514400, train loss: 3.896742343902588, val loss: 3.966572403907776, ETA in seconds: 1969971.432\n",
      "epoch: 514500, train loss: 3.9001938104629517, val loss: 3.970701789855957, ETA in seconds: 1969934.748\n",
      "epoch: 514600, train loss: 3.8984779357910155, val loss: 3.980591583251953, ETA in seconds: 1969911.034\n",
      "epoch: 514700, train loss: 3.888643980026245, val loss: 3.969726586341858, ETA in seconds: 1969873.957\n",
      "epoch: 514800, train loss: 3.8923049926757813, val loss: 3.9644724130630493, ETA in seconds: 1969835.353\n",
      "epoch: 514900, train loss: 3.8857099771499635, val loss: 3.96863968372345, ETA in seconds: 1969802.142\n",
      "epoch: 515000, train loss: 3.8945762634277346, val loss: 3.9674444675445555, ETA in seconds: 1969765.793\n",
      "epoch: 515100, train loss: 3.8822423934936525, val loss: 3.975875663757324, ETA in seconds: 1969736.737\n",
      "epoch: 515200, train loss: 3.8927234649658202, val loss: 3.9779012203216553, ETA in seconds: 1969706.076\n",
      "epoch: 515300, train loss: 3.8974881887435915, val loss: 3.9752923250198364, ETA in seconds: 1969664.633\n",
      "epoch: 515400, train loss: 3.894933557510376, val loss: 3.9654008865356447, ETA in seconds: 1969621.779\n",
      "epoch: 515500, train loss: 3.894903564453125, val loss: 3.9675392627716066, ETA in seconds: 1969649.599\n",
      "epoch: 515600, train loss: 3.8909533500671385, val loss: 3.966080570220947, ETA in seconds: 1969718.839\n",
      "epoch: 515700, train loss: 3.8964599847793577, val loss: 3.9755905866622925, ETA in seconds: 1969792.412\n",
      "epoch: 515800, train loss: 3.8883137702941895, val loss: 3.9780353307724, ETA in seconds: 1969860.169\n",
      "epoch: 515900, train loss: 3.893951106071472, val loss: 3.9633248090744018, ETA in seconds: 1969835.294\n",
      "epoch: 516000, train loss: 3.896600103378296, val loss: 3.974946618080139, ETA in seconds: 1969842.647\n",
      "epoch: 516100, train loss: 3.8957619667053223, val loss: 3.9655766248703004, ETA in seconds: 1969848.510\n",
      "epoch: 516200, train loss: 3.8975702047348024, val loss: 3.966139817237854, ETA in seconds: 1969833.213\n",
      "epoch: 516300, train loss: 3.886582112312317, val loss: 3.973869466781616, ETA in seconds: 1969808.643\n",
      "epoch: 516400, train loss: 3.886933970451355, val loss: 3.969925332069397, ETA in seconds: 1969834.511\n",
      "epoch: 516500, train loss: 3.898586559295654, val loss: 3.980205225944519, ETA in seconds: 1969802.135\n",
      "epoch: 516600, train loss: 3.889496326446533, val loss: 3.9804849147796633, ETA in seconds: 1969788.661\n",
      "epoch: 516700, train loss: 3.8941373109817503, val loss: 3.9773088932037353, ETA in seconds: 1969782.561\n",
      "epoch: 516800, train loss: 3.8918628692626953, val loss: 3.9731921672821047, ETA in seconds: 1969814.589\n",
      "epoch: 516900, train loss: 3.8956210613250732, val loss: 3.9720245599746704, ETA in seconds: 1969793.733\n",
      "epoch: 517000, train loss: 3.8973310708999636, val loss: 3.972154378890991, ETA in seconds: 1969739.159\n",
      "epoch: 517100, train loss: 3.892121124267578, val loss: 3.96377375125885, ETA in seconds: 1969675.949\n",
      "epoch: 517200, train loss: 3.892985391616821, val loss: 3.970514416694641, ETA in seconds: 1969619.741\n",
      "epoch: 517300, train loss: 3.895075225830078, val loss: 3.978934097290039, ETA in seconds: 1969603.417\n",
      "epoch: 517400, train loss: 3.8856437683105467, val loss: 3.968330192565918, ETA in seconds: 1969632.613\n",
      "epoch: 517500, train loss: 3.890701150894165, val loss: 3.966989278793335, ETA in seconds: 1969665.831\n",
      "epoch: 517600, train loss: 3.8915873527526856, val loss: 3.98152072429657, ETA in seconds: 1969665.383\n",
      "epoch: 517700, train loss: 3.895013451576233, val loss: 3.972975969314575, ETA in seconds: 1969671.595\n",
      "epoch: 517800, train loss: 3.8987175941467287, val loss: 3.9647861242294313, ETA in seconds: 1969627.354\n",
      "epoch: 517900, train loss: 3.8974750995635987, val loss: 3.9780956745147704, ETA in seconds: 1969597.512\n",
      "epoch: 518000, train loss: 3.8921621322631834, val loss: 3.971673679351807, ETA in seconds: 1969555.584\n",
      "epoch: 518100, train loss: 3.8957302570343018, val loss: 3.970898675918579, ETA in seconds: 1969512.639\n",
      "epoch: 518200, train loss: 3.890955924987793, val loss: 3.9693339109420775, ETA in seconds: 1969473.288\n",
      "epoch: 518300, train loss: 3.896298718452454, val loss: 3.9735599517822267, ETA in seconds: 1969446.337\n",
      "epoch: 518400, train loss: 3.8925410985946653, val loss: 3.9792978763580322, ETA in seconds: 1969437.071\n",
      "epoch: 518500, train loss: 3.8942178964614866, val loss: 3.9797447681427003, ETA in seconds: 1969471.189\n",
      "epoch: 518600, train loss: 3.895694708824158, val loss: 3.967780900001526, ETA in seconds: 1969501.958\n",
      "epoch: 518700, train loss: 3.89043710231781, val loss: 3.966774821281433, ETA in seconds: 1969529.442\n",
      "epoch: 518800, train loss: 3.8871667861938475, val loss: 3.9689833879470826, ETA in seconds: 1969560.026\n",
      "epoch: 518900, train loss: 3.8887598752975463, val loss: 3.9768912553787232, ETA in seconds: 1969590.779\n",
      "epoch: 519000, train loss: 3.8951365470886232, val loss: 3.9713507175445555, ETA in seconds: 1969560.248\n",
      "epoch: 519100, train loss: 3.897694706916809, val loss: 3.9587792634963987, ETA in seconds: 1969504.945\n",
      "epoch: 519200, train loss: 3.892122673988342, val loss: 3.9836395502090456, ETA in seconds: 1969466.321\n",
      "epoch: 519300, train loss: 3.8971473932266236, val loss: 3.963299608230591, ETA in seconds: 1969427.054\n",
      "epoch: 519400, train loss: 3.884776067733765, val loss: 3.9664075136184693, ETA in seconds: 1969381.690\n",
      "epoch: 519500, train loss: 3.881751775741577, val loss: 3.961235022544861, ETA in seconds: 1969341.416\n",
      "epoch: 519600, train loss: 3.8840182542800905, val loss: 3.960892391204834, ETA in seconds: 1969308.027\n",
      "epoch: 519700, train loss: 3.8925060033798218, val loss: 3.974611210823059, ETA in seconds: 1969295.293\n",
      "epoch: 519800, train loss: 3.883850860595703, val loss: 3.968186283111572, ETA in seconds: 1969256.954\n",
      "epoch: 519900, train loss: 3.8909782648086546, val loss: 3.9615199089050295, ETA in seconds: 1969203.867\n",
      "epoch: 520000, train loss: 3.9007278442382813, val loss: 3.9751585960388183, ETA in seconds: 1969158.142\n",
      "epoch: 520100, train loss: 3.8859954118728637, val loss: 3.965856647491455, ETA in seconds: 1969125.295\n",
      "epoch: 520200, train loss: 3.892706370353699, val loss: 3.9659329652786255, ETA in seconds: 1969082.541\n",
      "epoch: 520300, train loss: 3.8989667654037476, val loss: 3.9673392295837404, ETA in seconds: 1969060.933\n",
      "epoch: 520400, train loss: 3.8991937160491945, val loss: 3.972502017021179, ETA in seconds: 1969081.222\n",
      "epoch: 520500, train loss: 3.886178183555603, val loss: 3.973231482505798, ETA in seconds: 1969081.848\n",
      "epoch: 520600, train loss: 3.886912131309509, val loss: 3.9732404708862306, ETA in seconds: 1969043.073\n",
      "epoch: 520700, train loss: 3.896029996871948, val loss: 3.9660847663879393, ETA in seconds: 1968986.810\n",
      "epoch: 520800, train loss: 3.8849971532821654, val loss: 3.9791731357574465, ETA in seconds: 1968929.828\n",
      "epoch: 520900, train loss: 3.895869255065918, val loss: 3.9690070867538454, ETA in seconds: 1968874.984\n",
      "epoch: 521000, train loss: 3.892495131492615, val loss: 3.9701138496398927, ETA in seconds: 1968829.828\n",
      "epoch: 521100, train loss: 3.88853976726532, val loss: 3.982311153411865, ETA in seconds: 1968832.408\n",
      "epoch: 521200, train loss: 3.8879043102264403, val loss: 3.9719283819198608, ETA in seconds: 1968778.838\n",
      "epoch: 521300, train loss: 3.893394112586975, val loss: 3.9680458068847657, ETA in seconds: 1968725.583\n",
      "epoch: 521400, train loss: 3.8959224224090576, val loss: 3.965501236915588, ETA in seconds: 1968666.575\n",
      "epoch: 521500, train loss: 3.8938549995422362, val loss: 3.9698020696640013, ETA in seconds: 1968609.521\n",
      "epoch: 521600, train loss: 3.8938273906707765, val loss: 3.9708289384841917, ETA in seconds: 1968565.502\n",
      "epoch: 521700, train loss: 3.8940775632858275, val loss: 3.970215082168579, ETA in seconds: 1968520.445\n",
      "epoch: 521800, train loss: 3.892000102996826, val loss: 3.9762927770614622, ETA in seconds: 1968469.941\n",
      "epoch: 521900, train loss: 3.892800211906433, val loss: 3.97284471988678, ETA in seconds: 1968411.970\n",
      "epoch: 522000, train loss: 3.885231947898865, val loss: 3.9652268409729006, ETA in seconds: 1968352.758\n",
      "epoch: 522100, train loss: 3.894325017929077, val loss: 3.9689029932022093, ETA in seconds: 1968305.729\n",
      "epoch: 522200, train loss: 3.8900748014450075, val loss: 3.9679550170898437, ETA in seconds: 1968238.854\n",
      "epoch: 522300, train loss: 3.8908806324005125, val loss: 3.974307084083557, ETA in seconds: 1968197.608\n",
      "epoch: 522400, train loss: 3.8935838460922243, val loss: 3.9780059576034548, ETA in seconds: 1968136.672\n",
      "epoch: 522500, train loss: 3.903466725349426, val loss: 3.9733733177185058, ETA in seconds: 1968093.745\n",
      "epoch: 522600, train loss: 3.890783500671387, val loss: 3.9699920654296874, ETA in seconds: 1968026.841\n",
      "epoch: 522700, train loss: 3.8925426244735717, val loss: 3.979884958267212, ETA in seconds: 1967964.792\n",
      "epoch: 522800, train loss: 3.8999072074890138, val loss: 3.9621723890304565, ETA in seconds: 1967899.927\n",
      "epoch: 522900, train loss: 3.895216226577759, val loss: 3.9659101963043213, ETA in seconds: 1967832.056\n",
      "epoch: 523000, train loss: 3.901241397857666, val loss: 3.9684240341186525, ETA in seconds: 1967767.761\n",
      "epoch: 523100, train loss: 3.896123933792114, val loss: 3.9686456441879274, ETA in seconds: 1967710.363\n",
      "epoch: 523200, train loss: 3.8921047687530517, val loss: 3.970110058784485, ETA in seconds: 1967659.744\n",
      "epoch: 523300, train loss: 3.8918381214141844, val loss: 3.9626520395278932, ETA in seconds: 1967617.200\n",
      "epoch: 523400, train loss: 3.899292778968811, val loss: 3.9665864229202272, ETA in seconds: 1967548.836\n",
      "epoch: 523500, train loss: 3.892522621154785, val loss: 3.9615622997283935, ETA in seconds: 1967479.284\n",
      "epoch: 523600, train loss: 3.8913596868515015, val loss: 3.979008746147156, ETA in seconds: 1967423.780\n",
      "epoch: 523700, train loss: 3.8944445610046388, val loss: 3.9649344205856325, ETA in seconds: 1967370.495\n",
      "epoch: 523800, train loss: 3.8920181035995483, val loss: 3.9751318454742433, ETA in seconds: 1967323.654\n",
      "epoch: 523900, train loss: 3.8992388248443604, val loss: 3.97273428440094, ETA in seconds: 1967259.259\n",
      "epoch: 524000, train loss: 3.8953105449676513, val loss: 3.976579999923706, ETA in seconds: 1967196.631\n",
      "epoch: 524100, train loss: 3.892012929916382, val loss: 3.9782681465148926, ETA in seconds: 1967131.392\n",
      "epoch: 524200, train loss: 3.8929485082626343, val loss: 3.9643576622009276, ETA in seconds: 1967054.713\n",
      "epoch: 524300, train loss: 3.8962194442749025, val loss: 3.974416947364807, ETA in seconds: 1966978.513\n",
      "epoch: 524400, train loss: 3.896119213104248, val loss: 3.9804388523101806, ETA in seconds: 1966906.482\n",
      "epoch: 524500, train loss: 3.890298581123352, val loss: 3.965710997581482, ETA in seconds: 1966836.237\n",
      "epoch: 524600, train loss: 3.895620918273926, val loss: 3.9750044345855713, ETA in seconds: 1966783.489\n",
      "epoch: 524700, train loss: 3.896314859390259, val loss: 3.97638885974884, ETA in seconds: 1966724.471\n",
      "epoch: 524800, train loss: 3.885130286216736, val loss: 3.9790276527404784, ETA in seconds: 1966660.843\n",
      "epoch: 524900, train loss: 3.900646424293518, val loss: 3.972198557853699, ETA in seconds: 1966598.030\n",
      "epoch: 525000, train loss: 3.8958885192871096, val loss: 3.978240966796875, ETA in seconds: 1966533.334\n",
      "epoch: 525100, train loss: 3.891903877258301, val loss: 3.9718877553939818, ETA in seconds: 1966469.880\n",
      "epoch: 525200, train loss: 3.891375780105591, val loss: 3.9761945962905885, ETA in seconds: 1966405.472\n",
      "epoch: 525300, train loss: 3.8952574253082277, val loss: 3.968451714515686, ETA in seconds: 1966339.184\n",
      "epoch: 525400, train loss: 3.8890276193618774, val loss: 3.973696208000183, ETA in seconds: 1966331.174\n",
      "epoch: 525500, train loss: 3.899216866493225, val loss: 3.9651462554931642, ETA in seconds: 1966262.202\n",
      "epoch: 525600, train loss: 3.8927971601486204, val loss: 3.9808691263198854, ETA in seconds: 1966187.566\n",
      "epoch: 525700, train loss: 3.889138436317444, val loss: 3.970485234260559, ETA in seconds: 1966114.643\n",
      "epoch: 525800, train loss: 3.8966055870056153, val loss: 3.9642581939697266, ETA in seconds: 1966041.290\n",
      "epoch: 525900, train loss: 3.892379713058472, val loss: 3.9735973834991456, ETA in seconds: 1965975.405\n",
      "epoch: 526000, train loss: 3.891231060028076, val loss: 3.9712762355804445, ETA in seconds: 1965915.888\n",
      "epoch: 526100, train loss: 3.893415641784668, val loss: 3.9613272666931154, ETA in seconds: 1965908.980\n",
      "epoch: 526200, train loss: 3.886916494369507, val loss: 3.97669894695282, ETA in seconds: 1965850.592\n",
      "epoch: 526300, train loss: 3.888334941864014, val loss: 3.9743492364883424, ETA in seconds: 1965784.507\n",
      "epoch: 526400, train loss: 3.8979246854782104, val loss: 3.958505153656006, ETA in seconds: 1965717.941\n",
      "epoch: 526500, train loss: 3.8923819065093994, val loss: 3.967358374595642, ETA in seconds: 1965652.812\n",
      "epoch: 526600, train loss: 3.8969038009643553, val loss: 3.9702395677566527, ETA in seconds: 1965592.476\n",
      "epoch: 526700, train loss: 3.902942728996277, val loss: 3.9662049770355225, ETA in seconds: 1965588.155\n",
      "epoch: 526800, train loss: 3.8963616132736205, val loss: 3.9715981006622316, ETA in seconds: 1965523.171\n",
      "epoch: 526900, train loss: 3.898905372619629, val loss: 3.96746826171875, ETA in seconds: 1965489.152\n",
      "epoch: 527000, train loss: 3.8895280838012694, val loss: 3.9668174743652345, ETA in seconds: 1965480.782\n",
      "epoch: 527100, train loss: 3.8900400400161743, val loss: 3.9648133754730224, ETA in seconds: 1965424.528\n",
      "epoch: 527200, train loss: 3.8930614471435545, val loss: 3.973256468772888, ETA in seconds: 1965380.374\n",
      "epoch: 527300, train loss: 3.89254412651062, val loss: 3.975656247138977, ETA in seconds: 1965307.080\n",
      "epoch: 527400, train loss: 3.8920556783676146, val loss: 3.9733965158462525, ETA in seconds: 1965224.478\n",
      "epoch: 527500, train loss: 3.899203395843506, val loss: 3.9734108209609986, ETA in seconds: 1965207.001\n",
      "epoch: 527600, train loss: 3.8901947259902956, val loss: 3.9645218372344972, ETA in seconds: 1965195.355\n",
      "epoch: 527700, train loss: 3.9016961097717284, val loss: 3.9661110639572144, ETA in seconds: 1965186.671\n",
      "epoch: 527800, train loss: 3.9026970624923707, val loss: 3.976744604110718, ETA in seconds: 1965125.528\n",
      "epoch: 527900, train loss: 3.892204189300537, val loss: 3.972308945655823, ETA in seconds: 1965058.343\n",
      "epoch: 528000, train loss: 3.8858264207839968, val loss: 3.971030855178833, ETA in seconds: 1964991.895\n",
      "epoch: 528100, train loss: 3.8922053813934325, val loss: 3.9807330131530763, ETA in seconds: 1964921.964\n",
      "epoch: 528200, train loss: 3.8875470876693727, val loss: 3.9724385738372803, ETA in seconds: 1964866.881\n",
      "epoch: 528300, train loss: 3.880675029754639, val loss: 3.97197368144989, ETA in seconds: 1964824.129\n",
      "epoch: 528400, train loss: 3.889496088027954, val loss: 3.9697893857955933, ETA in seconds: 1964782.400\n",
      "epoch: 528500, train loss: 3.8864514350891115, val loss: 3.9622374773025513, ETA in seconds: 1964738.403\n",
      "epoch: 528600, train loss: 3.8994086265563963, val loss: 3.9680739879608153, ETA in seconds: 1964669.062\n",
      "epoch: 528700, train loss: 3.8964308738708495, val loss: 3.96679310798645, ETA in seconds: 1964618.250\n",
      "epoch: 528800, train loss: 3.8881215333938597, val loss: 3.98235342502594, ETA in seconds: 1964569.111\n",
      "epoch: 528900, train loss: 3.889474105834961, val loss: 3.9732118606567384, ETA in seconds: 1964562.239\n",
      "epoch: 529000, train loss: 3.8957956314086912, val loss: 3.9783021926879885, ETA in seconds: 1964493.511\n",
      "epoch: 529100, train loss: 3.901930332183838, val loss: 3.97706081867218, ETA in seconds: 1964428.914\n",
      "epoch: 529200, train loss: 3.886555480957031, val loss: 3.9654826164245605, ETA in seconds: 1964359.091\n",
      "epoch: 529300, train loss: 3.899585247039795, val loss: 3.9821359157562255, ETA in seconds: 1964288.039\n",
      "epoch: 529400, train loss: 3.894863176345825, val loss: 3.9713015079498293, ETA in seconds: 1964222.633\n",
      "epoch: 529500, train loss: 3.893067979812622, val loss: 3.9660247564315796, ETA in seconds: 1964141.916\n",
      "epoch: 529600, train loss: 3.895272445678711, val loss: 3.972839379310608, ETA in seconds: 1964077.537\n",
      "epoch: 529700, train loss: 3.8889214277267454, val loss: 3.973505449295044, ETA in seconds: 1964029.470\n",
      "epoch: 529800, train loss: 3.8946149587631225, val loss: 3.963813853263855, ETA in seconds: 1963998.102\n",
      "epoch: 529900, train loss: 3.8917835474014284, val loss: 3.9814414501190187, ETA in seconds: 1964019.599\n",
      "epoch: 530000, train loss: 3.884330940246582, val loss: 3.9803653001785277, ETA in seconds: 1964023.650\n",
      "epoch: 530100, train loss: 3.895438289642334, val loss: 3.967521595954895, ETA in seconds: 1964023.710\n",
      "epoch: 530200, train loss: 3.9025718212127685, val loss: 3.9581814050674438, ETA in seconds: 1964033.467\n",
      "epoch: 530300, train loss: 3.8947269439697267, val loss: 3.972766470909119, ETA in seconds: 1964036.803\n",
      "epoch: 530400, train loss: 3.893077826499939, val loss: 3.9705182313919067, ETA in seconds: 1964036.735\n",
      "epoch: 530500, train loss: 3.891634964942932, val loss: 3.971532201766968, ETA in seconds: 1963986.472\n",
      "epoch: 530600, train loss: 3.891854190826416, val loss: 3.967530059814453, ETA in seconds: 1963926.968\n",
      "epoch: 530700, train loss: 3.8916541814804075, val loss: 3.9853237867355347, ETA in seconds: 1963885.630\n",
      "epoch: 530800, train loss: 3.8854618787765505, val loss: 3.9798296689987183, ETA in seconds: 1963836.740\n",
      "epoch: 530900, train loss: 3.8973500967025756, val loss: 3.963469886779785, ETA in seconds: 1963774.903\n",
      "epoch: 531000, train loss: 3.898113226890564, val loss: 3.9672608613967895, ETA in seconds: 1963713.000\n",
      "epoch: 531100, train loss: 3.8964659929275514, val loss: 3.966580700874329, ETA in seconds: 1963654.439\n",
      "epoch: 531200, train loss: 3.896689033508301, val loss: 3.9711541652679445, ETA in seconds: 1963595.598\n",
      "epoch: 531300, train loss: 3.8936678886413576, val loss: 3.9791580200195313, ETA in seconds: 1963519.803\n",
      "epoch: 531400, train loss: 3.8955471992492674, val loss: 3.975391912460327, ETA in seconds: 1963452.109\n",
      "epoch: 531500, train loss: 3.898148512840271, val loss: 3.9716883182525633, ETA in seconds: 1963388.806\n",
      "epoch: 531600, train loss: 3.8911969900131225, val loss: 3.9773174285888673, ETA in seconds: 1963327.135\n",
      "epoch: 531700, train loss: 3.8978169441223143, val loss: 3.9691383123397825, ETA in seconds: 1963278.408\n",
      "epoch: 531800, train loss: 3.897679305076599, val loss: 3.9572471141815186, ETA in seconds: 1963275.643\n",
      "epoch: 531900, train loss: 3.889638137817383, val loss: 3.9768990993499758, ETA in seconds: 1963272.033\n",
      "epoch: 532000, train loss: 3.8941314935684206, val loss: 3.9824695587158203, ETA in seconds: 1963269.579\n",
      "epoch: 532100, train loss: 3.8897096872329713, val loss: 3.971990942955017, ETA in seconds: 1963265.859\n",
      "epoch: 532200, train loss: 3.893796610832214, val loss: 3.965265417098999, ETA in seconds: 1963251.167\n",
      "epoch: 532300, train loss: 3.896765351295471, val loss: 3.9680594205856323, ETA in seconds: 1963198.227\n",
      "epoch: 532400, train loss: 3.8997589349746704, val loss: 3.9637793779373167, ETA in seconds: 1963124.266\n",
      "epoch: 532500, train loss: 3.8972007751464846, val loss: 3.9749223947525025, ETA in seconds: 1963049.528\n",
      "epoch: 532600, train loss: 3.884349226951599, val loss: 3.9741670370101927, ETA in seconds: 1962974.397\n",
      "epoch: 532700, train loss: 3.9015560865402223, val loss: 3.9640273809432984, ETA in seconds: 1962892.354\n",
      "epoch: 532800, train loss: 3.8980509281158446, val loss: 3.972436809539795, ETA in seconds: 1962820.189\n",
      "epoch: 532900, train loss: 3.8967496395111083, val loss: 3.971767807006836, ETA in seconds: 1962758.821\n",
      "epoch: 533000, train loss: 3.890012788772583, val loss: 3.979501748085022, ETA in seconds: 1962693.599\n",
      "epoch: 533100, train loss: 3.8863563537597656, val loss: 3.9678632736206056, ETA in seconds: 1962628.066\n",
      "epoch: 533200, train loss: 3.891165328025818, val loss: 3.966737747192383, ETA in seconds: 1962558.813\n",
      "epoch: 533300, train loss: 3.897481656074524, val loss: 3.9667961597442627, ETA in seconds: 1962483.249\n",
      "epoch: 533400, train loss: 3.8994207859039305, val loss: 3.969087815284729, ETA in seconds: 1962411.995\n",
      "epoch: 533500, train loss: 3.8994506359100343, val loss: 3.9742194175720216, ETA in seconds: 1962342.406\n",
      "epoch: 533600, train loss: 3.896872806549072, val loss: 3.961051821708679, ETA in seconds: 1962268.118\n",
      "epoch: 533700, train loss: 3.8971468925476076, val loss: 3.9746548414230345, ETA in seconds: 1962186.093\n",
      "epoch: 533800, train loss: 3.893807601928711, val loss: 3.9591575145721434, ETA in seconds: 1962144.332\n",
      "epoch: 533900, train loss: 3.8970962285995485, val loss: 3.9658050298690797, ETA in seconds: 1962059.490\n",
      "epoch: 534000, train loss: 3.892779755592346, val loss: 3.9694218397140504, ETA in seconds: 1962029.801\n",
      "epoch: 534100, train loss: 3.893174743652344, val loss: 3.971597170829773, ETA in seconds: 1962053.898\n",
      "epoch: 534200, train loss: 3.895148205757141, val loss: 3.9693715572357178, ETA in seconds: 1962047.476\n",
      "epoch: 534300, train loss: 3.884771203994751, val loss: 3.967631435394287, ETA in seconds: 1962049.250\n",
      "epoch: 534400, train loss: 3.8941885232925415, val loss: 3.9772887229919434, ETA in seconds: 1961965.623\n",
      "epoch: 534500, train loss: 3.89132707118988, val loss: 3.964064192771912, ETA in seconds: 1961880.707\n",
      "epoch: 534600, train loss: 3.888160490989685, val loss: 3.9828149557113646, ETA in seconds: 1961807.844\n",
      "epoch: 534700, train loss: 3.903528022766113, val loss: 3.972995662689209, ETA in seconds: 1961731.240\n",
      "epoch: 534800, train loss: 3.8903558969497682, val loss: 3.959686517715454, ETA in seconds: 1961653.732\n",
      "epoch: 534900, train loss: 3.8956177711486815, val loss: 3.9670871257781983, ETA in seconds: 1961589.292\n",
      "epoch: 535000, train loss: 3.892266345024109, val loss: 3.9821404457092284, ETA in seconds: 1961523.086\n",
      "epoch: 535100, train loss: 3.898860812187195, val loss: 3.9734227895736693, ETA in seconds: 1961457.200\n",
      "epoch: 535200, train loss: 3.9001511096954347, val loss: 3.970496106147766, ETA in seconds: 1961389.655\n",
      "epoch: 535300, train loss: 3.8813457012176515, val loss: 3.968179631233215, ETA in seconds: 1961313.555\n",
      "epoch: 535400, train loss: 3.899042582511902, val loss: 3.95927357673645, ETA in seconds: 1961241.398\n",
      "epoch: 535500, train loss: 3.8981382846832275, val loss: 3.9752965688705446, ETA in seconds: 1961168.101\n",
      "epoch: 535600, train loss: 3.8859229564666746, val loss: 3.974388003349304, ETA in seconds: 1961117.327\n",
      "epoch: 535700, train loss: 3.8947713375091553, val loss: 3.966883063316345, ETA in seconds: 1961119.044\n",
      "epoch: 535800, train loss: 3.8997779369354246, val loss: 3.954871344566345, ETA in seconds: 1961112.839\n",
      "epoch: 535900, train loss: 3.8919385194778444, val loss: 3.9819233894348143, ETA in seconds: 1961107.791\n",
      "epoch: 536000, train loss: 3.8901270389556886, val loss: 3.9575847864151, ETA in seconds: 1961108.169\n",
      "epoch: 536100, train loss: 3.8945342302322388, val loss: 3.968778443336487, ETA in seconds: 1961111.115\n",
      "epoch: 536200, train loss: 3.896783876419067, val loss: 3.9661300659179686, ETA in seconds: 1961062.593\n",
      "epoch: 536300, train loss: 3.8843132734298704, val loss: 3.969964075088501, ETA in seconds: 1961053.233\n",
      "epoch: 536400, train loss: 3.8997196435928343, val loss: 3.9824529886245728, ETA in seconds: 1961047.554\n",
      "epoch: 536500, train loss: 3.8945024490356444, val loss: 3.966836762428284, ETA in seconds: 1961060.547\n",
      "epoch: 536600, train loss: 3.8965277910232543, val loss: 3.9698986530303957, ETA in seconds: 1960992.716\n",
      "epoch: 536700, train loss: 3.8936481952667235, val loss: 3.964053177833557, ETA in seconds: 1960948.290\n",
      "epoch: 536800, train loss: 3.897571158409119, val loss: 3.9763009309768678, ETA in seconds: 1960887.274\n",
      "epoch: 536900, train loss: 3.89107460975647, val loss: 3.972020959854126, ETA in seconds: 1960832.020\n",
      "epoch: 537000, train loss: 3.8923623085021974, val loss: 3.9730098485946654, ETA in seconds: 1960766.697\n",
      "epoch: 537100, train loss: 3.8977611780166628, val loss: 3.964134120941162, ETA in seconds: 1960704.300\n",
      "epoch: 537200, train loss: 3.889079761505127, val loss: 3.9638132095336913, ETA in seconds: 1960651.024\n",
      "epoch: 537300, train loss: 3.8844538927078247, val loss: 3.972199821472168, ETA in seconds: 1960644.020\n",
      "epoch: 537400, train loss: 3.8996713638305662, val loss: 3.97702157497406, ETA in seconds: 1960606.350\n",
      "epoch: 537500, train loss: 3.8869662523269652, val loss: 3.9675044298171995, ETA in seconds: 1960540.238\n",
      "epoch: 537600, train loss: 3.892649602890015, val loss: 3.9706635236740113, ETA in seconds: 1960467.750\n",
      "epoch: 537700, train loss: 3.894596529006958, val loss: 3.9648741483688354, ETA in seconds: 1960413.562\n",
      "epoch: 537800, train loss: 3.8934176683425905, val loss: 3.977176070213318, ETA in seconds: 1960390.233\n",
      "epoch: 537900, train loss: 3.8913782596588136, val loss: 3.965627574920654, ETA in seconds: 1960310.346\n",
      "epoch: 538000, train loss: 3.888105630874634, val loss: 3.9726195096969605, ETA in seconds: 1960231.698\n",
      "epoch: 538100, train loss: 3.9005196332931518, val loss: 3.9603453397750856, ETA in seconds: 1960167.174\n",
      "epoch: 538200, train loss: 3.898807168006897, val loss: 3.968787670135498, ETA in seconds: 1960099.970\n",
      "epoch: 538300, train loss: 3.8898161888122558, val loss: 3.9714534521102904, ETA in seconds: 1960036.083\n",
      "epoch: 538400, train loss: 3.8960251092910765, val loss: 3.97109591960907, ETA in seconds: 1959965.868\n",
      "epoch: 538500, train loss: 3.8942102670669554, val loss: 3.9700613021850586, ETA in seconds: 1959903.184\n",
      "epoch: 538600, train loss: 3.895856022834778, val loss: 3.967127299308777, ETA in seconds: 1959853.395\n",
      "epoch: 538700, train loss: 3.8977474927902223, val loss: 3.9745251417160032, ETA in seconds: 1959845.452\n",
      "epoch: 538800, train loss: 3.9081650257110594, val loss: 3.974602293968201, ETA in seconds: 1959798.227\n",
      "epoch: 538900, train loss: 3.896636152267456, val loss: 3.9660073280334474, ETA in seconds: 1959773.214\n",
      "epoch: 539000, train loss: 3.887678289413452, val loss: 3.9595579385757445, ETA in seconds: 1959696.924\n",
      "epoch: 539100, train loss: 3.897866654396057, val loss: 3.9667937040328978, ETA in seconds: 1959617.866\n",
      "epoch: 539200, train loss: 3.8991517782211305, val loss: 3.9764907360076904, ETA in seconds: 1959557.360\n",
      "epoch: 539300, train loss: 3.888119173049927, val loss: 3.966979432106018, ETA in seconds: 1959548.302\n",
      "epoch: 539400, train loss: 3.897918629646301, val loss: 3.976141905784607, ETA in seconds: 1959535.447\n",
      "epoch: 539500, train loss: 3.8960299491882324, val loss: 3.971160364151001, ETA in seconds: 1959524.971\n",
      "epoch: 539600, train loss: 3.8991634607315064, val loss: 3.976685047149658, ETA in seconds: 1959517.863\n",
      "epoch: 539700, train loss: 3.8831660985946654, val loss: 3.9703830003738405, ETA in seconds: 1959505.069\n",
      "epoch: 539800, train loss: 3.89488422870636, val loss: 3.97707085609436, ETA in seconds: 1959478.479\n",
      "epoch: 539900, train loss: 3.894664931297302, val loss: 3.9663153171539305, ETA in seconds: 1959392.151\n",
      "epoch: 540000, train loss: 3.892073917388916, val loss: 3.9692884922027587, ETA in seconds: 1959305.347\n",
      "epoch: 540100, train loss: 3.892559027671814, val loss: 3.9793527364730834, ETA in seconds: 1959215.893\n",
      "epoch: 540200, train loss: 3.8910379648208617, val loss: 3.969853901863098, ETA in seconds: 1959134.418\n",
      "epoch: 540300, train loss: 3.8849716424942016, val loss: 3.9718329906463623, ETA in seconds: 1959048.893\n",
      "epoch: 540400, train loss: 3.8902974843978884, val loss: 3.972525715827942, ETA in seconds: 1958969.370\n",
      "epoch: 540500, train loss: 3.8924364328384398, val loss: 3.964939594268799, ETA in seconds: 1958883.838\n",
      "epoch: 540600, train loss: 3.8881734132766725, val loss: 3.9709576845169066, ETA in seconds: 1958803.561\n",
      "epoch: 540700, train loss: 3.8905733108520506, val loss: 3.985843229293823, ETA in seconds: 1958725.673\n",
      "epoch: 540800, train loss: 3.887586808204651, val loss: 3.977374625205994, ETA in seconds: 1958649.896\n",
      "epoch: 540900, train loss: 3.8965291500091555, val loss: 3.976001572608948, ETA in seconds: 1958564.230\n",
      "epoch: 541000, train loss: 3.8930678606033324, val loss: 3.984255385398865, ETA in seconds: 1958512.141\n",
      "epoch: 541100, train loss: 3.8988240718841554, val loss: 3.9764513969421387, ETA in seconds: 1958522.411\n",
      "epoch: 541200, train loss: 3.8924887657165526, val loss: 3.9737783432006837, ETA in seconds: 1958518.854\n",
      "epoch: 541300, train loss: 3.8837568044662474, val loss: 3.969543123245239, ETA in seconds: 1958523.600\n",
      "epoch: 541400, train loss: 3.8853039741516113, val loss: 3.9778745651245115, ETA in seconds: 1958532.394\n",
      "epoch: 541500, train loss: 3.897228646278381, val loss: 3.97719349861145, ETA in seconds: 1958513.000\n",
      "epoch: 541600, train loss: 3.8920475482940673, val loss: 3.9635488510131838, ETA in seconds: 1958452.848\n",
      "epoch: 541700, train loss: 3.896976685523987, val loss: 3.964117932319641, ETA in seconds: 1958390.018\n",
      "epoch: 541800, train loss: 3.8972033739089964, val loss: 3.9706573724746703, ETA in seconds: 1958330.717\n",
      "epoch: 541900, train loss: 3.9060909748077393, val loss: 3.9775387048721313, ETA in seconds: 1958270.832\n",
      "epoch: 542000, train loss: 3.898956322669983, val loss: 3.9719893455505373, ETA in seconds: 1958217.330\n",
      "epoch: 542100, train loss: 3.896564507484436, val loss: 3.9800748586654664, ETA in seconds: 1958136.721\n",
      "epoch: 542200, train loss: 3.897118854522705, val loss: 3.977073621749878, ETA in seconds: 1958084.299\n",
      "epoch: 542300, train loss: 3.90552499294281, val loss: 3.9744161128997804, ETA in seconds: 1958070.509\n",
      "epoch: 542400, train loss: 3.9013516426086428, val loss: 3.981816291809082, ETA in seconds: 1958013.442\n",
      "epoch: 542500, train loss: 3.889590835571289, val loss: 3.974335789680481, ETA in seconds: 1958010.585\n",
      "epoch: 542600, train loss: 3.8912003517150877, val loss: 3.961784029006958, ETA in seconds: 1957999.629\n",
      "epoch: 542700, train loss: 3.8866364479064943, val loss: 3.9871592044830324, ETA in seconds: 1957922.759\n",
      "epoch: 542800, train loss: 3.8941429615020753, val loss: 3.9784754037857057, ETA in seconds: 1957857.531\n",
      "epoch: 542900, train loss: 3.8891985416412354, val loss: 3.982476758956909, ETA in seconds: 1957780.523\n",
      "epoch: 543000, train loss: 3.887110733985901, val loss: 3.961254286766052, ETA in seconds: 1957705.582\n",
      "epoch: 543100, train loss: 3.8946765422821046, val loss: 3.9769055604934693, ETA in seconds: 1957633.875\n",
      "epoch: 543200, train loss: 3.8995800971984864, val loss: 3.9731595039367678, ETA in seconds: 1957537.240\n",
      "epoch: 543300, train loss: 3.897297763824463, val loss: 3.968874716758728, ETA in seconds: 1957450.072\n",
      "epoch: 543400, train loss: 3.894368529319763, val loss: 3.9741281032562257, ETA in seconds: 1957391.757\n",
      "epoch: 543500, train loss: 3.8951001882553102, val loss: 3.9745133399963377, ETA in seconds: 1957314.663\n",
      "epoch: 543600, train loss: 3.8906102657318113, val loss: 3.976529836654663, ETA in seconds: 1957234.755\n",
      "epoch: 543700, train loss: 3.8893643617630005, val loss: 3.9737855911254885, ETA in seconds: 1957179.622\n",
      "epoch: 543800, train loss: 3.894447612762451, val loss: 3.966287446022034, ETA in seconds: 1957169.638\n",
      "epoch: 543900, train loss: 3.902436089515686, val loss: 3.982040548324585, ETA in seconds: 1957152.833\n",
      "epoch: 544000, train loss: 3.890374994277954, val loss: 3.9628557920455934, ETA in seconds: 1957143.162\n",
      "epoch: 544100, train loss: 3.8998540163040163, val loss: 3.9758990526199343, ETA in seconds: 1957072.150\n",
      "epoch: 544200, train loss: 3.888545346260071, val loss: 3.9691429138183594, ETA in seconds: 1957001.225\n",
      "epoch: 544300, train loss: 3.8912529706954957, val loss: 3.9760276079177856, ETA in seconds: 1956930.086\n",
      "epoch: 544400, train loss: 3.886119318008423, val loss: 3.976336216926575, ETA in seconds: 1956855.438\n",
      "epoch: 544500, train loss: 3.8894901037216187, val loss: 3.973642587661743, ETA in seconds: 1956788.603\n",
      "epoch: 544600, train loss: 3.8915154933929443, val loss: 3.9673522233963014, ETA in seconds: 1956711.028\n",
      "epoch: 544700, train loss: 3.8866491079330445, val loss: 3.967855668067932, ETA in seconds: 1956639.096\n",
      "epoch: 544800, train loss: 3.893941068649292, val loss: 3.9719130277633665, ETA in seconds: 1956568.170\n",
      "epoch: 544900, train loss: 3.8911251544952394, val loss: 3.9684277772903442, ETA in seconds: 1956490.129\n",
      "epoch: 545000, train loss: 3.8849684715271, val loss: 3.9624552965164184, ETA in seconds: 1956411.232\n",
      "epoch: 545100, train loss: 3.8914797782897947, val loss: 3.9753803491592405, ETA in seconds: 1956330.539\n",
      "epoch: 545200, train loss: 3.901740479469299, val loss: 3.9631423473358156, ETA in seconds: 1956248.822\n",
      "epoch: 545300, train loss: 3.897831392288208, val loss: 3.9713667392730714, ETA in seconds: 1956162.243\n",
      "epoch: 545400, train loss: 3.8951985120773314, val loss: 3.962591600418091, ETA in seconds: 1956090.378\n",
      "epoch: 545500, train loss: 3.881427597999573, val loss: 3.982378435134888, ETA in seconds: 1956034.492\n",
      "epoch: 545600, train loss: 3.893321919441223, val loss: 3.9722664594650268, ETA in seconds: 1955961.852\n",
      "epoch: 545700, train loss: 3.8953797578811646, val loss: 3.9679526567459105, ETA in seconds: 1955902.266\n",
      "epoch: 545800, train loss: 3.9022718906402587, val loss: 3.97561719417572, ETA in seconds: 1955882.559\n",
      "epoch: 545900, train loss: 3.9023473501205443, val loss: 3.9730496644973754, ETA in seconds: 1955818.123\n",
      "epoch: 546000, train loss: 3.8954994916915893, val loss: 3.9641344785690307, ETA in seconds: 1955718.025\n",
      "epoch: 546100, train loss: 3.9050516366958616, val loss: 3.9709872722625734, ETA in seconds: 1955652.377\n",
      "epoch: 546200, train loss: 3.893586015701294, val loss: 3.9731979370117188, ETA in seconds: 1955638.622\n",
      "epoch: 546300, train loss: 3.8933615684509277, val loss: 3.9593055486679076, ETA in seconds: 1955565.333\n",
      "epoch: 546400, train loss: 3.8874443054199217, val loss: 3.9653156042099, ETA in seconds: 1955475.721\n",
      "epoch: 546500, train loss: 3.9024332046508787, val loss: 3.9676673650741576, ETA in seconds: 1955383.675\n",
      "epoch: 546600, train loss: 3.900372791290283, val loss: 3.978971529006958, ETA in seconds: 1955293.902\n",
      "epoch: 546700, train loss: 3.90101842880249, val loss: 3.9683828353881836, ETA in seconds: 1955205.891\n",
      "epoch: 546800, train loss: 3.890841245651245, val loss: 3.9589120626449583, ETA in seconds: 1955194.512\n",
      "epoch: 546900, train loss: 3.888677453994751, val loss: 3.9595690250396727, ETA in seconds: 1955110.823\n",
      "epoch: 547000, train loss: 3.8886539697647096, val loss: 3.9583317756652834, ETA in seconds: 1955018.254\n",
      "epoch: 547100, train loss: 3.8931902647018433, val loss: 3.9644436120986937, ETA in seconds: 1954922.656\n",
      "epoch: 547200, train loss: 3.899951124191284, val loss: 3.9686872243881224, ETA in seconds: 1954837.861\n",
      "epoch: 547300, train loss: 3.89838547706604, val loss: 3.955817365646362, ETA in seconds: 1954740.792\n",
      "epoch: 547400, train loss: 3.882250428199768, val loss: 3.9675254106521605, ETA in seconds: 1954643.668\n",
      "epoch: 547500, train loss: 3.8981561183929445, val loss: 3.9695000648498535, ETA in seconds: 1954550.007\n",
      "epoch: 547600, train loss: 3.891416311264038, val loss: 3.972700023651123, ETA in seconds: 1954468.938\n",
      "epoch: 547700, train loss: 3.885739541053772, val loss: 3.9666891574859617, ETA in seconds: 1954439.715\n",
      "epoch: 547800, train loss: 3.887424373626709, val loss: 3.9847447872161865, ETA in seconds: 1954344.426\n",
      "epoch: 547900, train loss: 3.892814540863037, val loss: 3.9711748600006103, ETA in seconds: 1954243.738\n",
      "epoch: 548000, train loss: 3.8903316497802733, val loss: 3.968509387969971, ETA in seconds: 1954138.699\n",
      "epoch: 548100, train loss: 3.9061214685440064, val loss: 3.9684415102005004, ETA in seconds: 1954035.048\n",
      "epoch: 548200, train loss: 3.8976568460464476, val loss: 3.9679229736328123, ETA in seconds: 1953926.602\n",
      "epoch: 548300, train loss: 3.8902265548706056, val loss: 3.9613059997558593, ETA in seconds: 1953833.323\n",
      "epoch: 548400, train loss: 3.901625657081604, val loss: 3.969474720954895, ETA in seconds: 1953735.287\n",
      "epoch: 548500, train loss: 3.8913831233978273, val loss: 3.9678444147109984, ETA in seconds: 1953643.176\n",
      "epoch: 548600, train loss: 3.8975186109542848, val loss: 3.9695181846618652, ETA in seconds: 1953542.606\n",
      "epoch: 548700, train loss: 3.8967402935028077, val loss: 3.9778311252593994, ETA in seconds: 1953442.855\n",
      "epoch: 548800, train loss: 3.889337968826294, val loss: 3.961524486541748, ETA in seconds: 1953358.212\n",
      "epoch: 548900, train loss: 3.902185583114624, val loss: 3.958228349685669, ETA in seconds: 1953333.357\n",
      "epoch: 549000, train loss: 3.8888861656188967, val loss: 3.9584508419036863, ETA in seconds: 1953309.388\n",
      "epoch: 549100, train loss: 3.8875264167785644, val loss: 3.9684879541397096, ETA in seconds: 1953285.391\n",
      "epoch: 549200, train loss: 3.8867398262023927, val loss: 3.974231791496277, ETA in seconds: 1953255.793\n",
      "epoch: 549300, train loss: 3.909937047958374, val loss: 3.9655692338943482, ETA in seconds: 1953225.020\n",
      "epoch: 549400, train loss: 3.8919424533843996, val loss: 3.9759456157684325, ETA in seconds: 1953198.544\n",
      "epoch: 549500, train loss: 3.895561933517456, val loss: 3.9619856595993044, ETA in seconds: 1953170.017\n",
      "epoch: 549600, train loss: 3.894229555130005, val loss: 3.978959798812866, ETA in seconds: 1953145.747\n",
      "epoch: 549700, train loss: 3.8758093118667603, val loss: 3.976056122779846, ETA in seconds: 1953112.678\n",
      "epoch: 549800, train loss: 3.892323350906372, val loss: 3.9679468631744386, ETA in seconds: 1953015.090\n",
      "epoch: 549900, train loss: 3.8954259634017943, val loss: 3.9749760389328004, ETA in seconds: 1952914.335\n",
      "epoch: 550000, train loss: 3.8913514614105225, val loss: 3.964038443565369, ETA in seconds: 1952816.206\n",
      "epoch: 550100, train loss: 3.893713140487671, val loss: 3.9732319593429564, ETA in seconds: 1952724.372\n",
      "epoch: 550200, train loss: 3.893216848373413, val loss: 3.967621421813965, ETA in seconds: 1952645.406\n",
      "epoch: 550300, train loss: 3.8936105489730837, val loss: 3.9792291641235353, ETA in seconds: 1952623.289\n",
      "epoch: 550400, train loss: 3.889754033088684, val loss: 3.9721898317337034, ETA in seconds: 1952615.874\n",
      "epoch: 550500, train loss: 3.8933345556259153, val loss: 3.9716964960098267, ETA in seconds: 1952613.458\n",
      "epoch: 550600, train loss: 3.899441456794739, val loss: 3.9736024856567385, ETA in seconds: 1952591.195\n",
      "epoch: 550700, train loss: 3.8979191303253176, val loss: 3.9630123138427735, ETA in seconds: 1952512.575\n",
      "epoch: 550800, train loss: 3.8898255825042725, val loss: 3.9732427835464477, ETA in seconds: 1952406.460\n",
      "epoch: 550900, train loss: 3.895034599304199, val loss: 3.968344712257385, ETA in seconds: 1952303.451\n",
      "epoch: 551000, train loss: 3.8909553050994874, val loss: 3.9623364210128784, ETA in seconds: 1952262.985\n",
      "epoch: 551100, train loss: 3.9027405738830567, val loss: 3.983999824523926, ETA in seconds: 1952159.203\n",
      "epoch: 551200, train loss: 3.8922136783599854, val loss: 3.976070499420166, ETA in seconds: 1952082.574\n",
      "epoch: 551300, train loss: 3.8888121843338013, val loss: 3.967603158950806, ETA in seconds: 1951974.487\n",
      "epoch: 551400, train loss: 3.8980139017105104, val loss: 3.96818060874939, ETA in seconds: 1951860.778\n",
      "epoch: 551500, train loss: 3.899391007423401, val loss: 3.9695713758468627, ETA in seconds: 1951749.756\n",
      "epoch: 551600, train loss: 3.893637466430664, val loss: 3.973801517486572, ETA in seconds: 1951652.651\n",
      "epoch: 551700, train loss: 3.893609809875488, val loss: 3.9760082721710206, ETA in seconds: 1951538.839\n",
      "epoch: 551800, train loss: 3.890548896789551, val loss: 3.9762336730957033, ETA in seconds: 1951463.676\n",
      "epoch: 551900, train loss: 3.894283127784729, val loss: 3.9708229064941407, ETA in seconds: 1951436.737\n",
      "epoch: 552000, train loss: 3.8963013648986817, val loss: 3.982222652435303, ETA in seconds: 1951364.079\n",
      "epoch: 552100, train loss: 3.905542755126953, val loss: 3.974606084823608, ETA in seconds: 1951286.926\n",
      "epoch: 552200, train loss: 3.890273118019104, val loss: 3.966347646713257, ETA in seconds: 1951214.456\n",
      "epoch: 552300, train loss: 3.892272782325745, val loss: 3.9750796794891357, ETA in seconds: 1951109.257\n",
      "epoch: 552400, train loss: 3.894787049293518, val loss: 3.9748722314834595, ETA in seconds: 1951038.139\n",
      "epoch: 552500, train loss: 3.8937416791915895, val loss: 3.9702234506607055, ETA in seconds: 1951007.707\n",
      "epoch: 552600, train loss: 3.891431427001953, val loss: 3.981637716293335, ETA in seconds: 1950954.671\n",
      "epoch: 552700, train loss: 3.8960525512695314, val loss: 3.967032241821289, ETA in seconds: 1950880.418\n",
      "epoch: 552800, train loss: 3.9019879341125487, val loss: 3.9717430114746093, ETA in seconds: 1950852.863\n",
      "epoch: 552900, train loss: 3.896450090408325, val loss: 3.9821895360946655, ETA in seconds: 1950755.386\n",
      "epoch: 553000, train loss: 3.9008462905883787, val loss: 3.9635854005813598, ETA in seconds: 1950674.188\n",
      "epoch: 553100, train loss: 3.893380117416382, val loss: 3.9764624357223513, ETA in seconds: 1950647.841\n",
      "epoch: 553200, train loss: 3.8966241598129274, val loss: 3.97418098449707, ETA in seconds: 1950544.942\n",
      "epoch: 553300, train loss: 3.8822638750076295, val loss: 3.9775800704956055, ETA in seconds: 1950455.618\n",
      "epoch: 553400, train loss: 3.890994119644165, val loss: 3.97021746635437, ETA in seconds: 1950354.125\n",
      "epoch: 553500, train loss: 3.8987344980239866, val loss: 3.977185297012329, ETA in seconds: 1950255.955\n",
      "epoch: 553600, train loss: 3.8911704778671266, val loss: 3.9731684923171997, ETA in seconds: 1950155.492\n",
      "epoch: 553700, train loss: 3.891082787513733, val loss: 3.962659239768982, ETA in seconds: 1950048.067\n",
      "epoch: 553800, train loss: 3.881711792945862, val loss: 3.9690314292907716, ETA in seconds: 1949930.824\n",
      "epoch: 553900, train loss: 3.896035146713257, val loss: 3.9745521068573, ETA in seconds: 1949819.756\n",
      "epoch: 554000, train loss: 3.896750640869141, val loss: 3.9685502529144285, ETA in seconds: 1949718.665\n",
      "epoch: 554100, train loss: 3.898062539100647, val loss: 3.971529507637024, ETA in seconds: 1949621.502\n",
      "epoch: 554200, train loss: 3.8935563564300537, val loss: 3.96835834980011, ETA in seconds: 1949553.297\n",
      "epoch: 554300, train loss: 3.890702795982361, val loss: 3.9715652465820312, ETA in seconds: 1949537.114\n",
      "epoch: 554400, train loss: 3.9017719507217405, val loss: 3.9765466928482054, ETA in seconds: 1949442.729\n",
      "epoch: 554500, train loss: 3.892877793312073, val loss: 3.979418897628784, ETA in seconds: 1949340.867\n",
      "epoch: 554600, train loss: 3.8825378894805906, val loss: 3.9634753465652466, ETA in seconds: 1949239.926\n",
      "epoch: 554700, train loss: 3.893518090248108, val loss: 3.9695778369903563, ETA in seconds: 1949134.721\n",
      "epoch: 554800, train loss: 3.904767155647278, val loss: 3.973882722854614, ETA in seconds: 1949051.247\n",
      "epoch: 554900, train loss: 3.8838022232055662, val loss: 3.959621262550354, ETA in seconds: 1949017.448\n",
      "epoch: 555000, train loss: 3.897327351570129, val loss: 3.975206398963928, ETA in seconds: 1948952.384\n",
      "epoch: 555100, train loss: 3.8976781129837037, val loss: 3.9707719326019286, ETA in seconds: 1948843.357\n",
      "epoch: 555200, train loss: 3.8912196159362793, val loss: 3.9688753128051757, ETA in seconds: 1948740.223\n",
      "epoch: 555300, train loss: 3.8815935134887694, val loss: 3.9798478364944456, ETA in seconds: 1948676.962\n",
      "epoch: 555400, train loss: 3.8876925706863403, val loss: 3.973370671272278, ETA in seconds: 1948669.057\n",
      "epoch: 555500, train loss: 3.8953728914260863, val loss: 3.9860284090042115, ETA in seconds: 1948649.126\n",
      "epoch: 555600, train loss: 3.895609426498413, val loss: 3.96488196849823, ETA in seconds: 1948648.902\n",
      "epoch: 555700, train loss: 3.892471766471863, val loss: 3.968041920661926, ETA in seconds: 1948615.559\n",
      "epoch: 555800, train loss: 3.8998875856399535, val loss: 3.96955463886261, ETA in seconds: 1948594.133\n",
      "epoch: 555900, train loss: 3.8990830898284914, val loss: 3.9689301252365112, ETA in seconds: 1948563.645\n",
      "epoch: 556000, train loss: 3.886055421829224, val loss: 3.972860336303711, ETA in seconds: 1948545.961\n",
      "epoch: 556100, train loss: 3.890488696098328, val loss: 3.9633169174194336, ETA in seconds: 1948451.767\n",
      "epoch: 556200, train loss: 3.8971925258636473, val loss: 3.966646122932434, ETA in seconds: 1948359.465\n",
      "epoch: 556300, train loss: 3.903550219535828, val loss: 3.973824882507324, ETA in seconds: 1948252.384\n",
      "epoch: 556400, train loss: 3.892631506919861, val loss: 3.97744882106781, ETA in seconds: 1948150.914\n",
      "epoch: 556500, train loss: 3.8952773809432983, val loss: 3.9724578619003297, ETA in seconds: 1948053.785\n",
      "epoch: 556600, train loss: 3.890892195701599, val loss: 3.9723576068878175, ETA in seconds: 1947960.915\n",
      "epoch: 556700, train loss: 3.8919962882995605, val loss: 3.9797654867172243, ETA in seconds: 1947864.039\n",
      "epoch: 556800, train loss: 3.8930091857910156, val loss: 3.9637908697128297, ETA in seconds: 1947764.839\n",
      "epoch: 556900, train loss: 3.9003668785095216, val loss: 3.973459267616272, ETA in seconds: 1947664.438\n",
      "epoch: 557000, train loss: 3.894356870651245, val loss: 3.963467264175415, ETA in seconds: 1947563.751\n",
      "epoch: 557100, train loss: 3.9017157554626465, val loss: 3.972317862510681, ETA in seconds: 1947467.377\n",
      "epoch: 557200, train loss: 3.902420163154602, val loss: 3.967971158027649, ETA in seconds: 1947360.803\n",
      "epoch: 557300, train loss: 3.88995418548584, val loss: 3.973416876792908, ETA in seconds: 1947252.525\n",
      "epoch: 557400, train loss: 3.8921506881713865, val loss: 3.9736819505691527, ETA in seconds: 1947140.173\n",
      "epoch: 557500, train loss: 3.8953296422958372, val loss: 3.962560224533081, ETA in seconds: 1947076.077\n",
      "epoch: 557600, train loss: 3.896456575393677, val loss: 3.9670509815216066, ETA in seconds: 1946972.963\n",
      "epoch: 557700, train loss: 3.892479991912842, val loss: 3.9696219205856322, ETA in seconds: 1946867.853\n",
      "epoch: 557800, train loss: 3.896161365509033, val loss: 3.9636771202087404, ETA in seconds: 1946772.581\n",
      "epoch: 557900, train loss: 3.8983708381652833, val loss: 3.9669073820114136, ETA in seconds: 1946730.654\n",
      "epoch: 558000, train loss: 3.891891074180603, val loss: 3.9646866798400877, ETA in seconds: 1946688.646\n",
      "epoch: 558100, train loss: 3.892662596702576, val loss: 3.950797128677368, ETA in seconds: 1946647.604\n",
      "epoch: 558200, train loss: 3.8929044723510744, val loss: 3.9788460731506348, ETA in seconds: 1946609.993\n",
      "epoch: 558300, train loss: 3.8959754943847655, val loss: 3.961665987968445, ETA in seconds: 1946568.422\n",
      "epoch: 558400, train loss: 3.888862466812134, val loss: 3.970129156112671, ETA in seconds: 1946529.419\n",
      "epoch: 558500, train loss: 3.889264225959778, val loss: 3.9655641317367554, ETA in seconds: 1946430.511\n",
      "epoch: 558600, train loss: 3.881656765937805, val loss: 3.9624943494796754, ETA in seconds: 1946314.864\n",
      "epoch: 558700, train loss: 3.894865393638611, val loss: 3.961433506011963, ETA in seconds: 1946221.755\n",
      "epoch: 558800, train loss: 3.8873751878738405, val loss: 3.9658706903457643, ETA in seconds: 1946171.945\n",
      "epoch: 558900, train loss: 3.899622368812561, val loss: 3.95504367351532, ETA in seconds: 1946062.747\n",
      "epoch: 559000, train loss: 3.889223647117615, val loss: 3.9610158920288088, ETA in seconds: 1945953.882\n",
      "epoch: 559100, train loss: 3.8891940832138063, val loss: 3.9652111291885377, ETA in seconds: 1945843.954\n",
      "epoch: 559200, train loss: 3.9067490100860596, val loss: 3.9619080066680907, ETA in seconds: 1945739.329\n",
      "epoch: 559300, train loss: 3.8881272792816164, val loss: 3.960349416732788, ETA in seconds: 1945623.259\n",
      "epoch: 559400, train loss: 3.9010512828826904, val loss: 3.967783570289612, ETA in seconds: 1945518.043\n",
      "epoch: 559500, train loss: 3.90010142326355, val loss: 3.9743708848953245, ETA in seconds: 1945417.203\n",
      "epoch: 559600, train loss: 3.890363025665283, val loss: 3.975190544128418, ETA in seconds: 1945329.359\n",
      "epoch: 559700, train loss: 3.8907065868377684, val loss: 3.968825888633728, ETA in seconds: 1945265.365\n",
      "epoch: 559800, train loss: 3.890722370147705, val loss: 3.958996796607971, ETA in seconds: 1945138.125\n",
      "epoch: 559900, train loss: 3.89439914226532, val loss: 3.966415357589722, ETA in seconds: 1945034.592\n",
      "epoch: 560000, train loss: 3.8879117012023925, val loss: 3.9781970262527464, ETA in seconds: 1944929.299\n",
      "epoch: 560100, train loss: 3.8882387399673464, val loss: 3.9679059267044066, ETA in seconds: 1944809.633\n",
      "epoch: 560200, train loss: 3.8934151411056517, val loss: 3.9745593309402465, ETA in seconds: 1944699.268\n",
      "epoch: 560300, train loss: 3.894641661643982, val loss: 3.9745676279067994, ETA in seconds: 1944586.566\n",
      "epoch: 560400, train loss: 3.8962186336517335, val loss: 3.956795263290405, ETA in seconds: 1944500.433\n",
      "epoch: 560500, train loss: 3.879410672187805, val loss: 3.9634101390838623, ETA in seconds: 1944398.078\n",
      "epoch: 560600, train loss: 3.893133616447449, val loss: 3.9622487783432008, ETA in seconds: 1944270.545\n",
      "epoch: 560700, train loss: 3.8969574928283692, val loss: 3.96140193939209, ETA in seconds: 1944144.438\n",
      "epoch: 560800, train loss: 3.8902838706970213, val loss: 3.972140169143677, ETA in seconds: 1944027.456\n",
      "epoch: 560900, train loss: 3.8945054531097414, val loss: 3.9677931547164915, ETA in seconds: 1943911.985\n",
      "epoch: 561000, train loss: 3.897370982170105, val loss: 3.9652796745300294, ETA in seconds: 1943788.954\n",
      "epoch: 561100, train loss: 3.891687774658203, val loss: 3.976463508605957, ETA in seconds: 1943670.197\n",
      "epoch: 561200, train loss: 3.8877173185348513, val loss: 3.9654420137405397, ETA in seconds: 1943551.299\n",
      "epoch: 561300, train loss: 3.898313879966736, val loss: 3.979821228981018, ETA in seconds: 1943429.983\n",
      "epoch: 561400, train loss: 3.888276767730713, val loss: 3.9694164752960206, ETA in seconds: 1943341.108\n",
      "epoch: 561500, train loss: 3.8895272254943847, val loss: 3.9788267612457275, ETA in seconds: 1943292.431\n",
      "epoch: 561600, train loss: 3.897465538978577, val loss: 3.9789578199386595, ETA in seconds: 1943206.258\n",
      "epoch: 561700, train loss: 3.8938275814056396, val loss: 3.969879937171936, ETA in seconds: 1943107.337\n",
      "epoch: 561800, train loss: 3.8960352659225466, val loss: 3.976136827468872, ETA in seconds: 1942998.806\n",
      "epoch: 561900, train loss: 3.897067642211914, val loss: 3.982134222984314, ETA in seconds: 1942888.657\n",
      "epoch: 562000, train loss: 3.892683005332947, val loss: 3.9704388618469237, ETA in seconds: 1942775.805\n",
      "epoch: 562100, train loss: 3.8895729780197144, val loss: 3.971650409698486, ETA in seconds: 1942660.379\n",
      "epoch: 562200, train loss: 3.8987245321273805, val loss: 3.9675156354904173, ETA in seconds: 1942550.833\n",
      "epoch: 562300, train loss: 3.8882155656814574, val loss: 3.977757787704468, ETA in seconds: 1942462.147\n",
      "epoch: 562400, train loss: 3.8928817987442015, val loss: 3.9601165056228638, ETA in seconds: 1942352.706\n",
      "epoch: 562500, train loss: 3.890192723274231, val loss: 3.9709340810775755, ETA in seconds: 1942237.923\n",
      "epoch: 562600, train loss: 3.8920986890792846, val loss: 3.96930513381958, ETA in seconds: 1942130.917\n",
      "epoch: 562700, train loss: 3.8851964235305787, val loss: 3.967212772369385, ETA in seconds: 1942029.390\n",
      "epoch: 562800, train loss: 3.895783543586731, val loss: 3.9725763320922853, ETA in seconds: 1941924.163\n",
      "epoch: 562900, train loss: 3.886362981796265, val loss: 3.9630100250244142, ETA in seconds: 1941820.860\n",
      "epoch: 563000, train loss: 3.9013649225234985, val loss: 3.974393916130066, ETA in seconds: 1941714.049\n",
      "epoch: 563100, train loss: 3.8951284408569338, val loss: 3.9627296924591064, ETA in seconds: 1941603.963\n",
      "epoch: 563200, train loss: 3.8999561786651613, val loss: 3.9724571228027346, ETA in seconds: 1941491.680\n",
      "epoch: 563300, train loss: 3.8913047075271607, val loss: 3.979682779312134, ETA in seconds: 1941374.167\n",
      "epoch: 563400, train loss: 3.8873071670532227, val loss: 3.9680413246154784, ETA in seconds: 1941255.381\n",
      "epoch: 563500, train loss: 3.886258053779602, val loss: 3.9678520441055296, ETA in seconds: 1941143.440\n",
      "epoch: 563600, train loss: 3.8885395526885986, val loss: 3.9650681734085085, ETA in seconds: 1941032.604\n",
      "epoch: 563700, train loss: 3.8787508964538575, val loss: 3.967881774902344, ETA in seconds: 1940930.314\n",
      "epoch: 563800, train loss: 3.8984211921691894, val loss: 3.970326614379883, ETA in seconds: 1940814.019\n",
      "epoch: 563900, train loss: 3.897155451774597, val loss: 3.965903806686401, ETA in seconds: 1940697.953\n",
      "epoch: 564000, train loss: 3.8853296041488647, val loss: 3.95671968460083, ETA in seconds: 1940575.497\n",
      "epoch: 564100, train loss: 3.889075446128845, val loss: 3.964125418663025, ETA in seconds: 1940464.983\n",
      "epoch: 564200, train loss: 3.90016188621521, val loss: 3.9756168365478515, ETA in seconds: 1940356.597\n",
      "epoch: 564300, train loss: 3.897547483444214, val loss: 3.9612134456634522, ETA in seconds: 1940251.188\n",
      "epoch: 564400, train loss: 3.893890619277954, val loss: 3.9665224313735963, ETA in seconds: 1940193.202\n",
      "epoch: 564500, train loss: 3.8922915697097777, val loss: 3.971967577934265, ETA in seconds: 1940080.441\n",
      "epoch: 564600, train loss: 3.895674633979797, val loss: 3.971121835708618, ETA in seconds: 1939968.169\n",
      "epoch: 564700, train loss: 3.8899757862091064, val loss: 3.9722068309783936, ETA in seconds: 1939853.563\n",
      "epoch: 564800, train loss: 3.900164985656738, val loss: 3.9681253671646117, ETA in seconds: 1939742.087\n",
      "epoch: 564900, train loss: 3.8880743265151976, val loss: 3.975588154792786, ETA in seconds: 1939631.438\n",
      "epoch: 565000, train loss: 3.896382761001587, val loss: 3.9698923587799073, ETA in seconds: 1939511.595\n",
      "epoch: 565100, train loss: 3.8998214483261107, val loss: 3.9725576877593993, ETA in seconds: 1939394.413\n",
      "epoch: 565200, train loss: 3.893780994415283, val loss: 3.9681558847427367, ETA in seconds: 1939318.022\n",
      "epoch: 565300, train loss: 3.8977885484695434, val loss: 3.96917986869812, ETA in seconds: 1939278.575\n",
      "epoch: 565400, train loss: 3.895460867881775, val loss: 3.9723385095596315, ETA in seconds: 1939238.526\n",
      "epoch: 565500, train loss: 3.90458402633667, val loss: 3.9707305431365967, ETA in seconds: 1939132.731\n",
      "epoch: 565600, train loss: 3.9006935596466064, val loss: 3.9687898635864256, ETA in seconds: 1939004.326\n",
      "epoch: 565700, train loss: 3.8937074661254885, val loss: 3.968831777572632, ETA in seconds: 1938885.975\n",
      "epoch: 565800, train loss: 3.8911495923995973, val loss: 3.955716109275818, ETA in seconds: 1938762.708\n",
      "epoch: 565900, train loss: 3.895573592185974, val loss: 3.9641161441802977, ETA in seconds: 1938642.452\n",
      "epoch: 566000, train loss: 3.890337514877319, val loss: 3.969943618774414, ETA in seconds: 1938520.854\n",
      "epoch: 566100, train loss: 3.892545223236084, val loss: 3.962877702713013, ETA in seconds: 1938400.409\n",
      "epoch: 566200, train loss: 3.8918659687042236, val loss: 3.9713241815567017, ETA in seconds: 1938294.450\n",
      "epoch: 566300, train loss: 3.8939141511917112, val loss: 3.971659779548645, ETA in seconds: 1938227.457\n",
      "epoch: 566400, train loss: 3.8993638515472413, val loss: 3.9657172679901125, ETA in seconds: 1938097.512\n",
      "epoch: 566500, train loss: 3.8864030599594117, val loss: 3.9613019704818724, ETA in seconds: 1937977.184\n",
      "epoch: 566600, train loss: 3.9023534774780275, val loss: 3.959116053581238, ETA in seconds: 1937850.391\n",
      "epoch: 566700, train loss: 3.8891941785812376, val loss: 3.961744856834412, ETA in seconds: 1937723.909\n",
      "epoch: 566800, train loss: 3.8926238298416136, val loss: 3.9566710233688354, ETA in seconds: 1937600.627\n",
      "epoch: 566900, train loss: 3.8964106798172, val loss: 3.961508536338806, ETA in seconds: 1937468.433\n",
      "epoch: 567000, train loss: 3.892625665664673, val loss: 3.971916604042053, ETA in seconds: 1937337.910\n",
      "epoch: 567100, train loss: 3.8957605361938477, val loss: 3.9681644439697266, ETA in seconds: 1937240.536\n",
      "epoch: 567200, train loss: 3.897781491279602, val loss: 3.9774240493774413, ETA in seconds: 1937121.261\n",
      "epoch: 567300, train loss: 3.884937620162964, val loss: 3.9667543172836304, ETA in seconds: 1936985.937\n",
      "epoch: 567400, train loss: 3.888227343559265, val loss: 3.95870726108551, ETA in seconds: 1936853.129\n",
      "epoch: 567500, train loss: 3.899094653129578, val loss: 3.9624858856201173, ETA in seconds: 1936720.360\n",
      "epoch: 567600, train loss: 3.8831029653549196, val loss: 3.9635410070419312, ETA in seconds: 1936580.454\n",
      "epoch: 567700, train loss: 3.88508939743042, val loss: 3.961421799659729, ETA in seconds: 1936445.785\n",
      "epoch: 567800, train loss: 3.889708137512207, val loss: 3.961360955238342, ETA in seconds: 1936311.766\n",
      "epoch: 567900, train loss: 3.9057932615280153, val loss: 3.9668103218078614, ETA in seconds: 1936181.918\n",
      "epoch: 568000, train loss: 3.8919318199157713, val loss: 3.9669949293136595, ETA in seconds: 1936051.062\n",
      "epoch: 568100, train loss: 3.892287826538086, val loss: 3.974482607841492, ETA in seconds: 1935916.323\n",
      "epoch: 568200, train loss: 3.895620036125183, val loss: 3.966772174835205, ETA in seconds: 1935779.587\n",
      "epoch: 568300, train loss: 3.880677318572998, val loss: 3.9643398761749267, ETA in seconds: 1935643.830\n",
      "epoch: 568400, train loss: 3.894889497756958, val loss: 3.9620933294296266, ETA in seconds: 1935508.542\n",
      "epoch: 568500, train loss: 3.8895604610443115, val loss: 3.9717359066009523, ETA in seconds: 1935370.779\n",
      "epoch: 568600, train loss: 3.8958799123764036, val loss: 3.9733074426651003, ETA in seconds: 1935232.667\n",
      "epoch: 568700, train loss: 3.8926879167556763, val loss: 3.9702311754226685, ETA in seconds: 1935093.602\n",
      "epoch: 568800, train loss: 3.9101645231246946, val loss: 3.9701959609985353, ETA in seconds: 1934947.827\n",
      "epoch: 568900, train loss: 3.8908447504043577, val loss: 3.966044878959656, ETA in seconds: 1934807.786\n",
      "epoch: 569000, train loss: 3.8915892362594606, val loss: 3.9680245161056518, ETA in seconds: 1934737.699\n",
      "epoch: 569100, train loss: 3.891105628013611, val loss: 3.972807741165161, ETA in seconds: 1934597.266\n",
      "epoch: 569200, train loss: 3.892249011993408, val loss: 3.9755664348602293, ETA in seconds: 1934479.830\n",
      "epoch: 569300, train loss: 3.8981127977371215, val loss: 3.9669246673583984, ETA in seconds: 1934421.371\n",
      "epoch: 569400, train loss: 3.8941077470779417, val loss: 3.9719325065612794, ETA in seconds: 1934306.256\n",
      "epoch: 569500, train loss: 3.8905104637145995, val loss: 3.9548107147216798, ETA in seconds: 1934213.198\n",
      "epoch: 569600, train loss: 3.8994783639907835, val loss: 3.973308777809143, ETA in seconds: 1934094.767\n",
      "epoch: 569700, train loss: 3.8981668949127197, val loss: 3.960473322868347, ETA in seconds: 1933960.941\n",
      "epoch: 569800, train loss: 3.8830503702163695, val loss: 3.96854088306427, ETA in seconds: 1933839.065\n",
      "epoch: 569900, train loss: 3.892782759666443, val loss: 3.9657246828079225, ETA in seconds: 1933698.264\n",
      "epoch: 570000, train loss: 3.8893361568450926, val loss: 3.9754586458206176, ETA in seconds: 1933572.659\n",
      "epoch: 570100, train loss: 3.8993396282196047, val loss: 3.956265449523926, ETA in seconds: 1933440.310\n",
      "epoch: 570200, train loss: 3.8826892614364623, val loss: 3.965002155303955, ETA in seconds: 1933308.982\n",
      "epoch: 570300, train loss: 3.891569662094116, val loss: 3.959737229347229, ETA in seconds: 1933174.098\n",
      "epoch: 570400, train loss: 3.8938469648361207, val loss: 3.9755553245544433, ETA in seconds: 1933063.318\n",
      "epoch: 570500, train loss: 3.896571087837219, val loss: 3.9706936120986938, ETA in seconds: 1932947.049\n",
      "epoch: 570600, train loss: 3.8972545146942137, val loss: 3.976309657096863, ETA in seconds: 1932823.698\n",
      "epoch: 570700, train loss: 3.8915544748306274, val loss: 3.9665728330612184, ETA in seconds: 1932750.146\n",
      "epoch: 570800, train loss: 3.8878974437713625, val loss: 3.9734667778015136, ETA in seconds: 1932630.749\n",
      "epoch: 570900, train loss: 3.8979615449905394, val loss: 3.9691803455352783, ETA in seconds: 1932496.079\n",
      "epoch: 571000, train loss: 3.89207239151001, val loss: 3.9763734102249146, ETA in seconds: 1932377.500\n",
      "epoch: 571100, train loss: 3.8813370943069456, val loss: 3.9738619565963744, ETA in seconds: 1932287.091\n",
      "epoch: 571200, train loss: 3.8919029712677, val loss: 3.975616955757141, ETA in seconds: 1932135.536\n",
      "epoch: 571300, train loss: 3.889348793029785, val loss: 3.9714052200317385, ETA in seconds: 1931994.831\n",
      "epoch: 571400, train loss: 3.889770030975342, val loss: 3.970801281929016, ETA in seconds: 1931861.564\n",
      "epoch: 571500, train loss: 3.8984784841537476, val loss: 3.9622055768966673, ETA in seconds: 1931716.137\n",
      "epoch: 571600, train loss: 3.894231581687927, val loss: 3.9791109561920166, ETA in seconds: 1931632.091\n",
      "epoch: 571700, train loss: 3.900866222381592, val loss: 3.9725988626480104, ETA in seconds: 1931512.329\n",
      "epoch: 571800, train loss: 3.8952904462814333, val loss: 3.97690064907074, ETA in seconds: 1931392.034\n",
      "epoch: 571900, train loss: 3.894622564315796, val loss: 3.9698300123214723, ETA in seconds: 1931272.311\n",
      "epoch: 572000, train loss: 3.8865356922149656, val loss: 3.9701282739639283, ETA in seconds: 1931152.793\n",
      "epoch: 572100, train loss: 3.8925715923309325, val loss: 3.968644309043884, ETA in seconds: 1931032.930\n",
      "epoch: 572200, train loss: 3.8955275774002076, val loss: 3.9750564098358154, ETA in seconds: 1930907.463\n",
      "epoch: 572300, train loss: 3.8892736434936523, val loss: 3.975161147117615, ETA in seconds: 1930781.483\n",
      "epoch: 572400, train loss: 3.8949753046035767, val loss: 3.9747069835662843, ETA in seconds: 1930657.913\n",
      "epoch: 572500, train loss: 3.8966429948806764, val loss: 3.9614044427871704, ETA in seconds: 1930539.294\n",
      "epoch: 572600, train loss: 3.8889840602874757, val loss: 3.9793798446655275, ETA in seconds: 1930419.227\n",
      "epoch: 572700, train loss: 3.8873215675354005, val loss: 3.9681521892547607, ETA in seconds: 1930292.976\n",
      "epoch: 572800, train loss: 3.8884761810302733, val loss: 3.973823833465576, ETA in seconds: 1930194.200\n",
      "epoch: 572900, train loss: 3.9022712469100953, val loss: 3.9676158666610717, ETA in seconds: 1930133.066\n",
      "epoch: 573000, train loss: 3.889802575111389, val loss: 3.9727159261703493, ETA in seconds: 1930060.448\n",
      "epoch: 573100, train loss: 3.8968775033950807, val loss: 3.9681113958358765, ETA in seconds: 1929949.600\n",
      "epoch: 573200, train loss: 3.892446231842041, val loss: 3.9738161087036135, ETA in seconds: 1929810.147\n",
      "epoch: 573300, train loss: 3.888138246536255, val loss: 3.9674367666244508, ETA in seconds: 1929672.051\n",
      "epoch: 573400, train loss: 3.894112992286682, val loss: 3.958867883682251, ETA in seconds: 1929535.489\n",
      "epoch: 573500, train loss: 3.8924008131027223, val loss: 3.9656458854675294, ETA in seconds: 1929452.561\n",
      "epoch: 573600, train loss: 3.89019980430603, val loss: 3.9692214012145994, ETA in seconds: 1929326.272\n",
      "epoch: 573700, train loss: 3.89379460811615, val loss: 3.9600067377090453, ETA in seconds: 1929200.574\n",
      "epoch: 573800, train loss: 3.9116315841674805, val loss: 3.9657684803009032, ETA in seconds: 1929072.264\n",
      "epoch: 573900, train loss: 3.8922125816345217, val loss: 3.967665123939514, ETA in seconds: 1928942.387\n",
      "epoch: 574000, train loss: 3.898350977897644, val loss: 3.9723335027694704, ETA in seconds: 1928826.514\n",
      "epoch: 574100, train loss: 3.894316220283508, val loss: 3.9690789461135862, ETA in seconds: 1928755.234\n",
      "epoch: 574200, train loss: 3.889731597900391, val loss: 3.9671158313751222, ETA in seconds: 1928687.015\n",
      "epoch: 574300, train loss: 3.896655488014221, val loss: 3.969675326347351, ETA in seconds: 1928597.853\n",
      "epoch: 574400, train loss: 3.900153136253357, val loss: 3.9750603675842284, ETA in seconds: 1928464.920\n",
      "epoch: 574500, train loss: 3.8936580419540405, val loss: 3.974943161010742, ETA in seconds: 1928342.392\n",
      "epoch: 574600, train loss: 3.889061522483826, val loss: 3.970524001121521, ETA in seconds: 1928203.442\n",
      "epoch: 574700, train loss: 3.879898524284363, val loss: 3.973460388183594, ETA in seconds: 1928058.679\n",
      "epoch: 574800, train loss: 3.896964979171753, val loss: 3.9682074785232544, ETA in seconds: 1927939.708\n",
      "epoch: 574900, train loss: 3.9033775329589844, val loss: 3.962562394142151, ETA in seconds: 1927806.085\n",
      "epoch: 575000, train loss: 3.8906123876571654, val loss: 3.957881736755371, ETA in seconds: 1927674.215\n",
      "epoch: 575100, train loss: 3.8923044443130492, val loss: 3.969687342643738, ETA in seconds: 1927539.313\n",
      "epoch: 575200, train loss: 3.887287664413452, val loss: 3.9688390254974366, ETA in seconds: 1927412.304\n",
      "epoch: 575300, train loss: 3.89252507686615, val loss: 3.958298182487488, ETA in seconds: 1927295.337\n",
      "epoch: 575400, train loss: 3.897087049484253, val loss: 3.968018913269043, ETA in seconds: 1927175.455\n",
      "epoch: 575500, train loss: 3.891469955444336, val loss: 3.9724257946014405, ETA in seconds: 1927131.364\n",
      "epoch: 575600, train loss: 3.8929027795791624, val loss: 3.9626015186309815, ETA in seconds: 1927005.778\n",
      "epoch: 575700, train loss: 3.8843945026397706, val loss: 3.9628090143203734, ETA in seconds: 1926887.002\n",
      "epoch: 575800, train loss: 3.882941961288452, val loss: 3.9669269323349, ETA in seconds: 1926776.950\n",
      "epoch: 575900, train loss: 3.8942719459533692, val loss: 3.9718024253845217, ETA in seconds: 1926699.615\n",
      "epoch: 576000, train loss: 3.8930359840393067, val loss: 3.963608431816101, ETA in seconds: 1926595.452\n",
      "epoch: 576100, train loss: 3.8935129165649416, val loss: 3.968861532211304, ETA in seconds: 1926487.038\n",
      "epoch: 576200, train loss: 3.8934376001358033, val loss: 3.967335844039917, ETA in seconds: 1926378.110\n",
      "epoch: 576300, train loss: 3.895220470428467, val loss: 3.9594276428222654, ETA in seconds: 1926228.656\n",
      "epoch: 576400, train loss: 3.9010725736618044, val loss: 3.976243567466736, ETA in seconds: 1926086.125\n",
      "epoch: 576500, train loss: 3.900407862663269, val loss: 3.961993455886841, ETA in seconds: 1925939.630\n",
      "epoch: 576600, train loss: 3.8868750095367433, val loss: 3.9651557207107544, ETA in seconds: 1925899.272\n",
      "epoch: 576700, train loss: 3.8979180335998533, val loss: 3.977898192405701, ETA in seconds: 1925855.153\n",
      "epoch: 576800, train loss: 3.8885172843933105, val loss: 3.970705842971802, ETA in seconds: 1925812.557\n",
      "epoch: 576900, train loss: 3.894035220146179, val loss: 3.9695839405059816, ETA in seconds: 1925763.732\n",
      "epoch: 577000, train loss: 3.8873243808746336, val loss: 3.9642279148101807, ETA in seconds: 1925722.267\n",
      "epoch: 577100, train loss: 3.892255735397339, val loss: 3.9666415452957153, ETA in seconds: 1925681.314\n",
      "epoch: 577200, train loss: 3.8918492794036865, val loss: 3.97622549533844, ETA in seconds: 1925639.548\n",
      "epoch: 577300, train loss: 3.8864877939224245, val loss: 3.964888668060303, ETA in seconds: 1925599.525\n",
      "epoch: 577400, train loss: 3.89401113986969, val loss: 3.968655896186829, ETA in seconds: 1925514.362\n",
      "epoch: 577500, train loss: 3.8931140184402464, val loss: 3.966543364524841, ETA in seconds: 1925357.907\n",
      "epoch: 577600, train loss: 3.8975557327270507, val loss: 3.9810916423797607, ETA in seconds: 1925215.862\n",
      "epoch: 577700, train loss: 3.888074827194214, val loss: 3.9735202550888062, ETA in seconds: 1925075.905\n",
      "epoch: 577800, train loss: 3.896806812286377, val loss: 3.968779373168945, ETA in seconds: 1924927.365\n",
      "epoch: 577900, train loss: 3.8885684490203856, val loss: 3.9578224658966064, ETA in seconds: 1924795.178\n",
      "epoch: 578000, train loss: 3.8952216148376464, val loss: 3.9632603883743287, ETA in seconds: 1924659.778\n",
      "epoch: 578100, train loss: 3.8893988847732546, val loss: 3.9762789011001587, ETA in seconds: 1924525.319\n",
      "epoch: 578200, train loss: 3.891719651222229, val loss: 3.9789572238922117, ETA in seconds: 1924388.604\n",
      "epoch: 578300, train loss: 3.889470076560974, val loss: 3.9749502658843996, ETA in seconds: 1924248.205\n",
      "epoch: 578400, train loss: 3.892397737503052, val loss: 3.974512457847595, ETA in seconds: 1924113.589\n",
      "epoch: 578500, train loss: 3.889583778381348, val loss: 3.971703815460205, ETA in seconds: 1923975.891\n",
      "epoch: 578600, train loss: 3.885597515106201, val loss: 3.9680533170700074, ETA in seconds: 1923844.541\n",
      "epoch: 578700, train loss: 3.8878599405288696, val loss: 3.9656962156295776, ETA in seconds: 1923699.078\n",
      "epoch: 578800, train loss: 3.8953093528747558, val loss: 3.960682225227356, ETA in seconds: 1923576.384\n",
      "epoch: 578900, train loss: 3.892045783996582, val loss: 3.9714328765869142, ETA in seconds: 1923439.130\n",
      "epoch: 579000, train loss: 3.8858711957931518, val loss: 3.9504643440246583, ETA in seconds: 1923296.229\n",
      "epoch: 579100, train loss: 3.897382354736328, val loss: 3.9823578596115112, ETA in seconds: 1923151.085\n",
      "epoch: 579200, train loss: 3.8974689245224, val loss: 3.969397521018982, ETA in seconds: 1923011.057\n",
      "epoch: 579300, train loss: 3.8974724054336547, val loss: 3.974458599090576, ETA in seconds: 1922865.058\n",
      "epoch: 579400, train loss: 3.8816282033920286, val loss: 3.9719391584396364, ETA in seconds: 1922716.362\n",
      "epoch: 579500, train loss: 3.8998764038085936, val loss: 3.9765586137771605, ETA in seconds: 1922580.170\n",
      "epoch: 579600, train loss: 3.892368221282959, val loss: 3.9694047927856446, ETA in seconds: 1922434.731\n",
      "epoch: 579700, train loss: 3.8988774538040163, val loss: 3.9831189632415773, ETA in seconds: 1922294.586\n",
      "epoch: 579800, train loss: 3.8959155321121215, val loss: 3.969439077377319, ETA in seconds: 1922149.603\n",
      "epoch: 579900, train loss: 3.8943413972854612, val loss: 3.9850577116012573, ETA in seconds: 1922002.225\n",
      "epoch: 580000, train loss: 3.8945072650909425, val loss: 3.9766814708709717, ETA in seconds: 1921855.037\n",
      "epoch: 580100, train loss: 3.894926357269287, val loss: 3.9755834341049194, ETA in seconds: 1921716.324\n",
      "epoch: 580200, train loss: 3.8868318796157837, val loss: 3.9723949909210203, ETA in seconds: 1921572.131\n",
      "epoch: 580300, train loss: 3.8930805206298826, val loss: 3.9641659021377564, ETA in seconds: 1921417.559\n",
      "epoch: 580400, train loss: 3.895448899269104, val loss: 3.9750431537628175, ETA in seconds: 1921278.090\n",
      "epoch: 580500, train loss: 3.891340899467468, val loss: 3.9698745250701903, ETA in seconds: 1921130.997\n",
      "epoch: 580600, train loss: 3.896141695976257, val loss: 3.9737119674682617, ETA in seconds: 1920993.169\n",
      "epoch: 580700, train loss: 3.892910289764404, val loss: 3.9654514312744142, ETA in seconds: 1920909.536\n",
      "epoch: 580800, train loss: 3.887735867500305, val loss: 3.968355655670166, ETA in seconds: 1920826.112\n",
      "epoch: 580900, train loss: 3.899450945854187, val loss: 3.977092814445496, ETA in seconds: 1920748.169\n",
      "epoch: 581000, train loss: 3.8987369775772094, val loss: 3.9730613470077514, ETA in seconds: 1920656.492\n",
      "epoch: 581100, train loss: 3.9073819160461425, val loss: 3.9646138191223144, ETA in seconds: 1920508.341\n",
      "epoch: 581200, train loss: 3.883412003517151, val loss: 3.965517210960388, ETA in seconds: 1920364.426\n",
      "epoch: 581300, train loss: 3.9018519163131713, val loss: 3.9798279762268067, ETA in seconds: 1920222.420\n",
      "epoch: 581400, train loss: 3.902060055732727, val loss: 3.9727077722549438, ETA in seconds: 1920067.160\n",
      "epoch: 581500, train loss: 3.8986855506896974, val loss: 3.9743497133255006, ETA in seconds: 1919911.501\n",
      "epoch: 581600, train loss: 3.897498941421509, val loss: 3.9637043714523315, ETA in seconds: 1919759.372\n",
      "epoch: 581700, train loss: 3.8929269075393678, val loss: 3.9771441221237183, ETA in seconds: 1919608.396\n",
      "epoch: 581800, train loss: 3.892837715148926, val loss: 3.9699970722198485, ETA in seconds: 1919471.161\n",
      "epoch: 581900, train loss: 3.8951818704605103, val loss: 3.9727015256881715, ETA in seconds: 1919385.923\n",
      "epoch: 582000, train loss: 3.88843891620636, val loss: 3.968553566932678, ETA in seconds: 1919299.928\n",
      "epoch: 582100, train loss: 3.898345470428467, val loss: 3.9781989812850953, ETA in seconds: 1919217.016\n",
      "epoch: 582200, train loss: 3.887049174308777, val loss: 3.9567298173904417, ETA in seconds: 1919082.336\n",
      "epoch: 582300, train loss: 3.8993331670761107, val loss: 3.975752568244934, ETA in seconds: 1918931.094\n",
      "epoch: 582400, train loss: 3.8872505903244017, val loss: 3.9753483295440675, ETA in seconds: 1918781.710\n",
      "epoch: 582500, train loss: 3.89417622089386, val loss: 3.970859980583191, ETA in seconds: 1918667.886\n",
      "epoch: 582600, train loss: 3.8926211833953857, val loss: 3.974416661262512, ETA in seconds: 1918520.358\n",
      "epoch: 582700, train loss: 3.8891507625579833, val loss: 3.971227741241455, ETA in seconds: 1918371.161\n",
      "epoch: 582800, train loss: 3.8905524730682375, val loss: 3.9723601818084715, ETA in seconds: 1918230.043\n",
      "epoch: 582900, train loss: 3.8879253387451174, val loss: 3.963563108444214, ETA in seconds: 1918074.600\n",
      "epoch: 583000, train loss: 3.897665548324585, val loss: 3.977103519439697, ETA in seconds: 1917917.571\n",
      "epoch: 583100, train loss: 3.889445948600769, val loss: 3.981420660018921, ETA in seconds: 1917761.416\n",
      "epoch: 583200, train loss: 3.892858386039734, val loss: 3.973978590965271, ETA in seconds: 1917609.422\n",
      "epoch: 583300, train loss: 3.8980473041534425, val loss: 3.9748602628707888, ETA in seconds: 1917458.371\n",
      "epoch: 583400, train loss: 3.9006555795669557, val loss: 3.9716256856918335, ETA in seconds: 1917306.982\n",
      "epoch: 583500, train loss: 3.8869148015975954, val loss: 3.978435516357422, ETA in seconds: 1917153.455\n",
      "epoch: 583600, train loss: 3.895235776901245, val loss: 3.9813560962677004, ETA in seconds: 1917008.422\n",
      "epoch: 583700, train loss: 3.893920111656189, val loss: 3.966555118560791, ETA in seconds: 1916869.646\n",
      "epoch: 583800, train loss: 3.9070685863494874, val loss: 3.976660943031311, ETA in seconds: 1916723.468\n",
      "epoch: 583900, train loss: 3.897515320777893, val loss: 3.9806407928466796, ETA in seconds: 1916573.930\n",
      "epoch: 584000, train loss: 3.9043615579605104, val loss: 3.980014181137085, ETA in seconds: 1916432.284\n",
      "epoch: 584100, train loss: 3.8977442264556883, val loss: 3.9803757429122926, ETA in seconds: 1916292.978\n",
      "epoch: 584200, train loss: 3.887408971786499, val loss: 3.9666038751602173, ETA in seconds: 1916142.261\n",
      "epoch: 584300, train loss: 3.8918468236923216, val loss: 3.9688193798065186, ETA in seconds: 1915999.704\n",
      "epoch: 584400, train loss: 3.8926274299621584, val loss: 3.9673340797424315, ETA in seconds: 1915853.429\n",
      "epoch: 584500, train loss: 3.8961881399154663, val loss: 3.970233607292175, ETA in seconds: 1915706.844\n",
      "epoch: 584600, train loss: 3.8891185522079468, val loss: 3.9716727495193482, ETA in seconds: 1915556.216\n",
      "epoch: 584700, train loss: 3.888974165916443, val loss: 3.9685626745224, ETA in seconds: 1915405.579\n",
      "epoch: 584800, train loss: 3.8934950113296507, val loss: 3.968390154838562, ETA in seconds: 1915254.176\n",
      "epoch: 584900, train loss: 3.896731472015381, val loss: 3.9755982875823976, ETA in seconds: 1915102.057\n",
      "epoch: 585000, train loss: 3.8977723836898805, val loss: 3.979314422607422, ETA in seconds: 1914948.378\n",
      "epoch: 585100, train loss: 3.8967506170272825, val loss: 3.976656270027161, ETA in seconds: 1914872.071\n",
      "epoch: 585200, train loss: 3.8946482658386232, val loss: 3.9627837419509886, ETA in seconds: 1914804.584\n",
      "epoch: 585300, train loss: 3.893032264709473, val loss: 3.9719056844711305, ETA in seconds: 1914637.065\n",
      "epoch: 585400, train loss: 3.8931459903717043, val loss: 3.967372941970825, ETA in seconds: 1914470.707\n",
      "epoch: 585500, train loss: 3.897406888008118, val loss: 3.977610373497009, ETA in seconds: 1914319.731\n",
      "epoch: 585600, train loss: 3.892626714706421, val loss: 3.969312644004822, ETA in seconds: 1914155.335\n",
      "epoch: 585700, train loss: 3.8992838859558105, val loss: 3.9713262796401976, ETA in seconds: 1914045.424\n",
      "epoch: 585800, train loss: 3.890958547592163, val loss: 3.966733145713806, ETA in seconds: 1913894.962\n",
      "epoch: 585900, train loss: 3.8886578559875487, val loss: 3.9722809314727785, ETA in seconds: 1913741.595\n",
      "epoch: 586000, train loss: 3.8940459966659544, val loss: 3.9695910930633547, ETA in seconds: 1913575.072\n",
      "epoch: 586100, train loss: 3.884239912033081, val loss: 3.97546706199646, ETA in seconds: 1913433.111\n",
      "epoch: 586200, train loss: 3.887116289138794, val loss: 3.9772533416748046, ETA in seconds: 1913287.878\n",
      "epoch: 586300, train loss: 3.8912158489227293, val loss: 3.968530035018921, ETA in seconds: 1913139.225\n",
      "epoch: 586400, train loss: 3.8824979305267333, val loss: 3.963120937347412, ETA in seconds: 1912986.179\n",
      "epoch: 586500, train loss: 3.888265776634216, val loss: 3.978665852546692, ETA in seconds: 1912833.933\n",
      "epoch: 586600, train loss: 3.890960621833801, val loss: 3.9770265340805055, ETA in seconds: 1912683.526\n",
      "epoch: 586700, train loss: 3.892816972732544, val loss: 3.9727163553237914, ETA in seconds: 1912528.512\n",
      "epoch: 586800, train loss: 3.8886996507644653, val loss: 3.973611903190613, ETA in seconds: 1912360.585\n",
      "epoch: 586900, train loss: 3.904047989845276, val loss: 3.9713864803314207, ETA in seconds: 1912200.378\n",
      "epoch: 587000, train loss: 3.888716530799866, val loss: 3.9663150548934936, ETA in seconds: 1912047.194\n",
      "epoch: 587100, train loss: 3.8958216428756716, val loss: 3.9903409481048584, ETA in seconds: 1911900.375\n",
      "epoch: 587200, train loss: 3.9046951293945313, val loss: 3.9715136528015136, ETA in seconds: 1911754.621\n",
      "epoch: 587300, train loss: 3.8887686491012574, val loss: 3.986333799362183, ETA in seconds: 1911605.521\n",
      "epoch: 587400, train loss: 3.900154137611389, val loss: 3.9750694274902343, ETA in seconds: 1911451.898\n",
      "epoch: 587500, train loss: 3.896466088294983, val loss: 3.968232250213623, ETA in seconds: 1911312.040\n",
      "epoch: 587600, train loss: 3.893883490562439, val loss: 3.964766788482666, ETA in seconds: 1911152.782\n",
      "epoch: 587700, train loss: 3.8935934782028196, val loss: 3.9813740491867065, ETA in seconds: 1910994.306\n",
      "epoch: 587800, train loss: 3.8971026659011843, val loss: 3.96499981880188, ETA in seconds: 1910846.796\n",
      "epoch: 587900, train loss: 3.8897960901260378, val loss: 3.9736531019210815, ETA in seconds: 1910780.068\n",
      "epoch: 588000, train loss: 3.8829333066940306, val loss: 3.978225255012512, ETA in seconds: 1910689.966\n",
      "epoch: 588100, train loss: 3.8945525169372557, val loss: 3.9792145013809206, ETA in seconds: 1910627.857\n",
      "epoch: 588200, train loss: 3.8924902200698854, val loss: 3.983939290046692, ETA in seconds: 1910507.351\n",
      "epoch: 588300, train loss: 3.896150064468384, val loss: 3.976405143737793, ETA in seconds: 1910360.659\n",
      "epoch: 588400, train loss: 3.8830934047698973, val loss: 3.969117045402527, ETA in seconds: 1910255.037\n",
      "epoch: 588500, train loss: 3.8956215143203736, val loss: 3.9671361446380615, ETA in seconds: 1910197.058\n",
      "epoch: 588600, train loss: 3.8970452785491942, val loss: 3.9849709033966065, ETA in seconds: 1910114.550\n",
      "epoch: 588700, train loss: 3.8981441259384155, val loss: 3.975084924697876, ETA in seconds: 1909997.908\n",
      "epoch: 588800, train loss: 3.8958465099334716, val loss: 3.973690962791443, ETA in seconds: 1909830.327\n",
      "epoch: 588900, train loss: 3.8973071813583373, val loss: 3.981974220275879, ETA in seconds: 1909655.340\n",
      "epoch: 589000, train loss: 3.8934245824813845, val loss: 3.9735909938812255, ETA in seconds: 1909479.796\n",
      "epoch: 589100, train loss: 3.890823149681091, val loss: 3.9771280765533445, ETA in seconds: 1909358.953\n",
      "epoch: 589200, train loss: 3.895282244682312, val loss: 3.9679915428161623, ETA in seconds: 1909196.303\n",
      "epoch: 589300, train loss: 3.9049163579940798, val loss: 3.9817628383636476, ETA in seconds: 1909069.409\n",
      "epoch: 589400, train loss: 3.8955637216567993, val loss: 3.979809021949768, ETA in seconds: 1909005.340\n",
      "epoch: 589500, train loss: 3.8887497186660767, val loss: 3.9805553913116456, ETA in seconds: 1908940.660\n",
      "epoch: 589600, train loss: 3.8907832145690917, val loss: 3.9788493156433105, ETA in seconds: 1908866.095\n",
      "epoch: 589700, train loss: 3.8946933507919312, val loss: 3.9795441150665285, ETA in seconds: 1908802.653\n",
      "epoch: 589800, train loss: 3.892260026931763, val loss: 3.9769577264785765, ETA in seconds: 1908724.366\n",
      "epoch: 589900, train loss: 3.8964232444763183, val loss: 3.973018455505371, ETA in seconds: 1908632.826\n",
      "epoch: 590000, train loss: 3.8991897821426393, val loss: 3.975108027458191, ETA in seconds: 1908479.719\n",
      "epoch: 590100, train loss: 3.8861944675445557, val loss: 3.9729328632354735, ETA in seconds: 1908327.311\n",
      "epoch: 590200, train loss: 3.8928049325942995, val loss: 3.9534346580505373, ETA in seconds: 1908175.164\n",
      "epoch: 590300, train loss: 3.895732593536377, val loss: 3.972192645072937, ETA in seconds: 1908014.602\n",
      "epoch: 590400, train loss: 3.8906650304794312, val loss: 3.9621463775634767, ETA in seconds: 1907870.059\n",
      "epoch: 590500, train loss: 3.896142506599426, val loss: 3.977022385597229, ETA in seconds: 1907745.928\n",
      "epoch: 590600, train loss: 3.889508366584778, val loss: 3.972422647476196, ETA in seconds: 1907587.882\n",
      "epoch: 590700, train loss: 3.8941178798675535, val loss: 3.97062292098999, ETA in seconds: 1907422.296\n",
      "epoch: 590800, train loss: 3.888779616355896, val loss: 3.9630077600479128, ETA in seconds: 1907273.135\n",
      "epoch: 590900, train loss: 3.8956828117370605, val loss: 3.9685975551605224, ETA in seconds: 1907176.031\n",
      "epoch: 591000, train loss: 3.8878110885620116, val loss: 3.9705906391143797, ETA in seconds: 1907075.155\n",
      "epoch: 591100, train loss: 3.890475845336914, val loss: 3.9701198816299437, ETA in seconds: 1906922.337\n",
      "epoch: 591200, train loss: 3.8944547891616823, val loss: 3.980827045440674, ETA in seconds: 1906759.509\n",
      "epoch: 591300, train loss: 3.8873138666152953, val loss: 3.975721311569214, ETA in seconds: 1906584.994\n",
      "epoch: 591400, train loss: 3.8941864013671874, val loss: 3.9599743843078614, ETA in seconds: 1906417.804\n",
      "epoch: 591500, train loss: 3.893989419937134, val loss: 3.968127655982971, ETA in seconds: 1906270.230\n",
      "epoch: 591600, train loss: 3.8968441247940064, val loss: 3.9759702682495117, ETA in seconds: 1906117.453\n",
      "epoch: 591700, train loss: 3.8861101150512694, val loss: 3.983337903022766, ETA in seconds: 1905986.589\n",
      "epoch: 591800, train loss: 3.9041933536529543, val loss: 3.974390721321106, ETA in seconds: 1905892.553\n",
      "epoch: 591900, train loss: 3.88945529460907, val loss: 3.9643296241760253, ETA in seconds: 1905789.951\n",
      "epoch: 592000, train loss: 3.896985673904419, val loss: 3.972704458236694, ETA in seconds: 1905645.466\n",
      "epoch: 592100, train loss: 3.889119362831116, val loss: 3.9783300161361694, ETA in seconds: 1905545.873\n",
      "epoch: 592200, train loss: 3.8959179878234864, val loss: 3.9738428831100463, ETA in seconds: 1905447.213\n",
      "epoch: 592300, train loss: 3.897392416000366, val loss: 3.9734543800354003, ETA in seconds: 1905351.523\n",
      "epoch: 592400, train loss: 3.9022923946380614, val loss: 3.9815545082092285, ETA in seconds: 1905255.970\n",
      "epoch: 592500, train loss: 3.899032711982727, val loss: 3.970477056503296, ETA in seconds: 1905118.643\n",
      "epoch: 592600, train loss: 3.886841917037964, val loss: 3.9734535217285156, ETA in seconds: 1904953.202\n",
      "epoch: 592700, train loss: 3.897763895988464, val loss: 3.962415671348572, ETA in seconds: 1904803.503\n",
      "epoch: 592800, train loss: 3.898981237411499, val loss: 3.9559459924697875, ETA in seconds: 1904642.542\n",
      "epoch: 592900, train loss: 3.9042898416519165, val loss: 3.976630163192749, ETA in seconds: 1904472.963\n",
      "epoch: 593000, train loss: 3.901513195037842, val loss: 3.9694614171981812, ETA in seconds: 1904314.708\n",
      "epoch: 593100, train loss: 3.88980872631073, val loss: 3.9695470571517943, ETA in seconds: 1904145.242\n",
      "epoch: 593200, train loss: 3.894280958175659, val loss: 3.9884485244750976, ETA in seconds: 1903982.440\n",
      "epoch: 593300, train loss: 3.883481502532959, val loss: 3.97721688747406, ETA in seconds: 1903803.640\n",
      "epoch: 593400, train loss: 3.8955278158187867, val loss: 3.9786760330200197, ETA in seconds: 1903626.703\n",
      "epoch: 593500, train loss: 3.89019238948822, val loss: 3.9766613960266115, ETA in seconds: 1903471.897\n",
      "epoch: 593600, train loss: 3.896455240249634, val loss: 3.9764900684356688, ETA in seconds: 1903386.622\n",
      "epoch: 593700, train loss: 3.895604372024536, val loss: 3.982203650474548, ETA in seconds: 1903308.853\n",
      "epoch: 593800, train loss: 3.890071892738342, val loss: 3.9710134983062746, ETA in seconds: 1903194.625\n",
      "epoch: 593900, train loss: 3.8977362871170045, val loss: 3.974749231338501, ETA in seconds: 1903041.070\n",
      "epoch: 594000, train loss: 3.8951143980026246, val loss: 3.975829577445984, ETA in seconds: 1902886.097\n",
      "epoch: 594100, train loss: 3.897951126098633, val loss: 3.9703834772109987, ETA in seconds: 1902759.199\n",
      "epoch: 594200, train loss: 3.8979455709457396, val loss: 3.978739333152771, ETA in seconds: 1902621.246\n",
      "epoch: 594300, train loss: 3.8906556129455567, val loss: 3.9751596212387086, ETA in seconds: 1902477.642\n",
      "epoch: 594400, train loss: 3.8924694776535036, val loss: 3.9845453023910524, ETA in seconds: 1902345.626\n",
      "epoch: 594500, train loss: 3.893290305137634, val loss: 3.9767479419708254, ETA in seconds: 1902204.544\n",
      "epoch: 594600, train loss: 3.9121338367462157, val loss: 3.9800217390060424, ETA in seconds: 1902060.616\n",
      "epoch: 594700, train loss: 3.897113037109375, val loss: 3.9764267206192017, ETA in seconds: 1901939.369\n",
      "epoch: 594800, train loss: 3.895002317428589, val loss: 3.986960196495056, ETA in seconds: 1901856.424\n",
      "epoch: 594900, train loss: 3.894933342933655, val loss: 3.974707818031311, ETA in seconds: 1901701.647\n",
      "epoch: 595000, train loss: 3.8918413877487184, val loss: 3.9749395132064818, ETA in seconds: 1901560.363\n",
      "epoch: 595100, train loss: 3.893523025512695, val loss: 3.984992003440857, ETA in seconds: 1901415.085\n",
      "epoch: 595200, train loss: 3.8912044048309324, val loss: 3.9844858169555666, ETA in seconds: 1901266.663\n",
      "epoch: 595300, train loss: 3.8875306129455565, val loss: 3.969149374961853, ETA in seconds: 1901118.301\n",
      "epoch: 595400, train loss: 3.8848811864852903, val loss: 3.9708601236343384, ETA in seconds: 1900979.574\n",
      "epoch: 595500, train loss: 3.892804431915283, val loss: 3.9747402667999268, ETA in seconds: 1900845.842\n",
      "epoch: 595600, train loss: 3.8867589235305786, val loss: 3.971380519866943, ETA in seconds: 1900729.437\n",
      "epoch: 595700, train loss: 3.9005711555480955, val loss: 3.9765321254730224, ETA in seconds: 1900646.232\n",
      "epoch: 595800, train loss: 3.8970808744430543, val loss: 3.961609959602356, ETA in seconds: 1900502.875\n",
      "epoch: 595900, train loss: 3.888366222381592, val loss: 3.971642279624939, ETA in seconds: 1900383.002\n",
      "epoch: 596000, train loss: 3.89740571975708, val loss: 3.9660862922668456, ETA in seconds: 1900272.442\n",
      "epoch: 596100, train loss: 3.8897863626480103, val loss: 3.9719557762145996, ETA in seconds: 1900162.292\n",
      "epoch: 596200, train loss: 3.8998496532440186, val loss: 3.9829029560089113, ETA in seconds: 1900051.523\n",
      "epoch: 596300, train loss: 3.8955273628234863, val loss: 3.9680233001708984, ETA in seconds: 1899942.442\n",
      "epoch: 596400, train loss: 3.888684105873108, val loss: 3.9610862731933594, ETA in seconds: 1899829.692\n",
      "epoch: 596500, train loss: 3.88764066696167, val loss: 3.98200318813324, ETA in seconds: 1899727.489\n",
      "epoch: 596600, train loss: 3.8906080961227416, val loss: 3.974152708053589, ETA in seconds: 1899592.142\n",
      "epoch: 596700, train loss: 3.8920531034469605, val loss: 3.972188425064087, ETA in seconds: 1899456.643\n",
      "epoch: 596800, train loss: 3.884439206123352, val loss: 3.9734107732772825, ETA in seconds: 1899342.557\n",
      "epoch: 596900, train loss: 3.8949609279632567, val loss: 3.979483699798584, ETA in seconds: 1899186.056\n",
      "epoch: 597000, train loss: 3.8928688287734987, val loss: 3.971576118469238, ETA in seconds: 1899015.296\n",
      "epoch: 597100, train loss: 3.8938451528549196, val loss: 3.9732816219329834, ETA in seconds: 1898850.998\n",
      "epoch: 597200, train loss: 3.9011138439178468, val loss: 3.9769814729690554, ETA in seconds: 1898675.117\n",
      "epoch: 597300, train loss: 3.8971785068511964, val loss: 3.9846959114074707, ETA in seconds: 1898547.471\n",
      "epoch: 597400, train loss: 3.8940999507904053, val loss: 3.9780106782913207, ETA in seconds: 1898470.592\n",
      "epoch: 597500, train loss: 3.901438355445862, val loss: 3.965106463432312, ETA in seconds: 1898363.130\n",
      "epoch: 597600, train loss: 3.8823225021362306, val loss: 3.9727792263031008, ETA in seconds: 1898205.817\n",
      "epoch: 597700, train loss: 3.8955585956573486, val loss: 3.971257448196411, ETA in seconds: 1898061.252\n",
      "epoch: 597800, train loss: 3.8970574855804445, val loss: 3.968043804168701, ETA in seconds: 1897913.956\n",
      "epoch: 597900, train loss: 3.897523546218872, val loss: 3.970619797706604, ETA in seconds: 1897749.390\n",
      "epoch: 598000, train loss: 3.894041562080383, val loss: 3.9753119707107545, ETA in seconds: 1897576.148\n",
      "epoch: 598100, train loss: 3.8939510345458985, val loss: 3.970051097869873, ETA in seconds: 1897409.167\n",
      "epoch: 598200, train loss: 3.8840639352798463, val loss: 3.9812949895858765, ETA in seconds: 1897278.095\n",
      "epoch: 598300, train loss: 3.89797728061676, val loss: 3.98217134475708, ETA in seconds: 1897115.308\n",
      "epoch: 598400, train loss: 3.891002368927002, val loss: 3.976955032348633, ETA in seconds: 1896959.555\n",
      "epoch: 598500, train loss: 3.90001540184021, val loss: 3.9698668479919434, ETA in seconds: 1896784.143\n",
      "epoch: 598600, train loss: 3.9046387910842895, val loss: 3.9764968872070314, ETA in seconds: 1896609.004\n",
      "epoch: 598700, train loss: 3.892904448509216, val loss: 3.9739125251770018, ETA in seconds: 1896434.248\n",
      "epoch: 598800, train loss: 3.8897035837173464, val loss: 3.968173885345459, ETA in seconds: 1896269.738\n",
      "epoch: 598900, train loss: 3.897466468811035, val loss: 3.9722513437271116, ETA in seconds: 1896162.353\n",
      "epoch: 599000, train loss: 3.8973732471466063, val loss: 3.962027835845947, ETA in seconds: 1896050.225\n",
      "epoch: 599100, train loss: 3.900761127471924, val loss: 3.9688783168792723, ETA in seconds: 1895921.909\n",
      "epoch: 599200, train loss: 3.89550199508667, val loss: 3.9778960704803468, ETA in seconds: 1895764.498\n",
      "epoch: 599300, train loss: 3.9039029836654664, val loss: 3.9702665328979494, ETA in seconds: 1895599.111\n",
      "epoch: 599400, train loss: 3.886769723892212, val loss: 3.981636381149292, ETA in seconds: 1895468.415\n",
      "epoch: 599500, train loss: 3.8999302864074705, val loss: 3.9799733638763426, ETA in seconds: 1895282.352\n",
      "epoch: 599600, train loss: 3.8901131629943846, val loss: 3.9746894359588625, ETA in seconds: 1895104.255\n",
      "epoch: 599700, train loss: 3.8943873167037966, val loss: 3.974252200126648, ETA in seconds: 1894941.060\n",
      "epoch: 599800, train loss: 3.8991785287857055, val loss: 3.966647672653198, ETA in seconds: 1894771.423\n",
      "epoch: 599900, train loss: 3.893918251991272, val loss: 3.974618411064148, ETA in seconds: 1894667.469\n",
      "epoch: 600000, train loss: 3.8899631977081297, val loss: 3.9782373666763307, ETA in seconds: 1894561.758\n",
      "epoch: 600100, train loss: 3.8976173877716063, val loss: 3.976642346382141, ETA in seconds: 1894457.510\n",
      "epoch: 600200, train loss: 3.891538310050964, val loss: 3.9779253959655763, ETA in seconds: 1894307.276\n",
      "epoch: 600300, train loss: 3.8978012084960936, val loss: 3.9651079893112184, ETA in seconds: 1894158.222\n",
      "epoch: 600400, train loss: 3.8855051040649413, val loss: 3.97350070476532, ETA in seconds: 1894004.253\n",
      "epoch: 600500, train loss: 3.900309133529663, val loss: 3.974074697494507, ETA in seconds: 1893848.635\n",
      "epoch: 600600, train loss: 3.8896647453308106, val loss: 3.9696958541870115, ETA in seconds: 1893684.807\n",
      "epoch: 600700, train loss: 3.886664700508118, val loss: 3.964212441444397, ETA in seconds: 1893558.463\n",
      "epoch: 600800, train loss: 3.901840853691101, val loss: 3.9751469612121584, ETA in seconds: 1893425.839\n",
      "epoch: 600900, train loss: 3.8985767364501953, val loss: 3.975663423538208, ETA in seconds: 1893308.723\n",
      "epoch: 601000, train loss: 3.8877402782440185, val loss: 3.9807632207870483, ETA in seconds: 1893177.139\n",
      "epoch: 601100, train loss: 3.872329759597778, val loss: 3.9708319425582888, ETA in seconds: 1893044.486\n",
      "epoch: 601200, train loss: 3.9072230577468874, val loss: 3.9719982147216797, ETA in seconds: 1892909.839\n",
      "epoch: 601300, train loss: 3.8907115936279295, val loss: 3.970386242866516, ETA in seconds: 1892766.506\n",
      "epoch: 601400, train loss: 3.896997594833374, val loss: 3.968042993545532, ETA in seconds: 1892612.867\n",
      "epoch: 601500, train loss: 3.8871665716171266, val loss: 3.977241611480713, ETA in seconds: 1892492.079\n",
      "epoch: 601600, train loss: 3.8932612895965577, val loss: 3.966783618927002, ETA in seconds: 1892341.066\n",
      "epoch: 601700, train loss: 3.9050583839416504, val loss: 3.972452902793884, ETA in seconds: 1892194.920\n",
      "epoch: 601800, train loss: 3.891162157058716, val loss: 3.9573262214660643, ETA in seconds: 1892048.030\n",
      "epoch: 601900, train loss: 3.8960628271102906, val loss: 3.9804481983184816, ETA in seconds: 1891901.484\n",
      "epoch: 602000, train loss: 3.888409161567688, val loss: 3.9768667221069336, ETA in seconds: 1891732.031\n",
      "epoch: 602100, train loss: 3.8933366775512694, val loss: 3.9732503414154055, ETA in seconds: 1891602.798\n",
      "epoch: 602200, train loss: 3.8935348272323607, val loss: 3.9716971635818483, ETA in seconds: 1891448.938\n",
      "epoch: 602300, train loss: 3.8979624271392823, val loss: 3.9557289600372316, ETA in seconds: 1891274.053\n",
      "epoch: 602400, train loss: 3.895864987373352, val loss: 3.967086982727051, ETA in seconds: 1891102.034\n",
      "epoch: 602500, train loss: 3.907795548439026, val loss: 3.9771107912063597, ETA in seconds: 1890931.225\n",
      "epoch: 602600, train loss: 3.8867236375808716, val loss: 3.9700057744979858, ETA in seconds: 1890763.436\n",
      "epoch: 602700, train loss: 3.8950140953063963, val loss: 3.9746463775634764, ETA in seconds: 1890588.938\n",
      "epoch: 602800, train loss: 3.889050841331482, val loss: 3.967568778991699, ETA in seconds: 1890418.851\n",
      "epoch: 602900, train loss: 3.892652750015259, val loss: 3.973143792152405, ETA in seconds: 1890249.316\n",
      "epoch: 603000, train loss: 3.882851576805115, val loss: 3.973702907562256, ETA in seconds: 1890077.933\n",
      "epoch: 603100, train loss: 3.898597979545593, val loss: 3.9606452703475954, ETA in seconds: 1889911.682\n",
      "epoch: 603200, train loss: 3.8928116083145143, val loss: 3.966900086402893, ETA in seconds: 1889736.857\n",
      "epoch: 603300, train loss: 3.886232042312622, val loss: 3.9741881847381593, ETA in seconds: 1889563.945\n",
      "epoch: 603400, train loss: 3.8999813556671143, val loss: 3.9712480783462523, ETA in seconds: 1889396.000\n",
      "epoch: 603500, train loss: 3.8918922424316404, val loss: 3.9736701011657716, ETA in seconds: 1889236.294\n",
      "epoch: 603600, train loss: 3.894735884666443, val loss: 3.977045679092407, ETA in seconds: 1889067.484\n",
      "epoch: 603700, train loss: 3.8854992151260377, val loss: 3.9733458518981934, ETA in seconds: 1888889.671\n",
      "epoch: 603800, train loss: 3.884782600402832, val loss: 3.9625895500183104, ETA in seconds: 1888710.139\n",
      "epoch: 603900, train loss: 3.896387553215027, val loss: 3.967296814918518, ETA in seconds: 1888531.643\n",
      "epoch: 604000, train loss: 3.899874043464661, val loss: 3.966897463798523, ETA in seconds: 1888354.085\n",
      "epoch: 604100, train loss: 3.8942091941833494, val loss: 3.9716375350952147, ETA in seconds: 1888186.255\n",
      "epoch: 604200, train loss: 3.887981653213501, val loss: 3.9737371683120726, ETA in seconds: 1888014.702\n",
      "epoch: 604300, train loss: 3.8986419439315796, val loss: 3.963492250442505, ETA in seconds: 1887848.981\n",
      "epoch: 604400, train loss: 3.8797322273254395, val loss: 3.969701051712036, ETA in seconds: 1887683.118\n",
      "epoch: 604500, train loss: 3.895162296295166, val loss: 3.973692297935486, ETA in seconds: 1887500.929\n",
      "epoch: 604600, train loss: 3.892417287826538, val loss: 3.97246835231781, ETA in seconds: 1887322.710\n",
      "epoch: 604700, train loss: 3.888302230834961, val loss: 3.962455463409424, ETA in seconds: 1887146.605\n",
      "epoch: 604800, train loss: 3.903115224838257, val loss: 3.9601958990097046, ETA in seconds: 1886982.518\n",
      "epoch: 604900, train loss: 3.8972198247909544, val loss: 3.980175805091858, ETA in seconds: 1886812.203\n",
      "epoch: 605000, train loss: 3.887612533569336, val loss: 3.9469656467437746, ETA in seconds: 1886630.651\n",
      "epoch: 605100, train loss: 3.9029029607772827, val loss: 3.9626399993896486, ETA in seconds: 1886448.011\n",
      "epoch: 605200, train loss: 3.901500105857849, val loss: 3.9705400705337524, ETA in seconds: 1886269.606\n",
      "epoch: 605300, train loss: 3.894480323791504, val loss: 3.9696555137634277, ETA in seconds: 1886087.856\n",
      "epoch: 605400, train loss: 3.889179515838623, val loss: 3.964365768432617, ETA in seconds: 1885907.577\n",
      "epoch: 605500, train loss: 3.889326238632202, val loss: 3.9682620763778687, ETA in seconds: 1885755.893\n",
      "epoch: 605600, train loss: 3.8916284799575807, val loss: 3.9683420658111572, ETA in seconds: 1885631.599\n",
      "epoch: 605700, train loss: 3.902342963218689, val loss: 3.973587965965271, ETA in seconds: 1885501.684\n",
      "epoch: 605800, train loss: 3.8977813005447386, val loss: 3.9706546783447267, ETA in seconds: 1885303.233\n",
      "epoch: 605900, train loss: 3.8890126943588257, val loss: 3.9624928712844847, ETA in seconds: 1885108.631\n",
      "epoch: 606000, train loss: 3.8920563459396362, val loss: 3.969152903556824, ETA in seconds: 1884978.435\n",
      "epoch: 606100, train loss: 3.8918456077575683, val loss: 3.9713138580322265, ETA in seconds: 1884846.321\n",
      "epoch: 606200, train loss: 3.885970664024353, val loss: 3.9703914642333986, ETA in seconds: 1884731.500\n",
      "epoch: 606300, train loss: 3.893819284439087, val loss: 3.9850426197052, ETA in seconds: 1884539.198\n",
      "epoch: 606400, train loss: 3.8874226331710817, val loss: 3.9743770599365233, ETA in seconds: 1884341.907\n",
      "epoch: 606500, train loss: 3.895956301689148, val loss: 3.9664921283721926, ETA in seconds: 1884141.173\n",
      "epoch: 606600, train loss: 3.899427056312561, val loss: 3.9646126985549928, ETA in seconds: 1883945.253\n",
      "epoch: 606700, train loss: 3.897151470184326, val loss: 3.9661382675170898, ETA in seconds: 1883744.139\n",
      "epoch: 606800, train loss: 3.891270089149475, val loss: 3.9718767166137696, ETA in seconds: 1883546.628\n",
      "epoch: 606900, train loss: 3.89394097328186, val loss: 3.9697139978408815, ETA in seconds: 1883348.856\n",
      "epoch: 607000, train loss: 3.8988112926483156, val loss: 3.970172667503357, ETA in seconds: 1883167.186\n",
      "epoch: 607100, train loss: 3.8950483083724974, val loss: 3.9628257274627687, ETA in seconds: 1882996.473\n",
      "epoch: 607200, train loss: 3.8939950704574584, val loss: 3.9616726636886597, ETA in seconds: 1882815.341\n",
      "epoch: 607300, train loss: 3.9033146619796755, val loss: 3.969013977050781, ETA in seconds: 1882631.624\n",
      "epoch: 607400, train loss: 3.8921732902526855, val loss: 3.963085412979126, ETA in seconds: 1882462.240\n",
      "epoch: 607500, train loss: 3.8945232391357423, val loss: 3.9608091354370116, ETA in seconds: 1882274.607\n",
      "epoch: 607600, train loss: 3.8855438232421875, val loss: 3.960983920097351, ETA in seconds: 1882085.810\n",
      "epoch: 607700, train loss: 3.8861175298690798, val loss: 3.967255187034607, ETA in seconds: 1881903.249\n",
      "epoch: 607800, train loss: 3.885804796218872, val loss: 3.9645626068115236, ETA in seconds: 1881721.998\n",
      "epoch: 607900, train loss: 3.8959811210632322, val loss: 3.969468903541565, ETA in seconds: 1881538.369\n",
      "epoch: 608000, train loss: 3.891404628753662, val loss: 3.963620924949646, ETA in seconds: 1881348.008\n",
      "epoch: 608100, train loss: 3.897950839996338, val loss: 3.962062358856201, ETA in seconds: 1881160.528\n",
      "epoch: 608200, train loss: 3.8976638078689576, val loss: 3.9599982261657716, ETA in seconds: 1880966.839\n",
      "epoch: 608300, train loss: 3.9033470153808594, val loss: 3.961730480194092, ETA in seconds: 1880768.699\n",
      "epoch: 608400, train loss: 3.88974552154541, val loss: 3.9618256092071533, ETA in seconds: 1880615.494\n",
      "epoch: 608500, train loss: 3.8945651769638063, val loss: 3.9808874845504763, ETA in seconds: 1880432.997\n",
      "epoch: 608600, train loss: 3.88568229675293, val loss: 3.9638693809509276, ETA in seconds: 1880249.118\n",
      "epoch: 608700, train loss: 3.898424220085144, val loss: 3.977089023590088, ETA in seconds: 1880065.872\n",
      "epoch: 608800, train loss: 3.894511890411377, val loss: 3.975262689590454, ETA in seconds: 1879894.810\n",
      "epoch: 608900, train loss: 3.889775848388672, val loss: 3.965129518508911, ETA in seconds: 1879712.344\n",
      "epoch: 609000, train loss: 3.904169774055481, val loss: 3.975392460823059, ETA in seconds: 1879530.101\n",
      "epoch: 609100, train loss: 3.8857980728149415, val loss: 3.976417660713196, ETA in seconds: 1879347.111\n",
      "epoch: 609200, train loss: 3.8982149839401243, val loss: 3.956931781768799, ETA in seconds: 1879156.918\n",
      "epoch: 609300, train loss: 3.896259617805481, val loss: 3.963241457939148, ETA in seconds: 1878965.160\n",
      "epoch: 609400, train loss: 3.900272035598755, val loss: 3.957733917236328, ETA in seconds: 1878802.208\n",
      "epoch: 609500, train loss: 3.894048547744751, val loss: 3.9820685148239137, ETA in seconds: 1878606.352\n",
      "epoch: 609600, train loss: 3.902788233757019, val loss: 3.9508013486862184, ETA in seconds: 1878428.272\n",
      "epoch: 609700, train loss: 3.894266200065613, val loss: 3.9762441396713255, ETA in seconds: 1878257.674\n",
      "epoch: 609800, train loss: 3.8925583124160767, val loss: 3.9730550765991213, ETA in seconds: 1878120.220\n",
      "epoch: 609900, train loss: 3.888214612007141, val loss: 3.971443033218384, ETA in seconds: 1877935.027\n",
      "epoch: 610000, train loss: 3.899259901046753, val loss: 3.9693411350250245, ETA in seconds: 1877771.787\n",
      "epoch: 610100, train loss: 3.894168496131897, val loss: 3.962768816947937, ETA in seconds: 1877607.261\n",
      "epoch: 610200, train loss: 3.8944375038146974, val loss: 3.9667279720306396, ETA in seconds: 1877413.661\n",
      "epoch: 610300, train loss: 3.895707058906555, val loss: 3.9715567350387575, ETA in seconds: 1877212.309\n",
      "epoch: 610400, train loss: 3.8861907005310057, val loss: 3.964362645149231, ETA in seconds: 1877042.181\n",
      "epoch: 610500, train loss: 3.891506719589233, val loss: 3.967468762397766, ETA in seconds: 1876912.728\n",
      "epoch: 610600, train loss: 3.890044689178467, val loss: 3.9772345781326295, ETA in seconds: 1876802.999\n",
      "epoch: 610700, train loss: 3.888535976409912, val loss: 3.9548354387283324, ETA in seconds: 1876676.105\n",
      "epoch: 610800, train loss: 3.896924543380737, val loss: 3.97798273563385, ETA in seconds: 1876494.072\n",
      "epoch: 610900, train loss: 3.8948901414871218, val loss: 3.9581244230270385, ETA in seconds: 1876360.060\n",
      "epoch: 611000, train loss: 3.900169849395752, val loss: 3.9620712041854858, ETA in seconds: 1876228.858\n",
      "epoch: 611100, train loss: 3.8885854721069335, val loss: 3.967002248764038, ETA in seconds: 1876096.606\n",
      "epoch: 611200, train loss: 3.899163579940796, val loss: 3.969159460067749, ETA in seconds: 1875964.425\n",
      "epoch: 611300, train loss: 3.8880048513412477, val loss: 3.962361478805542, ETA in seconds: 1875811.488\n",
      "epoch: 611400, train loss: 3.8868853569030763, val loss: 3.970709776878357, ETA in seconds: 1875602.746\n",
      "epoch: 611500, train loss: 3.894684886932373, val loss: 3.9680994510650636, ETA in seconds: 1875414.501\n",
      "epoch: 611600, train loss: 3.8979623556137084, val loss: 3.9792537927627563, ETA in seconds: 1875207.570\n",
      "epoch: 611700, train loss: 3.8961345195770263, val loss: 3.9769136905670166, ETA in seconds: 1875011.026\n",
      "epoch: 611800, train loss: 3.887424039840698, val loss: 3.956025242805481, ETA in seconds: 1874816.496\n",
      "epoch: 611900, train loss: 3.888201355934143, val loss: 3.9708189249038695, ETA in seconds: 1874627.280\n",
      "epoch: 612000, train loss: 3.8888861179351806, val loss: 3.9681222200393678, ETA in seconds: 1874429.134\n",
      "epoch: 612100, train loss: 3.892893671989441, val loss: 3.9678515195846558, ETA in seconds: 1874232.908\n",
      "epoch: 612200, train loss: 3.8864125967025758, val loss: 3.9670181274414062, ETA in seconds: 1874035.149\n",
      "epoch: 612300, train loss: 3.886299753189087, val loss: 3.9765135288238525, ETA in seconds: 1873835.883\n",
      "epoch: 612400, train loss: 3.9044805765151978, val loss: 3.975373387336731, ETA in seconds: 1873637.033\n",
      "epoch: 612500, train loss: 3.8937847137451174, val loss: 3.969178247451782, ETA in seconds: 1873446.440\n",
      "epoch: 612600, train loss: 3.888791227340698, val loss: 3.968644142150879, ETA in seconds: 1873249.772\n",
      "epoch: 612700, train loss: 3.886371064186096, val loss: 3.96187903881073, ETA in seconds: 1873053.407\n",
      "epoch: 612800, train loss: 3.8984222173690797, val loss: 3.9656780481338503, ETA in seconds: 1872851.664\n",
      "epoch: 612900, train loss: 3.9007098197937013, val loss: 3.9574481725692747, ETA in seconds: 1872649.431\n",
      "epoch: 613000, train loss: 3.8892106294631956, val loss: 3.952342462539673, ETA in seconds: 1872445.923\n",
      "epoch: 613100, train loss: 3.891853952407837, val loss: 3.968850779533386, ETA in seconds: 1872241.076\n",
      "epoch: 613200, train loss: 3.8922677993774415, val loss: 3.9615548133850096, ETA in seconds: 1872036.305\n",
      "epoch: 613300, train loss: 3.907113194465637, val loss: 3.972017741203308, ETA in seconds: 1871824.854\n",
      "epoch: 613400, train loss: 3.8998105764389037, val loss: 3.9631360292434694, ETA in seconds: 1871613.155\n",
      "epoch: 613500, train loss: 3.904680919647217, val loss: 3.9726402282714846, ETA in seconds: 1871422.493\n",
      "epoch: 613600, train loss: 3.8925183057785033, val loss: 3.959297609329224, ETA in seconds: 1871223.671\n",
      "epoch: 613700, train loss: 3.893975520133972, val loss: 3.971684956550598, ETA in seconds: 1871089.445\n",
      "epoch: 613800, train loss: 3.905216360092163, val loss: 3.966745471954346, ETA in seconds: 1870877.386\n",
      "epoch: 613900, train loss: 3.89344961643219, val loss: 3.9657294750213623, ETA in seconds: 1870666.368\n",
      "epoch: 614000, train loss: 3.8964223623275758, val loss: 3.964557242393494, ETA in seconds: 1870497.033\n",
      "epoch: 614100, train loss: 3.894197463989258, val loss: 3.9542828798294067, ETA in seconds: 1870290.270\n",
      "epoch: 614200, train loss: 3.896348214149475, val loss: 3.971067428588867, ETA in seconds: 1870095.119\n",
      "epoch: 614300, train loss: 3.905574584007263, val loss: 3.970415210723877, ETA in seconds: 1869879.953\n",
      "epoch: 614400, train loss: 3.8881468296051027, val loss: 3.981157398223877, ETA in seconds: 1869663.063\n",
      "epoch: 614500, train loss: 3.893197536468506, val loss: 3.9632102251052856, ETA in seconds: 1869450.441\n",
      "epoch: 614600, train loss: 3.893070650100708, val loss: 3.969512915611267, ETA in seconds: 1869276.130\n",
      "epoch: 614700, train loss: 3.9027066230773926, val loss: 3.9712764263153075, ETA in seconds: 1869108.815\n",
      "epoch: 614800, train loss: 3.9004709720611572, val loss: 3.9816739559173584, ETA in seconds: 1868901.325\n",
      "epoch: 614900, train loss: 3.8876859188079833, val loss: 3.974421167373657, ETA in seconds: 1868700.056\n",
      "epoch: 615000, train loss: 3.8796790838241577, val loss: 3.9677072763442993, ETA in seconds: 1868491.494\n",
      "epoch: 615100, train loss: 3.9021876573562624, val loss: 3.971054458618164, ETA in seconds: 1868278.602\n",
      "epoch: 615200, train loss: 3.902459192276001, val loss: 3.9590095043182374, ETA in seconds: 1868070.455\n",
      "epoch: 615300, train loss: 3.891537094116211, val loss: 3.9609174728393555, ETA in seconds: 1867864.814\n",
      "epoch: 615400, train loss: 3.8848942518234253, val loss: 3.9747894763946534, ETA in seconds: 1867680.988\n",
      "epoch: 615500, train loss: 3.894457221031189, val loss: 3.9637808561325074, ETA in seconds: 1867543.936\n",
      "epoch: 615600, train loss: 3.8884357213974, val loss: 3.968720316886902, ETA in seconds: 1867365.311\n",
      "epoch: 615700, train loss: 3.904279661178589, val loss: 3.977639603614807, ETA in seconds: 1867179.094\n",
      "epoch: 615800, train loss: 3.888658142089844, val loss: 3.97491774559021, ETA in seconds: 1867006.660\n",
      "epoch: 615900, train loss: 3.8989686250686644, val loss: 3.9663475275039675, ETA in seconds: 1866821.759\n",
      "epoch: 616000, train loss: 3.9002299308776855, val loss: 3.968327021598816, ETA in seconds: 1866647.976\n",
      "epoch: 616100, train loss: 3.89684579372406, val loss: 3.9735333204269407, ETA in seconds: 1866462.103\n",
      "epoch: 616200, train loss: 3.8971101522445677, val loss: 3.960607624053955, ETA in seconds: 1866269.398\n",
      "epoch: 616300, train loss: 3.888788414001465, val loss: 3.970282030105591, ETA in seconds: 1866077.144\n",
      "epoch: 616400, train loss: 3.897132325172424, val loss: 3.9669202089309694, ETA in seconds: 1865892.538\n",
      "epoch: 616500, train loss: 3.8960058450698853, val loss: 3.9800283908843994, ETA in seconds: 1865694.557\n",
      "epoch: 616600, train loss: 3.9028803586959837, val loss: 3.95963568687439, ETA in seconds: 1865496.080\n",
      "epoch: 616700, train loss: 3.888994574546814, val loss: 3.965750551223755, ETA in seconds: 1865309.053\n",
      "epoch: 616800, train loss: 3.8917479276657105, val loss: 3.9722294330596926, ETA in seconds: 1865120.841\n",
      "epoch: 616900, train loss: 3.8953003883361816, val loss: 3.976272535324097, ETA in seconds: 1864932.028\n",
      "epoch: 617000, train loss: 3.8890432834625246, val loss: 3.973356747627258, ETA in seconds: 1864745.858\n",
      "epoch: 617100, train loss: 3.892347264289856, val loss: 3.967627453804016, ETA in seconds: 1864581.771\n",
      "epoch: 617200, train loss: 3.8900027751922606, val loss: 3.963863754272461, ETA in seconds: 1864413.295\n",
      "epoch: 617300, train loss: 3.8932036638259886, val loss: 3.9690176010131837, ETA in seconds: 1864243.685\n",
      "epoch: 617400, train loss: 3.8919880867004393, val loss: 3.964350771903992, ETA in seconds: 1864128.254\n",
      "epoch: 617500, train loss: 3.8952026844024656, val loss: 3.9778234720230103, ETA in seconds: 1863984.973\n",
      "epoch: 617600, train loss: 3.8995262384414673, val loss: 3.9869811296463014, ETA in seconds: 1863815.774\n",
      "epoch: 617700, train loss: 3.891860508918762, val loss: 3.979131269454956, ETA in seconds: 1863689.143\n",
      "epoch: 617800, train loss: 3.8922027826309202, val loss: 3.9742831230163573, ETA in seconds: 1863558.098\n",
      "epoch: 617900, train loss: 3.905694031715393, val loss: 3.974105405807495, ETA in seconds: 1863416.875\n",
      "epoch: 618000, train loss: 3.8875983715057374, val loss: 3.9636552572250365, ETA in seconds: 1863261.750\n",
      "epoch: 618100, train loss: 3.900754380226135, val loss: 3.9708086252212524, ETA in seconds: 1863083.470\n",
      "epoch: 618200, train loss: 3.8904163360595705, val loss: 3.969871997833252, ETA in seconds: 1862887.705\n",
      "epoch: 618300, train loss: 3.893464136123657, val loss: 3.9737959623336794, ETA in seconds: 1862686.037\n",
      "epoch: 618400, train loss: 3.8963268518447878, val loss: 3.971272897720337, ETA in seconds: 1862490.991\n",
      "epoch: 618500, train loss: 3.9103845357894897, val loss: 3.969279670715332, ETA in seconds: 1862311.694\n",
      "epoch: 618600, train loss: 3.886172556877136, val loss: 3.9750675916671754, ETA in seconds: 1862099.286\n",
      "epoch: 618700, train loss: 3.885660004615784, val loss: 3.980301523208618, ETA in seconds: 1861891.041\n",
      "epoch: 618800, train loss: 3.9002617597579956, val loss: 3.9695259809494017, ETA in seconds: 1861691.627\n",
      "epoch: 618900, train loss: 3.888391947746277, val loss: 3.96818425655365, ETA in seconds: 1861512.538\n",
      "epoch: 619000, train loss: 3.8977821350097654, val loss: 3.968430781364441, ETA in seconds: 1861370.735\n",
      "epoch: 619100, train loss: 3.888856530189514, val loss: 3.9782877445220945, ETA in seconds: 1861179.626\n",
      "epoch: 619200, train loss: 3.8839298725128173, val loss: 3.9682115077972413, ETA in seconds: 1860975.739\n",
      "epoch: 619300, train loss: 3.8935715436935423, val loss: 3.963177490234375, ETA in seconds: 1860771.561\n",
      "epoch: 619400, train loss: 3.8933752536773683, val loss: 3.9726902723312376, ETA in seconds: 1860567.455\n",
      "epoch: 619500, train loss: 3.894911456108093, val loss: 3.984345245361328, ETA in seconds: 1860364.628\n",
      "epoch: 619600, train loss: 3.894689154624939, val loss: 3.9723043918609617, ETA in seconds: 1860172.067\n",
      "epoch: 619700, train loss: 3.8913604259490966, val loss: 3.9640839099884033, ETA in seconds: 1859991.088\n",
      "epoch: 619800, train loss: 3.898561143875122, val loss: 3.965375781059265, ETA in seconds: 1859779.748\n",
      "epoch: 619900, train loss: 3.88669593334198, val loss: 3.9647712230682375, ETA in seconds: 1859574.581\n",
      "epoch: 620000, train loss: 3.891895079612732, val loss: 3.9690283060073854, ETA in seconds: 1859375.773\n",
      "epoch: 620100, train loss: 3.896514081954956, val loss: 3.9714530229568483, ETA in seconds: 1859173.118\n",
      "epoch: 620200, train loss: 3.8928834438323974, val loss: 3.972037696838379, ETA in seconds: 1858966.497\n",
      "epoch: 620300, train loss: 3.8857913255691527, val loss: 3.9575852155685425, ETA in seconds: 1858790.594\n",
      "epoch: 620400, train loss: 3.8833955764770507, val loss: 3.962479591369629, ETA in seconds: 1858609.080\n",
      "epoch: 620500, train loss: 3.8899441957473755, val loss: 3.9701924085617066, ETA in seconds: 1858421.672\n",
      "epoch: 620600, train loss: 3.8928492784500124, val loss: 3.9638219118118285, ETA in seconds: 1858220.953\n",
      "epoch: 620700, train loss: 3.8937878131866457, val loss: 3.9720617294311524, ETA in seconds: 1858025.991\n",
      "epoch: 620800, train loss: 3.889673852920532, val loss: 3.9702965259552, ETA in seconds: 1857852.578\n",
      "epoch: 620900, train loss: 3.889180827140808, val loss: 3.959262251853943, ETA in seconds: 1857707.324\n",
      "epoch: 621000, train loss: 3.898277449607849, val loss: 3.9662241458892824, ETA in seconds: 1857572.021\n",
      "epoch: 621100, train loss: 3.8910861253738402, val loss: 3.9610647916793824, ETA in seconds: 1857372.006\n",
      "epoch: 621200, train loss: 3.8852888107299806, val loss: 3.970509433746338, ETA in seconds: 1857174.811\n",
      "epoch: 621300, train loss: 3.8954504489898683, val loss: 3.964147663116455, ETA in seconds: 1856964.927\n",
      "epoch: 621400, train loss: 3.8855601072311403, val loss: 3.976617765426636, ETA in seconds: 1856761.789\n",
      "epoch: 621500, train loss: 3.9039072513580324, val loss: 3.972214412689209, ETA in seconds: 1856581.583\n",
      "epoch: 621600, train loss: 3.8865603685379027, val loss: 3.9791595220565794, ETA in seconds: 1856395.466\n",
      "epoch: 621700, train loss: 3.897128462791443, val loss: 3.9594991207122803, ETA in seconds: 1856244.128\n",
      "epoch: 621800, train loss: 3.894611692428589, val loss: 3.962992477416992, ETA in seconds: 1856113.625\n",
      "epoch: 621900, train loss: 3.8937730312347414, val loss: 3.970314383506775, ETA in seconds: 1855928.787\n",
      "epoch: 622000, train loss: 3.9000205993652344, val loss: 3.966607856750488, ETA in seconds: 1855738.923\n",
      "epoch: 622100, train loss: 3.887224626541138, val loss: 3.970105266571045, ETA in seconds: 1855534.651\n",
      "epoch: 622200, train loss: 3.8948078632354735, val loss: 3.958806777000427, ETA in seconds: 1855326.751\n",
      "epoch: 622300, train loss: 3.889609169960022, val loss: 3.9690110445022584, ETA in seconds: 1855119.652\n",
      "epoch: 622400, train loss: 3.891221284866333, val loss: 3.9642635345458985, ETA in seconds: 1854932.111\n",
      "epoch: 622500, train loss: 3.887556791305542, val loss: 3.9697861433029176, ETA in seconds: 1854723.658\n",
      "epoch: 622600, train loss: 3.8923665285110474, val loss: 3.96946177482605, ETA in seconds: 1854540.162\n",
      "epoch: 622700, train loss: 3.8866982221603394, val loss: 3.9703017473220825, ETA in seconds: 1854389.675\n",
      "epoch: 622800, train loss: 3.8876535892486572, val loss: 3.964666652679443, ETA in seconds: 1854221.245\n",
      "epoch: 622900, train loss: 3.892995238304138, val loss: 3.9769031286239622, ETA in seconds: 1854037.322\n",
      "epoch: 623000, train loss: 3.8887871742248534, val loss: 3.9707293272018434, ETA in seconds: 1853888.458\n",
      "epoch: 623100, train loss: 3.8979650259017946, val loss: 3.9676276206970216, ETA in seconds: 1853743.029\n",
      "epoch: 623200, train loss: 3.8842255115509032, val loss: 3.9674073219299317, ETA in seconds: 1853596.057\n",
      "epoch: 623300, train loss: 3.891767144203186, val loss: 3.975280427932739, ETA in seconds: 1853398.041\n",
      "epoch: 623400, train loss: 3.9012250185012816, val loss: 3.978266143798828, ETA in seconds: 1853199.604\n",
      "epoch: 623500, train loss: 3.897236156463623, val loss: 3.983188533782959, ETA in seconds: 1853019.899\n",
      "epoch: 623600, train loss: 3.882732105255127, val loss: 3.957540440559387, ETA in seconds: 1852839.607\n",
      "epoch: 623700, train loss: 3.896789813041687, val loss: 3.9851876735687255, ETA in seconds: 1852677.772\n",
      "epoch: 623800, train loss: 3.893028402328491, val loss: 3.9785810708999634, ETA in seconds: 1852488.975\n",
      "epoch: 623900, train loss: 3.895557236671448, val loss: 3.9639249801635743, ETA in seconds: 1852293.833\n",
      "epoch: 624000, train loss: 3.898158812522888, val loss: 3.967804431915283, ETA in seconds: 1852099.322\n",
      "epoch: 624100, train loss: 3.8906676530838014, val loss: 3.9757391691207884, ETA in seconds: 1851896.452\n",
      "epoch: 624200, train loss: 3.9046298980712892, val loss: 3.9666992902755736, ETA in seconds: 1851698.677\n",
      "epoch: 624300, train loss: 3.910175824165344, val loss: 3.9769243001937866, ETA in seconds: 1851491.157\n",
      "epoch: 624400, train loss: 3.8903429746627807, val loss: 3.9728941679000855, ETA in seconds: 1851287.416\n",
      "epoch: 624500, train loss: 3.89011492729187, val loss: 3.979290270805359, ETA in seconds: 1851083.555\n",
      "epoch: 624600, train loss: 3.89846293926239, val loss: 3.9735348224639893, ETA in seconds: 1850877.214\n",
      "epoch: 624700, train loss: 3.893805480003357, val loss: 3.981316828727722, ETA in seconds: 1850686.231\n",
      "epoch: 624800, train loss: 3.8930514812469483, val loss: 3.98091037273407, ETA in seconds: 1850487.954\n",
      "epoch: 624900, train loss: 3.889563488960266, val loss: 3.9773377180099487, ETA in seconds: 1850288.306\n",
      "epoch: 625000, train loss: 3.9000556230545045, val loss: 3.971563291549683, ETA in seconds: 1850084.050\n",
      "epoch: 625100, train loss: 3.894399642944336, val loss: 3.984772515296936, ETA in seconds: 1849885.795\n",
      "epoch: 625200, train loss: 3.8955904483795165, val loss: 3.968957543373108, ETA in seconds: 1849678.103\n",
      "epoch: 625300, train loss: 3.8927571535110475, val loss: 3.9715094804763793, ETA in seconds: 1849484.710\n",
      "epoch: 625400, train loss: 3.891190457344055, val loss: 3.9734470367431642, ETA in seconds: 1849283.799\n",
      "epoch: 625500, train loss: 3.9009081363677978, val loss: 3.971398186683655, ETA in seconds: 1849085.249\n",
      "epoch: 625600, train loss: 3.896430015563965, val loss: 3.9786465644836424, ETA in seconds: 1848922.329\n",
      "epoch: 625700, train loss: 3.894752788543701, val loss: 3.988004541397095, ETA in seconds: 1848712.915\n",
      "epoch: 625800, train loss: 3.8975420951843263, val loss: 3.96775369644165, ETA in seconds: 1848509.088\n",
      "epoch: 625900, train loss: 3.8950324058532715, val loss: 3.9692781925201417, ETA in seconds: 1848306.205\n",
      "epoch: 626000, train loss: 3.899026322364807, val loss: 3.972653794288635, ETA in seconds: 1848179.913\n",
      "epoch: 626100, train loss: 3.892980718612671, val loss: 3.973152446746826, ETA in seconds: 1848032.946\n",
      "epoch: 626200, train loss: 3.8876192808151244, val loss: 3.9619049787521363, ETA in seconds: 1847846.628\n",
      "epoch: 626300, train loss: 3.8906381130218506, val loss: 3.9778411626815795, ETA in seconds: 1847656.397\n",
      "epoch: 626400, train loss: 3.8983235120773316, val loss: 3.9734593868255614, ETA in seconds: 1847448.791\n",
      "epoch: 626500, train loss: 3.887286376953125, val loss: 3.973878288269043, ETA in seconds: 1847255.734\n",
      "epoch: 626600, train loss: 3.8877763986587524, val loss: 3.9660497903823853, ETA in seconds: 1847092.872\n",
      "epoch: 626700, train loss: 3.900842070579529, val loss: 3.9692692518234254, ETA in seconds: 1846928.994\n",
      "epoch: 626800, train loss: 3.8923821210861207, val loss: 3.9720449686050414, ETA in seconds: 1846767.410\n",
      "epoch: 626900, train loss: 3.891415333747864, val loss: 3.978362274169922, ETA in seconds: 1846608.116\n",
      "epoch: 627000, train loss: 3.8927272081375124, val loss: 3.9784372091293334, ETA in seconds: 1846445.869\n",
      "epoch: 627100, train loss: 3.8912450790405275, val loss: 3.9839492559432985, ETA in seconds: 1846284.118\n",
      "epoch: 627200, train loss: 3.8894872426986695, val loss: 3.9898239850997923, ETA in seconds: 1846077.516\n",
      "epoch: 627300, train loss: 3.896759605407715, val loss: 3.9663413763046265, ETA in seconds: 1845850.063\n",
      "epoch: 627400, train loss: 3.896842098236084, val loss: 3.970325469970703, ETA in seconds: 1845623.887\n",
      "epoch: 627500, train loss: 3.8989831686019896, val loss: 3.9667904376983643, ETA in seconds: 1845412.506\n",
      "epoch: 627600, train loss: 3.888745379447937, val loss: 3.973304867744446, ETA in seconds: 1845196.248\n",
      "epoch: 627700, train loss: 3.892901372909546, val loss: 3.9786860942840576, ETA in seconds: 1844978.002\n",
      "epoch: 627800, train loss: 3.8943437814712523, val loss: 3.976215434074402, ETA in seconds: 1844760.529\n",
      "epoch: 627900, train loss: 3.893983793258667, val loss: 3.98299925327301, ETA in seconds: 1844579.662\n",
      "epoch: 628000, train loss: 3.8901772260665894, val loss: 3.966105651855469, ETA in seconds: 1844420.333\n",
      "epoch: 628100, train loss: 3.9031868457794188, val loss: 3.9685904026031493, ETA in seconds: 1844258.736\n",
      "epoch: 628200, train loss: 3.899005961418152, val loss: 3.9808136940002443, ETA in seconds: 1844054.991\n",
      "epoch: 628300, train loss: 3.8942827224731444, val loss: 3.9700573444366456, ETA in seconds: 1843880.833\n",
      "epoch: 628400, train loss: 3.888416385650635, val loss: 3.979142665863037, ETA in seconds: 1843725.826\n",
      "epoch: 628500, train loss: 3.901138424873352, val loss: 3.953734517097473, ETA in seconds: 1843581.860\n",
      "epoch: 628600, train loss: 3.902434158325195, val loss: 3.9742743730545045, ETA in seconds: 1843421.782\n",
      "epoch: 628700, train loss: 3.8894817352294924, val loss: 3.9662997722625732, ETA in seconds: 1843241.805\n",
      "epoch: 628800, train loss: 3.8942038297653196, val loss: 3.9675987482070925, ETA in seconds: 1843033.548\n",
      "epoch: 628900, train loss: 3.903460216522217, val loss: 3.963575339317322, ETA in seconds: 1842870.366\n",
      "epoch: 629000, train loss: 3.883829927444458, val loss: 3.9627471208572387, ETA in seconds: 1842709.855\n",
      "epoch: 629100, train loss: 3.882331919670105, val loss: 3.9654850482940676, ETA in seconds: 1842548.676\n",
      "epoch: 629200, train loss: 3.894090270996094, val loss: 3.983956217765808, ETA in seconds: 1842388.873\n",
      "epoch: 629300, train loss: 3.896587872505188, val loss: 3.9611384868621826, ETA in seconds: 1842186.873\n",
      "epoch: 629400, train loss: 3.8937973260879515, val loss: 3.9755630254745484, ETA in seconds: 1841969.638\n",
      "epoch: 629500, train loss: 3.8985105037689207, val loss: 3.9623813152313234, ETA in seconds: 1841761.127\n",
      "epoch: 629600, train loss: 3.8881752729415893, val loss: 3.975038433074951, ETA in seconds: 1841556.389\n",
      "epoch: 629700, train loss: 3.8985258102416993, val loss: 3.9721948862075807, ETA in seconds: 1841325.253\n",
      "epoch: 629800, train loss: 3.9069003105163573, val loss: 3.9800240278244017, ETA in seconds: 1841094.298\n",
      "epoch: 629900, train loss: 3.8988755702972413, val loss: 3.965660285949707, ETA in seconds: 1840862.843\n",
      "epoch: 630000, train loss: 3.884617638587952, val loss: 3.9604974746704102, ETA in seconds: 1840650.763\n",
      "epoch: 630100, train loss: 3.8950443267822266, val loss: 3.9701265335083007, ETA in seconds: 1840461.472\n",
      "epoch: 630200, train loss: 3.8947015762329102, val loss: 3.967824864387512, ETA in seconds: 1840269.832\n",
      "epoch: 630300, train loss: 3.8939767599105837, val loss: 3.9704861879348754, ETA in seconds: 1840077.078\n",
      "epoch: 630400, train loss: 3.89609637260437, val loss: 3.973697233200073, ETA in seconds: 1839852.456\n",
      "epoch: 630500, train loss: 3.8921560764312746, val loss: 3.9718333959579466, ETA in seconds: 1839639.015\n",
      "epoch: 630600, train loss: 3.8979499340057373, val loss: 3.961367893218994, ETA in seconds: 1839425.888\n",
      "epoch: 630700, train loss: 3.8966606140136717, val loss: 3.959021306037903, ETA in seconds: 1839202.378\n",
      "epoch: 630800, train loss: 3.900051975250244, val loss: 3.975472903251648, ETA in seconds: 1838978.587\n",
      "epoch: 630900, train loss: 3.8898004293441772, val loss: 3.9724982261657713, ETA in seconds: 1838748.619\n",
      "epoch: 631000, train loss: 3.8909352302551268, val loss: 3.969222140312195, ETA in seconds: 1838513.530\n",
      "epoch: 631100, train loss: 3.891122841835022, val loss: 3.968536376953125, ETA in seconds: 1838288.536\n",
      "epoch: 631200, train loss: 3.895014834403992, val loss: 3.965578389167786, ETA in seconds: 1838077.190\n",
      "epoch: 631300, train loss: 3.8925485134124758, val loss: 3.975866270065308, ETA in seconds: 1837852.011\n",
      "epoch: 631400, train loss: 3.8977534770965576, val loss: 3.9667731285095216, ETA in seconds: 1837625.229\n",
      "epoch: 631500, train loss: 3.8960528135299684, val loss: 3.9791500329971314, ETA in seconds: 1837441.253\n",
      "epoch: 631600, train loss: 3.906903862953186, val loss: 3.96548068523407, ETA in seconds: 1837221.699\n",
      "epoch: 631700, train loss: 3.894854736328125, val loss: 3.9657458782196047, ETA in seconds: 1837001.464\n",
      "epoch: 631800, train loss: 3.8921040058135987, val loss: 3.959607148170471, ETA in seconds: 1836773.895\n",
      "epoch: 631900, train loss: 3.892379570007324, val loss: 3.9747414112091066, ETA in seconds: 1836546.806\n",
      "epoch: 632000, train loss: 3.887689304351807, val loss: 3.9802809953689575, ETA in seconds: 1836319.271\n",
      "epoch: 632100, train loss: 3.889291787147522, val loss: 3.9731648445129393, ETA in seconds: 1836093.079\n",
      "epoch: 632200, train loss: 3.892736887931824, val loss: 3.965717840194702, ETA in seconds: 1835872.319\n",
      "epoch: 632300, train loss: 3.8842275619506834, val loss: 3.975052523612976, ETA in seconds: 1835648.960\n",
      "epoch: 632400, train loss: 3.8868513107299805, val loss: 3.9778420448303224, ETA in seconds: 1835429.028\n",
      "epoch: 632500, train loss: 3.887464928627014, val loss: 3.9682867765426635, ETA in seconds: 1835214.549\n",
      "epoch: 632600, train loss: 3.8860753059387205, val loss: 3.962828588485718, ETA in seconds: 1834999.126\n",
      "epoch: 632700, train loss: 3.894847536087036, val loss: 3.961943340301514, ETA in seconds: 1834781.626\n",
      "epoch: 632800, train loss: 3.8953262329101563, val loss: 3.981929564476013, ETA in seconds: 1834562.565\n",
      "epoch: 632900, train loss: 3.8846292972564695, val loss: 3.987275791168213, ETA in seconds: 1834383.526\n",
      "epoch: 633000, train loss: 3.8949750661849976, val loss: 3.974355959892273, ETA in seconds: 1834164.014\n",
      "epoch: 633100, train loss: 3.8876004219055176, val loss: 3.9705934524536133, ETA in seconds: 1833948.892\n",
      "epoch: 633200, train loss: 3.889646625518799, val loss: 3.964108920097351, ETA in seconds: 1833720.589\n",
      "epoch: 633300, train loss: 3.9034729957580567, val loss: 3.9742485523223876, ETA in seconds: 1833497.262\n",
      "epoch: 633400, train loss: 3.892046070098877, val loss: 3.9773102760314942, ETA in seconds: 1833273.460\n",
      "epoch: 633500, train loss: 3.8806044101715087, val loss: 3.9775784254074096, ETA in seconds: 1833047.943\n",
      "epoch: 633600, train loss: 3.9077463865280153, val loss: 3.9733465671539308, ETA in seconds: 1832825.523\n",
      "epoch: 633700, train loss: 3.8959028244018556, val loss: 3.9742619037628173, ETA in seconds: 1832605.747\n",
      "epoch: 633800, train loss: 3.898823094367981, val loss: 3.974703240394592, ETA in seconds: 1832387.888\n",
      "epoch: 633900, train loss: 3.8999649047851563, val loss: 3.9748568296432496, ETA in seconds: 1832176.813\n",
      "epoch: 634000, train loss: 3.9032811164855956, val loss: 3.9706251859664916, ETA in seconds: 1831959.082\n",
      "epoch: 634100, train loss: 3.8881638765335085, val loss: 3.962729287147522, ETA in seconds: 1831741.967\n",
      "epoch: 634200, train loss: 3.9008949756622315, val loss: 3.980945420265198, ETA in seconds: 1831525.969\n",
      "epoch: 634300, train loss: 3.8987498044967652, val loss: 3.974098324775696, ETA in seconds: 1831303.236\n",
      "epoch: 634400, train loss: 3.898049759864807, val loss: 3.978825068473816, ETA in seconds: 1831080.429\n",
      "epoch: 634500, train loss: 3.8920571565628053, val loss: 3.9672147035598755, ETA in seconds: 1830858.191\n",
      "epoch: 634600, train loss: 3.901387882232666, val loss: 3.965881109237671, ETA in seconds: 1830640.644\n",
      "epoch: 634700, train loss: 3.885797882080078, val loss: 3.9766196250915526, ETA in seconds: 1830414.258\n",
      "epoch: 634800, train loss: 3.886124539375305, val loss: 3.9640859603881835, ETA in seconds: 1830214.223\n",
      "epoch: 634900, train loss: 3.883893609046936, val loss: 3.9693172693252565, ETA in seconds: 1830057.329\n",
      "epoch: 635000, train loss: 3.8867483615875242, val loss: 3.970368981361389, ETA in seconds: 1829839.383\n",
      "epoch: 635100, train loss: 3.893670845031738, val loss: 3.9782456874847414, ETA in seconds: 1829620.764\n",
      "epoch: 635200, train loss: 3.8898558378219605, val loss: 3.977082943916321, ETA in seconds: 1829408.606\n",
      "epoch: 635300, train loss: 3.888050150871277, val loss: 3.9703719854354858, ETA in seconds: 1829194.861\n",
      "epoch: 635400, train loss: 3.897167682647705, val loss: 3.9732561111450195, ETA in seconds: 1828986.056\n",
      "epoch: 635500, train loss: 3.900228571891785, val loss: 3.9766525983810426, ETA in seconds: 1828767.724\n",
      "epoch: 635600, train loss: 3.890553879737854, val loss: 3.981373167037964, ETA in seconds: 1828555.225\n",
      "epoch: 635700, train loss: 3.8853707790374754, val loss: 3.9678397178649902, ETA in seconds: 1828340.524\n",
      "epoch: 635800, train loss: 3.8915287017822267, val loss: 3.971122670173645, ETA in seconds: 1828134.886\n",
      "epoch: 635900, train loss: 3.8945902585983276, val loss: 3.979555130004883, ETA in seconds: 1827959.019\n",
      "epoch: 636000, train loss: 3.89412579536438, val loss: 3.9725131511688234, ETA in seconds: 1827781.021\n",
      "epoch: 636100, train loss: 3.892797327041626, val loss: 3.9737480163574217, ETA in seconds: 1827615.376\n",
      "epoch: 636200, train loss: 3.8910197257995605, val loss: 3.9769710540771483, ETA in seconds: 1827385.435\n",
      "epoch: 636300, train loss: 3.887941074371338, val loss: 3.976522135734558, ETA in seconds: 1827143.946\n",
      "epoch: 636400, train loss: 3.896975302696228, val loss: 3.9746426343917847, ETA in seconds: 1826940.229\n",
      "epoch: 636500, train loss: 3.895894479751587, val loss: 3.9838635683059693, ETA in seconds: 1826727.279\n",
      "epoch: 636600, train loss: 3.8930180311203, val loss: 3.9524726390838625, ETA in seconds: 1826517.388\n",
      "epoch: 636700, train loss: 3.9045135974884033, val loss: 3.977738046646118, ETA in seconds: 1826320.194\n",
      "epoch: 636800, train loss: 3.9030662059783934, val loss: 3.963398551940918, ETA in seconds: 1826129.750\n",
      "epoch: 636900, train loss: 3.8878162622451784, val loss: 3.968465805053711, ETA in seconds: 1825900.375\n",
      "epoch: 637000, train loss: 3.8901999235153197, val loss: 3.9710641622543337, ETA in seconds: 1825696.682\n",
      "epoch: 637100, train loss: 3.890395426750183, val loss: 3.9782724618911742, ETA in seconds: 1825514.299\n",
      "epoch: 637200, train loss: 3.898968505859375, val loss: 3.986099433898926, ETA in seconds: 1825330.602\n",
      "epoch: 637300, train loss: 3.896504783630371, val loss: 3.9808667421340944, ETA in seconds: 1825149.424\n",
      "epoch: 637400, train loss: 3.8933507680892943, val loss: 3.9808453798294066, ETA in seconds: 1824972.740\n",
      "epoch: 637500, train loss: 3.898013114929199, val loss: 3.9691126108169557, ETA in seconds: 1824816.231\n",
      "epoch: 637600, train loss: 3.8895233154296873, val loss: 3.9715364933013917, ETA in seconds: 1824659.212\n",
      "epoch: 637700, train loss: 3.8865264892578124, val loss: 3.965195322036743, ETA in seconds: 1824508.834\n",
      "epoch: 637800, train loss: 3.903605580329895, val loss: 3.9671493053436278, ETA in seconds: 1824314.851\n",
      "epoch: 637900, train loss: 3.9092130184173586, val loss: 3.973538064956665, ETA in seconds: 1824079.335\n",
      "epoch: 638000, train loss: 3.8884734869003297, val loss: 3.9776172399520875, ETA in seconds: 1823839.583\n",
      "epoch: 638100, train loss: 3.891408157348633, val loss: 3.9820536851882933, ETA in seconds: 1823595.588\n",
      "epoch: 638200, train loss: 3.9063876628875733, val loss: 3.9640154361724855, ETA in seconds: 1823372.895\n",
      "epoch: 638300, train loss: 3.89713397026062, val loss: 3.976140522956848, ETA in seconds: 1823145.860\n",
      "epoch: 638400, train loss: 3.8863894939422607, val loss: 3.9676059246063233, ETA in seconds: 1822905.440\n",
      "epoch: 638500, train loss: 3.8934120893478394, val loss: 3.973151755332947, ETA in seconds: 1822680.187\n",
      "epoch: 638600, train loss: 3.894975733757019, val loss: 3.969486427307129, ETA in seconds: 1822476.018\n",
      "epoch: 638700, train loss: 3.8838422298431396, val loss: 3.9771994590759276, ETA in seconds: 1822246.879\n",
      "epoch: 638800, train loss: 3.899769330024719, val loss: 3.964560699462891, ETA in seconds: 1822023.731\n",
      "epoch: 638900, train loss: 3.8909753561019897, val loss: 3.9709468364715574, ETA in seconds: 1821803.369\n",
      "epoch: 639000, train loss: 3.8948190212249756, val loss: 3.9605618000030516, ETA in seconds: 1821570.721\n",
      "epoch: 639100, train loss: 3.8877043962478637, val loss: 3.9678077697753906, ETA in seconds: 1821337.079\n",
      "epoch: 639200, train loss: 3.895163917541504, val loss: 3.9667960166931153, ETA in seconds: 1821102.212\n",
      "epoch: 639300, train loss: 3.894277501106262, val loss: 3.9712146520614624, ETA in seconds: 1820865.974\n",
      "epoch: 639400, train loss: 3.899572968482971, val loss: 3.9653162479400637, ETA in seconds: 1820631.345\n",
      "epoch: 639500, train loss: 3.8932498693466187, val loss: 3.9816877126693724, ETA in seconds: 1820396.126\n",
      "epoch: 639600, train loss: 3.8985711336135864, val loss: 3.97201886177063, ETA in seconds: 1820160.474\n",
      "epoch: 639700, train loss: 3.893270421028137, val loss: 3.9694851636886597, ETA in seconds: 1819960.845\n",
      "epoch: 639800, train loss: 3.8968605279922484, val loss: 3.9696637630462646, ETA in seconds: 1819777.893\n",
      "epoch: 639900, train loss: 3.890416169166565, val loss: 3.9634454965591432, ETA in seconds: 1819601.039\n",
      "epoch: 640000, train loss: 3.89086709022522, val loss: 3.9619753122329713, ETA in seconds: 1819414.219\n",
      "epoch: 640100, train loss: 3.9018635988235473, val loss: 3.976322340965271, ETA in seconds: 1819179.986\n",
      "epoch: 640200, train loss: 3.8930702209472656, val loss: 3.96703519821167, ETA in seconds: 1818940.889\n",
      "epoch: 640300, train loss: 3.895810842514038, val loss: 3.9755318403244018, ETA in seconds: 1818723.602\n",
      "epoch: 640400, train loss: 3.895621156692505, val loss: 3.9684287548065185, ETA in seconds: 1818540.907\n",
      "epoch: 640500, train loss: 3.8915124893188477, val loss: 3.9554953813552856, ETA in seconds: 1818343.605\n",
      "epoch: 640600, train loss: 3.886927533149719, val loss: 3.9770379781723024, ETA in seconds: 1818149.514\n",
      "epoch: 640700, train loss: 3.8895840883255004, val loss: 3.9718952655792235, ETA in seconds: 1817989.150\n",
      "epoch: 640800, train loss: 3.88639554977417, val loss: 3.9635655164718626, ETA in seconds: 1817822.114\n",
      "epoch: 640900, train loss: 3.899774599075317, val loss: 3.958686184883118, ETA in seconds: 1817650.975\n",
      "epoch: 641000, train loss: 3.8875248432159424, val loss: 3.969477081298828, ETA in seconds: 1817420.268\n",
      "epoch: 641100, train loss: 3.8949247360229493, val loss: 3.9669702768325807, ETA in seconds: 1817221.457\n",
      "epoch: 641200, train loss: 3.8897076845169067, val loss: 3.966132879257202, ETA in seconds: 1816998.381\n",
      "epoch: 641300, train loss: 3.883380627632141, val loss: 3.974251627922058, ETA in seconds: 1816764.765\n",
      "epoch: 641400, train loss: 3.884219241142273, val loss: 3.971109390258789, ETA in seconds: 1816531.520\n",
      "epoch: 641500, train loss: 3.893519878387451, val loss: 3.9771687269210814, ETA in seconds: 1816315.043\n",
      "epoch: 641600, train loss: 3.8896720170974732, val loss: 3.9842214822769164, ETA in seconds: 1816163.174\n",
      "epoch: 641700, train loss: 3.8916633129119873, val loss: 3.974011206626892, ETA in seconds: 1816007.623\n",
      "epoch: 641800, train loss: 3.895546221733093, val loss: 3.9619010210037233, ETA in seconds: 1815787.931\n",
      "epoch: 641900, train loss: 3.8888727188110352, val loss: 3.9603172302246095, ETA in seconds: 1815554.555\n",
      "epoch: 642000, train loss: 3.8905032873153687, val loss: 3.9655218839645388, ETA in seconds: 1815319.540\n",
      "epoch: 642100, train loss: 3.8955573797225953, val loss: 3.9674997568130492, ETA in seconds: 1815080.159\n",
      "epoch: 642200, train loss: 3.8977208852767946, val loss: 3.954818320274353, ETA in seconds: 1814864.933\n",
      "epoch: 642300, train loss: 3.8872972965240478, val loss: 3.966363024711609, ETA in seconds: 1814631.488\n",
      "epoch: 642400, train loss: 3.895786428451538, val loss: 3.9698013782501222, ETA in seconds: 1814399.944\n",
      "epoch: 642500, train loss: 3.8818867444992065, val loss: 3.9637719869613646, ETA in seconds: 1814164.210\n",
      "epoch: 642600, train loss: 3.8881316661834715, val loss: 3.9676971197128297, ETA in seconds: 1813934.993\n",
      "epoch: 642700, train loss: 3.8904436349868776, val loss: 3.9750314474105837, ETA in seconds: 1813712.782\n",
      "epoch: 642800, train loss: 3.896901845932007, val loss: 3.96886568069458, ETA in seconds: 1813479.150\n",
      "epoch: 642900, train loss: 3.8972620010375976, val loss: 3.9670119762420653, ETA in seconds: 1813253.582\n",
      "epoch: 643000, train loss: 3.891337752342224, val loss: 3.9569706439971926, ETA in seconds: 1813018.961\n",
      "epoch: 643100, train loss: 3.8943848848342895, val loss: 3.974569487571716, ETA in seconds: 1812794.544\n",
      "epoch: 643200, train loss: 3.8894662857055664, val loss: 3.973859262466431, ETA in seconds: 1812572.210\n",
      "epoch: 643300, train loss: 3.8934324979782104, val loss: 3.957669186592102, ETA in seconds: 1812339.457\n",
      "epoch: 643400, train loss: 3.8949968814849854, val loss: 3.9514511585235597, ETA in seconds: 1812099.396\n",
      "epoch: 643500, train loss: 3.8884235620498657, val loss: 3.96852593421936, ETA in seconds: 1811853.908\n",
      "epoch: 643600, train loss: 3.8926697254180906, val loss: 3.967976427078247, ETA in seconds: 1811615.886\n",
      "epoch: 643700, train loss: 3.905572295188904, val loss: 3.9706089019775392, ETA in seconds: 1811374.285\n",
      "epoch: 643800, train loss: 3.8890731811523436, val loss: 3.973411798477173, ETA in seconds: 1811128.440\n",
      "epoch: 643900, train loss: 3.8933480978012085, val loss: 3.9737951040267943, ETA in seconds: 1810879.218\n",
      "epoch: 644000, train loss: 3.8963083744049074, val loss: 3.9709068536758423, ETA in seconds: 1810637.760\n",
      "epoch: 644100, train loss: 3.889042329788208, val loss: 3.972624492645264, ETA in seconds: 1810397.398\n",
      "epoch: 644200, train loss: 3.9022774696350098, val loss: 3.964440369606018, ETA in seconds: 1810156.645\n",
      "epoch: 644300, train loss: 3.88705153465271, val loss: 3.980128765106201, ETA in seconds: 1809926.939\n",
      "epoch: 644400, train loss: 3.90030574798584, val loss: 3.9682862520217896, ETA in seconds: 1809681.016\n",
      "epoch: 644500, train loss: 3.8841408014297487, val loss: 3.9679688453674316, ETA in seconds: 1809465.904\n",
      "epoch: 644600, train loss: 3.8945569038391112, val loss: 3.9761807203292845, ETA in seconds: 1809273.041\n",
      "epoch: 644700, train loss: 3.905186676979065, val loss: 3.9825350761413576, ETA in seconds: 1809063.252\n",
      "epoch: 644800, train loss: 3.89259352684021, val loss: 3.967131733894348, ETA in seconds: 1808833.251\n",
      "epoch: 644900, train loss: 3.8948346853256224, val loss: 3.966279983520508, ETA in seconds: 1808634.242\n",
      "epoch: 645000, train loss: 3.898006296157837, val loss: 3.981564426422119, ETA in seconds: 1808397.908\n",
      "epoch: 645100, train loss: 3.893538212776184, val loss: 3.9675832033157348, ETA in seconds: 1808167.271\n",
      "epoch: 645200, train loss: 3.8963967323303224, val loss: 3.9707143545150756, ETA in seconds: 1807911.033\n",
      "epoch: 645300, train loss: 3.8894165992736816, val loss: 3.973876404762268, ETA in seconds: 1807679.360\n",
      "epoch: 645400, train loss: 3.898370099067688, val loss: 3.9767765283584593, ETA in seconds: 1807460.558\n",
      "epoch: 645500, train loss: 3.890846371650696, val loss: 3.982033610343933, ETA in seconds: 1807246.844\n",
      "epoch: 645600, train loss: 3.8972666025161744, val loss: 3.975095295906067, ETA in seconds: 1807035.688\n",
      "epoch: 645700, train loss: 3.898061466217041, val loss: 3.9706473112106324, ETA in seconds: 1806814.262\n",
      "epoch: 645800, train loss: 3.8851702213287354, val loss: 3.9737014293670656, ETA in seconds: 1806590.136\n",
      "epoch: 645900, train loss: 3.8983920335769655, val loss: 3.9859755754470827, ETA in seconds: 1806365.231\n",
      "epoch: 646000, train loss: 3.8906444787979124, val loss: 3.9673150300979616, ETA in seconds: 1806135.438\n",
      "epoch: 646100, train loss: 3.8922492027282716, val loss: 3.9782377243041993, ETA in seconds: 1805901.602\n",
      "epoch: 646200, train loss: 3.889226531982422, val loss: 3.9679898023605347, ETA in seconds: 1805660.224\n",
      "epoch: 646300, train loss: 3.887223482131958, val loss: 3.9645050525665284, ETA in seconds: 1805435.007\n",
      "epoch: 646400, train loss: 3.88734712600708, val loss: 3.9754651546478272, ETA in seconds: 1805206.611\n",
      "epoch: 646500, train loss: 3.893100309371948, val loss: 3.964293932914734, ETA in seconds: 1804976.872\n",
      "epoch: 646600, train loss: 3.8939582109451294, val loss: 3.9828478336334228, ETA in seconds: 1804740.966\n",
      "epoch: 646700, train loss: 3.8950254678726197, val loss: 3.9719560861587526, ETA in seconds: 1804494.615\n",
      "epoch: 646800, train loss: 3.8934481859207155, val loss: 3.966095232963562, ETA in seconds: 1804242.755\n",
      "epoch: 646900, train loss: 3.8947036027908326, val loss: 3.9566246032714845, ETA in seconds: 1804008.068\n",
      "epoch: 647000, train loss: 3.8895302295684813, val loss: 3.970143032073975, ETA in seconds: 1803814.632\n",
      "epoch: 647100, train loss: 3.893039011955261, val loss: 3.973747658729553, ETA in seconds: 1803574.625\n",
      "epoch: 647200, train loss: 3.8983508586883544, val loss: 3.970831370353699, ETA in seconds: 1803334.340\n",
      "epoch: 647300, train loss: 3.8959502220153808, val loss: 3.9787373781204223, ETA in seconds: 1803103.124\n",
      "epoch: 647400, train loss: 3.8924174547195434, val loss: 3.967278242111206, ETA in seconds: 1802870.338\n",
      "epoch: 647500, train loss: 3.890241765975952, val loss: 3.9645809173583983, ETA in seconds: 1802625.238\n",
      "epoch: 647600, train loss: 3.8904128551483153, val loss: 3.974895405769348, ETA in seconds: 1802376.819\n",
      "epoch: 647700, train loss: 3.894201946258545, val loss: 3.9746379613876344, ETA in seconds: 1802144.348\n",
      "epoch: 647800, train loss: 3.8929992437362673, val loss: 3.984383296966553, ETA in seconds: 1801911.488\n",
      "epoch: 647900, train loss: 3.888054704666138, val loss: 3.9716149091720583, ETA in seconds: 1801694.914\n",
      "epoch: 648000, train loss: 3.891498064994812, val loss: 3.9674433946609495, ETA in seconds: 1801450.707\n",
      "epoch: 648100, train loss: 3.9036096811294554, val loss: 3.967721176147461, ETA in seconds: 1801219.999\n",
      "epoch: 648200, train loss: 3.890339970588684, val loss: 3.9686564922332765, ETA in seconds: 1800987.787\n",
      "epoch: 648300, train loss: 3.8860776662826537, val loss: 3.9699976205825807, ETA in seconds: 1800746.240\n",
      "epoch: 648400, train loss: 3.8875050067901613, val loss: 3.972370457649231, ETA in seconds: 1800513.427\n",
      "epoch: 648500, train loss: 3.891482353210449, val loss: 3.97073233127594, ETA in seconds: 1800264.534\n",
      "epoch: 648600, train loss: 3.8883151054382323, val loss: 3.980732274055481, ETA in seconds: 1800001.798\n",
      "epoch: 648700, train loss: 3.8816136598587034, val loss: 3.9704455852508547, ETA in seconds: 1799750.314\n",
      "epoch: 648800, train loss: 3.889267349243164, val loss: 3.9798076868057253, ETA in seconds: 1799513.582\n",
      "epoch: 648900, train loss: 3.8887830495834352, val loss: 3.9764655351638796, ETA in seconds: 1799268.480\n",
      "epoch: 649000, train loss: 3.8961182594299317, val loss: 3.977437210083008, ETA in seconds: 1799030.528\n",
      "epoch: 649100, train loss: 3.9004348516464233, val loss: 3.9797918081283568, ETA in seconds: 1798781.856\n",
      "epoch: 649200, train loss: 3.894620966911316, val loss: 3.96846764087677, ETA in seconds: 1798519.999\n",
      "epoch: 649300, train loss: 3.8968364000320435, val loss: 3.9718517303466796, ETA in seconds: 1798262.090\n",
      "epoch: 649400, train loss: 3.894944095611572, val loss: 3.9715325832366943, ETA in seconds: 1798032.149\n",
      "epoch: 649500, train loss: 3.8916751623153685, val loss: 3.9801180839538572, ETA in seconds: 1797799.986\n",
      "epoch: 649600, train loss: 3.890336275100708, val loss: 3.971605086326599, ETA in seconds: 1797613.364\n",
      "epoch: 649700, train loss: 3.8880494594573975, val loss: 3.9757214784622192, ETA in seconds: 1797419.315\n",
      "epoch: 649800, train loss: 3.897072601318359, val loss: 3.9733439445495606, ETA in seconds: 1797221.326\n",
      "epoch: 649900, train loss: 3.9029755353927613, val loss: 3.9667701959609984, ETA in seconds: 1796988.183\n",
      "epoch: 650000, train loss: 3.892540431022644, val loss: 3.964849853515625, ETA in seconds: 1796753.719\n",
      "epoch: 650100, train loss: 3.9014046430587768, val loss: 3.9747534513473513, ETA in seconds: 1796567.646\n",
      "epoch: 650200, train loss: 3.892896795272827, val loss: 3.9703490495681764, ETA in seconds: 1796305.843\n",
      "epoch: 650300, train loss: 3.8932780504226683, val loss: 3.9660829067230225, ETA in seconds: 1796049.196\n",
      "epoch: 650400, train loss: 3.8846065998077393, val loss: 3.974382472038269, ETA in seconds: 1795796.828\n",
      "epoch: 650500, train loss: 3.895544242858887, val loss: 3.9566025018692015, ETA in seconds: 1795553.789\n",
      "epoch: 650600, train loss: 3.8950348138809203, val loss: 3.970989465713501, ETA in seconds: 1795288.994\n",
      "epoch: 650700, train loss: 3.90330765247345, val loss: 3.9721688270568847, ETA in seconds: 1795023.891\n",
      "epoch: 650800, train loss: 3.894448494911194, val loss: 3.96662061214447, ETA in seconds: 1794764.274\n",
      "epoch: 650900, train loss: 3.8945230722427366, val loss: 3.9587551832199095, ETA in seconds: 1794509.256\n",
      "epoch: 651000, train loss: 3.888277792930603, val loss: 3.959454798698425, ETA in seconds: 1794245.436\n",
      "epoch: 651100, train loss: 3.8894099235534667, val loss: 3.9708884000778197, ETA in seconds: 1793988.424\n",
      "epoch: 651200, train loss: 3.8960604906082152, val loss: 3.9718090295791626, ETA in seconds: 1793733.495\n",
      "epoch: 651300, train loss: 3.8942665338516234, val loss: 3.9805088520050047, ETA in seconds: 1793504.720\n",
      "epoch: 651400, train loss: 3.8942760229110718, val loss: 3.9714575529098513, ETA in seconds: 1793245.721\n",
      "epoch: 651500, train loss: 3.8839956521987915, val loss: 3.974821162223816, ETA in seconds: 1792982.701\n",
      "epoch: 651600, train loss: 3.89886531829834, val loss: 3.971363162994385, ETA in seconds: 1792722.690\n",
      "epoch: 651700, train loss: 3.8979401111602785, val loss: 3.969558334350586, ETA in seconds: 1792501.676\n",
      "epoch: 651800, train loss: 3.8940345287322997, val loss: 3.973421239852905, ETA in seconds: 1792239.228\n",
      "epoch: 651900, train loss: 3.8960040330886843, val loss: 3.981627655029297, ETA in seconds: 1791972.669\n",
      "epoch: 652000, train loss: 3.8885823488235474, val loss: 3.967258906364441, ETA in seconds: 1791712.635\n",
      "epoch: 652100, train loss: 3.894438934326172, val loss: 3.9672056913375853, ETA in seconds: 1791480.529\n",
      "epoch: 652200, train loss: 3.88416383266449, val loss: 3.9750006198883057, ETA in seconds: 1791220.878\n",
      "epoch: 652300, train loss: 3.8963014841079713, val loss: 3.9738537311553954, ETA in seconds: 1790956.686\n",
      "epoch: 652400, train loss: 3.8869200706481934, val loss: 3.978639507293701, ETA in seconds: 1790692.840\n",
      "epoch: 652500, train loss: 3.88943727016449, val loss: 3.9772902727127075, ETA in seconds: 1790453.204\n",
      "epoch: 652600, train loss: 3.8908486366271973, val loss: 3.9771016597747804, ETA in seconds: 1790195.584\n",
      "epoch: 652700, train loss: 3.8952212810516356, val loss: 3.961055779457092, ETA in seconds: 1789929.947\n",
      "epoch: 652800, train loss: 3.8962252140045166, val loss: 3.973969006538391, ETA in seconds: 1789681.392\n",
      "epoch: 652900, train loss: 3.9005586385726927, val loss: 3.9756311178207397, ETA in seconds: 1789444.500\n",
      "epoch: 653000, train loss: 3.9081029415130617, val loss: 3.9725152015686036, ETA in seconds: 1789206.652\n",
      "epoch: 653100, train loss: 3.891664242744446, val loss: 3.9754138231277465, ETA in seconds: 1788942.433\n",
      "epoch: 653200, train loss: 3.8896934032440185, val loss: 3.9798205614089968, ETA in seconds: 1788684.380\n",
      "epoch: 653300, train loss: 3.894568681716919, val loss: 3.9657952308654787, ETA in seconds: 1788420.497\n",
      "epoch: 653400, train loss: 3.8995121479034425, val loss: 3.977293038368225, ETA in seconds: 1788154.917\n",
      "epoch: 653500, train loss: 3.888063931465149, val loss: 3.969605255126953, ETA in seconds: 1787899.566\n",
      "epoch: 653600, train loss: 3.8877095937728883, val loss: 3.970820760726929, ETA in seconds: 1787703.275\n",
      "epoch: 653700, train loss: 3.9025768280029296, val loss: 3.9747731924057006, ETA in seconds: 1787444.335\n",
      "epoch: 653800, train loss: 3.893039035797119, val loss: 3.9704384326934816, ETA in seconds: 1787173.529\n",
      "epoch: 653900, train loss: 3.897597312927246, val loss: 3.9734238624572753, ETA in seconds: 1786909.238\n",
      "epoch: 654000, train loss: 3.8881776332855225, val loss: 3.9666549444198607, ETA in seconds: 1786660.892\n",
      "epoch: 654100, train loss: 3.8919357538223265, val loss: 3.9677597999572756, ETA in seconds: 1786413.382\n",
      "epoch: 654200, train loss: 3.891413927078247, val loss: 3.978353476524353, ETA in seconds: 1786149.583\n",
      "epoch: 654300, train loss: 3.8961711645126345, val loss: 3.9706879377365114, ETA in seconds: 1785893.425\n",
      "epoch: 654400, train loss: 3.889130878448486, val loss: 3.9743507385253904, ETA in seconds: 1785631.227\n",
      "epoch: 654500, train loss: 3.892866015434265, val loss: 3.978464651107788, ETA in seconds: 1785371.253\n",
      "epoch: 654600, train loss: 3.89678008556366, val loss: 3.970756983757019, ETA in seconds: 1785140.049\n",
      "epoch: 654700, train loss: 3.8967530965805053, val loss: 3.9663753271102906, ETA in seconds: 1784872.124\n",
      "epoch: 654800, train loss: 3.8966578245162964, val loss: 3.9743961095809937, ETA in seconds: 1784598.932\n",
      "epoch: 654900, train loss: 3.890984535217285, val loss: 3.9580559968948363, ETA in seconds: 1784324.226\n",
      "epoch: 655000, train loss: 3.8898945569992067, val loss: 3.970755124092102, ETA in seconds: 1784050.655\n",
      "epoch: 655100, train loss: 3.893564462661743, val loss: 3.9763028144836428, ETA in seconds: 1783802.289\n",
      "epoch: 655200, train loss: 3.8952256441116333, val loss: 3.9675522804260255, ETA in seconds: 1783566.302\n",
      "epoch: 655300, train loss: 3.8928370237350465, val loss: 3.970045638084412, ETA in seconds: 1783305.797\n",
      "epoch: 655400, train loss: 3.899741005897522, val loss: 3.971907615661621, ETA in seconds: 1783044.751\n",
      "epoch: 655500, train loss: 3.892712044715881, val loss: 3.979576921463013, ETA in seconds: 1782773.602\n",
      "epoch: 655600, train loss: 3.9029229879379272, val loss: 3.9689050912857056, ETA in seconds: 1782505.784\n",
      "epoch: 655700, train loss: 3.8917858362197877, val loss: 3.9740942478179933, ETA in seconds: 1782240.447\n",
      "epoch: 655800, train loss: 3.898646330833435, val loss: 3.9764819860458376, ETA in seconds: 1781984.407\n",
      "epoch: 655900, train loss: 3.8967490673065184, val loss: 3.9691449403762817, ETA in seconds: 1781724.048\n",
      "epoch: 656000, train loss: 3.895735263824463, val loss: 3.9677857160568237, ETA in seconds: 1781515.284\n",
      "epoch: 656100, train loss: 3.883406972885132, val loss: 3.974569535255432, ETA in seconds: 1781308.067\n",
      "epoch: 656200, train loss: 3.894213318824768, val loss: 3.9656760692596436, ETA in seconds: 1781099.443\n",
      "epoch: 656300, train loss: 3.892519497871399, val loss: 3.962146782875061, ETA in seconds: 1780889.170\n",
      "epoch: 656400, train loss: 3.8887397050857544, val loss: 3.9697513818740844, ETA in seconds: 1780678.016\n",
      "epoch: 656500, train loss: 3.8967265129089355, val loss: 3.9568318128585815, ETA in seconds: 1780468.785\n",
      "epoch: 656600, train loss: 3.898750877380371, val loss: 3.961891460418701, ETA in seconds: 1780259.238\n",
      "epoch: 656700, train loss: 3.8994268655776976, val loss: 3.9641745328903197, ETA in seconds: 1780051.885\n",
      "epoch: 656800, train loss: 3.8975871801376343, val loss: 3.970581865310669, ETA in seconds: 1779843.091\n",
      "epoch: 656900, train loss: 3.892556643486023, val loss: 3.965958857536316, ETA in seconds: 1779633.468\n",
      "epoch: 657000, train loss: 3.8841819047927855, val loss: 3.9596291303634645, ETA in seconds: 1779406.534\n",
      "epoch: 657100, train loss: 3.900152659416199, val loss: 3.9683608293533323, ETA in seconds: 1779152.455\n",
      "epoch: 657200, train loss: 3.89452748298645, val loss: 3.958818197250366, ETA in seconds: 1778922.921\n",
      "epoch: 657300, train loss: 3.8953794479370116, val loss: 3.9759427309036255, ETA in seconds: 1778660.750\n",
      "epoch: 657400, train loss: 3.884274625778198, val loss: 3.965729093551636, ETA in seconds: 1778414.254\n",
      "epoch: 657500, train loss: 3.8904255867004394, val loss: 3.965268087387085, ETA in seconds: 1778174.713\n",
      "epoch: 657600, train loss: 3.8987425565719604, val loss: 3.9723601818084715, ETA in seconds: 1777898.267\n",
      "epoch: 657700, train loss: 3.9049131870269775, val loss: 3.9650668144226073, ETA in seconds: 1777624.200\n",
      "epoch: 657800, train loss: 3.896285557746887, val loss: 3.963035821914673, ETA in seconds: 1777377.584\n",
      "epoch: 657900, train loss: 3.8946422576904296, val loss: 3.9676800966262817, ETA in seconds: 1777157.851\n",
      "epoch: 658000, train loss: 3.886697793006897, val loss: 3.967897582054138, ETA in seconds: 1776912.723\n",
      "epoch: 658100, train loss: 3.8971030950546264, val loss: 3.9661274194717406, ETA in seconds: 1776638.454\n",
      "epoch: 658200, train loss: 3.8970769166946413, val loss: 3.9692286968231203, ETA in seconds: 1776371.047\n",
      "epoch: 658300, train loss: 3.8904358625411986, val loss: 3.9693753480911256, ETA in seconds: 1776125.008\n",
      "epoch: 658400, train loss: 3.899817371368408, val loss: 3.960141730308533, ETA in seconds: 1775858.943\n",
      "epoch: 658500, train loss: 3.8962547540664674, val loss: 3.966564893722534, ETA in seconds: 1775583.622\n",
      "epoch: 658600, train loss: 3.894210147857666, val loss: 3.9650091409683226, ETA in seconds: 1775310.522\n",
      "epoch: 658700, train loss: 3.8877768754959106, val loss: 3.9741562366485597, ETA in seconds: 1775041.678\n",
      "epoch: 658800, train loss: 3.9000826835632325, val loss: 3.9677062034606934, ETA in seconds: 1774767.544\n",
      "epoch: 658900, train loss: 3.8910673379898073, val loss: 3.971876096725464, ETA in seconds: 1774495.441\n",
      "epoch: 659000, train loss: 3.903891134262085, val loss: 3.9692883491516113, ETA in seconds: 1774222.376\n",
      "epoch: 659100, train loss: 3.894075036048889, val loss: 3.975972580909729, ETA in seconds: 1773949.228\n",
      "epoch: 659200, train loss: 3.887765312194824, val loss: 3.9760379076004027, ETA in seconds: 1773671.507\n",
      "epoch: 659300, train loss: 3.8955490350723267, val loss: 3.982464838027954, ETA in seconds: 1773397.110\n",
      "epoch: 659400, train loss: 3.891387605667114, val loss: 3.967869448661804, ETA in seconds: 1773164.265\n",
      "epoch: 659500, train loss: 3.894387364387512, val loss: 3.9637473583221436, ETA in seconds: 1772914.600\n",
      "epoch: 659600, train loss: 3.8987688064575194, val loss: 3.9827363967895506, ETA in seconds: 1772670.759\n",
      "epoch: 659700, train loss: 3.892572355270386, val loss: 3.98515784740448, ETA in seconds: 1772404.065\n",
      "epoch: 659800, train loss: 3.8933939933776855, val loss: 3.9664730548858644, ETA in seconds: 1772150.943\n",
      "epoch: 659900, train loss: 3.8892226696014403, val loss: 3.9702410221099855, ETA in seconds: 1771933.486\n",
      "epoch: 660000, train loss: 3.892274808883667, val loss: 3.980352520942688, ETA in seconds: 1771685.083\n",
      "epoch: 660100, train loss: 3.8953418254852297, val loss: 3.973410224914551, ETA in seconds: 1771423.066\n",
      "epoch: 660200, train loss: 3.893877458572388, val loss: 3.971035671234131, ETA in seconds: 1771142.468\n",
      "epoch: 660300, train loss: 3.890388345718384, val loss: 3.9738180160522463, ETA in seconds: 1770862.719\n",
      "epoch: 660400, train loss: 3.888522481918335, val loss: 3.967066836357117, ETA in seconds: 1770609.122\n",
      "epoch: 660500, train loss: 3.899869418144226, val loss: 3.9723661661148073, ETA in seconds: 1770341.241\n",
      "epoch: 660600, train loss: 3.892097282409668, val loss: 3.975120782852173, ETA in seconds: 1770072.342\n",
      "epoch: 660700, train loss: 3.8948718547821044, val loss: 3.9704323291778563, ETA in seconds: 1769815.281\n",
      "epoch: 660800, train loss: 3.894935655593872, val loss: 3.9796724796295164, ETA in seconds: 1769587.580\n",
      "epoch: 660900, train loss: 3.886533570289612, val loss: 3.976367974281311, ETA in seconds: 1769322.068\n",
      "epoch: 661000, train loss: 3.8851656675338746, val loss: 3.9691112995147706, ETA in seconds: 1769059.157\n",
      "epoch: 661100, train loss: 3.895603561401367, val loss: 3.966357183456421, ETA in seconds: 1768795.530\n",
      "epoch: 661200, train loss: 3.898916482925415, val loss: 3.9714174270629883, ETA in seconds: 1768536.403\n",
      "epoch: 661300, train loss: 3.8969680309295653, val loss: 3.972101330757141, ETA in seconds: 1768286.250\n",
      "epoch: 661400, train loss: 3.8884357213974, val loss: 3.9731618642807005, ETA in seconds: 1768041.383\n",
      "epoch: 661500, train loss: 3.892407274246216, val loss: 3.9821213245391847, ETA in seconds: 1767768.623\n",
      "epoch: 661600, train loss: 3.89670193195343, val loss: 3.9645738124847414, ETA in seconds: 1767506.154\n",
      "epoch: 661700, train loss: 3.895469331741333, val loss: 3.9609996557235716, ETA in seconds: 1767286.176\n",
      "epoch: 661800, train loss: 3.8878925323486326, val loss: 3.983922553062439, ETA in seconds: 1767084.019\n",
      "epoch: 661900, train loss: 3.889419507980347, val loss: 3.9637861490249633, ETA in seconds: 1766884.835\n",
      "epoch: 662000, train loss: 3.8857290267944338, val loss: 3.9800140142440794, ETA in seconds: 1766684.710\n",
      "epoch: 662100, train loss: 3.894821572303772, val loss: 3.981962561607361, ETA in seconds: 1766485.413\n",
      "epoch: 662200, train loss: 3.8912534713745117, val loss: 3.962223196029663, ETA in seconds: 1766282.395\n",
      "epoch: 662300, train loss: 3.8931122541427614, val loss: 3.960422229766846, ETA in seconds: 1766078.351\n",
      "epoch: 662400, train loss: 3.8903805732727053, val loss: 3.9723108291625975, ETA in seconds: 1765874.112\n",
      "epoch: 662500, train loss: 3.8911040306091307, val loss: 3.9612497091293335, ETA in seconds: 1765676.047\n",
      "epoch: 662600, train loss: 3.8932873487472532, val loss: 3.9625648498535155, ETA in seconds: 1765471.082\n",
      "epoch: 662700, train loss: 3.8951372146606444, val loss: 3.9559023141860963, ETA in seconds: 1765216.615\n",
      "epoch: 662800, train loss: 3.8957542896270754, val loss: 3.968843674659729, ETA in seconds: 1764945.922\n",
      "epoch: 662900, train loss: 3.8891621112823485, val loss: 3.9640339851379394, ETA in seconds: 1764685.504\n",
      "epoch: 663000, train loss: 3.8918874502182006, val loss: 3.965484142303467, ETA in seconds: 1764413.014\n",
      "epoch: 663100, train loss: 3.8864891052246096, val loss: 3.9662269115448, ETA in seconds: 1764154.737\n",
      "epoch: 663200, train loss: 3.887921404838562, val loss: 3.97468683719635, ETA in seconds: 1763882.037\n",
      "epoch: 663300, train loss: 3.894012379646301, val loss: 3.966469955444336, ETA in seconds: 1763604.826\n",
      "epoch: 663400, train loss: 3.881979560852051, val loss: 3.9704699516296387, ETA in seconds: 1763329.268\n",
      "epoch: 663500, train loss: 3.8920610904693604, val loss: 3.9704020500183104, ETA in seconds: 1763052.723\n",
      "epoch: 663600, train loss: 3.8993017196655275, val loss: 3.9722689628601073, ETA in seconds: 1762772.527\n",
      "epoch: 663700, train loss: 3.893470525741577, val loss: 3.969000244140625, ETA in seconds: 1762494.828\n",
      "epoch: 663800, train loss: 3.8869811296463013, val loss: 3.9694904088974, ETA in seconds: 1762217.125\n",
      "epoch: 663900, train loss: 3.900539517402649, val loss: 3.9700536251068117, ETA in seconds: 1761940.240\n",
      "epoch: 664000, train loss: 3.897378349304199, val loss: 3.966607928276062, ETA in seconds: 1761665.251\n",
      "epoch: 664100, train loss: 3.8881036758422853, val loss: 3.9685758352279663, ETA in seconds: 1761413.915\n",
      "epoch: 664200, train loss: 3.901786208152771, val loss: 3.965634298324585, ETA in seconds: 1761142.812\n",
      "epoch: 664300, train loss: 3.885906195640564, val loss: 3.9618120908737184, ETA in seconds: 1760871.989\n",
      "epoch: 664400, train loss: 3.8899434089660643, val loss: 3.969356918334961, ETA in seconds: 1760620.201\n",
      "epoch: 664500, train loss: 3.8947876930236816, val loss: 3.96748206615448, ETA in seconds: 1760367.036\n",
      "epoch: 664600, train loss: 3.885036754608154, val loss: 3.9747420072555544, ETA in seconds: 1760117.028\n",
      "epoch: 664700, train loss: 3.895665407180786, val loss: 3.966707634925842, ETA in seconds: 1759837.749\n",
      "epoch: 664800, train loss: 3.898805522918701, val loss: 3.9696861743927, ETA in seconds: 1759553.264\n",
      "epoch: 664900, train loss: 3.906502366065979, val loss: 3.967593765258789, ETA in seconds: 1759282.851\n",
      "epoch: 665000, train loss: 3.895742917060852, val loss: 3.9692005157470702, ETA in seconds: 1759035.800\n",
      "epoch: 665100, train loss: 3.8891179084777834, val loss: 3.957618069648743, ETA in seconds: 1758750.036\n",
      "epoch: 665200, train loss: 3.8986096382141113, val loss: 3.9782979011535646, ETA in seconds: 1758475.419\n",
      "epoch: 665300, train loss: 3.894442009925842, val loss: 3.970855212211609, ETA in seconds: 1758196.337\n",
      "epoch: 665400, train loss: 3.9069313526153566, val loss: 3.9691021919250487, ETA in seconds: 1757917.689\n",
      "epoch: 665500, train loss: 3.89252347946167, val loss: 3.9685355186462403, ETA in seconds: 1757648.016\n",
      "epoch: 665600, train loss: 3.8991238832473756, val loss: 3.97255802154541, ETA in seconds: 1757393.694\n",
      "epoch: 665700, train loss: 3.895652174949646, val loss: 3.9730509996414183, ETA in seconds: 1757137.367\n",
      "epoch: 665800, train loss: 3.8857234716415405, val loss: 3.9635928869247437, ETA in seconds: 1756884.768\n",
      "epoch: 665900, train loss: 3.8940844774246215, val loss: 3.9673689126968386, ETA in seconds: 1756626.813\n",
      "epoch: 666000, train loss: 3.884676766395569, val loss: 3.9799728631973266, ETA in seconds: 1756352.271\n",
      "epoch: 666100, train loss: 3.8920737743377685, val loss: 3.9800516843795775, ETA in seconds: 1756086.275\n",
      "epoch: 666200, train loss: 3.895167350769043, val loss: 3.9661006927490234, ETA in seconds: 1755807.778\n",
      "epoch: 666300, train loss: 3.899666976928711, val loss: 3.966610383987427, ETA in seconds: 1755520.502\n",
      "epoch: 666400, train loss: 3.897995662689209, val loss: 3.9784533977508545, ETA in seconds: 1755237.304\n",
      "epoch: 666500, train loss: 3.8884546041488646, val loss: 3.963146138191223, ETA in seconds: 1754959.594\n",
      "epoch: 666600, train loss: 3.892829442024231, val loss: 3.9803359270095826, ETA in seconds: 1754730.931\n",
      "epoch: 666700, train loss: 3.8904035568237303, val loss: 3.9697299003601074, ETA in seconds: 1754503.087\n",
      "epoch: 666800, train loss: 3.8939043283462524, val loss: 3.9744481325149534, ETA in seconds: 1754279.403\n",
      "epoch: 666900, train loss: 3.887848687171936, val loss: 3.9760172367095947, ETA in seconds: 1754052.473\n",
      "epoch: 667000, train loss: 3.898135042190552, val loss: 3.9768155813217163, ETA in seconds: 1753825.056\n",
      "epoch: 667100, train loss: 3.890795683860779, val loss: 3.9701401472091673, ETA in seconds: 1753595.006\n",
      "epoch: 667200, train loss: 3.8979727745056154, val loss: 3.9746795654296876, ETA in seconds: 1753364.407\n",
      "epoch: 667300, train loss: 3.891239547729492, val loss: 3.9704288244247437, ETA in seconds: 1753133.771\n",
      "epoch: 667400, train loss: 3.8879420280456545, val loss: 3.980431056022644, ETA in seconds: 1752865.779\n",
      "epoch: 667500, train loss: 3.8939446449279784, val loss: 3.9781864404678347, ETA in seconds: 1752603.392\n",
      "epoch: 667600, train loss: 3.902480888366699, val loss: 3.96367769241333, ETA in seconds: 1752361.012\n",
      "epoch: 667700, train loss: 3.8951701402664183, val loss: 3.9728596448898315, ETA in seconds: 1752137.146\n",
      "epoch: 667800, train loss: 3.8914138317108153, val loss: 3.974764275550842, ETA in seconds: 1751916.395\n",
      "epoch: 667900, train loss: 3.886237382888794, val loss: 3.970895767211914, ETA in seconds: 1751677.847\n",
      "epoch: 668000, train loss: 3.8924246311187742, val loss: 3.9769139766693113, ETA in seconds: 1751453.792\n",
      "epoch: 668100, train loss: 3.899051332473755, val loss: 3.970737671852112, ETA in seconds: 1751207.909\n",
      "epoch: 668200, train loss: 3.8962586641311647, val loss: 3.970884346961975, ETA in seconds: 1750961.937\n",
      "epoch: 668300, train loss: 3.8892749309539796, val loss: 3.9705625057220457, ETA in seconds: 1750730.388\n",
      "epoch: 668400, train loss: 3.8991952657699587, val loss: 3.9716861724853514, ETA in seconds: 1750447.179\n",
      "epoch: 668500, train loss: 3.8990582466125487, val loss: 3.974141526222229, ETA in seconds: 1750173.489\n",
      "epoch: 668600, train loss: 3.8983898162841797, val loss: 3.9737953662872316, ETA in seconds: 1749940.981\n",
      "epoch: 668700, train loss: 3.88952214717865, val loss: 3.966519999504089, ETA in seconds: 1749722.162\n",
      "epoch: 668800, train loss: 3.898131084442139, val loss: 3.9661115646362304, ETA in seconds: 1749509.720\n",
      "epoch: 668900, train loss: 3.892741894721985, val loss: 3.9692447662353514, ETA in seconds: 1749294.022\n",
      "epoch: 669000, train loss: 3.895205283164978, val loss: 3.9696966409683228, ETA in seconds: 1749075.749\n",
      "epoch: 669100, train loss: 3.890452003479004, val loss: 3.9756619453430178, ETA in seconds: 1748858.762\n",
      "epoch: 669200, train loss: 3.886691522598267, val loss: 3.9708440780639647, ETA in seconds: 1748644.672\n",
      "epoch: 669300, train loss: 3.891387414932251, val loss: 3.9596892833709716, ETA in seconds: 1748435.013\n",
      "epoch: 669400, train loss: 3.895637845993042, val loss: 3.975682020187378, ETA in seconds: 1748226.508\n",
      "epoch: 669500, train loss: 3.899075078964233, val loss: 3.9726151704788206, ETA in seconds: 1748008.596\n",
      "epoch: 669600, train loss: 3.8892385244369505, val loss: 3.9616796493530275, ETA in seconds: 1747791.342\n",
      "epoch: 669700, train loss: 3.8950064420700072, val loss: 3.981630969047546, ETA in seconds: 1747581.621\n",
      "epoch: 669800, train loss: 3.8919069528579713, val loss: 3.9716382741928102, ETA in seconds: 1747337.597\n",
      "epoch: 669900, train loss: 3.8903101444244386, val loss: 3.9853480577468874, ETA in seconds: 1747053.524\n",
      "epoch: 670000, train loss: 3.8953649282455443, val loss: 3.962329077720642, ETA in seconds: 1746784.588\n",
      "epoch: 670100, train loss: 3.8946957111358644, val loss: 3.966339683532715, ETA in seconds: 1746567.022\n",
      "epoch: 670200, train loss: 3.895812511444092, val loss: 3.9681292533874513, ETA in seconds: 1746354.081\n",
      "epoch: 670300, train loss: 3.8934950828552246, val loss: 3.972219395637512, ETA in seconds: 1746142.836\n",
      "epoch: 670400, train loss: 3.892344856262207, val loss: 3.9870744943618774, ETA in seconds: 1745910.976\n",
      "epoch: 670500, train loss: 3.899528646469116, val loss: 3.978792929649353, ETA in seconds: 1745674.662\n",
      "epoch: 670600, train loss: 3.890283632278442, val loss: 3.9718825340271, ETA in seconds: 1745442.140\n",
      "epoch: 670700, train loss: 3.9005613565444945, val loss: 3.975983905792236, ETA in seconds: 1745187.079\n",
      "epoch: 670800, train loss: 3.8897579669952393, val loss: 3.974550247192383, ETA in seconds: 1744912.280\n",
      "epoch: 670900, train loss: 3.889664912223816, val loss: 3.969139909744263, ETA in seconds: 1744643.253\n",
      "epoch: 671000, train loss: 3.8891422510147096, val loss: 3.9672210216522217, ETA in seconds: 1744358.468\n",
      "epoch: 671100, train loss: 3.8882168769836425, val loss: 3.9783430814743044, ETA in seconds: 1744080.612\n",
      "epoch: 671200, train loss: 3.887307596206665, val loss: 3.976081895828247, ETA in seconds: 1743804.535\n",
      "epoch: 671300, train loss: 3.893413209915161, val loss: 3.9854443788528444, ETA in seconds: 1743530.389\n",
      "epoch: 671400, train loss: 3.894507908821106, val loss: 3.9632453680038453, ETA in seconds: 1743266.357\n",
      "epoch: 671500, train loss: 3.8921967267990114, val loss: 3.969361448287964, ETA in seconds: 1743018.569\n",
      "epoch: 671600, train loss: 3.892514634132385, val loss: 3.966166853904724, ETA in seconds: 1742735.128\n",
      "epoch: 671700, train loss: 3.8980403661727907, val loss: 3.9717482328414917, ETA in seconds: 1742453.221\n",
      "epoch: 671800, train loss: 3.889194107055664, val loss: 3.9731319665908815, ETA in seconds: 1742192.379\n",
      "epoch: 671900, train loss: 3.886991262435913, val loss: 3.9731478452682496, ETA in seconds: 1741912.920\n",
      "epoch: 672000, train loss: 3.891410064697266, val loss: 3.988082933425903, ETA in seconds: 1741626.927\n",
      "epoch: 672100, train loss: 3.8953827142715456, val loss: 3.973919725418091, ETA in seconds: 1741339.744\n",
      "epoch: 672200, train loss: 3.897491383552551, val loss: 3.9659992694854735, ETA in seconds: 1741046.439\n",
      "epoch: 672300, train loss: 3.896924042701721, val loss: 3.9668617486953734, ETA in seconds: 1740770.245\n",
      "epoch: 672400, train loss: 3.9040940284729, val loss: 3.9730986833572386, ETA in seconds: 1740474.292\n",
      "epoch: 672500, train loss: 3.8987205982208253, val loss: 3.9656549215316774, ETA in seconds: 1740187.327\n",
      "epoch: 672600, train loss: 3.8944218873977663, val loss: 3.9751517295837404, ETA in seconds: 1739892.860\n",
      "epoch: 672700, train loss: 3.881529998779297, val loss: 3.9737563133239746, ETA in seconds: 1739624.999\n",
      "epoch: 672800, train loss: 3.903197169303894, val loss: 3.973905825614929, ETA in seconds: 1739356.843\n",
      "epoch: 672900, train loss: 3.9004486322402956, val loss: 3.9802577257156373, ETA in seconds: 1739079.262\n",
      "epoch: 673000, train loss: 3.888814377784729, val loss: 3.9776108503341674, ETA in seconds: 1738848.651\n",
      "epoch: 673100, train loss: 3.8930407762527466, val loss: 3.9574196577072143, ETA in seconds: 1738618.675\n",
      "epoch: 673200, train loss: 3.895581603050232, val loss: 3.96903133392334, ETA in seconds: 1738370.687\n",
      "epoch: 673300, train loss: 3.889030623435974, val loss: 3.966360402107239, ETA in seconds: 1738075.591\n",
      "epoch: 673400, train loss: 3.8957290649414062, val loss: 3.9642009496688844, ETA in seconds: 1737782.231\n",
      "epoch: 673500, train loss: 3.8997588634490965, val loss: 3.9635946273803713, ETA in seconds: 1737487.542\n",
      "epoch: 673600, train loss: 3.8928187608718874, val loss: 3.979505681991577, ETA in seconds: 1737196.186\n",
      "epoch: 673700, train loss: 3.896219086647034, val loss: 3.9663159608840943, ETA in seconds: 1736906.019\n",
      "epoch: 673800, train loss: 3.8968748092651366, val loss: 3.985019564628601, ETA in seconds: 1736626.027\n",
      "epoch: 673900, train loss: 3.8875885248184203, val loss: 3.97178852558136, ETA in seconds: 1736334.919\n",
      "epoch: 674000, train loss: 3.888989567756653, val loss: 3.9661239862442015, ETA in seconds: 1736044.685\n",
      "epoch: 674100, train loss: 3.89149329662323, val loss: 3.9606873750686646, ETA in seconds: 1735744.524\n",
      "epoch: 674200, train loss: 3.8884190797805784, val loss: 3.976836085319519, ETA in seconds: 1735443.764\n",
      "epoch: 674300, train loss: 3.888656759262085, val loss: 3.971501421928406, ETA in seconds: 1735169.881\n",
      "epoch: 674400, train loss: 3.893032121658325, val loss: 3.964072108268738, ETA in seconds: 1734904.207\n",
      "epoch: 674500, train loss: 3.894455337524414, val loss: 3.9746554136276244, ETA in seconds: 1734638.881\n",
      "epoch: 674600, train loss: 3.8979357719421386, val loss: 3.9650009155273436, ETA in seconds: 1734359.228\n",
      "epoch: 674700, train loss: 3.8901931762695314, val loss: 3.9660499572753904, ETA in seconds: 1734076.864\n",
      "epoch: 674800, train loss: 3.8862839460372927, val loss: 3.9768481254577637, ETA in seconds: 1733790.349\n",
      "epoch: 674900, train loss: 3.898660159111023, val loss: 3.9844006061553956, ETA in seconds: 1733503.221\n",
      "epoch: 675000, train loss: 3.8830442905426024, val loss: 3.969359874725342, ETA in seconds: 1733212.465\n",
      "epoch: 675100, train loss: 3.890266227722168, val loss: 3.968639922142029, ETA in seconds: 1732942.002\n",
      "epoch: 675200, train loss: 3.8848934173583984, val loss: 3.966321516036987, ETA in seconds: 1732707.134\n",
      "epoch: 675300, train loss: 3.8875537395477293, val loss: 3.9653366088867186, ETA in seconds: 1732469.105\n",
      "epoch: 675400, train loss: 3.8944904088973997, val loss: 3.9749759912490843, ETA in seconds: 1732232.466\n",
      "epoch: 675500, train loss: 3.880168342590332, val loss: 3.9720762729644776, ETA in seconds: 1731973.723\n",
      "epoch: 675600, train loss: 3.8992329120635985, val loss: 3.9627134084701536, ETA in seconds: 1731722.356\n",
      "epoch: 675700, train loss: 3.8936182498931884, val loss: 3.9737539768218992, ETA in seconds: 1731480.968\n",
      "epoch: 675800, train loss: 3.88692102432251, val loss: 3.9691595792770387, ETA in seconds: 1731240.796\n",
      "epoch: 675900, train loss: 3.894982409477234, val loss: 3.972389650344849, ETA in seconds: 1730998.377\n",
      "epoch: 676000, train loss: 3.8950093746185304, val loss: 3.963697385787964, ETA in seconds: 1730757.882\n",
      "epoch: 676100, train loss: 3.885979962348938, val loss: 3.9606883764266967, ETA in seconds: 1730516.762\n",
      "epoch: 676200, train loss: 3.893244910240173, val loss: 3.9625548362731933, ETA in seconds: 1730272.979\n",
      "epoch: 676300, train loss: 3.8982505559921266, val loss: 3.9651867151260376, ETA in seconds: 1730030.970\n",
      "epoch: 676400, train loss: 3.894738721847534, val loss: 3.9695910692214964, ETA in seconds: 1729790.659\n",
      "epoch: 676500, train loss: 3.895366358757019, val loss: 3.9679758310317994, ETA in seconds: 1729540.927\n",
      "epoch: 676600, train loss: 3.8915284395217897, val loss: 3.970394802093506, ETA in seconds: 1729236.014\n",
      "epoch: 676700, train loss: 3.90141875743866, val loss: 3.971572017669678, ETA in seconds: 1728949.640\n",
      "epoch: 676800, train loss: 3.894591760635376, val loss: 3.971504330635071, ETA in seconds: 1728671.152\n",
      "epoch: 676900, train loss: 3.898297166824341, val loss: 3.9610868215560915, ETA in seconds: 1728395.868\n",
      "epoch: 677000, train loss: 3.891014504432678, val loss: 3.9700429439544678, ETA in seconds: 1728107.517\n",
      "epoch: 677100, train loss: 3.89631769657135, val loss: 3.9667248249053957, ETA in seconds: 1727831.769\n",
      "epoch: 677200, train loss: 3.8950306177139282, val loss: 3.971742630004883, ETA in seconds: 1727585.929\n",
      "epoch: 677300, train loss: 3.8939014673233032, val loss: 3.9779367208480836, ETA in seconds: 1727363.447\n",
      "epoch: 677400, train loss: 3.9007331848144533, val loss: 3.968312120437622, ETA in seconds: 1727130.962\n",
      "epoch: 677500, train loss: 3.884511351585388, val loss: 3.9736787557601927, ETA in seconds: 1726896.579\n",
      "epoch: 677600, train loss: 3.8930225372314453, val loss: 3.9780631065368652, ETA in seconds: 1726596.675\n",
      "epoch: 677700, train loss: 3.8932465076446534, val loss: 3.9753836154937745, ETA in seconds: 1726313.980\n",
      "epoch: 677800, train loss: 3.8931331396102906, val loss: 3.9825553417205812, ETA in seconds: 1726039.198\n",
      "epoch: 677900, train loss: 3.8973809242248536, val loss: 3.9818125009536742, ETA in seconds: 1725763.878\n",
      "epoch: 678000, train loss: 3.8988286733627318, val loss: 3.971338701248169, ETA in seconds: 1725488.994\n",
      "epoch: 678100, train loss: 3.8895612716674806, val loss: 3.9700698137283323, ETA in seconds: 1725223.518\n",
      "epoch: 678200, train loss: 3.892956519126892, val loss: 3.97166485786438, ETA in seconds: 1724949.019\n",
      "epoch: 678300, train loss: 3.8994854927062987, val loss: 3.966577959060669, ETA in seconds: 1724673.500\n",
      "epoch: 678400, train loss: 3.893785309791565, val loss: 3.96955828666687, ETA in seconds: 1724382.125\n",
      "epoch: 678500, train loss: 3.895276737213135, val loss: 3.9625930070877073, ETA in seconds: 1724087.534\n",
      "epoch: 678600, train loss: 3.8900783538818358, val loss: 3.9654610633850096, ETA in seconds: 1723793.402\n",
      "epoch: 678700, train loss: 3.894490885734558, val loss: 3.968488311767578, ETA in seconds: 1723519.824\n",
      "epoch: 678800, train loss: 3.898518514633179, val loss: 3.964579153060913, ETA in seconds: 1723239.650\n",
      "epoch: 678900, train loss: 3.8941006660461426, val loss: 3.968364405632019, ETA in seconds: 1722947.049\n",
      "epoch: 679000, train loss: 3.8920380592346193, val loss: 3.971496915817261, ETA in seconds: 1722664.221\n",
      "epoch: 679100, train loss: 3.8948590040206907, val loss: 3.9689345836639403, ETA in seconds: 1722368.254\n",
      "epoch: 679200, train loss: 3.896682333946228, val loss: 3.9693447828292845, ETA in seconds: 1722067.829\n",
      "epoch: 679300, train loss: 3.8847857236862184, val loss: 3.969062399864197, ETA in seconds: 1721769.097\n",
      "epoch: 679400, train loss: 3.89015679359436, val loss: 3.9718559980392456, ETA in seconds: 1721468.371\n",
      "epoch: 679500, train loss: 3.90121169090271, val loss: 3.9695897579193113, ETA in seconds: 1721167.867\n",
      "epoch: 679600, train loss: 3.8943480491638183, val loss: 3.970585823059082, ETA in seconds: 1720867.747\n",
      "epoch: 679700, train loss: 3.882370376586914, val loss: 3.9676706314086916, ETA in seconds: 1720574.321\n",
      "epoch: 679800, train loss: 3.897275614738464, val loss: 3.9750281095504763, ETA in seconds: 1720304.034\n",
      "epoch: 679900, train loss: 3.9071304321289064, val loss: 3.983311104774475, ETA in seconds: 1720021.666\n",
      "epoch: 680000, train loss: 3.906594753265381, val loss: 3.974188208580017, ETA in seconds: 1719772.739\n",
      "epoch: 680100, train loss: 3.890296149253845, val loss: 3.982027363777161, ETA in seconds: 1719477.887\n",
      "epoch: 680200, train loss: 3.8948933839797975, val loss: 3.9743118286132812, ETA in seconds: 1719167.301\n",
      "epoch: 680300, train loss: 3.8901575565338136, val loss: 3.9725385904312134, ETA in seconds: 1718867.294\n",
      "epoch: 680400, train loss: 3.900477910041809, val loss: 3.968391704559326, ETA in seconds: 1718585.828\n",
      "epoch: 680500, train loss: 3.8950810194015504, val loss: 3.9616673469543455, ETA in seconds: 1718330.724\n",
      "epoch: 680600, train loss: 3.8986105918884277, val loss: 3.9769447803497315, ETA in seconds: 1718039.763\n",
      "epoch: 680700, train loss: 3.8898703813552857, val loss: 3.968487572669983, ETA in seconds: 1717754.251\n",
      "epoch: 680800, train loss: 3.8927948474884033, val loss: 3.9718716144561768, ETA in seconds: 1717472.779\n",
      "epoch: 680900, train loss: 3.8990421056747437, val loss: 3.9689998865127563, ETA in seconds: 1717191.179\n",
      "epoch: 681000, train loss: 3.8898189067840576, val loss: 3.9619340658187867, ETA in seconds: 1716906.778\n",
      "epoch: 681100, train loss: 3.890130305290222, val loss: 3.969237494468689, ETA in seconds: 1716627.201\n",
      "epoch: 681200, train loss: 3.899493145942688, val loss: 3.9716247797012327, ETA in seconds: 1716340.007\n",
      "epoch: 681300, train loss: 3.8944257736206054, val loss: 3.9664915561676026, ETA in seconds: 1716037.813\n",
      "epoch: 681400, train loss: 3.894589328765869, val loss: 3.9631611108779907, ETA in seconds: 1715737.910\n",
      "epoch: 681500, train loss: 3.9004679679870606, val loss: 3.9785218715667723, ETA in seconds: 1715433.389\n",
      "epoch: 681600, train loss: 3.889784908294678, val loss: 3.9772048950195313, ETA in seconds: 1715128.826\n",
      "epoch: 681700, train loss: 3.899156427383423, val loss: 3.979967451095581, ETA in seconds: 1714825.768\n",
      "epoch: 681800, train loss: 3.8856659650802614, val loss: 3.9719369173049928, ETA in seconds: 1714523.972\n",
      "epoch: 681900, train loss: 3.8942776918411255, val loss: 3.967637467384338, ETA in seconds: 1714230.789\n",
      "epoch: 682000, train loss: 3.889604640007019, val loss: 3.966470813751221, ETA in seconds: 1713938.684\n",
      "epoch: 682100, train loss: 3.890100288391113, val loss: 3.97400062084198, ETA in seconds: 1713654.581\n",
      "epoch: 682200, train loss: 3.8931972980499268, val loss: 3.96780207157135, ETA in seconds: 1713372.110\n",
      "epoch: 682300, train loss: 3.8930486679077148, val loss: 3.976433348655701, ETA in seconds: 1713082.989\n",
      "epoch: 682400, train loss: 3.894913911819458, val loss: 3.9554078340530396, ETA in seconds: 1712785.887\n",
      "epoch: 682500, train loss: 3.889455032348633, val loss: 3.9740206003189087, ETA in seconds: 1712494.279\n",
      "epoch: 682600, train loss: 3.8923993825912477, val loss: 3.971378946304321, ETA in seconds: 1712199.593\n",
      "epoch: 682700, train loss: 3.897529196739197, val loss: 3.9696705102920533, ETA in seconds: 1711898.279\n",
      "epoch: 682800, train loss: 3.894337868690491, val loss: 3.980590033531189, ETA in seconds: 1711596.535\n",
      "epoch: 682900, train loss: 3.8925307750701905, val loss: 3.956687164306641, ETA in seconds: 1711301.153\n",
      "epoch: 683000, train loss: 3.8916451215744017, val loss: 3.9624866247177124, ETA in seconds: 1711005.779\n",
      "epoch: 683100, train loss: 3.89397509098053, val loss: 3.977799415588379, ETA in seconds: 1710696.320\n",
      "epoch: 683200, train loss: 3.8887499570846558, val loss: 3.9756875038146973, ETA in seconds: 1710417.145\n",
      "epoch: 683300, train loss: 3.901495122909546, val loss: 3.965919923782349, ETA in seconds: 1710162.939\n",
      "epoch: 683400, train loss: 3.885661005973816, val loss: 3.9631525039672852, ETA in seconds: 1709865.291\n",
      "epoch: 683500, train loss: 3.8974138259887696, val loss: 3.9663722991943358, ETA in seconds: 1709556.857\n",
      "epoch: 683600, train loss: 3.8920979261398316, val loss: 3.9628326892852783, ETA in seconds: 1709272.325\n",
      "epoch: 683700, train loss: 3.896842122077942, val loss: 3.971242094039917, ETA in seconds: 1708977.471\n",
      "epoch: 683800, train loss: 3.8910313129425047, val loss: 3.9621663093566895, ETA in seconds: 1708673.716\n",
      "epoch: 683900, train loss: 3.8895047426223757, val loss: 3.9665775299072266, ETA in seconds: 1708399.560\n",
      "epoch: 684000, train loss: 3.8950745344161986, val loss: 3.9672720670700072, ETA in seconds: 1708094.356\n",
      "epoch: 684100, train loss: 3.897817611694336, val loss: 3.962687540054321, ETA in seconds: 1707778.964\n",
      "epoch: 684200, train loss: 3.897437310218811, val loss: 3.97608106136322, ETA in seconds: 1707483.581\n",
      "epoch: 684300, train loss: 3.8979525804519652, val loss: 3.9779616832733153, ETA in seconds: 1707200.751\n",
      "epoch: 684400, train loss: 3.8937305450439452, val loss: 3.9645566940307617, ETA in seconds: 1706913.044\n",
      "epoch: 684500, train loss: 3.895884466171265, val loss: 3.9610678911209107, ETA in seconds: 1706628.337\n",
      "epoch: 684600, train loss: 3.8973015785217284, val loss: 3.9654090642929076, ETA in seconds: 1706328.693\n",
      "epoch: 684700, train loss: 3.8991364240646362, val loss: 3.971675086021423, ETA in seconds: 1706018.773\n",
      "epoch: 684800, train loss: 3.8912389993667604, val loss: 3.9642273664474486, ETA in seconds: 1705718.831\n",
      "epoch: 684900, train loss: 3.8971940517425536, val loss: 3.9627319812774657, ETA in seconds: 1705432.904\n",
      "epoch: 685000, train loss: 3.892393374443054, val loss: 3.973890686035156, ETA in seconds: 1705126.438\n",
      "epoch: 685100, train loss: 3.8931875228881836, val loss: 3.9669737100601195, ETA in seconds: 1704817.103\n",
      "epoch: 685200, train loss: 3.8927179098129274, val loss: 3.9562217473983763, ETA in seconds: 1704530.127\n",
      "epoch: 685300, train loss: 3.8915802478790282, val loss: 3.964658737182617, ETA in seconds: 1704224.491\n",
      "epoch: 685400, train loss: 3.8958734273910522, val loss: 3.962843632698059, ETA in seconds: 1703914.781\n",
      "epoch: 685500, train loss: 3.889809560775757, val loss: 3.9711891174316407, ETA in seconds: 1703605.161\n",
      "epoch: 685600, train loss: 3.8931947231292723, val loss: 3.9702043771743774, ETA in seconds: 1703296.824\n",
      "epoch: 685700, train loss: 3.892988920211792, val loss: 3.9573968410491944, ETA in seconds: 1702983.954\n",
      "epoch: 685800, train loss: 3.902697706222534, val loss: 3.9743053913116455, ETA in seconds: 1702676.274\n",
      "epoch: 685900, train loss: 3.8905120611190798, val loss: 3.9745758771896362, ETA in seconds: 1702393.772\n",
      "epoch: 686000, train loss: 3.900953006744385, val loss: 3.9716242074966432, ETA in seconds: 1702103.837\n",
      "epoch: 686100, train loss: 3.8874120235443117, val loss: 3.9701203107833862, ETA in seconds: 1701815.956\n",
      "epoch: 686200, train loss: 3.898360180854797, val loss: 3.9721869468688964, ETA in seconds: 1701517.708\n",
      "epoch: 686300, train loss: 3.897252345085144, val loss: 3.965130400657654, ETA in seconds: 1701217.558\n",
      "epoch: 686400, train loss: 3.896787738800049, val loss: 3.965468740463257, ETA in seconds: 1700914.675\n",
      "epoch: 686500, train loss: 3.899248647689819, val loss: 3.9700921535491944, ETA in seconds: 1700605.775\n",
      "epoch: 686600, train loss: 3.892359805107117, val loss: 3.962812972068787, ETA in seconds: 1700296.126\n",
      "epoch: 686700, train loss: 3.8908116102218626, val loss: 3.95942804813385, ETA in seconds: 1700031.626\n",
      "epoch: 686800, train loss: 3.8885316848754883, val loss: 3.9683918714523316, ETA in seconds: 1699749.139\n",
      "epoch: 686900, train loss: 3.895118474960327, val loss: 3.9742911577224733, ETA in seconds: 1699484.617\n",
      "epoch: 687000, train loss: 3.892866826057434, val loss: 3.967818331718445, ETA in seconds: 1699186.085\n",
      "epoch: 687100, train loss: 3.890193295478821, val loss: 3.962464952468872, ETA in seconds: 1698888.989\n",
      "epoch: 687200, train loss: 3.8970417022705077, val loss: 3.973814821243286, ETA in seconds: 1698586.860\n",
      "epoch: 687300, train loss: 3.898240160942078, val loss: 3.9575101852416994, ETA in seconds: 1698273.460\n",
      "epoch: 687400, train loss: 3.893224835395813, val loss: 3.9727319955825804, ETA in seconds: 1697982.373\n",
      "epoch: 687500, train loss: 3.897451138496399, val loss: 3.969002604484558, ETA in seconds: 1697661.584\n",
      "epoch: 687600, train loss: 3.898376798629761, val loss: 3.9729214668273927, ETA in seconds: 1697374.092\n",
      "epoch: 687700, train loss: 3.892779278755188, val loss: 3.975752258300781, ETA in seconds: 1697078.980\n",
      "epoch: 687800, train loss: 3.8937999486923216, val loss: 3.9659552335739137, ETA in seconds: 1696770.792\n",
      "epoch: 687900, train loss: 3.8873199462890624, val loss: 3.9596731185913088, ETA in seconds: 1696453.425\n",
      "epoch: 688000, train loss: 3.890908622741699, val loss: 3.959285020828247, ETA in seconds: 1696148.272\n",
      "epoch: 688100, train loss: 3.8912582635879516, val loss: 3.9765663623809813, ETA in seconds: 1695881.694\n",
      "epoch: 688200, train loss: 3.890329647064209, val loss: 3.9803273677825928, ETA in seconds: 1695648.022\n",
      "epoch: 688300, train loss: 3.8967188358306886, val loss: 3.958683657646179, ETA in seconds: 1695408.993\n",
      "epoch: 688400, train loss: 3.8888177394866945, val loss: 3.9675940036773683, ETA in seconds: 1695117.930\n",
      "epoch: 688500, train loss: 3.8936562299728394, val loss: 3.9615896701812745, ETA in seconds: 1694798.439\n",
      "epoch: 688600, train loss: 3.8925474882125854, val loss: 3.948979449272156, ETA in seconds: 1694534.060\n",
      "epoch: 688700, train loss: 3.8936049699783326, val loss: 3.966475176811218, ETA in seconds: 1694252.534\n",
      "epoch: 688800, train loss: 3.887351226806641, val loss: 3.9661900281906126, ETA in seconds: 1693945.304\n",
      "epoch: 688900, train loss: 3.895875334739685, val loss: 3.970268487930298, ETA in seconds: 1693647.298\n",
      "epoch: 689000, train loss: 3.8972153663635254, val loss: 3.9812793731689453, ETA in seconds: 1693343.664\n",
      "epoch: 689100, train loss: 3.8917398929595945, val loss: 3.9727657318115233, ETA in seconds: 1693039.441\n",
      "epoch: 689200, train loss: 3.893778395652771, val loss: 3.9749709606170653, ETA in seconds: 1692736.780\n",
      "epoch: 689300, train loss: 3.887125253677368, val loss: 3.966640663146973, ETA in seconds: 1692436.929\n",
      "epoch: 689400, train loss: 3.896015429496765, val loss: 3.966926836967468, ETA in seconds: 1692144.565\n",
      "epoch: 689500, train loss: 3.8915209054946898, val loss: 3.9767312526702883, ETA in seconds: 1691869.235\n",
      "epoch: 689600, train loss: 3.8950706243515016, val loss: 3.96978018283844, ETA in seconds: 1691625.926\n",
      "epoch: 689700, train loss: 3.899922227859497, val loss: 3.966246819496155, ETA in seconds: 1691382.576\n",
      "epoch: 689800, train loss: 3.896148157119751, val loss: 3.9746835231781006, ETA in seconds: 1691137.162\n",
      "epoch: 689900, train loss: 3.890038824081421, val loss: 3.9602919816970825, ETA in seconds: 1690865.934\n",
      "epoch: 690000, train loss: 3.8925681591033934, val loss: 3.9660063982009888, ETA in seconds: 1690623.512\n",
      "epoch: 690100, train loss: 3.885324764251709, val loss: 3.9779309272766112, ETA in seconds: 1690369.400\n",
      "epoch: 690200, train loss: 3.895096468925476, val loss: 3.9766616582870484, ETA in seconds: 1690110.665\n",
      "epoch: 690300, train loss: 3.8927335023880003, val loss: 3.9797570943832397, ETA in seconds: 1689846.761\n",
      "epoch: 690400, train loss: 3.8938283920288086, val loss: 3.9603301286697388, ETA in seconds: 1689571.098\n",
      "epoch: 690500, train loss: 3.8907528638839723, val loss: 3.973829412460327, ETA in seconds: 1689287.120\n",
      "epoch: 690600, train loss: 3.899926042556763, val loss: 3.973286819458008, ETA in seconds: 1688974.068\n",
      "epoch: 690700, train loss: 3.8954166650772093, val loss: 3.9745433568954467, ETA in seconds: 1688689.732\n",
      "epoch: 690800, train loss: 3.879854989051819, val loss: 3.9770058393478394, ETA in seconds: 1688396.025\n",
      "epoch: 690900, train loss: 3.90579731464386, val loss: 3.9793829202651976, ETA in seconds: 1688085.609\n",
      "epoch: 691000, train loss: 3.892786955833435, val loss: 3.966094207763672, ETA in seconds: 1687810.095\n",
      "epoch: 691100, train loss: 3.8919679641723635, val loss: 3.9783414602279663, ETA in seconds: 1687569.989\n",
      "epoch: 691200, train loss: 3.890047025680542, val loss: 3.982180190086365, ETA in seconds: 1687330.980\n",
      "epoch: 691300, train loss: 3.899816608428955, val loss: 3.9608491897583007, ETA in seconds: 1687074.796\n",
      "epoch: 691400, train loss: 3.8854124307632447, val loss: 3.9609957218170164, ETA in seconds: 1686815.193\n",
      "epoch: 691500, train loss: 3.8985986948013305, val loss: 3.9750749826431275, ETA in seconds: 1686546.283\n",
      "epoch: 691600, train loss: 3.8903276681900025, val loss: 3.9647414684295654, ETA in seconds: 1686276.997\n",
      "epoch: 691700, train loss: 3.889381504058838, val loss: 3.9699726819992067, ETA in seconds: 1686014.884\n",
      "epoch: 691800, train loss: 3.892853832244873, val loss: 3.9691226959228514, ETA in seconds: 1685749.955\n",
      "epoch: 691900, train loss: 3.8882524490356447, val loss: 3.977694010734558, ETA in seconds: 1685485.987\n",
      "epoch: 692000, train loss: 3.8977043867111205, val loss: 3.97439284324646, ETA in seconds: 1685198.224\n",
      "epoch: 692100, train loss: 3.8935090780258177, val loss: 3.968354368209839, ETA in seconds: 1684891.253\n",
      "epoch: 692200, train loss: 3.8883522033691404, val loss: 3.9630269765853883, ETA in seconds: 1684592.146\n",
      "epoch: 692300, train loss: 3.888218331336975, val loss: 3.96249372959137, ETA in seconds: 1684283.094\n",
      "epoch: 692400, train loss: 3.8961684942245483, val loss: 3.966196823120117, ETA in seconds: 1683979.947\n",
      "epoch: 692500, train loss: 3.8824089050292967, val loss: 3.97288818359375, ETA in seconds: 1683675.384\n",
      "epoch: 692600, train loss: 3.8919299125671385, val loss: 3.9763176679611205, ETA in seconds: 1683358.786\n",
      "epoch: 692700, train loss: 3.892119264602661, val loss: 3.9774609327316286, ETA in seconds: 1683056.158\n",
      "epoch: 692800, train loss: 3.888762927055359, val loss: 3.975378131866455, ETA in seconds: 1682760.708\n",
      "epoch: 692900, train loss: 3.8958351612091064, val loss: 3.9606892108917235, ETA in seconds: 1682462.689\n",
      "epoch: 693000, train loss: 3.900310230255127, val loss: 3.9726151704788206, ETA in seconds: 1682159.440\n",
      "epoch: 693100, train loss: 3.894077730178833, val loss: 3.9679678440093995, ETA in seconds: 1681868.642\n",
      "epoch: 693200, train loss: 3.8862637996673586, val loss: 3.964598536491394, ETA in seconds: 1681552.461\n",
      "epoch: 693300, train loss: 3.8848796606063845, val loss: 3.960397243499756, ETA in seconds: 1681229.189\n",
      "epoch: 693400, train loss: 3.8849138259887694, val loss: 3.964809036254883, ETA in seconds: 1680917.456\n",
      "epoch: 693500, train loss: 3.887250375747681, val loss: 3.980678749084473, ETA in seconds: 1680612.740\n",
      "epoch: 693600, train loss: 3.9033287286758425, val loss: 3.9671037912368776, ETA in seconds: 1680308.168\n",
      "epoch: 693700, train loss: 3.89316246509552, val loss: 3.9636648178100584, ETA in seconds: 1680001.019\n",
      "epoch: 693800, train loss: 3.8973248958587647, val loss: 3.9672280073165895, ETA in seconds: 1679694.348\n",
      "epoch: 693900, train loss: 3.892373037338257, val loss: 3.967047119140625, ETA in seconds: 1679406.705\n",
      "epoch: 694000, train loss: 3.885837936401367, val loss: 3.971151828765869, ETA in seconds: 1679142.915\n",
      "epoch: 694100, train loss: 3.890256714820862, val loss: 3.973486089706421, ETA in seconds: 1678872.282\n",
      "epoch: 694200, train loss: 3.887467622756958, val loss: 3.9759986877441404, ETA in seconds: 1678601.289\n",
      "epoch: 694300, train loss: 3.8917853832244873, val loss: 3.9748898267745973, ETA in seconds: 1678327.555\n",
      "epoch: 694400, train loss: 3.8861333608627318, val loss: 3.956243395805359, ETA in seconds: 1678054.078\n",
      "epoch: 694500, train loss: 3.9004238128662108, val loss: 3.972281980514526, ETA in seconds: 1677778.248\n",
      "epoch: 694600, train loss: 3.895499753952026, val loss: 3.968482732772827, ETA in seconds: 1677503.318\n",
      "epoch: 694700, train loss: 3.8950536012649537, val loss: 3.9651923894882204, ETA in seconds: 1677228.327\n",
      "epoch: 694800, train loss: 3.8959298610687254, val loss: 3.9779455423355103, ETA in seconds: 1676952.803\n",
      "epoch: 694900, train loss: 3.8905690193176268, val loss: 3.966619086265564, ETA in seconds: 1676650.497\n",
      "epoch: 695000, train loss: 3.8900530099868775, val loss: 3.957713413238525, ETA in seconds: 1676330.061\n",
      "epoch: 695100, train loss: 3.884867262840271, val loss: 3.9783128023147585, ETA in seconds: 1676013.586\n",
      "epoch: 695200, train loss: 3.8923207998275755, val loss: 3.9692932844161986, ETA in seconds: 1675703.700\n",
      "epoch: 695300, train loss: 3.896682929992676, val loss: 3.9757248878479006, ETA in seconds: 1675389.535\n",
      "epoch: 695400, train loss: 3.8988470792770387, val loss: 3.972844099998474, ETA in seconds: 1675063.938\n",
      "epoch: 695500, train loss: 3.9021281242370605, val loss: 3.972557544708252, ETA in seconds: 1674753.962\n",
      "epoch: 695600, train loss: 3.8956276178359985, val loss: 3.981284809112549, ETA in seconds: 1674430.636\n",
      "epoch: 695700, train loss: 3.8923969984054567, val loss: 3.9741239309310914, ETA in seconds: 1674108.173\n",
      "epoch: 695800, train loss: 3.888554263114929, val loss: 3.9599513530731203, ETA in seconds: 1673816.373\n",
      "epoch: 695900, train loss: 3.8865795373916625, val loss: 3.9877893209457396, ETA in seconds: 1673559.962\n",
      "epoch: 696000, train loss: 3.899937105178833, val loss: 3.9810304403305055, ETA in seconds: 1673302.398\n",
      "epoch: 696100, train loss: 3.8987343072891236, val loss: 3.956740212440491, ETA in seconds: 1673013.241\n",
      "epoch: 696200, train loss: 3.8943342924118043, val loss: 3.954187273979187, ETA in seconds: 1672721.828\n",
      "epoch: 696300, train loss: 3.8969549894332887, val loss: 3.9670274019241334, ETA in seconds: 1672409.907\n",
      "epoch: 696400, train loss: 3.894300866127014, val loss: 3.9682121753692625, ETA in seconds: 1672084.422\n",
      "epoch: 696500, train loss: 3.9023281574249267, val loss: 3.965242695808411, ETA in seconds: 1671759.104\n",
      "epoch: 696600, train loss: 3.892578363418579, val loss: 3.975783085823059, ETA in seconds: 1671433.293\n",
      "epoch: 696700, train loss: 3.8873361110687257, val loss: 3.9602967739105224, ETA in seconds: 1671110.155\n",
      "epoch: 696800, train loss: 3.8880373001098634, val loss: 3.971544122695923, ETA in seconds: 1670785.797\n",
      "epoch: 696900, train loss: 3.889062738418579, val loss: 3.960332655906677, ETA in seconds: 1670461.868\n",
      "epoch: 697000, train loss: 3.8912923097610475, val loss: 3.9702286958694457, ETA in seconds: 1670154.789\n",
      "epoch: 697100, train loss: 3.8930969715118406, val loss: 3.971489357948303, ETA in seconds: 1669846.160\n",
      "epoch: 697200, train loss: 3.89564995765686, val loss: 3.964433026313782, ETA in seconds: 1669528.691\n",
      "epoch: 697300, train loss: 3.896892023086548, val loss: 3.974510645866394, ETA in seconds: 1669202.171\n",
      "epoch: 697400, train loss: 3.8994470596313477, val loss: 3.9742275714874267, ETA in seconds: 1668873.352\n",
      "epoch: 697500, train loss: 3.8835685729980467, val loss: 3.958474063873291, ETA in seconds: 1668545.068\n",
      "epoch: 697600, train loss: 3.893854761123657, val loss: 3.9719481468200684, ETA in seconds: 1668212.067\n",
      "epoch: 697700, train loss: 3.893428826332092, val loss: 3.9617917776107787, ETA in seconds: 1667903.308\n",
      "epoch: 697800, train loss: 3.8978545665740967, val loss: 3.96086790561676, ETA in seconds: 1667596.374\n",
      "epoch: 697900, train loss: 3.9073972940444945, val loss: 3.9689799547195435, ETA in seconds: 1667291.147\n",
      "epoch: 698000, train loss: 3.8973397970199586, val loss: 3.9587408542633056, ETA in seconds: 1666984.366\n",
      "epoch: 698100, train loss: 3.8916588306427, val loss: 3.96385395526886, ETA in seconds: 1666677.206\n",
      "epoch: 698200, train loss: 3.906484913825989, val loss: 3.9622602224349976, ETA in seconds: 1666373.027\n",
      "epoch: 698300, train loss: 3.8953275203704836, val loss: 3.9707221746444703, ETA in seconds: 1666067.459\n",
      "epoch: 698400, train loss: 3.9004546642303466, val loss: 3.9759464502334594, ETA in seconds: 1665761.798\n",
      "epoch: 698500, train loss: 3.8960344791412354, val loss: 3.977762508392334, ETA in seconds: 1665452.406\n",
      "epoch: 698600, train loss: 3.8819934844970705, val loss: 3.970624804496765, ETA in seconds: 1665139.005\n",
      "epoch: 698700, train loss: 3.8981148481369017, val loss: 3.968729758262634, ETA in seconds: 1664813.306\n",
      "epoch: 698800, train loss: 3.8927151918411256, val loss: 3.970489239692688, ETA in seconds: 1664518.758\n",
      "epoch: 698900, train loss: 3.902990961074829, val loss: 3.9767709255218504, ETA in seconds: 1664233.364\n",
      "epoch: 699000, train loss: 3.9020016193389893, val loss: 3.966599678993225, ETA in seconds: 1663953.409\n",
      "epoch: 699100, train loss: 3.8952792167663572, val loss: 3.980370020866394, ETA in seconds: 1663671.206\n",
      "epoch: 699200, train loss: 3.8923486471176147, val loss: 3.972867488861084, ETA in seconds: 1663387.823\n",
      "epoch: 699300, train loss: 3.8972241163253782, val loss: 3.970213532447815, ETA in seconds: 1663107.410\n",
      "epoch: 699400, train loss: 3.8936232566833495, val loss: 3.9637186765670775, ETA in seconds: 1662830.605\n",
      "epoch: 699500, train loss: 3.8891428232192995, val loss: 3.9823618173599242, ETA in seconds: 1662511.353\n",
      "epoch: 699600, train loss: 3.892316961288452, val loss: 3.970259428024292, ETA in seconds: 1662194.467\n",
      "epoch: 699700, train loss: 3.8923669338226317, val loss: 3.976022219657898, ETA in seconds: 1661875.050\n",
      "epoch: 699800, train loss: 3.8825336694717407, val loss: 3.969245195388794, ETA in seconds: 1661552.203\n",
      "epoch: 699900, train loss: 3.8990304231643678, val loss: 3.9643691539764405, ETA in seconds: 1661228.063\n",
      "epoch: 700000, train loss: 3.894258451461792, val loss: 3.9670823097229, ETA in seconds: 1660905.450\n",
      "epoch: 700100, train loss: 3.8921459436416628, val loss: 3.968679428100586, ETA in seconds: 1660581.361\n",
      "epoch: 700200, train loss: 3.9025119066238405, val loss: 3.9762781381607057, ETA in seconds: 1660254.687\n",
      "epoch: 700300, train loss: 3.8927337169647216, val loss: 3.9668034076690675, ETA in seconds: 1659948.563\n",
      "epoch: 700400, train loss: 3.8970724821090696, val loss: 3.9736517429351808, ETA in seconds: 1659661.755\n",
      "epoch: 700500, train loss: 3.894467282295227, val loss: 3.973540472984314, ETA in seconds: 1659373.755\n",
      "epoch: 700600, train loss: 3.893096852302551, val loss: 3.978028655052185, ETA in seconds: 1659086.794\n",
      "epoch: 700700, train loss: 3.885587978363037, val loss: 3.9725996017456056, ETA in seconds: 1658799.043\n",
      "epoch: 700800, train loss: 3.8966990232467653, val loss: 3.977999448776245, ETA in seconds: 1658510.218\n",
      "epoch: 700900, train loss: 3.887730622291565, val loss: 3.9753719568252563, ETA in seconds: 1658218.927\n",
      "epoch: 701000, train loss: 3.892820692062378, val loss: 3.9635055541992186, ETA in seconds: 1657888.587\n",
      "epoch: 701100, train loss: 3.89616219997406, val loss: 3.9810808658599854, ETA in seconds: 1657555.781\n",
      "epoch: 701200, train loss: 3.886598563194275, val loss: 3.974596953392029, ETA in seconds: 1657263.611\n",
      "epoch: 701300, train loss: 3.894481348991394, val loss: 3.9765153408050535, ETA in seconds: 1656950.255\n",
      "epoch: 701400, train loss: 3.889190340042114, val loss: 3.9766391277313233, ETA in seconds: 1656616.100\n",
      "epoch: 701500, train loss: 3.873602557182312, val loss: 3.972311568260193, ETA in seconds: 1656280.546\n",
      "epoch: 701600, train loss: 3.8865139722824096, val loss: 3.9684062242507934, ETA in seconds: 1655992.418\n",
      "epoch: 701700, train loss: 3.8889029741287233, val loss: 3.9722137212753297, ETA in seconds: 1655705.951\n",
      "epoch: 701800, train loss: 3.8908640146255493, val loss: 3.963264536857605, ETA in seconds: 1655396.906\n",
      "epoch: 701900, train loss: 3.8994069337844848, val loss: 3.9737008810043335, ETA in seconds: 1655056.995\n",
      "epoch: 702000, train loss: 3.8928597927093507, val loss: 3.9721049070358276, ETA in seconds: 1654718.240\n",
      "epoch: 702100, train loss: 3.8880264520645142, val loss: 3.955839967727661, ETA in seconds: 1654379.919\n",
      "epoch: 702200, train loss: 3.8938413143157957, val loss: 3.9729025602340697, ETA in seconds: 1654042.082\n",
      "epoch: 702300, train loss: 3.8920707941055297, val loss: 3.9766320466995237, ETA in seconds: 1653706.001\n",
      "epoch: 702400, train loss: 3.8876218795776367, val loss: 3.9667173862457275, ETA in seconds: 1653367.376\n",
      "epoch: 702500, train loss: 3.894374704360962, val loss: 3.9697211027145385, ETA in seconds: 1653027.939\n",
      "epoch: 702600, train loss: 3.8976596117019655, val loss: 3.9773488521575926, ETA in seconds: 1652688.305\n",
      "epoch: 702700, train loss: 3.889832043647766, val loss: 3.9641340732574464, ETA in seconds: 1652349.226\n",
      "epoch: 702800, train loss: 3.8945169687271117, val loss: 3.981183671951294, ETA in seconds: 1652010.026\n",
      "epoch: 702900, train loss: 3.893175148963928, val loss: 3.9824305534362794, ETA in seconds: 1651669.452\n",
      "epoch: 703000, train loss: 3.89385085105896, val loss: 3.961130475997925, ETA in seconds: 1651334.577\n",
      "epoch: 703100, train loss: 3.891923975944519, val loss: 3.963766646385193, ETA in seconds: 1650993.651\n",
      "epoch: 703200, train loss: 3.8942078590393066, val loss: 3.964288854598999, ETA in seconds: 1650652.268\n",
      "epoch: 703300, train loss: 3.883927607536316, val loss: 3.956302785873413, ETA in seconds: 1650320.344\n",
      "epoch: 703400, train loss: 3.8849931240081785, val loss: 3.970588445663452, ETA in seconds: 1650027.326\n",
      "epoch: 703500, train loss: 3.8941222429275513, val loss: 3.9684383869171143, ETA in seconds: 1649732.944\n",
      "epoch: 703600, train loss: 3.893052816390991, val loss: 3.9760865449905394, ETA in seconds: 1649439.853\n",
      "epoch: 703700, train loss: 3.8984917879104612, val loss: 3.9689340114593508, ETA in seconds: 1649149.298\n",
      "epoch: 703800, train loss: 3.8961329221725465, val loss: 3.967455291748047, ETA in seconds: 1648855.222\n",
      "epoch: 703900, train loss: 3.893147873878479, val loss: 3.972043180465698, ETA in seconds: 1648562.500\n",
      "epoch: 704000, train loss: 3.8879934549331665, val loss: 3.977809023857117, ETA in seconds: 1648268.375\n",
      "epoch: 704100, train loss: 3.8892749071121218, val loss: 3.9762596130371093, ETA in seconds: 1647973.844\n",
      "epoch: 704200, train loss: 3.884124684333801, val loss: 3.9593021631240846, ETA in seconds: 1647680.275\n",
      "epoch: 704300, train loss: 3.889728474617004, val loss: 3.9654167175292967, ETA in seconds: 1647387.820\n",
      "epoch: 704400, train loss: 3.8887607574462892, val loss: 3.9809297561645507, ETA in seconds: 1647092.383\n",
      "epoch: 704500, train loss: 3.9058122873306274, val loss: 3.960973834991455, ETA in seconds: 1646778.171\n",
      "epoch: 704600, train loss: 3.8874430894851684, val loss: 3.967317485809326, ETA in seconds: 1646437.699\n",
      "epoch: 704700, train loss: 3.89039089679718, val loss: 3.9726494789123534, ETA in seconds: 1646094.971\n",
      "epoch: 704800, train loss: 3.891784644126892, val loss: 3.9721960544586183, ETA in seconds: 1645753.040\n",
      "epoch: 704900, train loss: 3.895977568626404, val loss: 3.9649554014205934, ETA in seconds: 1645417.878\n",
      "epoch: 705000, train loss: 3.8971446990966796, val loss: 3.9734262943267824, ETA in seconds: 1645076.513\n",
      "epoch: 705100, train loss: 3.8885016441345215, val loss: 3.9741143465042112, ETA in seconds: 1644743.288\n",
      "epoch: 705200, train loss: 3.8908227682113647, val loss: 3.980626702308655, ETA in seconds: 1644451.001\n",
      "epoch: 705300, train loss: 3.8943395614624023, val loss: 3.9737722873687744, ETA in seconds: 1644157.106\n",
      "epoch: 705400, train loss: 3.8963886499404907, val loss: 3.960145926475525, ETA in seconds: 1643863.979\n",
      "epoch: 705500, train loss: 3.8979258060455324, val loss: 3.977902364730835, ETA in seconds: 1643570.072\n",
      "epoch: 705600, train loss: 3.889058542251587, val loss: 3.9800134181976317, ETA in seconds: 1643258.779\n",
      "epoch: 705700, train loss: 3.8930076360702515, val loss: 3.973550033569336, ETA in seconds: 1642920.023\n",
      "epoch: 705800, train loss: 3.899003100395203, val loss: 3.9758720636367797, ETA in seconds: 1642577.749\n",
      "epoch: 705900, train loss: 3.898602104187012, val loss: 3.963161659240723, ETA in seconds: 1642234.189\n",
      "epoch: 706000, train loss: 3.8946976900100707, val loss: 3.9732618808746336, ETA in seconds: 1641891.911\n",
      "epoch: 706100, train loss: 3.8992510557174684, val loss: 3.9710307359695434, ETA in seconds: 1641550.044\n",
      "epoch: 706200, train loss: 3.8978629112243652, val loss: 3.9591270685195923, ETA in seconds: 1641205.846\n",
      "epoch: 706300, train loss: 3.885422468185425, val loss: 3.9727355241775513, ETA in seconds: 1640866.531\n",
      "epoch: 706400, train loss: 3.896160387992859, val loss: 3.9670368432998657, ETA in seconds: 1640523.629\n",
      "epoch: 706500, train loss: 3.8875341415405273, val loss: 3.9783680200576783, ETA in seconds: 1640179.961\n",
      "epoch: 706600, train loss: 3.90078284740448, val loss: 3.9748674869537353, ETA in seconds: 1639835.209\n",
      "epoch: 706700, train loss: 3.8953429460525513, val loss: 3.969913649559021, ETA in seconds: 1639511.451\n",
      "epoch: 706800, train loss: 3.895737719535828, val loss: 3.9769877910614015, ETA in seconds: 1639214.543\n",
      "epoch: 706900, train loss: 3.898128080368042, val loss: 3.969598913192749, ETA in seconds: 1638918.186\n",
      "epoch: 707000, train loss: 3.895897364616394, val loss: 3.974673843383789, ETA in seconds: 1638623.389\n",
      "epoch: 707100, train loss: 3.8932126998901366, val loss: 3.9728969812393187, ETA in seconds: 1638333.341\n",
      "epoch: 707200, train loss: 3.8923266649246218, val loss: 3.977415943145752, ETA in seconds: 1638037.017\n",
      "epoch: 707300, train loss: 3.8935890436172484, val loss: 3.9673491954803466, ETA in seconds: 1637738.902\n",
      "epoch: 707400, train loss: 3.903375768661499, val loss: 3.976177453994751, ETA in seconds: 1637392.662\n",
      "epoch: 707500, train loss: 3.885683512687683, val loss: 3.978938341140747, ETA in seconds: 1637049.523\n",
      "epoch: 707600, train loss: 3.8959783792495726, val loss: 3.9703691482543944, ETA in seconds: 1636714.830\n",
      "epoch: 707700, train loss: 3.891078734397888, val loss: 3.9752652645111084, ETA in seconds: 1636392.491\n",
      "epoch: 707800, train loss: 3.898800015449524, val loss: 3.9677491903305055, ETA in seconds: 1636099.264\n",
      "epoch: 707900, train loss: 3.8824859380722048, val loss: 3.976300597190857, ETA in seconds: 1635803.630\n",
      "epoch: 708000, train loss: 3.897018241882324, val loss: 3.971583127975464, ETA in seconds: 1635514.303\n",
      "epoch: 708100, train loss: 3.9023566484451293, val loss: 3.9583950996398927, ETA in seconds: 1635173.831\n",
      "epoch: 708200, train loss: 3.903100347518921, val loss: 3.97510244846344, ETA in seconds: 1634824.351\n",
      "epoch: 708300, train loss: 3.9124108791351317, val loss: 3.969907855987549, ETA in seconds: 1634475.996\n",
      "epoch: 708400, train loss: 3.8845885753631593, val loss: 3.9680493593215944, ETA in seconds: 1634130.657\n",
      "epoch: 708500, train loss: 3.887039852142334, val loss: 3.9732715606689455, ETA in seconds: 1633789.164\n",
      "epoch: 708600, train loss: 3.897544813156128, val loss: 3.963767099380493, ETA in seconds: 1633435.254\n",
      "epoch: 708700, train loss: 3.899759888648987, val loss: 3.983266735076904, ETA in seconds: 1633098.716\n",
      "epoch: 708800, train loss: 3.890410614013672, val loss: 3.971721959114075, ETA in seconds: 1632800.334\n",
      "epoch: 708900, train loss: 3.8951773405075074, val loss: 3.9663954973220825, ETA in seconds: 1632479.395\n",
      "epoch: 709000, train loss: 3.896040654182434, val loss: 3.975167489051819, ETA in seconds: 1632156.572\n",
      "epoch: 709100, train loss: 3.888053560256958, val loss: 3.9680517673492433, ETA in seconds: 1631808.893\n",
      "epoch: 709200, train loss: 3.9029444217681886, val loss: 3.971281051635742, ETA in seconds: 1631463.884\n",
      "epoch: 709300, train loss: 3.901504373550415, val loss: 3.9641167163848876, ETA in seconds: 1631124.285\n",
      "epoch: 709400, train loss: 3.9026296615600584, val loss: 3.9711266279220583, ETA in seconds: 1630791.965\n",
      "epoch: 709500, train loss: 3.889466643333435, val loss: 3.971967005729675, ETA in seconds: 1630455.179\n",
      "epoch: 709600, train loss: 3.8980390310287474, val loss: 3.9812169790267946, ETA in seconds: 1630123.510\n",
      "epoch: 709700, train loss: 3.8963584184646605, val loss: 3.9653282165527344, ETA in seconds: 1629806.893\n",
      "epoch: 709800, train loss: 3.897277498245239, val loss: 3.983130192756653, ETA in seconds: 1629461.658\n",
      "epoch: 709900, train loss: 3.8860744714736937, val loss: 3.9688504934310913, ETA in seconds: 1629130.801\n",
      "epoch: 710000, train loss: 3.8969125270843508, val loss: 3.973967432975769, ETA in seconds: 1628829.056\n",
      "epoch: 710100, train loss: 3.8888476848602296, val loss: 3.967622399330139, ETA in seconds: 1628508.275\n",
      "epoch: 710200, train loss: 3.886423444747925, val loss: 3.969666361808777, ETA in seconds: 1628167.269\n",
      "epoch: 710300, train loss: 3.8917088747024535, val loss: 3.979959011077881, ETA in seconds: 1627830.356\n",
      "epoch: 710400, train loss: 3.895923066139221, val loss: 3.956362557411194, ETA in seconds: 1627486.319\n",
      "epoch: 710500, train loss: 3.8888004779815675, val loss: 3.976890730857849, ETA in seconds: 1627162.546\n",
      "epoch: 710600, train loss: 3.8932263135910032, val loss: 3.9834463119506838, ETA in seconds: 1626846.284\n",
      "epoch: 710700, train loss: 3.8943251609802245, val loss: 3.9747090578079223, ETA in seconds: 1626499.907\n",
      "epoch: 710800, train loss: 3.8931942701339723, val loss: 3.9721062183380127, ETA in seconds: 1626178.851\n",
      "epoch: 710900, train loss: 3.8942880868911742, val loss: 3.9737565755844115, ETA in seconds: 1625858.176\n",
      "epoch: 711000, train loss: 3.893076848983765, val loss: 3.969135355949402, ETA in seconds: 1625552.092\n",
      "epoch: 711100, train loss: 3.9080472230911254, val loss: 3.9682809352874755, ETA in seconds: 1625200.119\n",
      "epoch: 711200, train loss: 3.8964179515838624, val loss: 3.97440242767334, ETA in seconds: 1624858.254\n",
      "epoch: 711300, train loss: 3.8816189765930176, val loss: 3.968206453323364, ETA in seconds: 1624510.657\n",
      "epoch: 711400, train loss: 3.895896553993225, val loss: 3.973717141151428, ETA in seconds: 1624163.298\n",
      "epoch: 711500, train loss: 3.897205424308777, val loss: 3.9771613836288453, ETA in seconds: 1623810.827\n",
      "epoch: 711600, train loss: 3.8987279415130613, val loss: 3.9724878311157226, ETA in seconds: 1623470.064\n",
      "epoch: 711700, train loss: 3.9064356803894045, val loss: 3.9696011304855348, ETA in seconds: 1623172.185\n",
      "epoch: 711800, train loss: 3.8978806257247927, val loss: 3.9696991443634033, ETA in seconds: 1622860.866\n",
      "epoch: 711900, train loss: 3.9009297370910643, val loss: 3.9706087827682497, ETA in seconds: 1622533.119\n",
      "epoch: 712000, train loss: 3.8865823268890383, val loss: 3.966389608383179, ETA in seconds: 1622206.171\n",
      "epoch: 712100, train loss: 3.893862175941467, val loss: 3.974933457374573, ETA in seconds: 1621880.331\n",
      "epoch: 712200, train loss: 3.895337176322937, val loss: 3.9716891050338745, ETA in seconds: 1621571.336\n",
      "epoch: 712300, train loss: 3.8895310878753664, val loss: 3.9613041400909426, ETA in seconds: 1621272.830\n",
      "epoch: 712400, train loss: 3.884862947463989, val loss: 3.974173069000244, ETA in seconds: 1620944.463\n",
      "epoch: 712500, train loss: 3.896632742881775, val loss: 3.9690774202346804, ETA in seconds: 1620613.970\n",
      "epoch: 712600, train loss: 3.892204427719116, val loss: 3.9801605701446534, ETA in seconds: 1620287.543\n",
      "epoch: 712700, train loss: 3.8945486545562744, val loss: 3.970069932937622, ETA in seconds: 1619957.997\n",
      "epoch: 712800, train loss: 3.8996486902236938, val loss: 3.968643045425415, ETA in seconds: 1619628.794\n",
      "epoch: 712900, train loss: 3.8979198217391966, val loss: 3.985471272468567, ETA in seconds: 1619301.757\n",
      "epoch: 713000, train loss: 3.8814546346664427, val loss: 3.965422487258911, ETA in seconds: 1618956.080\n",
      "epoch: 713100, train loss: 3.8861523628234864, val loss: 3.9694177150726317, ETA in seconds: 1618620.550\n",
      "epoch: 713200, train loss: 3.896363544464111, val loss: 3.976114869117737, ETA in seconds: 1618286.549\n",
      "epoch: 713300, train loss: 3.89249587059021, val loss: 3.9694797515869142, ETA in seconds: 1617989.271\n",
      "epoch: 713400, train loss: 3.9017717838287354, val loss: 3.9791144371032714, ETA in seconds: 1617650.089\n",
      "epoch: 713500, train loss: 3.8942773818969725, val loss: 3.9681264638900755, ETA in seconds: 1617309.265\n",
      "epoch: 713600, train loss: 3.893897604942322, val loss: 3.9829774379730223, ETA in seconds: 1616966.961\n",
      "epoch: 713700, train loss: 3.8913017749786376, val loss: 3.968809795379639, ETA in seconds: 1616676.011\n",
      "epoch: 713800, train loss: 3.8961050271987916, val loss: 3.9758836030960083, ETA in seconds: 1616350.119\n",
      "epoch: 713900, train loss: 3.8908010005950926, val loss: 3.972225618362427, ETA in seconds: 1616064.417\n",
      "epoch: 714000, train loss: 3.894551944732666, val loss: 3.979321599006653, ETA in seconds: 1615756.084\n",
      "epoch: 714100, train loss: 3.898431587219238, val loss: 3.971677374839783, ETA in seconds: 1615417.257\n",
      "epoch: 714200, train loss: 3.8929200887680055, val loss: 3.967592549324036, ETA in seconds: 1615072.296\n",
      "epoch: 714300, train loss: 3.887994337081909, val loss: 3.9768337488174437, ETA in seconds: 1614720.294\n",
      "epoch: 714400, train loss: 3.892333483695984, val loss: 3.96022572517395, ETA in seconds: 1614379.642\n",
      "epoch: 714500, train loss: 3.8933467149734495, val loss: 3.9675916910171507, ETA in seconds: 1614065.504\n",
      "epoch: 714600, train loss: 3.901898241043091, val loss: 3.9766963958740233, ETA in seconds: 1613731.368\n",
      "epoch: 714700, train loss: 3.891377854347229, val loss: 3.9598864555358886, ETA in seconds: 1613428.419\n",
      "epoch: 714800, train loss: 3.890140247344971, val loss: 3.9773066520690916, ETA in seconds: 1613071.145\n",
      "epoch: 714900, train loss: 3.8962871074676513, val loss: 3.978144407272339, ETA in seconds: 1612755.272\n",
      "epoch: 715000, train loss: 3.890133833885193, val loss: 3.9748080492019655, ETA in seconds: 1612429.958\n",
      "epoch: 715100, train loss: 3.8848434925079345, val loss: 3.963489055633545, ETA in seconds: 1612117.136\n",
      "epoch: 715200, train loss: 3.8899789094924926, val loss: 3.9658863306045533, ETA in seconds: 1611803.430\n",
      "epoch: 715300, train loss: 3.8889529943466186, val loss: 3.975137186050415, ETA in seconds: 1611489.898\n",
      "epoch: 715400, train loss: 3.893553042411804, val loss: 3.9658029317855834, ETA in seconds: 1611178.502\n",
      "epoch: 715500, train loss: 3.8900168180465697, val loss: 3.9691149473190306, ETA in seconds: 1610864.613\n",
      "epoch: 715600, train loss: 3.889735198020935, val loss: 3.9731571435928346, ETA in seconds: 1610503.814\n",
      "epoch: 715700, train loss: 3.8915937423706053, val loss: 3.9725780963897703, ETA in seconds: 1610155.485\n",
      "epoch: 715800, train loss: 3.8871172666549683, val loss: 3.977036380767822, ETA in seconds: 1609801.173\n",
      "epoch: 715900, train loss: 3.8863362789154055, val loss: 3.9740679025650025, ETA in seconds: 1609477.512\n",
      "epoch: 716000, train loss: 3.885895109176636, val loss: 3.977703022956848, ETA in seconds: 1609167.321\n",
      "epoch: 716100, train loss: 3.8831942796707155, val loss: 3.9808018445968627, ETA in seconds: 1608858.763\n",
      "epoch: 716200, train loss: 3.8899890184402466, val loss: 3.9728925466537475, ETA in seconds: 1608548.463\n",
      "epoch: 716300, train loss: 3.896410346031189, val loss: 3.976187062263489, ETA in seconds: 1608236.990\n",
      "epoch: 716400, train loss: 3.895126795768738, val loss: 3.9756977558135986, ETA in seconds: 1607924.977\n",
      "epoch: 716500, train loss: 3.8893768548965455, val loss: 3.974988579750061, ETA in seconds: 1607595.993\n",
      "epoch: 716600, train loss: 3.893514895439148, val loss: 3.96857590675354, ETA in seconds: 1607249.100\n",
      "epoch: 716700, train loss: 3.88957679271698, val loss: 3.9763455152511598, ETA in seconds: 1606893.098\n",
      "epoch: 716800, train loss: 3.8860424041748045, val loss: 3.9648847341537476, ETA in seconds: 1606542.357\n",
      "epoch: 716900, train loss: 3.8891492605209352, val loss: 3.9755748748779296, ETA in seconds: 1606190.800\n",
      "epoch: 717000, train loss: 3.8986470460891725, val loss: 3.973731589317322, ETA in seconds: 1605848.632\n",
      "epoch: 717100, train loss: 3.8982875108718873, val loss: 3.9839317083358763, ETA in seconds: 1605488.843\n",
      "epoch: 717200, train loss: 3.8936397552490236, val loss: 3.971670913696289, ETA in seconds: 1605134.926\n",
      "epoch: 717300, train loss: 3.8940837383270264, val loss: 3.966663932800293, ETA in seconds: 1604779.199\n",
      "epoch: 717400, train loss: 3.894826579093933, val loss: 3.9627341985702516, ETA in seconds: 1604433.863\n",
      "epoch: 717500, train loss: 3.8978007078170775, val loss: 3.98131902217865, ETA in seconds: 1604122.639\n",
      "epoch: 717600, train loss: 3.881997513771057, val loss: 3.974078583717346, ETA in seconds: 1603809.021\n",
      "epoch: 717700, train loss: 3.8900558233261107, val loss: 3.966277837753296, ETA in seconds: 1603498.620\n",
      "epoch: 717800, train loss: 3.8823031902313234, val loss: 3.9799034357070924, ETA in seconds: 1603162.455\n",
      "epoch: 717900, train loss: 3.9017868041992188, val loss: 3.983508491516113, ETA in seconds: 1602820.161\n",
      "epoch: 718000, train loss: 3.8919849395751953, val loss: 3.9774408102035523, ETA in seconds: 1602488.336\n",
      "epoch: 718100, train loss: 3.8909345626831056, val loss: 3.98587019443512, ETA in seconds: 1602176.822\n",
      "epoch: 718200, train loss: 3.9010931730270384, val loss: 3.9703559398651125, ETA in seconds: 1601861.210\n",
      "epoch: 718300, train loss: 3.8869242668151855, val loss: 3.9780792951583863, ETA in seconds: 1601546.297\n",
      "epoch: 718400, train loss: 3.89228150844574, val loss: 3.9846652269363405, ETA in seconds: 1601231.250\n",
      "epoch: 718500, train loss: 3.898240876197815, val loss: 3.9705904006958006, ETA in seconds: 1600915.362\n",
      "epoch: 718600, train loss: 3.892813801765442, val loss: 3.974284791946411, ETA in seconds: 1600600.730\n",
      "epoch: 718700, train loss: 3.8928802728652956, val loss: 3.9721380710601806, ETA in seconds: 1600287.829\n",
      "epoch: 718800, train loss: 3.9026015996932983, val loss: 3.988902187347412, ETA in seconds: 1599939.278\n",
      "epoch: 718900, train loss: 3.9002906322479247, val loss: 3.971920108795166, ETA in seconds: 1599578.557\n",
      "epoch: 719000, train loss: 3.8983116865158083, val loss: 3.975360870361328, ETA in seconds: 1599217.673\n",
      "epoch: 719100, train loss: 3.887502336502075, val loss: 3.9698643684387207, ETA in seconds: 1598868.589\n",
      "epoch: 719200, train loss: 3.8973422288894652, val loss: 3.977074956893921, ETA in seconds: 1598556.146\n",
      "epoch: 719300, train loss: 3.897462558746338, val loss: 3.976480221748352, ETA in seconds: 1598203.573\n",
      "epoch: 719400, train loss: 3.8894582271575926, val loss: 3.976605200767517, ETA in seconds: 1597851.676\n",
      "epoch: 719500, train loss: 3.8858216524124147, val loss: 3.963138771057129, ETA in seconds: 1597503.381\n",
      "epoch: 719600, train loss: 3.889280843734741, val loss: 3.9647527933120728, ETA in seconds: 1597184.240\n",
      "epoch: 719700, train loss: 3.8886526823043823, val loss: 3.966543364524841, ETA in seconds: 1596864.157\n",
      "epoch: 719800, train loss: 3.891988182067871, val loss: 3.9726651191711424, ETA in seconds: 1596542.982\n",
      "epoch: 719900, train loss: 3.89072208404541, val loss: 3.9746765375137327, ETA in seconds: 1596193.950\n",
      "epoch: 720000, train loss: 3.896479940414429, val loss: 3.9874921560287477, ETA in seconds: 1595870.078\n",
      "epoch: 720100, train loss: 3.896099328994751, val loss: 3.961620831489563, ETA in seconds: 1595523.449\n",
      "epoch: 720200, train loss: 3.8978294372558593, val loss: 3.9701929092407227, ETA in seconds: 1595179.490\n",
      "epoch: 720300, train loss: 3.897874903678894, val loss: 3.970912313461304, ETA in seconds: 1594816.669\n",
      "epoch: 720400, train loss: 3.891619420051575, val loss: 3.9716646671295166, ETA in seconds: 1594453.385\n",
      "epoch: 720500, train loss: 3.890635442733765, val loss: 3.967304515838623, ETA in seconds: 1594092.506\n",
      "epoch: 720600, train loss: 3.897646450996399, val loss: 3.966864252090454, ETA in seconds: 1593731.036\n",
      "epoch: 720700, train loss: 3.8808107137680055, val loss: 3.968700575828552, ETA in seconds: 1593374.335\n",
      "epoch: 720800, train loss: 3.8936458349227907, val loss: 3.970810604095459, ETA in seconds: 1593028.947\n",
      "epoch: 720900, train loss: 3.8978038787841798, val loss: 3.981088948249817, ETA in seconds: 1592707.049\n",
      "epoch: 721000, train loss: 3.8915766000747682, val loss: 3.9745779752731325, ETA in seconds: 1592385.158\n",
      "epoch: 721100, train loss: 3.891898822784424, val loss: 3.967776393890381, ETA in seconds: 1592062.678\n",
      "epoch: 721200, train loss: 3.8878620862960815, val loss: 3.9668528556823732, ETA in seconds: 1591739.935\n",
      "epoch: 721300, train loss: 3.894236183166504, val loss: 3.968747544288635, ETA in seconds: 1591422.760\n",
      "epoch: 721400, train loss: 3.892704224586487, val loss: 3.9766786336898803, ETA in seconds: 1591103.097\n",
      "epoch: 721500, train loss: 3.892147183418274, val loss: 3.9696313619613646, ETA in seconds: 1590782.672\n",
      "epoch: 721600, train loss: 3.899627709388733, val loss: 3.961003065109253, ETA in seconds: 1590462.813\n",
      "epoch: 721700, train loss: 3.9027393102645873, val loss: 3.964510989189148, ETA in seconds: 1590142.347\n",
      "epoch: 721800, train loss: 3.8936376094818117, val loss: 3.962732124328613, ETA in seconds: 1589821.865\n",
      "epoch: 721900, train loss: 3.9001323699951174, val loss: 3.9684953689575195, ETA in seconds: 1589505.977\n",
      "epoch: 722000, train loss: 3.8885284900665282, val loss: 3.9786092042922974, ETA in seconds: 1589183.998\n",
      "epoch: 722100, train loss: 3.887934851646423, val loss: 3.9691349029541017, ETA in seconds: 1588857.968\n",
      "epoch: 722200, train loss: 3.8966814517974853, val loss: 3.973519706726074, ETA in seconds: 1588533.704\n",
      "epoch: 722300, train loss: 3.8892903089523316, val loss: 3.961449646949768, ETA in seconds: 1588208.632\n",
      "epoch: 722400, train loss: 3.8892539739608765, val loss: 3.9613646984100344, ETA in seconds: 1587874.823\n",
      "epoch: 722500, train loss: 3.8834451913833616, val loss: 3.96824905872345, ETA in seconds: 1587537.204\n",
      "epoch: 722600, train loss: 3.9007838487625124, val loss: 3.974507999420166, ETA in seconds: 1587216.304\n",
      "epoch: 722700, train loss: 3.8816379070281983, val loss: 3.975317192077637, ETA in seconds: 1586878.955\n",
      "epoch: 722800, train loss: 3.901457667350769, val loss: 3.9640666961669924, ETA in seconds: 1586538.067\n",
      "epoch: 722900, train loss: 3.8961079120635986, val loss: 3.9838817596435545, ETA in seconds: 1586213.281\n",
      "epoch: 723000, train loss: 3.892237091064453, val loss: 3.9686460971832274, ETA in seconds: 1585887.262\n",
      "epoch: 723100, train loss: 3.904780960083008, val loss: 3.9728320360183718, ETA in seconds: 1585568.000\n",
      "epoch: 723200, train loss: 3.8828171730041503, val loss: 3.970672678947449, ETA in seconds: 1585245.952\n",
      "epoch: 723300, train loss: 3.8993138313293456, val loss: 3.975267696380615, ETA in seconds: 1584921.507\n",
      "epoch: 723400, train loss: 3.8864993333816527, val loss: 3.9591463088989256, ETA in seconds: 1584598.073\n",
      "epoch: 723500, train loss: 3.8912684679031373, val loss: 3.9612862348556517, ETA in seconds: 1584274.236\n",
      "epoch: 723600, train loss: 3.896736907958984, val loss: 3.979179525375366, ETA in seconds: 1583949.947\n",
      "epoch: 723700, train loss: 3.8911134719848635, val loss: 3.9633635520935058, ETA in seconds: 1583627.522\n",
      "epoch: 723800, train loss: 3.896335816383362, val loss: 3.970965266227722, ETA in seconds: 1583263.631\n",
      "epoch: 723900, train loss: 3.8902206420898438, val loss: 3.973895025253296, ETA in seconds: 1582892.091\n",
      "epoch: 724000, train loss: 3.8948918104171755, val loss: 3.9650811910629273, ETA in seconds: 1582527.085\n",
      "epoch: 724100, train loss: 3.890915131568909, val loss: 3.977993893623352, ETA in seconds: 1582161.948\n",
      "epoch: 724200, train loss: 3.886193037033081, val loss: 3.980701518058777, ETA in seconds: 1581792.047\n",
      "epoch: 724300, train loss: 3.9014626502990724, val loss: 3.980016803741455, ETA in seconds: 1581419.854\n",
      "epoch: 724400, train loss: 3.8930038690567015, val loss: 3.980562615394592, ETA in seconds: 1581050.372\n",
      "epoch: 724500, train loss: 3.882358264923096, val loss: 3.971845030784607, ETA in seconds: 1580680.962\n",
      "epoch: 724600, train loss: 3.8932039976119994, val loss: 3.9690881252288817, ETA in seconds: 1580313.134\n",
      "epoch: 724700, train loss: 3.8987780809402466, val loss: 3.9753005504608154, ETA in seconds: 1579948.898\n",
      "epoch: 724800, train loss: 3.8882306814193726, val loss: 3.9788048028945924, ETA in seconds: 1579620.849\n",
      "epoch: 724900, train loss: 3.8915826320648192, val loss: 3.9712411165237427, ETA in seconds: 1579313.915\n",
      "epoch: 725000, train loss: 3.890125846862793, val loss: 3.973136043548584, ETA in seconds: 1578959.518\n",
      "epoch: 725100, train loss: 3.8919687509536742, val loss: 3.9764946937561034, ETA in seconds: 1578588.733\n",
      "epoch: 725200, train loss: 3.8912970542907717, val loss: 3.9694235801696776, ETA in seconds: 1578214.616\n",
      "epoch: 725300, train loss: 3.900623154640198, val loss: 3.969724011421204, ETA in seconds: 1577867.343\n",
      "epoch: 725400, train loss: 3.8898645401000977, val loss: 3.98186309337616, ETA in seconds: 1577549.532\n",
      "epoch: 725500, train loss: 3.8828543186187745, val loss: 3.977580451965332, ETA in seconds: 1577218.316\n",
      "epoch: 725600, train loss: 3.8968043327331543, val loss: 3.9865133285522463, ETA in seconds: 1576905.742\n",
      "epoch: 725700, train loss: 3.9006935358047485, val loss: 3.9751768589019774, ETA in seconds: 1576600.858\n",
      "epoch: 725800, train loss: 3.896772837638855, val loss: 3.9678690433502197, ETA in seconds: 1576285.286\n",
      "epoch: 725900, train loss: 3.891899061203003, val loss: 3.9683968782424928, ETA in seconds: 1575974.082\n",
      "epoch: 726000, train loss: 3.8938379287719727, val loss: 3.96891667842865, ETA in seconds: 1575640.953\n",
      "epoch: 726100, train loss: 3.8943672657012938, val loss: 3.981169247627258, ETA in seconds: 1575325.781\n",
      "epoch: 726200, train loss: 3.892128825187683, val loss: 3.975518727302551, ETA in seconds: 1575010.748\n",
      "epoch: 726300, train loss: 3.8919091701507567, val loss: 3.974339985847473, ETA in seconds: 1574648.255\n",
      "epoch: 726400, train loss: 3.901233124732971, val loss: 3.961294150352478, ETA in seconds: 1574275.520\n",
      "epoch: 726500, train loss: 3.8928099155426024, val loss: 3.967611074447632, ETA in seconds: 1573905.912\n",
      "epoch: 726600, train loss: 3.8897475242614745, val loss: 3.9672998666763304, ETA in seconds: 1573532.586\n",
      "epoch: 726700, train loss: 3.8977314710617064, val loss: 3.968782734870911, ETA in seconds: 1573164.741\n",
      "epoch: 726800, train loss: 3.8994737386703493, val loss: 3.970037889480591, ETA in seconds: 1572789.825\n",
      "epoch: 726900, train loss: 3.8804203033447267, val loss: 3.971972942352295, ETA in seconds: 1572414.436\n",
      "epoch: 727000, train loss: 3.885661554336548, val loss: 3.970768189430237, ETA in seconds: 1572084.206\n",
      "epoch: 727100, train loss: 3.8924041748046876, val loss: 3.9765502214431763, ETA in seconds: 1571753.687\n",
      "epoch: 727200, train loss: 3.8977699518203734, val loss: 3.980874490737915, ETA in seconds: 1571422.616\n",
      "epoch: 727300, train loss: 3.8963507652282714, val loss: 3.974480485916138, ETA in seconds: 1571091.978\n",
      "epoch: 727400, train loss: 3.8883476734161375, val loss: 3.9728958368301392, ETA in seconds: 1570760.885\n",
      "epoch: 727500, train loss: 3.9051592111587525, val loss: 3.9739506959915163, ETA in seconds: 1570428.695\n",
      "epoch: 727600, train loss: 3.9043488025665285, val loss: 3.965784692764282, ETA in seconds: 1570100.712\n",
      "epoch: 727700, train loss: 3.885392999649048, val loss: 3.980646824836731, ETA in seconds: 1569749.853\n",
      "epoch: 727800, train loss: 3.893729329109192, val loss: 3.9784821271896362, ETA in seconds: 1569364.939\n",
      "epoch: 727900, train loss: 3.8977231979370117, val loss: 3.9740440368652346, ETA in seconds: 1569012.939\n",
      "epoch: 728000, train loss: 3.8919113636016847, val loss: 3.971921610832214, ETA in seconds: 1568694.167\n",
      "epoch: 728100, train loss: 3.897157001495361, val loss: 3.972140002250671, ETA in seconds: 1568362.108\n",
      "epoch: 728200, train loss: 3.899317407608032, val loss: 3.9738489627838134, ETA in seconds: 1568046.374\n",
      "epoch: 728300, train loss: 3.8825599431991575, val loss: 3.981886124610901, ETA in seconds: 1567690.648\n",
      "epoch: 728400, train loss: 3.900208020210266, val loss: 3.972407078742981, ETA in seconds: 1567346.041\n",
      "epoch: 728500, train loss: 3.9070349454879763, val loss: 3.969298791885376, ETA in seconds: 1567006.193\n",
      "epoch: 728600, train loss: 3.8941520929336546, val loss: 3.9699618577957154, ETA in seconds: 1566690.989\n",
      "epoch: 728700, train loss: 3.8947710037231444, val loss: 3.9554736614227295, ETA in seconds: 1566375.081\n",
      "epoch: 728800, train loss: 3.894558143615723, val loss: 3.9738367795944214, ETA in seconds: 1565997.943\n",
      "epoch: 728900, train loss: 3.8816767930984497, val loss: 3.958634877204895, ETA in seconds: 1565615.684\n",
      "epoch: 729000, train loss: 3.893609309196472, val loss: 3.9693542957305907, ETA in seconds: 1565229.058\n",
      "epoch: 729100, train loss: 3.891138529777527, val loss: 3.979590559005737, ETA in seconds: 1564844.375\n",
      "epoch: 729200, train loss: 3.8945634603500365, val loss: 3.980205774307251, ETA in seconds: 1564479.853\n",
      "epoch: 729300, train loss: 3.894460940361023, val loss: 3.9799824237823485, ETA in seconds: 1564106.648\n",
      "epoch: 729400, train loss: 3.897870635986328, val loss: 3.982701301574707, ETA in seconds: 1563735.144\n",
      "epoch: 729500, train loss: 3.8904972314834594, val loss: 3.9765507221221923, ETA in seconds: 1563362.936\n",
      "epoch: 729600, train loss: 3.882605457305908, val loss: 3.9687192916870115, ETA in seconds: 1562982.982\n",
      "epoch: 729700, train loss: 3.895011472702026, val loss: 3.976622700691223, ETA in seconds: 1562604.654\n",
      "epoch: 729800, train loss: 3.9006557703018188, val loss: 3.982247495651245, ETA in seconds: 1562267.085\n",
      "epoch: 729900, train loss: 3.884846043586731, val loss: 3.9710829973220827, ETA in seconds: 1561928.545\n",
      "epoch: 730000, train loss: 3.884171462059021, val loss: 3.9699514150619506, ETA in seconds: 1561593.375\n",
      "epoch: 730100, train loss: 3.8998228549957275, val loss: 3.9786153078079223, ETA in seconds: 1561229.633\n",
      "epoch: 730200, train loss: 3.8889917373657226, val loss: 3.96558735370636, ETA in seconds: 1560862.109\n",
      "epoch: 730300, train loss: 3.89117751121521, val loss: 3.971360611915588, ETA in seconds: 1560481.944\n",
      "epoch: 730400, train loss: 3.889879322052002, val loss: 3.9741695880889893, ETA in seconds: 1560101.485\n",
      "epoch: 730500, train loss: 3.8874141216278075, val loss: 3.980613112449646, ETA in seconds: 1559732.613\n",
      "epoch: 730600, train loss: 3.895470881462097, val loss: 3.9797351121902467, ETA in seconds: 1559376.491\n",
      "epoch: 730700, train loss: 3.8973908185958863, val loss: 3.9696789741516114, ETA in seconds: 1559037.611\n",
      "epoch: 730800, train loss: 3.9007290840148925, val loss: 3.9675845861434937, ETA in seconds: 1558701.008\n",
      "epoch: 730900, train loss: 3.8877007246017454, val loss: 3.9787298679351806, ETA in seconds: 1558362.524\n",
      "epoch: 731000, train loss: 3.881815457344055, val loss: 3.97347674369812, ETA in seconds: 1558023.319\n",
      "epoch: 731100, train loss: 3.8801831722259523, val loss: 3.971595549583435, ETA in seconds: 1557683.154\n",
      "epoch: 731200, train loss: 3.888993978500366, val loss: 3.9728270292282106, ETA in seconds: 1557343.869\n",
      "epoch: 731300, train loss: 3.895902633666992, val loss: 3.9704272985458373, ETA in seconds: 1557004.403\n",
      "epoch: 731400, train loss: 3.8909959316253664, val loss: 3.970093822479248, ETA in seconds: 1556660.856\n",
      "epoch: 731500, train loss: 3.898843264579773, val loss: 3.978806400299072, ETA in seconds: 1556286.286\n",
      "epoch: 731600, train loss: 3.899473023414612, val loss: 3.976994752883911, ETA in seconds: 1555923.976\n",
      "epoch: 731700, train loss: 3.894802618026733, val loss: 3.9721349716186523, ETA in seconds: 1555561.877\n",
      "epoch: 731800, train loss: 3.891688871383667, val loss: 3.9780600547790526, ETA in seconds: 1555193.324\n",
      "epoch: 731900, train loss: 3.898081088066101, val loss: 3.9744688510894775, ETA in seconds: 1554812.289\n",
      "epoch: 732000, train loss: 3.8962751388549806, val loss: 3.9844151020050047, ETA in seconds: 1554437.382\n",
      "epoch: 732100, train loss: 3.890661883354187, val loss: 3.9832175970077515, ETA in seconds: 1554059.558\n",
      "epoch: 732200, train loss: 3.8934882402420046, val loss: 3.976804423332214, ETA in seconds: 1553691.752\n",
      "epoch: 732300, train loss: 3.9012890577316286, val loss: 3.96788866519928, ETA in seconds: 1553321.245\n",
      "epoch: 732400, train loss: 3.894556474685669, val loss: 3.9736642360687258, ETA in seconds: 1552954.966\n",
      "epoch: 732500, train loss: 3.894013595581055, val loss: 3.9643328189849854, ETA in seconds: 1552583.835\n",
      "epoch: 732600, train loss: 3.8959718704223634, val loss: 3.975645923614502, ETA in seconds: 1552209.929\n",
      "epoch: 732700, train loss: 3.88842294216156, val loss: 3.974232292175293, ETA in seconds: 1551847.883\n",
      "epoch: 732800, train loss: 3.8943569660186768, val loss: 3.9730380535125733, ETA in seconds: 1551479.842\n",
      "epoch: 732900, train loss: 3.890324521064758, val loss: 3.9695891380310058, ETA in seconds: 1551102.831\n",
      "epoch: 733000, train loss: 3.890385937690735, val loss: 3.9802103519439695, ETA in seconds: 1550724.136\n",
      "epoch: 733100, train loss: 3.8917633056640626, val loss: 3.968060827255249, ETA in seconds: 1550344.858\n",
      "epoch: 733200, train loss: 3.8866576194763183, val loss: 3.9654449701309202, ETA in seconds: 1549966.951\n",
      "epoch: 733300, train loss: 3.8956446170806887, val loss: 3.9841437339782715, ETA in seconds: 1549588.594\n",
      "epoch: 733400, train loss: 3.888936400413513, val loss: 3.982358145713806, ETA in seconds: 1549206.188\n",
      "epoch: 733500, train loss: 3.8954759359359743, val loss: 3.9664151430130006, ETA in seconds: 1548842.345\n",
      "epoch: 733600, train loss: 3.899488592147827, val loss: 3.9736020088195803, ETA in seconds: 1548482.882\n",
      "epoch: 733700, train loss: 3.9027103662490843, val loss: 3.966128706932068, ETA in seconds: 1548139.189\n",
      "epoch: 733800, train loss: 3.895861268043518, val loss: 3.9651484727859496, ETA in seconds: 1547794.682\n",
      "epoch: 733900, train loss: 3.89412043094635, val loss: 3.980825090408325, ETA in seconds: 1547450.659\n",
      "epoch: 734000, train loss: 3.9026001691818237, val loss: 3.975501608848572, ETA in seconds: 1547106.242\n",
      "epoch: 734100, train loss: 3.8989718675613405, val loss: 3.969355320930481, ETA in seconds: 1546762.595\n",
      "epoch: 734200, train loss: 3.8957732200622557, val loss: 3.9815113067626955, ETA in seconds: 1546423.016\n",
      "epoch: 734300, train loss: 3.89953351020813, val loss: 3.972907471656799, ETA in seconds: 1546050.728\n",
      "epoch: 734400, train loss: 3.8985713958740233, val loss: 3.9699309349060057, ETA in seconds: 1545683.801\n",
      "epoch: 734500, train loss: 3.889670205116272, val loss: 3.9698972940444945, ETA in seconds: 1545316.610\n",
      "epoch: 734600, train loss: 3.9016506910324096, val loss: 3.974543046951294, ETA in seconds: 1544956.000\n",
      "epoch: 734700, train loss: 3.892376351356506, val loss: 3.963260531425476, ETA in seconds: 1544580.627\n",
      "epoch: 734800, train loss: 3.8922643423080445, val loss: 3.9717119216918944, ETA in seconds: 1544205.790\n",
      "epoch: 734900, train loss: 3.8917067289352416, val loss: 3.9671194076538088, ETA in seconds: 1543824.704\n",
      "epoch: 735000, train loss: 3.888274574279785, val loss: 3.9721887350082397, ETA in seconds: 1543450.484\n",
      "epoch: 735100, train loss: 3.890783357620239, val loss: 3.9674097299575806, ETA in seconds: 1543067.671\n",
      "epoch: 735200, train loss: 3.897754502296448, val loss: 3.9804962635040284, ETA in seconds: 1542696.142\n",
      "epoch: 735300, train loss: 3.893883728981018, val loss: 3.979657769203186, ETA in seconds: 1542314.517\n",
      "epoch: 735400, train loss: 3.890883946418762, val loss: 3.975593900680542, ETA in seconds: 1541952.435\n",
      "epoch: 735500, train loss: 3.8883174657821655, val loss: 3.9790406465530395, ETA in seconds: 1541592.882\n",
      "epoch: 735600, train loss: 3.89599986076355, val loss: 3.9829626083374023, ETA in seconds: 1541208.674\n",
      "epoch: 735700, train loss: 3.8922543048858644, val loss: 3.976406121253967, ETA in seconds: 1540830.924\n",
      "epoch: 735800, train loss: 3.891930365562439, val loss: 3.9736544132232665, ETA in seconds: 1540465.496\n",
      "epoch: 735900, train loss: 3.896524524688721, val loss: 3.9695129871368406, ETA in seconds: 1540095.101\n",
      "epoch: 736000, train loss: 3.8975481033325194, val loss: 3.9720012187957763, ETA in seconds: 1539729.193\n",
      "epoch: 736100, train loss: 3.8968443155288695, val loss: 3.972405123710632, ETA in seconds: 1539414.355\n",
      "epoch: 736200, train loss: 3.8892549753189085, val loss: 3.983850288391113, ETA in seconds: 1539047.884\n",
      "epoch: 736300, train loss: 3.901240420341492, val loss: 3.972383904457092, ETA in seconds: 1538681.039\n",
      "epoch: 736400, train loss: 3.8900391817092896, val loss: 3.970170760154724, ETA in seconds: 1538311.233\n",
      "epoch: 736500, train loss: 3.9036959648132323, val loss: 3.973367428779602, ETA in seconds: 1537949.522\n",
      "epoch: 736600, train loss: 3.888975143432617, val loss: 3.9704503297805784, ETA in seconds: 1537583.412\n",
      "epoch: 736700, train loss: 3.8959120988845823, val loss: 3.9832298517227174, ETA in seconds: 1537214.186\n",
      "epoch: 736800, train loss: 3.894707131385803, val loss: 3.9820260047912597, ETA in seconds: 1536848.680\n",
      "epoch: 736900, train loss: 3.8924261808395384, val loss: 3.9733641147613525, ETA in seconds: 1536490.100\n",
      "epoch: 737000, train loss: 3.885348916053772, val loss: 3.9852527618408202, ETA in seconds: 1536140.327\n",
      "epoch: 737100, train loss: 3.891450047492981, val loss: 3.9736477375030517, ETA in seconds: 1535789.753\n",
      "epoch: 737200, train loss: 3.8884097576141357, val loss: 3.9825621604919434, ETA in seconds: 1535440.605\n",
      "epoch: 737300, train loss: 3.885439920425415, val loss: 3.9748871088027955, ETA in seconds: 1535090.773\n",
      "epoch: 737400, train loss: 3.8941076040267943, val loss: 3.972795104980469, ETA in seconds: 1534741.714\n",
      "epoch: 737500, train loss: 3.8981916666030885, val loss: 3.9753114223480224, ETA in seconds: 1534391.317\n",
      "epoch: 737600, train loss: 3.894039440155029, val loss: 3.971821427345276, ETA in seconds: 1534025.519\n",
      "epoch: 737700, train loss: 3.8957099676132203, val loss: 3.9721553564071654, ETA in seconds: 1533650.891\n",
      "epoch: 737800, train loss: 3.8984725952148436, val loss: 3.97172155380249, ETA in seconds: 1533258.898\n",
      "epoch: 737900, train loss: 3.894339108467102, val loss: 3.980486345291138, ETA in seconds: 1532872.195\n",
      "epoch: 738000, train loss: 3.8889833450317384, val loss: 3.9713728189468385, ETA in seconds: 1532487.660\n",
      "epoch: 738100, train loss: 3.8913825273513796, val loss: 3.9781386137008665, ETA in seconds: 1532129.971\n",
      "epoch: 738200, train loss: 3.8770006895065308, val loss: 3.978207731246948, ETA in seconds: 1531739.017\n",
      "epoch: 738300, train loss: 3.882134032249451, val loss: 3.979929232597351, ETA in seconds: 1531364.584\n",
      "epoch: 738400, train loss: 3.8980101346969604, val loss: 3.9834252119064333, ETA in seconds: 1530998.284\n",
      "epoch: 738500, train loss: 3.8774459600448608, val loss: 3.9795641899108887, ETA in seconds: 1530649.849\n",
      "epoch: 738600, train loss: 3.8998038291931154, val loss: 3.970911908149719, ETA in seconds: 1530300.891\n",
      "epoch: 738700, train loss: 3.8974096059799193, val loss: 3.9798997402191163, ETA in seconds: 1529953.825\n",
      "epoch: 738800, train loss: 3.8935333728790282, val loss: 3.971574068069458, ETA in seconds: 1529614.692\n",
      "epoch: 738900, train loss: 3.8945473432540894, val loss: 3.9803877592086794, ETA in seconds: 1529221.757\n",
      "epoch: 739000, train loss: 3.8951746940612795, val loss: 3.975854992866516, ETA in seconds: 1528840.474\n",
      "epoch: 739100, train loss: 3.8960291385650634, val loss: 3.9761921882629396, ETA in seconds: 1528451.898\n",
      "epoch: 739200, train loss: 3.8974799871444703, val loss: 3.9802716255187987, ETA in seconds: 1528049.150\n",
      "epoch: 739300, train loss: 3.9028508186340334, val loss: 3.985017275810242, ETA in seconds: 1527671.848\n",
      "epoch: 739400, train loss: 3.8974366426467895, val loss: 3.9768097162246705, ETA in seconds: 1527286.433\n",
      "epoch: 739500, train loss: 3.894812750816345, val loss: 3.9782018184661867, ETA in seconds: 1526893.697\n",
      "epoch: 739600, train loss: 3.899686574935913, val loss: 3.975085949897766, ETA in seconds: 1526497.854\n",
      "epoch: 739700, train loss: 3.8866690158843995, val loss: 3.972658562660217, ETA in seconds: 1526102.729\n",
      "epoch: 739800, train loss: 3.8832502126693726, val loss: 3.9771613836288453, ETA in seconds: 1525707.949\n",
      "epoch: 739900, train loss: 3.893548011779785, val loss: 3.976247191429138, ETA in seconds: 1525314.486\n",
      "epoch: 740000, train loss: 3.8861949682235717, val loss: 3.970411229133606, ETA in seconds: 1524923.895\n",
      "epoch: 740100, train loss: 3.893718457221985, val loss: 3.9689647912979127, ETA in seconds: 1524535.943\n",
      "epoch: 740200, train loss: 3.8931193590164184, val loss: 3.975151300430298, ETA in seconds: 1524143.619\n",
      "epoch: 740300, train loss: 3.889428496360779, val loss: 3.9746469020843507, ETA in seconds: 1523748.862\n",
      "epoch: 740400, train loss: 3.88890426158905, val loss: 3.9797008275985717, ETA in seconds: 1523353.300\n",
      "epoch: 740500, train loss: 3.905762195587158, val loss: 3.9692410707473753, ETA in seconds: 1522957.037\n",
      "epoch: 740600, train loss: 3.8913848876953123, val loss: 3.97903573513031, ETA in seconds: 1522564.253\n",
      "epoch: 740700, train loss: 3.892018723487854, val loss: 3.9788829565048216, ETA in seconds: 1522169.946\n",
      "epoch: 740800, train loss: 3.8918819427490234, val loss: 3.9635281801223754, ETA in seconds: 1521779.375\n",
      "epoch: 740900, train loss: 3.8904000759124755, val loss: 3.9665680885314942, ETA in seconds: 1521384.740\n",
      "epoch: 741000, train loss: 3.899345541000366, val loss: 3.975458598136902, ETA in seconds: 1520991.519\n",
      "epoch: 741100, train loss: 3.8918777227401735, val loss: 3.971969246864319, ETA in seconds: 1520598.016\n",
      "epoch: 741200, train loss: 3.8977452754974364, val loss: 3.9604071140289308, ETA in seconds: 1520202.486\n",
      "epoch: 741300, train loss: 3.8997993230819703, val loss: 3.9728193283081055, ETA in seconds: 1519806.227\n",
      "epoch: 741400, train loss: 3.896986413002014, val loss: 3.968467378616333, ETA in seconds: 1519426.697\n",
      "epoch: 741500, train loss: 3.894888091087341, val loss: 3.9637869596481323, ETA in seconds: 1519038.845\n",
      "epoch: 741600, train loss: 3.896811580657959, val loss: 3.9692129135131835, ETA in seconds: 1518646.171\n",
      "epoch: 741700, train loss: 3.890643310546875, val loss: 3.973712134361267, ETA in seconds: 1518254.082\n",
      "epoch: 741800, train loss: 3.8873843431472777, val loss: 3.9701541900634765, ETA in seconds: 1517879.391\n",
      "epoch: 741900, train loss: 3.8891963720321656, val loss: 3.9714848041534423, ETA in seconds: 1517494.217\n",
      "epoch: 742000, train loss: 3.900463032722473, val loss: 3.975797200202942, ETA in seconds: 1517100.837\n",
      "epoch: 742100, train loss: 3.8920323848724365, val loss: 3.9678884506225587, ETA in seconds: 1516700.641\n",
      "epoch: 742200, train loss: 3.8927584886550903, val loss: 3.973409056663513, ETA in seconds: 1516303.756\n",
      "epoch: 742300, train loss: 3.8945605516433717, val loss: 3.9663010120391844, ETA in seconds: 1515907.589\n",
      "epoch: 742400, train loss: 3.8962839126586912, val loss: 3.9795215368270873, ETA in seconds: 1515528.711\n",
      "epoch: 742500, train loss: 3.891676330566406, val loss: 3.978054571151733, ETA in seconds: 1515150.903\n",
      "epoch: 742600, train loss: 3.891121506690979, val loss: 3.97802095413208, ETA in seconds: 1514774.421\n",
      "epoch: 742700, train loss: 3.8953513383865355, val loss: 3.9770694732666017, ETA in seconds: 1514396.276\n",
      "epoch: 742800, train loss: 3.880029225349426, val loss: 3.9742552995681764, ETA in seconds: 1514019.086\n",
      "epoch: 742900, train loss: 3.890733170509338, val loss: 3.975784420967102, ETA in seconds: 1513644.634\n",
      "epoch: 743000, train loss: 3.9024943351745605, val loss: 3.970456528663635, ETA in seconds: 1513266.139\n",
      "epoch: 743100, train loss: 3.897199583053589, val loss: 3.9602198362350465, ETA in seconds: 1512887.548\n",
      "epoch: 743200, train loss: 3.894795632362366, val loss: 3.965064454078674, ETA in seconds: 1512498.306\n",
      "epoch: 743300, train loss: 3.8916918992996217, val loss: 3.968645691871643, ETA in seconds: 1512114.721\n",
      "epoch: 743400, train loss: 3.8934611320495605, val loss: 3.96517391204834, ETA in seconds: 1511734.314\n",
      "epoch: 743500, train loss: 3.8825554609298707, val loss: 3.9627936840057374, ETA in seconds: 1511346.062\n",
      "epoch: 743600, train loss: 3.8898324251174925, val loss: 3.967949891090393, ETA in seconds: 1510947.836\n",
      "epoch: 743700, train loss: 3.8954440355300903, val loss: 3.9781352519989013, ETA in seconds: 1510547.643\n",
      "epoch: 743800, train loss: 3.8979463815689086, val loss: 3.964918613433838, ETA in seconds: 1510148.518\n",
      "epoch: 743900, train loss: 3.8982131481170654, val loss: 3.9695037364959718, ETA in seconds: 1509751.766\n",
      "epoch: 744000, train loss: 3.8917434930801393, val loss: 3.9732150793075562, ETA in seconds: 1509355.460\n",
      "epoch: 744100, train loss: 3.8926002264022825, val loss: 3.9795860052108765, ETA in seconds: 1508960.042\n",
      "epoch: 744200, train loss: 3.890502381324768, val loss: 3.9762149810791017, ETA in seconds: 1508575.783\n",
      "epoch: 744300, train loss: 3.9005202531814573, val loss: 3.974150013923645, ETA in seconds: 1508182.112\n",
      "epoch: 744400, train loss: 3.8996533870697023, val loss: 3.9775617122650146, ETA in seconds: 1507782.574\n",
      "epoch: 744500, train loss: 3.8909351348876955, val loss: 3.980163836479187, ETA in seconds: 1507386.535\n",
      "epoch: 744600, train loss: 3.88765127658844, val loss: 3.9664860725402833, ETA in seconds: 1507012.893\n",
      "epoch: 744700, train loss: 3.8941170930862428, val loss: 3.976944637298584, ETA in seconds: 1506655.979\n",
      "epoch: 744800, train loss: 3.8897039890289307, val loss: 3.9686083793640137, ETA in seconds: 1506297.910\n",
      "epoch: 744900, train loss: 3.8916496992111207, val loss: 3.962183213233948, ETA in seconds: 1505941.268\n",
      "epoch: 745000, train loss: 3.894316625595093, val loss: 3.9804803371429442, ETA in seconds: 1505584.129\n",
      "epoch: 745100, train loss: 3.89157452583313, val loss: 3.971096324920654, ETA in seconds: 1505227.022\n",
      "epoch: 745200, train loss: 3.897170400619507, val loss: 3.963983917236328, ETA in seconds: 1504868.615\n",
      "epoch: 745300, train loss: 3.901462936401367, val loss: 3.9603724479675293, ETA in seconds: 1504510.430\n",
      "epoch: 745400, train loss: 3.898630738258362, val loss: 3.9730526924133303, ETA in seconds: 1504149.255\n",
      "epoch: 745500, train loss: 3.8883065700531008, val loss: 3.968372654914856, ETA in seconds: 1503750.988\n",
      "epoch: 745600, train loss: 3.8943104982376098, val loss: 3.9670197010040282, ETA in seconds: 1503363.046\n",
      "epoch: 745700, train loss: 3.8878133296966553, val loss: 3.971287703514099, ETA in seconds: 1502984.176\n",
      "epoch: 745800, train loss: 3.891994047164917, val loss: 3.9717036724090575, ETA in seconds: 1502580.839\n",
      "epoch: 745900, train loss: 3.889982509613037, val loss: 3.9752621412277223, ETA in seconds: 1502176.598\n",
      "epoch: 746000, train loss: 3.9006677150726317, val loss: 3.9708441734313964, ETA in seconds: 1501770.917\n",
      "epoch: 746100, train loss: 3.884007287025452, val loss: 3.9615630149841308, ETA in seconds: 1501367.239\n",
      "epoch: 746200, train loss: 3.891709041595459, val loss: 3.9751196622848513, ETA in seconds: 1500966.549\n",
      "epoch: 746300, train loss: 3.8926414251327515, val loss: 3.9686282873153687, ETA in seconds: 1500560.899\n",
      "epoch: 746400, train loss: 3.8937445402145388, val loss: 3.9749928951263427, ETA in seconds: 1500154.670\n",
      "epoch: 746500, train loss: 3.8943665742874147, val loss: 3.9676979780197144, ETA in seconds: 1499749.575\n",
      "epoch: 746600, train loss: 3.8980873346328737, val loss: 3.971474123001099, ETA in seconds: 1499337.941\n",
      "epoch: 746700, train loss: 3.89515335559845, val loss: 3.961135172843933, ETA in seconds: 1498925.278\n",
      "epoch: 746800, train loss: 3.885667562484741, val loss: 3.9689697265625, ETA in seconds: 1498536.883\n",
      "epoch: 746900, train loss: 3.8886965990066527, val loss: 3.966427206993103, ETA in seconds: 1498155.637\n",
      "epoch: 747000, train loss: 3.8956461191177367, val loss: 3.9720996141433718, ETA in seconds: 1497771.050\n",
      "epoch: 747100, train loss: 3.889995837211609, val loss: 3.964543604850769, ETA in seconds: 1497391.820\n",
      "epoch: 747200, train loss: 3.899915361404419, val loss: 3.967610549926758, ETA in seconds: 1496997.800\n",
      "epoch: 747300, train loss: 3.900881552696228, val loss: 3.9585580348968508, ETA in seconds: 1496611.039\n",
      "epoch: 747400, train loss: 3.8848903656005858, val loss: 3.961736249923706, ETA in seconds: 1496233.843\n",
      "epoch: 747500, train loss: 3.8831140756607057, val loss: 3.958328700065613, ETA in seconds: 1495871.100\n",
      "epoch: 747600, train loss: 3.89342041015625, val loss: 3.969867157936096, ETA in seconds: 1495514.200\n",
      "epoch: 747700, train loss: 3.896468710899353, val loss: 3.972252607345581, ETA in seconds: 1495140.537\n",
      "epoch: 747800, train loss: 3.8933311462402345, val loss: 3.9712793588638307, ETA in seconds: 1494750.977\n",
      "epoch: 747900, train loss: 3.8945327281951903, val loss: 3.960797071456909, ETA in seconds: 1494368.282\n",
      "epoch: 748000, train loss: 3.8898008108139037, val loss: 3.9711836338043214, ETA in seconds: 1493972.528\n",
      "epoch: 748100, train loss: 3.8974807500839233, val loss: 3.970956492424011, ETA in seconds: 1493584.448\n",
      "epoch: 748200, train loss: 3.886130714416504, val loss: 3.9615371942520143, ETA in seconds: 1493182.755\n",
      "epoch: 748300, train loss: 3.892552208900452, val loss: 3.9622151613235475, ETA in seconds: 1492788.584\n",
      "epoch: 748400, train loss: 3.8887349367141724, val loss: 3.964339017868042, ETA in seconds: 1492434.488\n",
      "epoch: 748500, train loss: 3.8923588037490844, val loss: 3.973198485374451, ETA in seconds: 1492027.430\n",
      "epoch: 748600, train loss: 3.8959381580352783, val loss: 3.960717868804932, ETA in seconds: 1491619.409\n",
      "epoch: 748700, train loss: 3.8937466621398924, val loss: 3.9755385398864744, ETA in seconds: 1491233.123\n",
      "epoch: 748800, train loss: 3.8898670196533205, val loss: 3.9697038888931275, ETA in seconds: 1490836.849\n",
      "epoch: 748900, train loss: 3.887035846710205, val loss: 3.9610606908798216, ETA in seconds: 1490443.772\n",
      "epoch: 749000, train loss: 3.893507742881775, val loss: 3.97059543132782, ETA in seconds: 1490054.862\n",
      "epoch: 749100, train loss: 3.8971354484558107, val loss: 3.958242344856262, ETA in seconds: 1489674.538\n",
      "epoch: 749200, train loss: 3.890520691871643, val loss: 3.9741384983062744, ETA in seconds: 1489287.145\n",
      "epoch: 749300, train loss: 3.8896164178848265, val loss: 3.9685258150100706, ETA in seconds: 1488899.398\n",
      "epoch: 749400, train loss: 3.8876766204833983, val loss: 3.9687909126281737, ETA in seconds: 1488507.645\n",
      "epoch: 749500, train loss: 3.897387170791626, val loss: 3.962549614906311, ETA in seconds: 1488121.543\n",
      "epoch: 749600, train loss: 3.9025344371795656, val loss: 3.962877655029297, ETA in seconds: 1487717.142\n",
      "epoch: 749700, train loss: 3.8934090614318846, val loss: 3.9690911054611204, ETA in seconds: 1487305.351\n",
      "epoch: 749800, train loss: 3.9039263010025023, val loss: 3.9752758026123045, ETA in seconds: 1486898.682\n",
      "epoch: 749900, train loss: 3.887268590927124, val loss: 3.9723868370056152, ETA in seconds: 1486496.105\n",
      "epoch: 750000, train loss: 3.897023415565491, val loss: 3.970135712623596, ETA in seconds: 1486087.830\n",
      "epoch: 750100, train loss: 3.894206476211548, val loss: 3.9796167612075806, ETA in seconds: 1485690.690\n",
      "epoch: 750200, train loss: 3.8871546268463133, val loss: 3.969982385635376, ETA in seconds: 1485305.614\n",
      "epoch: 750300, train loss: 3.8948474407196043, val loss: 3.9740241289138796, ETA in seconds: 1484905.276\n",
      "epoch: 750400, train loss: 3.8848782777786255, val loss: 3.972083234786987, ETA in seconds: 1484521.718\n",
      "epoch: 750500, train loss: 3.899057388305664, val loss: 3.9694851875305175, ETA in seconds: 1484151.644\n",
      "epoch: 750600, train loss: 3.8988638401031492, val loss: 3.9736902475357057, ETA in seconds: 1483781.573\n",
      "epoch: 750700, train loss: 3.885848879814148, val loss: 3.972467541694641, ETA in seconds: 1483411.672\n",
      "epoch: 750800, train loss: 3.889722990989685, val loss: 3.9579087257385255, ETA in seconds: 1483042.637\n",
      "epoch: 750900, train loss: 3.893939971923828, val loss: 3.9759883642196656, ETA in seconds: 1482655.601\n",
      "epoch: 751000, train loss: 3.9039016008377074, val loss: 3.973450207710266, ETA in seconds: 1482273.253\n",
      "epoch: 751100, train loss: 3.9017122745513917, val loss: 3.964060068130493, ETA in seconds: 1481889.968\n",
      "epoch: 751200, train loss: 3.901865839958191, val loss: 3.984840679168701, ETA in seconds: 1481488.713\n",
      "epoch: 751300, train loss: 3.8862142086029055, val loss: 3.9663668155670164, ETA in seconds: 1481089.999\n",
      "epoch: 751400, train loss: 3.893991994857788, val loss: 3.9665851354599, ETA in seconds: 1480702.631\n",
      "epoch: 751500, train loss: 3.9004300355911257, val loss: 3.9785686254501345, ETA in seconds: 1480314.028\n",
      "epoch: 751600, train loss: 3.8914823293685914, val loss: 3.9650511026382445, ETA in seconds: 1479919.871\n",
      "epoch: 751700, train loss: 3.889425349235535, val loss: 3.9704699516296387, ETA in seconds: 1479516.418\n",
      "epoch: 751800, train loss: 3.8896491289138795, val loss: 3.9649365186691283, ETA in seconds: 1479113.692\n",
      "epoch: 751900, train loss: 3.8872064352035522, val loss: 3.9701271772384645, ETA in seconds: 1478711.068\n",
      "epoch: 752000, train loss: 3.8986721992492677, val loss: 3.9620338439941407, ETA in seconds: 1478322.490\n",
      "epoch: 752100, train loss: 3.8976043462753296, val loss: 3.9734161853790284, ETA in seconds: 1477945.743\n",
      "epoch: 752200, train loss: 3.897117328643799, val loss: 3.965568733215332, ETA in seconds: 1477562.100\n",
      "epoch: 752300, train loss: 3.8880900621414183, val loss: 3.962520217895508, ETA in seconds: 1477160.058\n",
      "epoch: 752400, train loss: 3.896487092971802, val loss: 3.971182441711426, ETA in seconds: 1476789.208\n",
      "epoch: 752500, train loss: 3.8973658323287963, val loss: 3.9765058517456056, ETA in seconds: 1476439.999\n",
      "epoch: 752600, train loss: 3.8900670528411867, val loss: 3.973548746109009, ETA in seconds: 1476073.442\n",
      "epoch: 752700, train loss: 3.8997482538223265, val loss: 3.977804183959961, ETA in seconds: 1475664.927\n",
      "epoch: 752800, train loss: 3.884014701843262, val loss: 3.9761465787887573, ETA in seconds: 1475268.561\n",
      "epoch: 752900, train loss: 3.886734223365784, val loss: 3.9683648586273192, ETA in seconds: 1474872.748\n",
      "epoch: 753000, train loss: 3.896938371658325, val loss: 3.9833436489105223, ETA in seconds: 1474478.508\n",
      "epoch: 753100, train loss: 3.8960153818130494, val loss: 3.9694379806518554, ETA in seconds: 1474091.861\n",
      "epoch: 753200, train loss: 3.8955830097198487, val loss: 3.98420729637146, ETA in seconds: 1473707.835\n",
      "epoch: 753300, train loss: 3.88946418762207, val loss: 3.9676825761795045, ETA in seconds: 1473295.392\n",
      "epoch: 753400, train loss: 3.894692897796631, val loss: 3.955331301689148, ETA in seconds: 1472903.938\n",
      "epoch: 753500, train loss: 3.8919079542160033, val loss: 3.9632989883422853, ETA in seconds: 1472494.694\n",
      "epoch: 753600, train loss: 3.8948964834213258, val loss: 3.959115409851074, ETA in seconds: 1472082.439\n",
      "epoch: 753700, train loss: 3.895040512084961, val loss: 3.9754696607589723, ETA in seconds: 1471687.211\n",
      "epoch: 753800, train loss: 3.89528169631958, val loss: 3.968655157089233, ETA in seconds: 1471279.528\n",
      "epoch: 753900, train loss: 3.890759539604187, val loss: 3.983673167228699, ETA in seconds: 1470870.445\n",
      "epoch: 754000, train loss: 3.892016959190369, val loss: 3.969580554962158, ETA in seconds: 1470469.449\n",
      "epoch: 754100, train loss: 3.900201678276062, val loss: 3.980151915550232, ETA in seconds: 1470067.748\n",
      "epoch: 754200, train loss: 3.886372709274292, val loss: 3.9663534641265867, ETA in seconds: 1469656.806\n",
      "epoch: 754300, train loss: 3.890770101547241, val loss: 3.9646581649780273, ETA in seconds: 1469256.778\n",
      "epoch: 754400, train loss: 3.8953566789627074, val loss: 3.9802462577819826, ETA in seconds: 1468887.566\n",
      "epoch: 754500, train loss: 3.8959249019622804, val loss: 3.979766082763672, ETA in seconds: 1468469.775\n",
      "epoch: 754600, train loss: 3.897506356239319, val loss: 3.9787790775299072, ETA in seconds: 1468059.319\n",
      "epoch: 754700, train loss: 3.8902307033538817, val loss: 3.9697646379470823, ETA in seconds: 1467663.322\n",
      "epoch: 754800, train loss: 3.8951088428497314, val loss: 3.9751616716384888, ETA in seconds: 1467254.571\n",
      "epoch: 754900, train loss: 3.888120341300964, val loss: 3.9816387414932253, ETA in seconds: 1466856.413\n",
      "epoch: 755000, train loss: 3.9007084608078, val loss: 3.96520574092865, ETA in seconds: 1466477.006\n",
      "epoch: 755100, train loss: 3.8953104496002195, val loss: 3.9738903045654297, ETA in seconds: 1466094.049\n",
      "epoch: 755200, train loss: 3.89312264919281, val loss: 3.969950485229492, ETA in seconds: 1465683.802\n",
      "epoch: 755300, train loss: 3.9060662508010866, val loss: 3.973990964889526, ETA in seconds: 1465291.417\n",
      "epoch: 755400, train loss: 3.894906258583069, val loss: 3.971944880485535, ETA in seconds: 1464898.910\n",
      "epoch: 755500, train loss: 3.9047937631607055, val loss: 3.973574733734131, ETA in seconds: 1464505.170\n",
      "epoch: 755600, train loss: 3.9053399085998537, val loss: 3.967577886581421, ETA in seconds: 1464111.162\n",
      "epoch: 755700, train loss: 3.8921164751052855, val loss: 3.9784555196762086, ETA in seconds: 1463703.786\n",
      "epoch: 755800, train loss: 3.8932415723800657, val loss: 3.978530764579773, ETA in seconds: 1463303.148\n",
      "epoch: 755900, train loss: 3.891587805747986, val loss: 3.9667645931243896, ETA in seconds: 1462934.092\n",
      "epoch: 756000, train loss: 3.8885490894317627, val loss: 3.9755109786987304, ETA in seconds: 1462525.091\n",
      "epoch: 756100, train loss: 3.88886444568634, val loss: 3.9669233083724977, ETA in seconds: 1462121.979\n",
      "epoch: 756200, train loss: 3.8936866521835327, val loss: 3.969291877746582, ETA in seconds: 1461709.825\n",
      "epoch: 756300, train loss: 3.8979356050491334, val loss: 3.9778090000152586, ETA in seconds: 1461299.666\n",
      "epoch: 756400, train loss: 3.8948266983032225, val loss: 3.979367733001709, ETA in seconds: 1460902.717\n",
      "epoch: 756500, train loss: 3.8918917179107666, val loss: 3.96782591342926, ETA in seconds: 1460522.031\n",
      "epoch: 756600, train loss: 3.8904909610748293, val loss: 3.9795299530029298, ETA in seconds: 1460125.083\n",
      "epoch: 756700, train loss: 3.890704035758972, val loss: 3.9721936225891112, ETA in seconds: 1459729.626\n",
      "epoch: 756800, train loss: 3.8893073558807374, val loss: 3.978683423995972, ETA in seconds: 1459346.811\n",
      "epoch: 756900, train loss: 3.8897600412368774, val loss: 3.9708498239517214, ETA in seconds: 1458963.537\n",
      "epoch: 757000, train loss: 3.8940999031066896, val loss: 3.9664177656173707, ETA in seconds: 1458580.989\n",
      "epoch: 757100, train loss: 3.8899232387542724, val loss: 3.968094277381897, ETA in seconds: 1458197.738\n",
      "epoch: 757200, train loss: 3.895883846282959, val loss: 3.9683130264282225, ETA in seconds: 1457814.340\n",
      "epoch: 757300, train loss: 3.895280122756958, val loss: 3.977105164527893, ETA in seconds: 1457433.145\n",
      "epoch: 757400, train loss: 3.890996313095093, val loss: 3.9697463512420654, ETA in seconds: 1457048.939\n",
      "epoch: 757500, train loss: 3.8928584337234495, val loss: 3.971996212005615, ETA in seconds: 1456664.416\n",
      "epoch: 757600, train loss: 3.8918275594711305, val loss: 3.9609750509262085, ETA in seconds: 1456282.128\n",
      "epoch: 757700, train loss: 3.8948898315429688, val loss: 3.979472041130066, ETA in seconds: 1455913.266\n",
      "epoch: 757800, train loss: 3.889924502372742, val loss: 3.96443886756897, ETA in seconds: 1455534.536\n",
      "epoch: 757900, train loss: 3.8983992099761964, val loss: 3.9754756450653077, ETA in seconds: 1455154.068\n",
      "epoch: 758000, train loss: 3.8864725112915037, val loss: 3.9697039842605593, ETA in seconds: 1454771.427\n",
      "epoch: 758100, train loss: 3.89220187664032, val loss: 3.972260284423828, ETA in seconds: 1454388.631\n",
      "epoch: 758200, train loss: 3.896489691734314, val loss: 3.9647364377975465, ETA in seconds: 1454006.674\n",
      "epoch: 758300, train loss: 3.8928175926208497, val loss: 3.9699098110198974, ETA in seconds: 1453624.071\n",
      "epoch: 758400, train loss: 3.8992736101150514, val loss: 3.96357901096344, ETA in seconds: 1453241.404\n",
      "epoch: 758500, train loss: 3.895015025138855, val loss: 3.9683238744735716, ETA in seconds: 1452860.139\n",
      "epoch: 758600, train loss: 3.888914775848389, val loss: 3.961891841888428, ETA in seconds: 1452464.793\n",
      "epoch: 758700, train loss: 3.8914861917495727, val loss: 3.964978790283203, ETA in seconds: 1452055.069\n",
      "epoch: 758800, train loss: 3.9054948329925536, val loss: 3.957126092910767, ETA in seconds: 1451645.024\n",
      "epoch: 758900, train loss: 3.895242786407471, val loss: 3.980412173271179, ETA in seconds: 1451218.012\n",
      "epoch: 759000, train loss: 3.89051673412323, val loss: 3.967039918899536, ETA in seconds: 1450789.024\n",
      "epoch: 759100, train loss: 3.8915451526641847, val loss: 3.969879984855652, ETA in seconds: 1450368.142\n",
      "epoch: 759200, train loss: 3.884675073623657, val loss: 3.9747327089309694, ETA in seconds: 1449942.266\n",
      "epoch: 759300, train loss: 3.9040416955947874, val loss: 3.9810567140579223, ETA in seconds: 1449512.341\n",
      "epoch: 759400, train loss: 3.895346450805664, val loss: 3.97022864818573, ETA in seconds: 1449080.887\n",
      "epoch: 759500, train loss: 3.8923173189163207, val loss: 3.972832131385803, ETA in seconds: 1448651.100\n",
      "epoch: 759600, train loss: 3.8926207065582275, val loss: 3.9925282478332518, ETA in seconds: 1448234.645\n",
      "epoch: 759700, train loss: 3.899314451217651, val loss: 3.9745368719100953, ETA in seconds: 1447833.351\n",
      "epoch: 759800, train loss: 3.884942412376404, val loss: 3.9613108158111574, ETA in seconds: 1447449.148\n",
      "epoch: 759900, train loss: 3.888385462760925, val loss: 3.978995847702026, ETA in seconds: 1447060.747\n",
      "epoch: 760000, train loss: 3.8963698625564573, val loss: 3.960391473770142, ETA in seconds: 1446662.300\n",
      "epoch: 760100, train loss: 3.894845747947693, val loss: 3.976266360282898, ETA in seconds: 1446235.909\n",
      "epoch: 760200, train loss: 3.8894336223602295, val loss: 3.9846208810806276, ETA in seconds: 1445809.668\n",
      "epoch: 760300, train loss: 3.8917657852172853, val loss: 3.9682480096817017, ETA in seconds: 1445379.350\n",
      "epoch: 760400, train loss: 3.8902871370315553, val loss: 3.9725765228271483, ETA in seconds: 1444952.071\n",
      "epoch: 760500, train loss: 3.8916601181030273, val loss: 3.9719849824905396, ETA in seconds: 1444527.776\n",
      "epoch: 760600, train loss: 3.889590620994568, val loss: 3.982088541984558, ETA in seconds: 1444101.590\n",
      "epoch: 760700, train loss: 3.8979456663131713, val loss: 3.9783422470092775, ETA in seconds: 1443675.076\n",
      "epoch: 760800, train loss: 3.888232445716858, val loss: 3.969909143447876, ETA in seconds: 1443247.641\n",
      "epoch: 760900, train loss: 3.8936545610427857, val loss: 3.9713237047195435, ETA in seconds: 1442823.563\n",
      "epoch: 761000, train loss: 3.8922552108764648, val loss: 3.977297306060791, ETA in seconds: 1442401.357\n",
      "epoch: 761100, train loss: 3.899139881134033, val loss: 3.9756698846817016, ETA in seconds: 1441981.250\n",
      "epoch: 761200, train loss: 3.8896534204483033, val loss: 3.9577785968780517, ETA in seconds: 1441571.874\n",
      "epoch: 761300, train loss: 3.8962384939193724, val loss: 3.9757044315338135, ETA in seconds: 1441183.604\n",
      "epoch: 761400, train loss: 3.8985725164413454, val loss: 3.9784279584884645, ETA in seconds: 1440781.643\n",
      "epoch: 761500, train loss: 3.8889479637145996, val loss: 3.9700121879577637, ETA in seconds: 1440359.762\n",
      "epoch: 761600, train loss: 3.894406485557556, val loss: 3.974511909484863, ETA in seconds: 1439938.773\n",
      "epoch: 761700, train loss: 3.8937615394592284, val loss: 3.965617609024048, ETA in seconds: 1439518.252\n",
      "epoch: 761800, train loss: 3.8972829818725585, val loss: 3.973680114746094, ETA in seconds: 1439105.331\n",
      "epoch: 761900, train loss: 3.883690929412842, val loss: 3.9750316381454467, ETA in seconds: 1438714.334\n",
      "epoch: 762000, train loss: 3.8985474586486815, val loss: 3.9840124607086183, ETA in seconds: 1438320.872\n",
      "epoch: 762100, train loss: 3.891310381889343, val loss: 3.9847293376922606, ETA in seconds: 1437928.849\n",
      "epoch: 762200, train loss: 3.902373957633972, val loss: 3.9764548540115356, ETA in seconds: 1437535.015\n",
      "epoch: 762300, train loss: 3.8923419952392577, val loss: 3.9697853088378907, ETA in seconds: 1437140.476\n",
      "epoch: 762400, train loss: 3.8991343498229982, val loss: 3.980230617523193, ETA in seconds: 1436746.958\n",
      "epoch: 762500, train loss: 3.8829607009887694, val loss: 3.9758769035339356, ETA in seconds: 1436354.471\n",
      "epoch: 762600, train loss: 3.8942059755325316, val loss: 3.9667921543121336, ETA in seconds: 1435965.158\n",
      "epoch: 762700, train loss: 3.8929984092712404, val loss: 3.9650495767593386, ETA in seconds: 1435551.396\n",
      "epoch: 762800, train loss: 3.895709824562073, val loss: 3.9716976642608643, ETA in seconds: 1435156.674\n",
      "epoch: 762900, train loss: 3.900235986709595, val loss: 3.972701907157898, ETA in seconds: 1434738.170\n",
      "epoch: 763000, train loss: 3.894752597808838, val loss: 3.9656178951263428, ETA in seconds: 1434329.622\n",
      "epoch: 763100, train loss: 3.8930722951889036, val loss: 3.9781518936157227, ETA in seconds: 1433941.290\n",
      "epoch: 763200, train loss: 3.8881838083267213, val loss: 3.966430139541626, ETA in seconds: 1433551.401\n",
      "epoch: 763300, train loss: 3.889610505104065, val loss: 3.96788284778595, ETA in seconds: 1433162.518\n",
      "epoch: 763400, train loss: 3.900176692008972, val loss: 3.9598041772842407, ETA in seconds: 1432772.622\n",
      "epoch: 763500, train loss: 3.8949846506118773, val loss: 3.962185788154602, ETA in seconds: 1432383.107\n",
      "epoch: 763600, train loss: 3.8907570838928223, val loss: 3.9695693016052247, ETA in seconds: 1431990.620\n",
      "epoch: 763700, train loss: 3.907067155838013, val loss: 3.9646947383880615, ETA in seconds: 1431598.484\n",
      "epoch: 763800, train loss: 3.9000152111053468, val loss: 3.9653302669525146, ETA in seconds: 1431185.353\n",
      "epoch: 763900, train loss: 3.89789137840271, val loss: 3.9821229934692384, ETA in seconds: 1430800.319\n",
      "epoch: 764000, train loss: 3.8900131225585937, val loss: 3.960784983634949, ETA in seconds: 1430406.966\n",
      "epoch: 764100, train loss: 3.8829715490341186, val loss: 3.9712859392166138, ETA in seconds: 1430012.414\n",
      "epoch: 764200, train loss: 3.8915844440460203, val loss: 3.962082862854004, ETA in seconds: 1429609.505\n",
      "epoch: 764300, train loss: 3.9046842098236083, val loss: 3.9751496076583863, ETA in seconds: 1429181.190\n",
      "epoch: 764400, train loss: 3.887787961959839, val loss: 3.9718395709991454, ETA in seconds: 1428748.376\n",
      "epoch: 764500, train loss: 3.8931419134140013, val loss: 3.972544860839844, ETA in seconds: 1428318.043\n",
      "epoch: 764600, train loss: 3.8986783742904665, val loss: 3.987506651878357, ETA in seconds: 1427889.792\n",
      "epoch: 764700, train loss: 3.892459845542908, val loss: 3.9659929990768434, ETA in seconds: 1427457.976\n",
      "epoch: 764800, train loss: 3.8998899459838867, val loss: 3.977423667907715, ETA in seconds: 1427040.356\n",
      "epoch: 764900, train loss: 3.8913212776184083, val loss: 3.9686937808990477, ETA in seconds: 1426650.847\n",
      "epoch: 765000, train loss: 3.877313566207886, val loss: 3.9789544343948364, ETA in seconds: 1426262.927\n",
      "epoch: 765100, train loss: 3.904230260848999, val loss: 3.9792153358459474, ETA in seconds: 1425871.276\n",
      "epoch: 765200, train loss: 3.8913711309432983, val loss: 3.9696502923965453, ETA in seconds: 1425480.575\n",
      "epoch: 765300, train loss: 3.895918297767639, val loss: 3.968625807762146, ETA in seconds: 1425088.683\n",
      "epoch: 765400, train loss: 3.884386897087097, val loss: 3.974249076843262, ETA in seconds: 1424697.678\n",
      "epoch: 765500, train loss: 3.892270636558533, val loss: 3.965440368652344, ETA in seconds: 1424305.832\n",
      "epoch: 765600, train loss: 3.8897753953933716, val loss: 3.9703302145004273, ETA in seconds: 1423894.135\n",
      "epoch: 765700, train loss: 3.8992557525634766, val loss: 3.973838448524475, ETA in seconds: 1423492.274\n",
      "epoch: 765800, train loss: 3.8961014986038207, val loss: 3.967317748069763, ETA in seconds: 1423059.689\n",
      "epoch: 765900, train loss: 3.8885732173919676, val loss: 3.9786818504333494, ETA in seconds: 1422626.315\n",
      "epoch: 766000, train loss: 3.897280740737915, val loss: 3.971093273162842, ETA in seconds: 1422194.087\n",
      "epoch: 766100, train loss: 3.8862416744232178, val loss: 3.97493999004364, ETA in seconds: 1421759.583\n",
      "epoch: 766200, train loss: 3.88806836605072, val loss: 3.964326024055481, ETA in seconds: 1421329.314\n",
      "epoch: 766300, train loss: 3.891170525550842, val loss: 3.97763307094574, ETA in seconds: 1420913.103\n",
      "epoch: 766400, train loss: 3.88766930103302, val loss: 3.9728373527526855, ETA in seconds: 1420515.425\n",
      "epoch: 766500, train loss: 3.8947293043136595, val loss: 3.975711464881897, ETA in seconds: 1420117.016\n",
      "epoch: 766600, train loss: 3.8939404487609863, val loss: 3.975890326499939, ETA in seconds: 1419718.809\n",
      "epoch: 766700, train loss: 3.890125775337219, val loss: 3.968567156791687, ETA in seconds: 1419319.769\n",
      "epoch: 766800, train loss: 3.89444899559021, val loss: 3.961212992668152, ETA in seconds: 1418920.399\n",
      "epoch: 766900, train loss: 3.889393615722656, val loss: 3.9605986356735228, ETA in seconds: 1418524.042\n",
      "epoch: 767000, train loss: 3.891462969779968, val loss: 3.970728802680969, ETA in seconds: 1418125.307\n",
      "epoch: 767100, train loss: 3.8868195056915282, val loss: 3.972567057609558, ETA in seconds: 1417725.794\n",
      "epoch: 767200, train loss: 3.8867600440979, val loss: 3.968274688720703, ETA in seconds: 1417326.914\n",
      "epoch: 767300, train loss: 3.887884569168091, val loss: 3.96119384765625, ETA in seconds: 1416914.129\n",
      "epoch: 767400, train loss: 3.897178030014038, val loss: 3.9712267398834227, ETA in seconds: 1416477.011\n",
      "epoch: 767500, train loss: 3.888060760498047, val loss: 3.9704254150390623, ETA in seconds: 1416047.247\n",
      "epoch: 767600, train loss: 3.88255569934845, val loss: 3.9637420415878295, ETA in seconds: 1415623.546\n",
      "epoch: 767700, train loss: 3.900397205352783, val loss: 3.9569381713867187, ETA in seconds: 1415177.412\n",
      "epoch: 767800, train loss: 3.897484874725342, val loss: 3.972205948829651, ETA in seconds: 1414746.377\n",
      "epoch: 767900, train loss: 3.879958415031433, val loss: 3.979751372337341, ETA in seconds: 1414326.571\n",
      "epoch: 768000, train loss: 3.8852755308151243, val loss: 3.970241975784302, ETA in seconds: 1413886.015\n",
      "epoch: 768100, train loss: 3.897887349128723, val loss: 3.967763614654541, ETA in seconds: 1413445.054\n",
      "epoch: 768200, train loss: 3.8947451591491697, val loss: 3.965810775756836, ETA in seconds: 1413016.134\n",
      "epoch: 768300, train loss: 3.898867130279541, val loss: 3.969774913787842, ETA in seconds: 1412577.502\n",
      "epoch: 768400, train loss: 3.899008560180664, val loss: 3.9684550046920775, ETA in seconds: 1412140.820\n",
      "epoch: 768500, train loss: 3.897892475128174, val loss: 3.976202702522278, ETA in seconds: 1411703.726\n",
      "epoch: 768600, train loss: 3.891491746902466, val loss: 3.9700237989425657, ETA in seconds: 1411269.992\n",
      "epoch: 768700, train loss: 3.885688543319702, val loss: 3.976963186264038, ETA in seconds: 1410845.491\n",
      "epoch: 768800, train loss: 3.8906464099884035, val loss: 3.9699300050735475, ETA in seconds: 1410441.992\n",
      "epoch: 768900, train loss: 3.8985175848007203, val loss: 3.9779019355773926, ETA in seconds: 1410039.820\n",
      "epoch: 769000, train loss: 3.8857069492340086, val loss: 3.9724217653274536, ETA in seconds: 1409637.116\n",
      "epoch: 769100, train loss: 3.901742672920227, val loss: 3.9759987592697144, ETA in seconds: 1409252.170\n",
      "epoch: 769200, train loss: 3.889416241645813, val loss: 3.967407703399658, ETA in seconds: 1408828.499\n",
      "epoch: 769300, train loss: 3.8886276721954345, val loss: 3.977345013618469, ETA in seconds: 1408406.012\n",
      "epoch: 769400, train loss: 3.901178503036499, val loss: 3.97175076007843, ETA in seconds: 1407982.002\n",
      "epoch: 769500, train loss: 3.895697736740112, val loss: 3.9712583303451536, ETA in seconds: 1407560.586\n",
      "epoch: 769600, train loss: 3.887649393081665, val loss: 3.9789946556091307, ETA in seconds: 1407129.680\n",
      "epoch: 769700, train loss: 3.8874690532684326, val loss: 3.975567579269409, ETA in seconds: 1406702.319\n",
      "epoch: 769800, train loss: 3.8965397119522094, val loss: 3.9633829593658447, ETA in seconds: 1406275.568\n",
      "epoch: 769900, train loss: 3.890140700340271, val loss: 3.967706251144409, ETA in seconds: 1405844.082\n",
      "epoch: 770000, train loss: 3.886892485618591, val loss: 3.972579336166382, ETA in seconds: 1405414.764\n",
      "epoch: 770100, train loss: 3.8829434633255007, val loss: 3.983513855934143, ETA in seconds: 1404988.276\n",
      "epoch: 770200, train loss: 3.8921812057495115, val loss: 3.9708287477493287, ETA in seconds: 1404562.792\n",
      "epoch: 770300, train loss: 3.902024173736572, val loss: 3.9674853324890136, ETA in seconds: 1404132.621\n",
      "epoch: 770400, train loss: 3.892907428741455, val loss: 3.9769468784332274, ETA in seconds: 1403693.804\n",
      "epoch: 770500, train loss: 3.888542652130127, val loss: 3.9706103801727295, ETA in seconds: 1403258.014\n",
      "epoch: 770600, train loss: 3.8948059558868406, val loss: 3.9689779043197633, ETA in seconds: 1402817.428\n",
      "epoch: 770700, train loss: 3.8954574346542357, val loss: 3.975771427154541, ETA in seconds: 1402375.633\n",
      "epoch: 770800, train loss: 3.890072202682495, val loss: 3.975741910934448, ETA in seconds: 1401935.448\n",
      "epoch: 770900, train loss: 3.8903200149536135, val loss: 3.971386122703552, ETA in seconds: 1401499.180\n",
      "epoch: 771000, train loss: 3.8939798355102537, val loss: 3.9797622680664064, ETA in seconds: 1401058.424\n",
      "epoch: 771100, train loss: 3.8972007751464846, val loss: 3.970231819152832, ETA in seconds: 1400617.621\n",
      "epoch: 771200, train loss: 3.8878059864044188, val loss: 3.9853076696395875, ETA in seconds: 1400170.668\n",
      "epoch: 771300, train loss: 3.895162057876587, val loss: 3.9771293878555296, ETA in seconds: 1399731.724\n",
      "epoch: 771400, train loss: 3.8960158109664915, val loss: 3.976255440711975, ETA in seconds: 1399302.288\n",
      "epoch: 771500, train loss: 3.8981619119644164, val loss: 3.9807362794876098, ETA in seconds: 1398871.676\n",
      "epoch: 771600, train loss: 3.903451919555664, val loss: 3.970189118385315, ETA in seconds: 1398444.786\n",
      "epoch: 771700, train loss: 3.904818797111511, val loss: 3.9761125326156614, ETA in seconds: 1398018.236\n",
      "epoch: 771800, train loss: 3.8894545078277587, val loss: 3.9696260929107665, ETA in seconds: 1397590.803\n",
      "epoch: 771900, train loss: 3.896020245552063, val loss: 3.9787714958190916, ETA in seconds: 1397160.813\n",
      "epoch: 772000, train loss: 3.8904897451400755, val loss: 3.9802595138549806, ETA in seconds: 1396724.016\n",
      "epoch: 772100, train loss: 3.8982811689376833, val loss: 3.965076446533203, ETA in seconds: 1396301.851\n",
      "epoch: 772200, train loss: 3.90200731754303, val loss: 3.971250367164612, ETA in seconds: 1395913.750\n",
      "epoch: 772300, train loss: 3.8865146160125734, val loss: 3.971590828895569, ETA in seconds: 1395509.886\n",
      "epoch: 772400, train loss: 3.8960538148880004, val loss: 3.970005679130554, ETA in seconds: 1395110.196\n",
      "epoch: 772500, train loss: 3.8915313959121702, val loss: 3.974095678329468, ETA in seconds: 1394685.782\n",
      "epoch: 772600, train loss: 3.891750860214233, val loss: 3.962834119796753, ETA in seconds: 1394239.525\n",
      "epoch: 772700, train loss: 3.890197229385376, val loss: 3.971696376800537, ETA in seconds: 1393794.917\n",
      "epoch: 772800, train loss: 3.8882941961288453, val loss: 3.975089263916016, ETA in seconds: 1393350.768\n",
      "epoch: 772900, train loss: 3.8931715726852416, val loss: 3.9709697008132934, ETA in seconds: 1392934.027\n",
      "epoch: 773000, train loss: 3.882712745666504, val loss: 3.97406587600708, ETA in seconds: 1392542.023\n",
      "epoch: 773100, train loss: 3.8981885433197023, val loss: 3.9780276298522947, ETA in seconds: 1392149.244\n",
      "epoch: 773200, train loss: 3.899176597595215, val loss: 3.9672850131988526, ETA in seconds: 1391732.783\n",
      "epoch: 773300, train loss: 3.888996386528015, val loss: 3.969062638282776, ETA in seconds: 1391293.777\n",
      "epoch: 773400, train loss: 3.8785863876342774, val loss: 3.982808828353882, ETA in seconds: 1390855.127\n",
      "epoch: 773500, train loss: 3.89843807220459, val loss: 3.9760490894317626, ETA in seconds: 1390451.756\n",
      "epoch: 773600, train loss: 3.882321906089783, val loss: 3.9784761905670165, ETA in seconds: 1389999.551\n",
      "epoch: 773700, train loss: 3.8893887042999267, val loss: 3.9720035791397095, ETA in seconds: 1389562.348\n",
      "epoch: 773800, train loss: 3.893475317955017, val loss: 3.9696882724761964, ETA in seconds: 1389138.079\n",
      "epoch: 773900, train loss: 3.8942572116851806, val loss: 3.9734009504318237, ETA in seconds: 1388702.317\n",
      "epoch: 774000, train loss: 3.8994689702987673, val loss: 3.965978670120239, ETA in seconds: 1388269.276\n",
      "epoch: 774100, train loss: 3.890449357032776, val loss: 3.9674949169158937, ETA in seconds: 1387836.983\n",
      "epoch: 774200, train loss: 3.8816475868225098, val loss: 3.969107222557068, ETA in seconds: 1387392.406\n",
      "epoch: 774300, train loss: 3.8914830684661865, val loss: 3.9711578130722045, ETA in seconds: 1386945.498\n",
      "epoch: 774400, train loss: 3.9002506256103517, val loss: 3.965615653991699, ETA in seconds: 1386500.411\n",
      "epoch: 774500, train loss: 3.8919052839279176, val loss: 3.9698753118515016, ETA in seconds: 1386055.824\n",
      "epoch: 774600, train loss: 3.876118040084839, val loss: 3.9689266681671143, ETA in seconds: 1385612.905\n",
      "epoch: 774700, train loss: 3.8931607723236086, val loss: 3.967018175125122, ETA in seconds: 1385199.366\n",
      "epoch: 774800, train loss: 3.8925463438034056, val loss: 3.967292308807373, ETA in seconds: 1384752.057\n",
      "epoch: 774900, train loss: 3.8788196325302122, val loss: 3.967366099357605, ETA in seconds: 1384301.567\n",
      "epoch: 775000, train loss: 3.889262247085571, val loss: 3.955988335609436, ETA in seconds: 1383854.173\n",
      "epoch: 775100, train loss: 3.894790768623352, val loss: 3.96575243473053, ETA in seconds: 1383409.070\n",
      "epoch: 775200, train loss: 3.8903012752532957, val loss: 3.9772663831710817, ETA in seconds: 1382954.477\n",
      "epoch: 775300, train loss: 3.8813680171966554, val loss: 3.975158190727234, ETA in seconds: 1382505.495\n",
      "epoch: 775400, train loss: 3.8925906896591185, val loss: 3.9676459312438963, ETA in seconds: 1382052.834\n",
      "epoch: 775500, train loss: 3.9037636518478394, val loss: 3.9610936880111693, ETA in seconds: 1381600.240\n",
      "epoch: 775600, train loss: 3.882352519035339, val loss: 3.979562520980835, ETA in seconds: 1381157.196\n",
      "epoch: 775700, train loss: 3.889863109588623, val loss: 3.9721869468688964, ETA in seconds: 1380719.852\n",
      "epoch: 775800, train loss: 3.8920733451843263, val loss: 3.971767449378967, ETA in seconds: 1380269.352\n",
      "epoch: 775900, train loss: 3.881603813171387, val loss: 3.9743672370910645, ETA in seconds: 1379818.943\n",
      "epoch: 776000, train loss: 3.8918860912323, val loss: 3.970906710624695, ETA in seconds: 1379364.407\n",
      "epoch: 776100, train loss: 3.892829751968384, val loss: 3.9692331075668337, ETA in seconds: 1378916.147\n",
      "epoch: 776200, train loss: 3.8912912607192993, val loss: 3.969121217727661, ETA in seconds: 1378466.388\n",
      "epoch: 776300, train loss: 3.8955798387527465, val loss: 3.9814894676208494, ETA in seconds: 1378064.519\n",
      "epoch: 776400, train loss: 3.901543664932251, val loss: 3.9828446388244627, ETA in seconds: 1377660.985\n",
      "epoch: 776500, train loss: 3.8888243198394776, val loss: 3.9614574670791627, ETA in seconds: 1377253.877\n",
      "epoch: 776600, train loss: 3.8837159395217897, val loss: 3.9692546129226685, ETA in seconds: 1376813.317\n",
      "epoch: 776700, train loss: 3.8968984603881838, val loss: 3.9674329280853273, ETA in seconds: 1376362.459\n",
      "epoch: 776800, train loss: 3.8920066118240357, val loss: 3.9560184717178344, ETA in seconds: 1375912.320\n",
      "epoch: 776900, train loss: 3.8888865232467653, val loss: 3.9772118091583253, ETA in seconds: 1375465.495\n",
      "epoch: 777000, train loss: 3.900688910484314, val loss: 3.960249996185303, ETA in seconds: 1375021.385\n",
      "epoch: 777100, train loss: 3.888581919670105, val loss: 3.97275550365448, ETA in seconds: 1374597.634\n",
      "epoch: 777200, train loss: 3.8950523376464843, val loss: 3.974958968162537, ETA in seconds: 1374156.594\n",
      "epoch: 777300, train loss: 3.8829949140548705, val loss: 3.962903594970703, ETA in seconds: 1373713.861\n",
      "epoch: 777400, train loss: 3.887049698829651, val loss: 3.959814929962158, ETA in seconds: 1373264.599\n",
      "epoch: 777500, train loss: 3.8976264715194704, val loss: 3.9741591215133667, ETA in seconds: 1372815.136\n",
      "epoch: 777600, train loss: 3.899292826652527, val loss: 3.9697866678237914, ETA in seconds: 1372368.615\n",
      "epoch: 777700, train loss: 3.8925868034362794, val loss: 3.9734517097473145, ETA in seconds: 1371918.822\n",
      "epoch: 777800, train loss: 3.8864831209182737, val loss: 3.968304085731506, ETA in seconds: 1371468.193\n",
      "epoch: 777900, train loss: 3.903372883796692, val loss: 3.970674967765808, ETA in seconds: 1371016.671\n",
      "epoch: 778000, train loss: 3.8949230670928956, val loss: 3.977339243888855, ETA in seconds: 1370565.808\n",
      "epoch: 778100, train loss: 3.8960493087768553, val loss: 3.970785212516785, ETA in seconds: 1370114.411\n",
      "epoch: 778200, train loss: 3.896448755264282, val loss: 3.98122239112854, ETA in seconds: 1369672.395\n",
      "epoch: 778300, train loss: 3.8930267810821535, val loss: 3.9733569622039795, ETA in seconds: 1369239.208\n",
      "epoch: 778400, train loss: 3.879114103317261, val loss: 3.9671466588974, ETA in seconds: 1368820.948\n",
      "epoch: 778500, train loss: 3.89079864025116, val loss: 3.9693362951278686, ETA in seconds: 1368403.263\n",
      "epoch: 778600, train loss: 3.8984699726104735, val loss: 3.968026876449585, ETA in seconds: 1367987.041\n",
      "epoch: 778700, train loss: 3.8851916074752806, val loss: 3.9575311660766603, ETA in seconds: 1367565.396\n",
      "epoch: 778800, train loss: 3.8996756076812744, val loss: 3.9779240131378173, ETA in seconds: 1367112.167\n",
      "epoch: 778900, train loss: 3.8939690589904785, val loss: 3.978299856185913, ETA in seconds: 1366676.118\n",
      "epoch: 779000, train loss: 3.89610013961792, val loss: 3.963859963417053, ETA in seconds: 1366226.276\n",
      "epoch: 779100, train loss: 3.90123245716095, val loss: 3.9728904724121095, ETA in seconds: 1365775.066\n",
      "epoch: 779200, train loss: 3.8977221727371214, val loss: 3.963092565536499, ETA in seconds: 1365363.389\n",
      "epoch: 779300, train loss: 3.9019116401672362, val loss: 3.9651458263397217, ETA in seconds: 1364912.606\n",
      "epoch: 779400, train loss: 3.8847543239593505, val loss: 3.9599061727523805, ETA in seconds: 1364462.130\n",
      "epoch: 779500, train loss: 3.8847950220108034, val loss: 3.9676473140716553, ETA in seconds: 1364006.912\n",
      "epoch: 779600, train loss: 3.9020302057266236, val loss: 3.973020315170288, ETA in seconds: 1363555.475\n",
      "epoch: 779700, train loss: 3.9018193960189818, val loss: 3.9810091733932493, ETA in seconds: 1363108.577\n",
      "epoch: 779800, train loss: 3.8896706342697143, val loss: 3.9722954750061037, ETA in seconds: 1362651.917\n",
      "epoch: 779900, train loss: 3.894548201560974, val loss: 3.9676223039627074, ETA in seconds: 1362210.034\n",
      "epoch: 780000, train loss: 3.895824837684631, val loss: 3.969255208969116, ETA in seconds: 1361759.037\n",
      "epoch: 780100, train loss: 3.886370062828064, val loss: 3.982002830505371, ETA in seconds: 1361317.351\n",
      "epoch: 780200, train loss: 3.8907409429550173, val loss: 3.972330164909363, ETA in seconds: 1360872.638\n",
      "epoch: 780300, train loss: 3.8877914428710936, val loss: 3.9575942039489744, ETA in seconds: 1360417.745\n",
      "epoch: 780400, train loss: 3.8909050464630126, val loss: 3.9751980543136596, ETA in seconds: 1359960.045\n",
      "epoch: 780500, train loss: 3.8985336780548097, val loss: 3.9656784534454346, ETA in seconds: 1359501.197\n",
      "epoch: 780600, train loss: 3.885466122627258, val loss: 3.9630335330963136, ETA in seconds: 1359043.605\n",
      "epoch: 780700, train loss: 3.8927518367767333, val loss: 3.9750494003295898, ETA in seconds: 1358598.740\n",
      "epoch: 780800, train loss: 3.8923009395599366, val loss: 3.9783671855926515, ETA in seconds: 1358158.570\n",
      "epoch: 780900, train loss: 3.889310026168823, val loss: 3.9773031949996946, ETA in seconds: 1357717.508\n",
      "epoch: 781000, train loss: 3.896190142631531, val loss: 3.9597249746322634, ETA in seconds: 1357272.206\n",
      "epoch: 781100, train loss: 3.904633808135986, val loss: 3.9693172216415404, ETA in seconds: 1356819.788\n",
      "epoch: 781200, train loss: 3.8882643699646, val loss: 3.975416970252991, ETA in seconds: 1356364.743\n",
      "epoch: 781300, train loss: 3.9010177850723267, val loss: 3.9650545835494997, ETA in seconds: 1355912.103\n",
      "epoch: 781400, train loss: 3.8964383363723756, val loss: 3.960900902748108, ETA in seconds: 1355486.078\n",
      "epoch: 781500, train loss: 3.8903778076171873, val loss: 3.9666568517684935, ETA in seconds: 1355082.315\n",
      "epoch: 781600, train loss: 3.8857402563095094, val loss: 3.9738123178482057, ETA in seconds: 1354676.016\n",
      "epoch: 781700, train loss: 3.892184615135193, val loss: 3.973980355262756, ETA in seconds: 1354245.533\n",
      "epoch: 781800, train loss: 3.891288685798645, val loss: 3.9680078506469725, ETA in seconds: 1353785.464\n",
      "epoch: 781900, train loss: 3.893939161300659, val loss: 3.9785079956054688, ETA in seconds: 1353353.715\n",
      "epoch: 782000, train loss: 3.8925924777984617, val loss: 3.973380613327026, ETA in seconds: 1352939.938\n",
      "epoch: 782100, train loss: 3.9000356435775756, val loss: 3.98290433883667, ETA in seconds: 1352528.850\n",
      "epoch: 782200, train loss: 3.889791703224182, val loss: 3.968579578399658, ETA in seconds: 1352118.049\n",
      "epoch: 782300, train loss: 3.893396735191345, val loss: 3.9704767227172852, ETA in seconds: 1351694.868\n",
      "epoch: 782400, train loss: 3.8843676090240478, val loss: 3.9576290130615233, ETA in seconds: 1351242.741\n",
      "epoch: 782500, train loss: 3.8950482606887817, val loss: 3.973162031173706, ETA in seconds: 1350807.032\n",
      "epoch: 782600, train loss: 3.88717577457428, val loss: 3.963310432434082, ETA in seconds: 1350383.464\n",
      "epoch: 782700, train loss: 3.888298439979553, val loss: 3.9621718406677244, ETA in seconds: 1349965.774\n",
      "epoch: 782800, train loss: 3.8870351552963256, val loss: 3.9618128061294557, ETA in seconds: 1349532.116\n",
      "epoch: 782900, train loss: 3.8914830446243287, val loss: 3.967654585838318, ETA in seconds: 1349107.277\n",
      "epoch: 783000, train loss: 3.894561839103699, val loss: 3.9770869970321656, ETA in seconds: 1348681.201\n",
      "epoch: 783100, train loss: 3.8940487623214723, val loss: 3.964896821975708, ETA in seconds: 1348254.983\n",
      "epoch: 783200, train loss: 3.8921210289001467, val loss: 3.9757903099060057, ETA in seconds: 1347827.095\n",
      "epoch: 783300, train loss: 3.8968973636627195, val loss: 3.977030611038208, ETA in seconds: 1347398.723\n",
      "epoch: 783400, train loss: 3.8987024784088136, val loss: 3.9723185777664183, ETA in seconds: 1346974.546\n",
      "epoch: 783500, train loss: 3.897911477088928, val loss: 3.9686014652252197, ETA in seconds: 1346546.464\n",
      "epoch: 783600, train loss: 3.895177984237671, val loss: 3.9703544855117796, ETA in seconds: 1346117.683\n",
      "epoch: 783700, train loss: 3.8908684253692627, val loss: 3.9706719398498533, ETA in seconds: 1345689.730\n",
      "epoch: 783800, train loss: 3.8799195289611816, val loss: 3.9817987442016602, ETA in seconds: 1345260.668\n",
      "epoch: 783900, train loss: 3.889213538169861, val loss: 3.964259481430054, ETA in seconds: 1344831.132\n",
      "epoch: 784000, train loss: 3.892437553405762, val loss: 3.9673479318618776, ETA in seconds: 1344404.884\n",
      "epoch: 784100, train loss: 3.9004522562026978, val loss: 3.963500952720642, ETA in seconds: 1343975.857\n",
      "epoch: 784200, train loss: 3.893076491355896, val loss: 3.9599212884902952, ETA in seconds: 1343525.995\n",
      "epoch: 784300, train loss: 3.88747091293335, val loss: 3.969374418258667, ETA in seconds: 1343059.571\n",
      "epoch: 784400, train loss: 3.899606943130493, val loss: 3.958641600608826, ETA in seconds: 1342589.481\n",
      "epoch: 784500, train loss: 3.898188924789429, val loss: 3.97302508354187, ETA in seconds: 1342118.142\n",
      "epoch: 784600, train loss: 3.896698760986328, val loss: 3.980581498146057, ETA in seconds: 1341646.119\n",
      "epoch: 784700, train loss: 3.8917471408843993, val loss: 3.971000146865845, ETA in seconds: 1341179.055\n",
      "epoch: 784800, train loss: 3.8949916839599608, val loss: 3.9770612716674805, ETA in seconds: 1340711.628\n",
      "epoch: 784900, train loss: 3.8890294075012206, val loss: 3.9762229442596437, ETA in seconds: 1340245.438\n",
      "epoch: 785000, train loss: 3.897188091278076, val loss: 3.965544867515564, ETA in seconds: 1339778.515\n",
      "epoch: 785100, train loss: 3.9018563985824586, val loss: 3.9736072540283205, ETA in seconds: 1339310.916\n",
      "epoch: 785200, train loss: 3.8854471683502196, val loss: 3.9784579277038574, ETA in seconds: 1338845.233\n",
      "epoch: 785300, train loss: 3.8898716449737547, val loss: 3.971920204162598, ETA in seconds: 1338377.181\n",
      "epoch: 785400, train loss: 3.8835219383239745, val loss: 3.972285342216492, ETA in seconds: 1337909.680\n",
      "epoch: 785500, train loss: 3.9013691425323485, val loss: 3.9707958459854127, ETA in seconds: 1337445.058\n",
      "epoch: 785600, train loss: 3.90147602558136, val loss: 3.9858474254608156, ETA in seconds: 1336979.543\n",
      "epoch: 785700, train loss: 3.8918209314346313, val loss: 3.9588860511779784, ETA in seconds: 1336510.908\n",
      "epoch: 785800, train loss: 3.900536322593689, val loss: 3.985068154335022, ETA in seconds: 1336051.815\n",
      "epoch: 785900, train loss: 3.9047459602355956, val loss: 3.9647765159606934, ETA in seconds: 1335587.627\n",
      "epoch: 786000, train loss: 3.8981089115142824, val loss: 3.9736281394958497, ETA in seconds: 1335123.557\n",
      "epoch: 786100, train loss: 3.8926455974578857, val loss: 3.972526264190674, ETA in seconds: 1334661.129\n",
      "epoch: 786200, train loss: 3.8876306772232057, val loss: 3.9717228412628174, ETA in seconds: 1334205.561\n",
      "epoch: 786300, train loss: 3.889303684234619, val loss: 3.9723459482192993, ETA in seconds: 1333736.676\n",
      "epoch: 786400, train loss: 3.885365295410156, val loss: 3.9647085428237916, ETA in seconds: 1333271.223\n",
      "epoch: 786500, train loss: 3.900098943710327, val loss: 3.9590536832809446, ETA in seconds: 1332810.963\n",
      "epoch: 786600, train loss: 3.8955140829086305, val loss: 3.964418148994446, ETA in seconds: 1332358.615\n",
      "epoch: 786700, train loss: 3.8965119123458862, val loss: 3.985067081451416, ETA in seconds: 1331900.018\n",
      "epoch: 786800, train loss: 3.8885441064834594, val loss: 3.9776214122772218, ETA in seconds: 1331429.194\n",
      "epoch: 786900, train loss: 3.898184084892273, val loss: 3.9577612400054933, ETA in seconds: 1330964.226\n",
      "epoch: 787000, train loss: 3.897087335586548, val loss: 3.964231276512146, ETA in seconds: 1330506.344\n",
      "epoch: 787100, train loss: 3.902390646934509, val loss: 3.9781643629074095, ETA in seconds: 1330053.500\n",
      "epoch: 787200, train loss: 3.89770827293396, val loss: 3.967049312591553, ETA in seconds: 1329600.291\n",
      "epoch: 787300, train loss: 3.8922401428222657, val loss: 3.9679913759231566, ETA in seconds: 1329159.461\n",
      "epoch: 787400, train loss: 3.885997247695923, val loss: 3.991153526306152, ETA in seconds: 1328688.837\n",
      "epoch: 787500, train loss: 3.889435911178589, val loss: 3.966855192184448, ETA in seconds: 1328222.788\n",
      "epoch: 787600, train loss: 3.890796446800232, val loss: 3.964411664009094, ETA in seconds: 1327768.540\n",
      "epoch: 787700, train loss: 3.891702580451965, val loss: 3.9646682500839234, ETA in seconds: 1327313.868\n",
      "epoch: 787800, train loss: 3.8855266094207765, val loss: 3.976792073249817, ETA in seconds: 1326855.253\n",
      "epoch: 787900, train loss: 3.8963414669036864, val loss: 3.963821029663086, ETA in seconds: 1326391.498\n",
      "epoch: 788000, train loss: 3.8980871200561524, val loss: 3.9727083444595337, ETA in seconds: 1325925.718\n",
      "epoch: 788100, train loss: 3.8951967000961303, val loss: 3.9872747898101806, ETA in seconds: 1325462.669\n",
      "epoch: 788200, train loss: 3.890943145751953, val loss: 3.972178030014038, ETA in seconds: 1325027.528\n",
      "epoch: 788300, train loss: 3.8970888376235964, val loss: 3.968272161483765, ETA in seconds: 1324559.164\n",
      "epoch: 788400, train loss: 3.8954201698303224, val loss: 3.9733261346817015, ETA in seconds: 1324087.536\n",
      "epoch: 788500, train loss: 3.890820360183716, val loss: 3.9766738176345826, ETA in seconds: 1323635.797\n",
      "epoch: 788600, train loss: 3.8984359741210937, val loss: 3.9644878625869753, ETA in seconds: 1323171.247\n",
      "epoch: 788700, train loss: 3.887105369567871, val loss: 3.9614384889602663, ETA in seconds: 1322697.336\n",
      "epoch: 788800, train loss: 3.891928195953369, val loss: 3.974257731437683, ETA in seconds: 1322231.058\n",
      "epoch: 788900, train loss: 3.9001948833465576, val loss: 3.9760220766067507, ETA in seconds: 1321779.155\n",
      "epoch: 789000, train loss: 3.89658784866333, val loss: 3.9744987010955812, ETA in seconds: 1321307.743\n",
      "epoch: 789100, train loss: 3.9024290323257445, val loss: 3.9719918012619018, ETA in seconds: 1320839.784\n",
      "epoch: 789200, train loss: 3.901887083053589, val loss: 3.974205565452576, ETA in seconds: 1320375.625\n",
      "epoch: 789300, train loss: 3.8963733673095704, val loss: 3.97709538936615, ETA in seconds: 1319907.804\n",
      "epoch: 789400, train loss: 3.89784951210022, val loss: 3.9653234004974367, ETA in seconds: 1319432.769\n",
      "epoch: 789500, train loss: 3.8982441902160643, val loss: 3.974418115615845, ETA in seconds: 1318962.427\n",
      "epoch: 789600, train loss: 3.894976544380188, val loss: 3.9681535243988035, ETA in seconds: 1318502.887\n",
      "epoch: 789700, train loss: 3.892902064323425, val loss: 3.966987705230713, ETA in seconds: 1318044.605\n",
      "epoch: 789800, train loss: 3.892682361602783, val loss: 3.9796547174453734, ETA in seconds: 1317568.833\n",
      "epoch: 789900, train loss: 3.8913498163223266, val loss: 3.9673359870910643, ETA in seconds: 1317091.591\n",
      "epoch: 790000, train loss: 3.8922613143920897, val loss: 3.9723408460617065, ETA in seconds: 1316614.011\n",
      "epoch: 790100, train loss: 3.9033785581588747, val loss: 3.9640314102172853, ETA in seconds: 1316145.700\n",
      "epoch: 790200, train loss: 3.89435555934906, val loss: 3.9703919649124146, ETA in seconds: 1315671.099\n",
      "epoch: 790300, train loss: 3.897098970413208, val loss: 3.976976776123047, ETA in seconds: 1315197.950\n",
      "epoch: 790400, train loss: 3.8928150177001952, val loss: 3.9661763429641725, ETA in seconds: 1314768.790\n",
      "epoch: 790500, train loss: 3.8952587366104128, val loss: 3.9710613250732423, ETA in seconds: 1314343.449\n",
      "epoch: 790600, train loss: 3.8969559192657472, val loss: 3.9817524194717406, ETA in seconds: 1313912.016\n",
      "epoch: 790700, train loss: 3.891507649421692, val loss: 3.965520405769348, ETA in seconds: 1313476.909\n",
      "epoch: 790800, train loss: 3.8865640640258787, val loss: 3.9642093658447264, ETA in seconds: 1313014.705\n",
      "epoch: 790900, train loss: 3.896253228187561, val loss: 3.967588114738464, ETA in seconds: 1312540.326\n",
      "epoch: 791000, train loss: 3.8950095415115356, val loss: 3.971855044364929, ETA in seconds: 1312067.896\n",
      "epoch: 791100, train loss: 3.8898810386657714, val loss: 3.9894388198852537, ETA in seconds: 1311595.405\n",
      "epoch: 791200, train loss: 3.896441411972046, val loss: 3.9752434968948362, ETA in seconds: 1311126.843\n",
      "epoch: 791300, train loss: 3.893094563484192, val loss: 3.971091389656067, ETA in seconds: 1310659.206\n",
      "epoch: 791400, train loss: 3.8902546882629396, val loss: 3.9749234676361085, ETA in seconds: 1310200.147\n",
      "epoch: 791500, train loss: 3.893591284751892, val loss: 3.9738343000411986, ETA in seconds: 1309750.783\n",
      "epoch: 791600, train loss: 3.8910625696182253, val loss: 3.9634091377258303, ETA in seconds: 1309280.656\n",
      "epoch: 791700, train loss: 3.884567975997925, val loss: 3.958349680900574, ETA in seconds: 1308812.759\n",
      "epoch: 791800, train loss: 3.9007359027862547, val loss: 3.975490379333496, ETA in seconds: 1308337.851\n",
      "epoch: 791900, train loss: 3.896963930130005, val loss: 3.9738288164138793, ETA in seconds: 1307858.808\n",
      "epoch: 792000, train loss: 3.8866395235061644, val loss: 3.9737112760543822, ETA in seconds: 1307380.232\n",
      "epoch: 792100, train loss: 3.8890785455703734, val loss: 3.973459005355835, ETA in seconds: 1306911.336\n",
      "epoch: 792200, train loss: 3.896589684486389, val loss: 3.975735306739807, ETA in seconds: 1306459.861\n",
      "epoch: 792300, train loss: 3.8798635721206667, val loss: 3.962904453277588, ETA in seconds: 1305983.543\n",
      "epoch: 792400, train loss: 3.8883787870407103, val loss: 3.9736505270004274, ETA in seconds: 1305513.235\n",
      "epoch: 792500, train loss: 3.8919700622558593, val loss: 3.9706812858581544, ETA in seconds: 1305044.988\n",
      "epoch: 792600, train loss: 3.9001109838485717, val loss: 3.971020150184631, ETA in seconds: 1304580.972\n",
      "epoch: 792700, train loss: 3.8902440547943113, val loss: 3.9843528270721436, ETA in seconds: 1304120.797\n",
      "epoch: 792800, train loss: 3.8952405214309693, val loss: 3.9811232566833494, ETA in seconds: 1303650.251\n",
      "epoch: 792900, train loss: 3.8875491857528686, val loss: 3.974512982368469, ETA in seconds: 1303177.379\n",
      "epoch: 793000, train loss: 3.8927754402160644, val loss: 3.969832921028137, ETA in seconds: 1302710.220\n",
      "epoch: 793100, train loss: 3.8839203357696532, val loss: 3.979397010803223, ETA in seconds: 1302238.081\n",
      "epoch: 793200, train loss: 3.888028049468994, val loss: 3.9689766645431517, ETA in seconds: 1301772.050\n",
      "epoch: 793300, train loss: 3.8996782541275024, val loss: 3.9788924932479857, ETA in seconds: 1301301.461\n",
      "epoch: 793400, train loss: 3.8945151567459106, val loss: 3.958890199661255, ETA in seconds: 1300824.740\n",
      "epoch: 793500, train loss: 3.894430232048035, val loss: 3.970340538024902, ETA in seconds: 1300348.828\n",
      "epoch: 793600, train loss: 3.8896397590637206, val loss: 3.978058934211731, ETA in seconds: 1299872.673\n",
      "epoch: 793700, train loss: 3.888418412208557, val loss: 3.971098613739014, ETA in seconds: 1299413.574\n",
      "epoch: 793800, train loss: 3.896743392944336, val loss: 3.963038468360901, ETA in seconds: 1298958.852\n",
      "epoch: 793900, train loss: 3.8894450664520264, val loss: 3.9801739692687987, ETA in seconds: 1298487.152\n",
      "epoch: 794000, train loss: 3.8987210273742674, val loss: 3.977043104171753, ETA in seconds: 1298011.903\n",
      "epoch: 794100, train loss: 3.892269992828369, val loss: 3.9859766244888304, ETA in seconds: 1297537.216\n",
      "epoch: 794200, train loss: 3.8868873834609987, val loss: 3.974351167678833, ETA in seconds: 1297067.725\n",
      "epoch: 794300, train loss: 3.8940512895584107, val loss: 3.977059507369995, ETA in seconds: 1296625.277\n",
      "epoch: 794400, train loss: 3.895612025260925, val loss: 3.9760796070098876, ETA in seconds: 1296184.855\n",
      "epoch: 794500, train loss: 3.8987955331802366, val loss: 3.9869476079940798, ETA in seconds: 1295742.599\n",
      "epoch: 794600, train loss: 3.8924262046813967, val loss: 3.9668299198150634, ETA in seconds: 1295299.486\n",
      "epoch: 794700, train loss: 3.8949787855148315, val loss: 3.9614165782928468, ETA in seconds: 1294831.207\n",
      "epoch: 794800, train loss: 3.8890332698822023, val loss: 3.976202440261841, ETA in seconds: 1294358.223\n",
      "epoch: 794900, train loss: 3.889292335510254, val loss: 3.9773123025894166, ETA in seconds: 1293884.117\n",
      "epoch: 795000, train loss: 3.8857976675033568, val loss: 3.9828657150268554, ETA in seconds: 1293414.151\n",
      "epoch: 795100, train loss: 3.901432180404663, val loss: 3.97004771232605, ETA in seconds: 1292943.320\n",
      "epoch: 795200, train loss: 3.8973008155822755, val loss: 3.9788917779922484, ETA in seconds: 1292465.750\n",
      "epoch: 795300, train loss: 3.893311071395874, val loss: 3.969370484352112, ETA in seconds: 1291989.306\n",
      "epoch: 795400, train loss: 3.8980749368667604, val loss: 3.965850067138672, ETA in seconds: 1291521.352\n",
      "epoch: 795500, train loss: 3.8880535125732423, val loss: 3.9600356817245483, ETA in seconds: 1291058.223\n",
      "epoch: 795600, train loss: 3.8924765586853027, val loss: 3.976718378067017, ETA in seconds: 1290588.534\n",
      "epoch: 795700, train loss: 3.893642258644104, val loss: 3.963324499130249, ETA in seconds: 1290109.403\n",
      "epoch: 795800, train loss: 3.8897273778915404, val loss: 3.9724839448928835, ETA in seconds: 1289635.847\n",
      "epoch: 795900, train loss: 3.88819682598114, val loss: 3.978284239768982, ETA in seconds: 1289156.261\n",
      "epoch: 796000, train loss: 3.8951390743255616, val loss: 3.9733028411865234, ETA in seconds: 1288695.680\n",
      "epoch: 796100, train loss: 3.8903994798660277, val loss: 3.970508909225464, ETA in seconds: 1288222.071\n",
      "epoch: 796200, train loss: 3.893175554275513, val loss: 3.9683099746704102, ETA in seconds: 1287763.960\n",
      "epoch: 796300, train loss: 3.8906980752944946, val loss: 3.9739888668060304, ETA in seconds: 1287314.348\n",
      "epoch: 796400, train loss: 3.8943217754364015, val loss: 3.9660313367843627, ETA in seconds: 1286829.918\n",
      "epoch: 796500, train loss: 3.8999019622802735, val loss: 3.9688198804855346, ETA in seconds: 1286355.335\n",
      "epoch: 796600, train loss: 3.882061409950256, val loss: 3.9716982364654543, ETA in seconds: 1285888.220\n",
      "epoch: 796700, train loss: 3.8938169479370117, val loss: 3.9775270223617554, ETA in seconds: 1285421.071\n",
      "epoch: 796800, train loss: 3.886509585380554, val loss: 3.9823569774627687, ETA in seconds: 1284944.548\n",
      "epoch: 796900, train loss: 3.8872503995895387, val loss: 3.9819896936416628, ETA in seconds: 1284465.188\n",
      "epoch: 797000, train loss: 3.891057825088501, val loss: 3.9610927581787108, ETA in seconds: 1283982.125\n",
      "epoch: 797100, train loss: 3.905114507675171, val loss: 3.9693129539489744, ETA in seconds: 1283506.545\n",
      "epoch: 797200, train loss: 3.892307424545288, val loss: 3.972380042076111, ETA in seconds: 1283027.805\n",
      "epoch: 797300, train loss: 3.893036460876465, val loss: 3.956755042076111, ETA in seconds: 1282559.777\n",
      "epoch: 797400, train loss: 3.893839406967163, val loss: 3.9742263078689577, ETA in seconds: 1282075.893\n",
      "epoch: 797500, train loss: 3.8931829929351807, val loss: 3.9685013055801392, ETA in seconds: 1281592.516\n",
      "epoch: 797600, train loss: 3.889863204956055, val loss: 3.967813587188721, ETA in seconds: 1281122.313\n",
      "epoch: 797700, train loss: 3.8970773220062256, val loss: 3.9704704523086547, ETA in seconds: 1280643.600\n",
      "epoch: 797800, train loss: 3.894869160652161, val loss: 3.971663737297058, ETA in seconds: 1280163.239\n",
      "epoch: 797900, train loss: 3.8853296041488647, val loss: 3.963802671432495, ETA in seconds: 1279689.265\n",
      "epoch: 798000, train loss: 3.8979288816452025, val loss: 3.9691675186157225, ETA in seconds: 1279217.678\n",
      "epoch: 798100, train loss: 3.8904188871383667, val loss: 3.9730878114700316, ETA in seconds: 1278744.289\n",
      "epoch: 798200, train loss: 3.891687512397766, val loss: 3.963245391845703, ETA in seconds: 1278272.880\n",
      "epoch: 798300, train loss: 3.8923720359802245, val loss: 3.9652921199798583, ETA in seconds: 1277798.438\n",
      "epoch: 798400, train loss: 3.8946497678756713, val loss: 3.967028999328613, ETA in seconds: 1277335.550\n",
      "epoch: 798500, train loss: 3.9025071144104, val loss: 3.9671666860580443, ETA in seconds: 1276883.360\n",
      "epoch: 798600, train loss: 3.8948275566101076, val loss: 3.9664098739624025, ETA in seconds: 1276428.816\n",
      "epoch: 798700, train loss: 3.9016164541244507, val loss: 3.956896495819092, ETA in seconds: 1275984.693\n",
      "epoch: 798800, train loss: 3.8851842641830445, val loss: 3.968930172920227, ETA in seconds: 1275530.243\n",
      "epoch: 798900, train loss: 3.88363893032074, val loss: 3.97302827835083, ETA in seconds: 1275074.887\n",
      "epoch: 799000, train loss: 3.8979751586914064, val loss: 3.9559324026107787, ETA in seconds: 1274622.985\n",
      "epoch: 799100, train loss: 3.899578094482422, val loss: 3.968892812728882, ETA in seconds: 1274169.769\n",
      "epoch: 799200, train loss: 3.890139675140381, val loss: 3.9700711488723757, ETA in seconds: 1273711.503\n",
      "epoch: 799300, train loss: 3.8985671043395995, val loss: 3.9569917917251587, ETA in seconds: 1273257.893\n",
      "epoch: 799400, train loss: 3.898351860046387, val loss: 3.9723503828048705, ETA in seconds: 1272808.895\n",
      "epoch: 799500, train loss: 3.8812188386917112, val loss: 3.960014009475708, ETA in seconds: 1272330.569\n",
      "epoch: 799600, train loss: 3.8935304641723634, val loss: 3.9815083026885985, ETA in seconds: 1271843.415\n",
      "epoch: 799700, train loss: 3.8960753202438356, val loss: 3.970068836212158, ETA in seconds: 1271358.677\n",
      "epoch: 799800, train loss: 3.8889573335647585, val loss: 3.9716832637786865, ETA in seconds: 1270871.189\n",
      "epoch: 799900, train loss: 3.8918615341186524, val loss: 3.9673570156097413, ETA in seconds: 1270388.707\n",
      "epoch: 800000, train loss: 3.8979684591293333, val loss: 3.9741839170455933, ETA in seconds: 1269941.524\n",
      "epoch: 800100, train loss: 3.888579559326172, val loss: 3.9758363246917723, ETA in seconds: 1269489.404\n",
      "epoch: 800200, train loss: 3.89769561290741, val loss: 3.971403932571411, ETA in seconds: 1269033.062\n",
      "epoch: 800300, train loss: 3.891034460067749, val loss: 3.9710769653320312, ETA in seconds: 1268547.592\n",
      "epoch: 800400, train loss: 3.8902453184127808, val loss: 3.97156617641449, ETA in seconds: 1268077.250\n",
      "epoch: 800500, train loss: 3.8935081720352174, val loss: 3.9660176038742065, ETA in seconds: 1267601.416\n",
      "epoch: 800600, train loss: 3.8899418830871584, val loss: 3.964013671875, ETA in seconds: 1267145.133\n",
      "epoch: 800700, train loss: 3.8989171504974367, val loss: 3.979650282859802, ETA in seconds: 1266687.724\n",
      "epoch: 800800, train loss: 3.9086101770401003, val loss: 3.9755489587783814, ETA in seconds: 1266235.042\n",
      "epoch: 800900, train loss: 3.887551188468933, val loss: 3.9763750076293944, ETA in seconds: 1265776.197\n",
      "epoch: 801000, train loss: 3.8901755809783936, val loss: 3.9733500719070434, ETA in seconds: 1265292.098\n",
      "epoch: 801100, train loss: 3.8920129537582397, val loss: 3.9763455629348754, ETA in seconds: 1264827.475\n",
      "epoch: 801200, train loss: 3.887719416618347, val loss: 3.974394178390503, ETA in seconds: 1264340.416\n",
      "epoch: 801300, train loss: 3.8977006912231444, val loss: 3.966398739814758, ETA in seconds: 1263852.726\n",
      "epoch: 801400, train loss: 3.888974976539612, val loss: 3.9688852310180662, ETA in seconds: 1263365.616\n",
      "epoch: 801500, train loss: 3.8937029361724855, val loss: 3.972861623764038, ETA in seconds: 1262878.064\n",
      "epoch: 801600, train loss: 3.8888051748275756, val loss: 3.9664361000061037, ETA in seconds: 1262390.511\n",
      "epoch: 801700, train loss: 3.8923549890518188, val loss: 3.9682846784591677, ETA in seconds: 1261914.369\n",
      "epoch: 801800, train loss: 3.886783075332642, val loss: 3.978365349769592, ETA in seconds: 1261441.927\n",
      "epoch: 801900, train loss: 3.891625452041626, val loss: 3.9657962560653686, ETA in seconds: 1260963.080\n",
      "epoch: 802000, train loss: 3.899446439743042, val loss: 3.973279619216919, ETA in seconds: 1260475.940\n",
      "epoch: 802100, train loss: 3.8873032569885253, val loss: 3.973751950263977, ETA in seconds: 1259991.782\n",
      "epoch: 802200, train loss: 3.895581030845642, val loss: 3.975108814239502, ETA in seconds: 1259504.874\n",
      "epoch: 802300, train loss: 3.896314811706543, val loss: 3.959771513938904, ETA in seconds: 1259023.274\n",
      "epoch: 802400, train loss: 3.889788007736206, val loss: 3.9792229413986204, ETA in seconds: 1258535.537\n",
      "epoch: 802500, train loss: 3.890279221534729, val loss: 3.9730653524398805, ETA in seconds: 1258049.911\n",
      "epoch: 802600, train loss: 3.897226572036743, val loss: 3.9812628030776978, ETA in seconds: 1257571.494\n",
      "epoch: 802700, train loss: 3.8909724235534666, val loss: 3.971909236907959, ETA in seconds: 1257111.487\n",
      "epoch: 802800, train loss: 3.889834833145142, val loss: 3.9727970361709595, ETA in seconds: 1256650.682\n",
      "epoch: 802900, train loss: 3.8922799110412596, val loss: 3.9695006132125856, ETA in seconds: 1256201.590\n",
      "epoch: 803000, train loss: 3.8915530920028685, val loss: 3.9724402904510496, ETA in seconds: 1255731.231\n",
      "epoch: 803100, train loss: 3.89626784324646, val loss: 3.9744757652282714, ETA in seconds: 1255244.917\n",
      "epoch: 803200, train loss: 3.891858720779419, val loss: 3.9678112745285032, ETA in seconds: 1254760.809\n",
      "epoch: 803300, train loss: 3.8950710535049438, val loss: 3.970014214515686, ETA in seconds: 1254274.727\n",
      "epoch: 803400, train loss: 3.8848968029022215, val loss: 3.9751713991165163, ETA in seconds: 1253787.304\n",
      "epoch: 803500, train loss: 3.8894713640213014, val loss: 3.9679834127426146, ETA in seconds: 1253308.977\n",
      "epoch: 803600, train loss: 3.8893836975097655, val loss: 3.9697585821151735, ETA in seconds: 1252848.064\n",
      "epoch: 803700, train loss: 3.8917080640792845, val loss: 3.974646711349487, ETA in seconds: 1252386.971\n",
      "epoch: 803800, train loss: 3.892307448387146, val loss: 3.97584388256073, ETA in seconds: 1251924.354\n",
      "epoch: 803900, train loss: 3.899457263946533, val loss: 3.9761263847351076, ETA in seconds: 1251461.254\n",
      "epoch: 804000, train loss: 3.8947110414505004, val loss: 3.959556150436401, ETA in seconds: 1250997.411\n",
      "epoch: 804100, train loss: 3.910521364212036, val loss: 3.9660007476806642, ETA in seconds: 1250540.786\n",
      "epoch: 804200, train loss: 3.897453188896179, val loss: 3.97333824634552, ETA in seconds: 1250071.838\n",
      "epoch: 804300, train loss: 3.8917792081832885, val loss: 3.9788721561431886, ETA in seconds: 1249588.157\n",
      "epoch: 804400, train loss: 3.895631504058838, val loss: 3.9674675703048705, ETA in seconds: 1249107.309\n",
      "epoch: 804500, train loss: 3.8908929347991945, val loss: 3.9815431594848634, ETA in seconds: 1248619.054\n",
      "epoch: 804600, train loss: 3.895822358131409, val loss: 3.964574670791626, ETA in seconds: 1248129.463\n",
      "epoch: 804700, train loss: 3.889403486251831, val loss: 3.966546630859375, ETA in seconds: 1247638.753\n",
      "epoch: 804800, train loss: 3.893065404891968, val loss: 3.9669132947921755, ETA in seconds: 1247148.912\n",
      "epoch: 804900, train loss: 3.8977864742279054, val loss: 3.969872832298279, ETA in seconds: 1246661.886\n",
      "epoch: 805000, train loss: 3.886637330055237, val loss: 3.9536612749099733, ETA in seconds: 1246174.293\n",
      "epoch: 805100, train loss: 3.8942247867584228, val loss: 3.9770702362060546, ETA in seconds: 1245683.452\n",
      "epoch: 805200, train loss: 3.8917080402374267, val loss: 3.9709299325942995, ETA in seconds: 1245192.819\n",
      "epoch: 805300, train loss: 3.8914558410644533, val loss: 3.984675931930542, ETA in seconds: 1244701.833\n",
      "epoch: 805400, train loss: 3.8979973554611207, val loss: 3.9627565860748293, ETA in seconds: 1244209.565\n",
      "epoch: 805500, train loss: 3.900725483894348, val loss: 3.9648845911026003, ETA in seconds: 1243716.351\n",
      "epoch: 805600, train loss: 3.8960436582565308, val loss: 3.9779956102371217, ETA in seconds: 1243226.163\n",
      "epoch: 805700, train loss: 3.8906276702880858, val loss: 3.969517183303833, ETA in seconds: 1242734.236\n",
      "epoch: 805800, train loss: 3.8889511585235597, val loss: 3.9893445491790773, ETA in seconds: 1242258.488\n",
      "epoch: 805900, train loss: 3.89149649143219, val loss: 3.965944528579712, ETA in seconds: 1241804.123\n",
      "epoch: 806000, train loss: 3.8961952924728394, val loss: 3.979789209365845, ETA in seconds: 1241335.752\n",
      "epoch: 806100, train loss: 3.8933624744415285, val loss: 3.9738346576690673, ETA in seconds: 1240843.783\n",
      "epoch: 806200, train loss: 3.8931488752365113, val loss: 3.9845369338989256, ETA in seconds: 1240357.616\n",
      "epoch: 806300, train loss: 3.899936318397522, val loss: 3.981492447853088, ETA in seconds: 1239871.035\n",
      "epoch: 806400, train loss: 3.89569628238678, val loss: 3.9719566822052004, ETA in seconds: 1239375.892\n",
      "epoch: 806500, train loss: 3.892964220046997, val loss: 3.975515532493591, ETA in seconds: 1238890.971\n",
      "epoch: 806600, train loss: 3.9050966262817384, val loss: 3.975599241256714, ETA in seconds: 1238389.002\n",
      "epoch: 806700, train loss: 3.902569794654846, val loss: 3.972727108001709, ETA in seconds: 1237892.877\n",
      "epoch: 806800, train loss: 3.8935944080352782, val loss: 3.9727540731430055, ETA in seconds: 1237416.600\n",
      "epoch: 806900, train loss: 3.901624393463135, val loss: 3.9758230686187743, ETA in seconds: 1236967.027\n",
      "epoch: 807000, train loss: 3.890764045715332, val loss: 3.9703554630279543, ETA in seconds: 1236492.478\n",
      "epoch: 807100, train loss: 3.8889046907424927, val loss: 3.972606897354126, ETA in seconds: 1236005.622\n",
      "epoch: 807200, train loss: 3.8937814235687256, val loss: 3.9768893003463743, ETA in seconds: 1235508.304\n",
      "epoch: 807300, train loss: 3.8969289302825927, val loss: 3.964589333534241, ETA in seconds: 1235017.564\n",
      "epoch: 807400, train loss: 3.8905693531036376, val loss: 3.9664977312088014, ETA in seconds: 1234527.961\n",
      "epoch: 807500, train loss: 3.892904019355774, val loss: 3.9702807903289794, ETA in seconds: 1234038.525\n",
      "epoch: 807600, train loss: 3.894021511077881, val loss: 3.9667631149291993, ETA in seconds: 1233550.793\n",
      "epoch: 807700, train loss: 3.894414281845093, val loss: 3.9721246719360352, ETA in seconds: 1233059.849\n",
      "epoch: 807800, train loss: 3.8841812372207642, val loss: 3.9679112672805785, ETA in seconds: 1232571.811\n",
      "epoch: 807900, train loss: 3.9004505395889284, val loss: 3.968112015724182, ETA in seconds: 1232079.124\n",
      "epoch: 808000, train loss: 3.897260546684265, val loss: 3.965463900566101, ETA in seconds: 1231587.986\n",
      "epoch: 808100, train loss: 3.8953810691833497, val loss: 3.9865381479263307, ETA in seconds: 1231094.796\n",
      "epoch: 808200, train loss: 3.8894933700561523, val loss: 3.9643002271652223, ETA in seconds: 1230600.155\n",
      "epoch: 808300, train loss: 3.9008963108062744, val loss: 3.975957679748535, ETA in seconds: 1230111.045\n",
      "epoch: 808400, train loss: 3.8971702098846435, val loss: 3.960373950004578, ETA in seconds: 1229618.815\n",
      "epoch: 808500, train loss: 3.886978793144226, val loss: 3.971911644935608, ETA in seconds: 1229125.705\n",
      "epoch: 808600, train loss: 3.8940855979919435, val loss: 3.9665167570114135, ETA in seconds: 1228640.945\n",
      "epoch: 808700, train loss: 3.8969510078430174, val loss: 3.969350481033325, ETA in seconds: 1228155.034\n",
      "epoch: 808800, train loss: 3.8847131729125977, val loss: 3.954374313354492, ETA in seconds: 1227669.083\n",
      "epoch: 808900, train loss: 3.8934577465057374, val loss: 3.963500452041626, ETA in seconds: 1227182.985\n",
      "epoch: 809000, train loss: 3.8939714431762695, val loss: 3.9677582502365114, ETA in seconds: 1226687.860\n",
      "epoch: 809100, train loss: 3.8902286529541015, val loss: 3.964707684516907, ETA in seconds: 1226191.131\n",
      "epoch: 809200, train loss: 3.886401891708374, val loss: 3.9632306337356566, ETA in seconds: 1225698.057\n",
      "epoch: 809300, train loss: 3.900039720535278, val loss: 3.9681111097335817, ETA in seconds: 1225209.008\n",
      "epoch: 809400, train loss: 3.896829891204834, val loss: 3.9735793352127073, ETA in seconds: 1224714.019\n",
      "epoch: 809500, train loss: 3.898184657096863, val loss: 3.977657103538513, ETA in seconds: 1224230.387\n",
      "epoch: 809600, train loss: 3.905239701271057, val loss: 3.97407009601593, ETA in seconds: 1223743.355\n",
      "epoch: 809700, train loss: 3.88894727230072, val loss: 3.9641311407089233, ETA in seconds: 1223250.095\n",
      "epoch: 809800, train loss: 3.8979294300079346, val loss: 3.9733546495437624, ETA in seconds: 1222756.373\n",
      "epoch: 809900, train loss: 3.8938928604125977, val loss: 3.960875701904297, ETA in seconds: 1222257.514\n",
      "epoch: 810000, train loss: 3.8901626110076903, val loss: 3.981637382507324, ETA in seconds: 1221759.645\n",
      "epoch: 810100, train loss: 3.8981037616729735, val loss: 3.969508171081543, ETA in seconds: 1221265.839\n",
      "epoch: 810200, train loss: 3.895344924926758, val loss: 3.964376926422119, ETA in seconds: 1220767.736\n",
      "epoch: 810300, train loss: 3.89423246383667, val loss: 3.96735737323761, ETA in seconds: 1220270.550\n",
      "epoch: 810400, train loss: 3.8979678392410277, val loss: 3.9624478340148928, ETA in seconds: 1219772.636\n",
      "epoch: 810500, train loss: 3.89554705619812, val loss: 3.9684566259384155, ETA in seconds: 1219275.871\n",
      "epoch: 810600, train loss: 3.89751250743866, val loss: 3.972365379333496, ETA in seconds: 1218776.522\n",
      "epoch: 810700, train loss: 3.892636442184448, val loss: 3.965868353843689, ETA in seconds: 1218279.485\n",
      "epoch: 810800, train loss: 3.8999079704284667, val loss: 3.9694848537445067, ETA in seconds: 1217782.229\n",
      "epoch: 810900, train loss: 3.8913708686828614, val loss: 3.9664596557617187, ETA in seconds: 1217289.899\n",
      "epoch: 811000, train loss: 3.8978673934936525, val loss: 3.965359997749329, ETA in seconds: 1216803.741\n",
      "epoch: 811100, train loss: 3.8900845050811768, val loss: 3.984460949897766, ETA in seconds: 1216332.126\n",
      "epoch: 811200, train loss: 3.887594223022461, val loss: 3.970932197570801, ETA in seconds: 1215859.972\n",
      "epoch: 811300, train loss: 3.8922711849212646, val loss: 3.977561044692993, ETA in seconds: 1215387.243\n",
      "epoch: 811400, train loss: 3.893413019180298, val loss: 3.969382381439209, ETA in seconds: 1214914.252\n",
      "epoch: 811500, train loss: 3.8996016502380373, val loss: 3.968243217468262, ETA in seconds: 1214441.246\n",
      "epoch: 811600, train loss: 3.8961058855056763, val loss: 3.9775372743606567, ETA in seconds: 1213969.813\n",
      "epoch: 811700, train loss: 3.899561047554016, val loss: 3.9733893156051634, ETA in seconds: 1213496.603\n",
      "epoch: 811800, train loss: 3.886025309562683, val loss: 3.9699145317077638, ETA in seconds: 1213022.517\n",
      "epoch: 811900, train loss: 3.886255168914795, val loss: 3.953321695327759, ETA in seconds: 1212548.799\n",
      "epoch: 812000, train loss: 3.886485528945923, val loss: 3.9816290378570556, ETA in seconds: 1212074.623\n",
      "epoch: 812100, train loss: 3.8852055311203, val loss: 3.9713582515716555, ETA in seconds: 1211608.408\n",
      "epoch: 812200, train loss: 3.8991586446762083, val loss: 3.967140960693359, ETA in seconds: 1211128.846\n",
      "epoch: 812300, train loss: 3.8974488258361815, val loss: 3.9617315530776978, ETA in seconds: 1210648.342\n",
      "epoch: 812400, train loss: 3.8881946086883543, val loss: 3.9606673955917358, ETA in seconds: 1210155.844\n",
      "epoch: 812500, train loss: 3.893631029129028, val loss: 3.9705994606018065, ETA in seconds: 1209666.057\n",
      "epoch: 812600, train loss: 3.8886521816253663, val loss: 3.9710764408111574, ETA in seconds: 1209172.580\n",
      "epoch: 812700, train loss: 3.8975209474563597, val loss: 3.9749191045761108, ETA in seconds: 1208667.883\n",
      "epoch: 812800, train loss: 3.8910106897354124, val loss: 3.9769269466400146, ETA in seconds: 1208170.838\n",
      "epoch: 812900, train loss: 3.8940060138702393, val loss: 3.970865321159363, ETA in seconds: 1207679.440\n",
      "epoch: 813000, train loss: 3.892788553237915, val loss: 3.9706667184829714, ETA in seconds: 1207179.392\n",
      "epoch: 813100, train loss: 3.890932035446167, val loss: 3.9678685903549193, ETA in seconds: 1206680.812\n",
      "epoch: 813200, train loss: 3.887937903404236, val loss: 3.975998592376709, ETA in seconds: 1206170.096\n",
      "epoch: 813300, train loss: 3.9034634590148927, val loss: 3.961214876174927, ETA in seconds: 1205669.156\n",
      "epoch: 813400, train loss: 3.899147701263428, val loss: 3.976538300514221, ETA in seconds: 1205169.435\n",
      "epoch: 813500, train loss: 3.8976402044296266, val loss: 3.963461661338806, ETA in seconds: 1204679.030\n",
      "epoch: 813600, train loss: 3.8950082063674927, val loss: 3.9777154684066773, ETA in seconds: 1204171.230\n",
      "epoch: 813700, train loss: 3.8926156759262085, val loss: 3.9675673246383667, ETA in seconds: 1203677.776\n",
      "epoch: 813800, train loss: 3.8814181327819823, val loss: 3.970146131515503, ETA in seconds: 1203184.878\n",
      "epoch: 813900, train loss: 3.8917173385620116, val loss: 3.97247633934021, ETA in seconds: 1202683.320\n",
      "epoch: 814000, train loss: 3.9014197826385497, val loss: 3.963957405090332, ETA in seconds: 1202189.738\n",
      "epoch: 814100, train loss: 3.8906580448150634, val loss: 3.9738600969314577, ETA in seconds: 1201686.018\n",
      "epoch: 814200, train loss: 3.893889045715332, val loss: 3.970947742462158, ETA in seconds: 1201200.906\n",
      "epoch: 814300, train loss: 3.893291473388672, val loss: 3.9662031173706054, ETA in seconds: 1200703.371\n",
      "epoch: 814400, train loss: 3.892328119277954, val loss: 3.972270202636719, ETA in seconds: 1200208.859\n",
      "epoch: 814500, train loss: 3.8954071283340452, val loss: 3.968601393699646, ETA in seconds: 1199712.812\n",
      "epoch: 814600, train loss: 3.8922350883483885, val loss: 3.9662436485290526, ETA in seconds: 1199211.528\n",
      "epoch: 814700, train loss: 3.8943953990936278, val loss: 3.983407473564148, ETA in seconds: 1198713.127\n",
      "epoch: 814800, train loss: 3.890362286567688, val loss: 3.973572039604187, ETA in seconds: 1198217.220\n",
      "epoch: 814900, train loss: 3.8980340003967284, val loss: 3.9619186162948608, ETA in seconds: 1197717.867\n",
      "epoch: 815000, train loss: 3.9054363489151003, val loss: 3.9703897714614866, ETA in seconds: 1197216.456\n",
      "epoch: 815100, train loss: 3.8906039714813234, val loss: 3.96414794921875, ETA in seconds: 1196715.073\n",
      "epoch: 815200, train loss: 3.8919763565063477, val loss: 3.9681029081344605, ETA in seconds: 1196224.102\n",
      "epoch: 815300, train loss: 3.897641658782959, val loss: 3.980022692680359, ETA in seconds: 1195744.588\n",
      "epoch: 815400, train loss: 3.886626100540161, val loss: 3.9696370124816895, ETA in seconds: 1195264.382\n",
      "epoch: 815500, train loss: 3.909276819229126, val loss: 3.9727381467819214, ETA in seconds: 1194786.498\n",
      "epoch: 815600, train loss: 3.885643482208252, val loss: 3.962230682373047, ETA in seconds: 1194306.240\n",
      "epoch: 815700, train loss: 3.899689245223999, val loss: 3.9712733745574953, ETA in seconds: 1193825.057\n",
      "epoch: 815800, train loss: 3.888331985473633, val loss: 3.9749981641769407, ETA in seconds: 1193344.350\n",
      "epoch: 815900, train loss: 3.896566939353943, val loss: 3.963535928726196, ETA in seconds: 1192862.841\n",
      "epoch: 816000, train loss: 3.89357476234436, val loss: 3.9590038537979124, ETA in seconds: 1192381.062\n",
      "epoch: 816100, train loss: 3.8909130811691286, val loss: 3.9760856866836547, ETA in seconds: 1191901.150\n",
      "epoch: 816200, train loss: 3.88714394569397, val loss: 3.9701233863830567, ETA in seconds: 1191419.397\n",
      "epoch: 816300, train loss: 3.8921028852462767, val loss: 3.962014579772949, ETA in seconds: 1190937.193\n",
      "epoch: 816400, train loss: 3.892155909538269, val loss: 3.9696011781692504, ETA in seconds: 1190455.212\n",
      "epoch: 816500, train loss: 3.8904478788375854, val loss: 3.967941737174988, ETA in seconds: 1189969.198\n",
      "epoch: 816600, train loss: 3.893309998512268, val loss: 3.961639642715454, ETA in seconds: 1189483.250\n",
      "epoch: 816700, train loss: 3.8953281164169313, val loss: 3.9697752952575684, ETA in seconds: 1188999.156\n",
      "epoch: 816800, train loss: 3.8901095390319824, val loss: 3.962711524963379, ETA in seconds: 1188506.257\n",
      "epoch: 816900, train loss: 3.8972665309906005, val loss: 3.957304000854492, ETA in seconds: 1187998.132\n",
      "epoch: 817000, train loss: 3.891791653633118, val loss: 3.960439682006836, ETA in seconds: 1187486.478\n",
      "epoch: 817100, train loss: 3.893017840385437, val loss: 3.9767155408859254, ETA in seconds: 1186975.760\n",
      "epoch: 817200, train loss: 3.887841272354126, val loss: 3.9607706546783445, ETA in seconds: 1186472.292\n",
      "epoch: 817300, train loss: 3.8864529609680174, val loss: 3.979107165336609, ETA in seconds: 1185985.784\n",
      "epoch: 817400, train loss: 3.9033204078674317, val loss: 3.9808117389678954, ETA in seconds: 1185477.271\n",
      "epoch: 817500, train loss: 3.8956798791885374, val loss: 3.9669684886932375, ETA in seconds: 1184963.505\n",
      "epoch: 817600, train loss: 3.8906431198120117, val loss: 3.9759827852249146, ETA in seconds: 1184455.137\n",
      "epoch: 817700, train loss: 3.9002975463867187, val loss: 3.983082962036133, ETA in seconds: 1183941.046\n",
      "epoch: 817800, train loss: 3.8945882320404053, val loss: 3.973168158531189, ETA in seconds: 1183426.784\n",
      "epoch: 817900, train loss: 3.8908156156539917, val loss: 3.980546545982361, ETA in seconds: 1182921.866\n",
      "epoch: 818000, train loss: 3.8921048641204834, val loss: 3.970956063270569, ETA in seconds: 1182420.110\n",
      "epoch: 818100, train loss: 3.8955410957336425, val loss: 3.97665331363678, ETA in seconds: 1181903.764\n",
      "epoch: 818200, train loss: 3.887531352043152, val loss: 3.980629086494446, ETA in seconds: 1181394.440\n",
      "epoch: 818300, train loss: 3.888469910621643, val loss: 3.9657827615737915, ETA in seconds: 1180888.026\n",
      "epoch: 818400, train loss: 3.9037901878356935, val loss: 3.969840884208679, ETA in seconds: 1180384.883\n",
      "epoch: 818500, train loss: 3.895920753479004, val loss: 3.968702292442322, ETA in seconds: 1179912.510\n",
      "epoch: 818600, train loss: 3.8900205850601197, val loss: 3.9723242998123167, ETA in seconds: 1179397.157\n",
      "epoch: 818700, train loss: 3.8891897439956664, val loss: 3.9678242206573486, ETA in seconds: 1178881.189\n",
      "epoch: 818800, train loss: 3.8865336894989015, val loss: 3.96848521232605, ETA in seconds: 1178367.044\n",
      "epoch: 818900, train loss: 3.893541669845581, val loss: 3.973978328704834, ETA in seconds: 1177857.502\n",
      "epoch: 819000, train loss: 3.8864728450775146, val loss: 3.9698896408081055, ETA in seconds: 1177349.096\n",
      "epoch: 819100, train loss: 3.8972431898117064, val loss: 3.9773762464523315, ETA in seconds: 1176841.074\n",
      "epoch: 819200, train loss: 3.8962525129318237, val loss: 3.9756317853927614, ETA in seconds: 1176331.738\n",
      "epoch: 819300, train loss: 3.895021152496338, val loss: 3.9698373317718505, ETA in seconds: 1175819.242\n",
      "epoch: 819400, train loss: 3.8923534870147707, val loss: 3.9842419147491457, ETA in seconds: 1175336.934\n",
      "epoch: 819500, train loss: 3.888044333457947, val loss: 3.9787938356399537, ETA in seconds: 1174849.936\n",
      "epoch: 819600, train loss: 3.890576457977295, val loss: 3.974810242652893, ETA in seconds: 1174337.546\n",
      "epoch: 819700, train loss: 3.8922147512435914, val loss: 3.9696239709854124, ETA in seconds: 1173835.861\n",
      "epoch: 819800, train loss: 3.8899734735488893, val loss: 3.978615927696228, ETA in seconds: 1173332.397\n",
      "epoch: 819900, train loss: 3.8912065267562865, val loss: 3.974639368057251, ETA in seconds: 1172828.254\n",
      "epoch: 820000, train loss: 3.894122505187988, val loss: 3.966573476791382, ETA in seconds: 1172323.975\n",
      "epoch: 820100, train loss: 3.8989184141159057, val loss: 3.964248013496399, ETA in seconds: 1171811.454\n",
      "epoch: 820200, train loss: 3.890990972518921, val loss: 3.9721950769424437, ETA in seconds: 1171302.072\n",
      "epoch: 820300, train loss: 3.8924612283706663, val loss: 3.9622689485549927, ETA in seconds: 1170793.179\n",
      "epoch: 820400, train loss: 3.881292128562927, val loss: 3.9625460624694826, ETA in seconds: 1170284.181\n",
      "epoch: 820500, train loss: 3.893717908859253, val loss: 3.9680185079574586, ETA in seconds: 1169772.117\n",
      "epoch: 820600, train loss: 3.897161674499512, val loss: 3.969780921936035, ETA in seconds: 1169279.981\n",
      "epoch: 820700, train loss: 3.892848539352417, val loss: 3.977308678627014, ETA in seconds: 1168803.619\n",
      "epoch: 820800, train loss: 3.8891542673110964, val loss: 3.975584101676941, ETA in seconds: 1168313.544\n",
      "epoch: 820900, train loss: 3.8852349281311036, val loss: 3.9732194185256957, ETA in seconds: 1167801.659\n",
      "epoch: 821000, train loss: 3.8926743984222414, val loss: 3.969273900985718, ETA in seconds: 1167287.217\n",
      "epoch: 821100, train loss: 3.888105320930481, val loss: 3.9753036499023438, ETA in seconds: 1166767.747\n",
      "epoch: 821200, train loss: 3.888045597076416, val loss: 3.9777075052261353, ETA in seconds: 1166249.675\n",
      "epoch: 821300, train loss: 3.8885327100753786, val loss: 3.962888550758362, ETA in seconds: 1165730.796\n",
      "epoch: 821400, train loss: 3.8953771352767945, val loss: 3.9608782291412354, ETA in seconds: 1165214.944\n",
      "epoch: 821500, train loss: 3.8916850805282595, val loss: 3.9687615394592286, ETA in seconds: 1164694.971\n",
      "epoch: 821600, train loss: 3.8981156826019285, val loss: 3.9804172039031984, ETA in seconds: 1164174.335\n",
      "epoch: 821700, train loss: 3.8912862062454225, val loss: 3.9692701578140257, ETA in seconds: 1163653.413\n",
      "epoch: 821800, train loss: 3.8954452753067015, val loss: 3.9791156530380247, ETA in seconds: 1163134.538\n",
      "epoch: 821900, train loss: 3.903221678733826, val loss: 3.970802092552185, ETA in seconds: 1162616.584\n",
      "epoch: 822000, train loss: 3.895187568664551, val loss: 3.9771802186965943, ETA in seconds: 1162096.110\n",
      "epoch: 822100, train loss: 3.8945295810699463, val loss: 3.9734983682632445, ETA in seconds: 1161580.758\n",
      "epoch: 822200, train loss: 3.9020687341690063, val loss: 3.9694753646850587, ETA in seconds: 1161058.423\n",
      "epoch: 822300, train loss: 3.8904051542282105, val loss: 3.9811639547348023, ETA in seconds: 1160532.123\n",
      "epoch: 822400, train loss: 3.8893251180648805, val loss: 3.973354530334473, ETA in seconds: 1160007.136\n",
      "epoch: 822500, train loss: 3.903393840789795, val loss: 3.969227123260498, ETA in seconds: 1159484.755\n",
      "epoch: 822600, train loss: 3.8913912057876585, val loss: 3.9685434103012085, ETA in seconds: 1158961.922\n",
      "epoch: 822700, train loss: 3.8965110301971437, val loss: 3.98700213432312, ETA in seconds: 1158441.494\n",
      "epoch: 822800, train loss: 3.891906476020813, val loss: 3.9721935510635378, ETA in seconds: 1157931.995\n",
      "epoch: 822900, train loss: 3.893973684310913, val loss: 3.9753703117370605, ETA in seconds: 1157416.481\n",
      "epoch: 823000, train loss: 3.896895146369934, val loss: 3.9627220392227174, ETA in seconds: 1156909.669\n",
      "epoch: 823100, train loss: 3.892645263671875, val loss: 3.9723873138427734, ETA in seconds: 1156391.923\n",
      "epoch: 823200, train loss: 3.8876714944839477, val loss: 3.969028949737549, ETA in seconds: 1155863.181\n",
      "epoch: 823300, train loss: 3.887968730926514, val loss: 3.972717213630676, ETA in seconds: 1155334.521\n",
      "epoch: 823400, train loss: 3.883148527145386, val loss: 3.970891571044922, ETA in seconds: 1154807.393\n",
      "epoch: 823500, train loss: 3.898077201843262, val loss: 3.975980305671692, ETA in seconds: 1154289.163\n",
      "epoch: 823600, train loss: 3.8974290609359743, val loss: 3.968538522720337, ETA in seconds: 1153772.983\n",
      "epoch: 823700, train loss: 3.8980897426605225, val loss: 3.9781907320022585, ETA in seconds: 1153275.358\n",
      "epoch: 823800, train loss: 3.889860248565674, val loss: 3.970891809463501, ETA in seconds: 1152777.748\n",
      "epoch: 823900, train loss: 3.883488655090332, val loss: 3.9621660709381104, ETA in seconds: 1152282.805\n",
      "epoch: 824000, train loss: 3.88426878452301, val loss: 3.9711753845214846, ETA in seconds: 1151784.579\n",
      "epoch: 824100, train loss: 3.886627125740051, val loss: 3.97871789932251, ETA in seconds: 1151287.347\n",
      "epoch: 824200, train loss: 3.8953778743743896, val loss: 3.9689183712005613, ETA in seconds: 1150789.552\n",
      "epoch: 824300, train loss: 3.895780348777771, val loss: 3.9644619703292845, ETA in seconds: 1150286.428\n",
      "epoch: 824400, train loss: 3.8935300588607786, val loss: 3.9735120296478272, ETA in seconds: 1149754.700\n",
      "epoch: 824500, train loss: 3.901097369194031, val loss: 3.9611430168151855, ETA in seconds: 1149233.594\n",
      "epoch: 824600, train loss: 3.898042893409729, val loss: 3.9818748235702515, ETA in seconds: 1148735.582\n",
      "epoch: 824700, train loss: 3.888278269767761, val loss: 3.979206895828247, ETA in seconds: 1148232.581\n",
      "epoch: 824800, train loss: 3.8852863550186156, val loss: 3.9775396823883056, ETA in seconds: 1147719.016\n",
      "epoch: 824900, train loss: 3.8936946392059326, val loss: 3.9749270915985107, ETA in seconds: 1147193.162\n",
      "epoch: 825000, train loss: 3.8900453805923463, val loss: 3.973586344718933, ETA in seconds: 1146666.907\n",
      "epoch: 825100, train loss: 3.8882998704910277, val loss: 3.9781532764434813, ETA in seconds: 1146142.135\n",
      "epoch: 825200, train loss: 3.9013646125793455, val loss: 3.9729724407196043, ETA in seconds: 1145617.549\n",
      "epoch: 825300, train loss: 3.8979844093322753, val loss: 3.9643980741500853, ETA in seconds: 1145094.026\n",
      "epoch: 825400, train loss: 3.887943077087402, val loss: 3.9687247276306152, ETA in seconds: 1144567.057\n",
      "epoch: 825500, train loss: 3.8973525285720827, val loss: 3.9590997457504273, ETA in seconds: 1144038.236\n",
      "epoch: 825600, train loss: 3.8971235513687135, val loss: 3.9757711410522463, ETA in seconds: 1143505.280\n",
      "epoch: 825700, train loss: 3.883477973937988, val loss: 3.9700644969940186, ETA in seconds: 1142982.502\n",
      "epoch: 825800, train loss: 3.897487688064575, val loss: 3.958781695365906, ETA in seconds: 1142465.922\n",
      "epoch: 825900, train loss: 3.899572253227234, val loss: 3.9638324975967407, ETA in seconds: 1141939.152\n",
      "epoch: 826000, train loss: 3.8909399032592775, val loss: 3.975698709487915, ETA in seconds: 1141408.569\n",
      "epoch: 826100, train loss: 3.8994994163513184, val loss: 3.967953324317932, ETA in seconds: 1140881.683\n",
      "epoch: 826200, train loss: 3.891965889930725, val loss: 3.9661585092544556, ETA in seconds: 1140353.958\n",
      "epoch: 826300, train loss: 3.8918996095657348, val loss: 3.967537045478821, ETA in seconds: 1139831.284\n",
      "epoch: 826400, train loss: 3.8980082750320433, val loss: 3.973178267478943, ETA in seconds: 1139306.911\n",
      "epoch: 826500, train loss: 3.8922653675079344, val loss: 3.975471568107605, ETA in seconds: 1138779.367\n",
      "epoch: 826600, train loss: 3.8977373123168944, val loss: 3.9704391241073607, ETA in seconds: 1138253.347\n",
      "epoch: 826700, train loss: 3.8966054916381836, val loss: 3.975143814086914, ETA in seconds: 1137728.408\n",
      "epoch: 826800, train loss: 3.894294500350952, val loss: 3.9737064123153685, ETA in seconds: 1137200.251\n",
      "epoch: 826900, train loss: 3.889399695396423, val loss: 3.9631408214569093, ETA in seconds: 1136679.171\n",
      "epoch: 827000, train loss: 3.8886170387268066, val loss: 3.9791947841644286, ETA in seconds: 1136148.518\n",
      "epoch: 827100, train loss: 3.8826194286346434, val loss: 3.967420792579651, ETA in seconds: 1135618.206\n",
      "epoch: 827200, train loss: 3.8919721603393556, val loss: 3.9723828077316283, ETA in seconds: 1135087.900\n",
      "epoch: 827300, train loss: 3.898429012298584, val loss: 3.972663927078247, ETA in seconds: 1134561.844\n",
      "epoch: 827400, train loss: 3.8991474866867066, val loss: 3.9658151388168337, ETA in seconds: 1134031.604\n",
      "epoch: 827500, train loss: 3.899485540390015, val loss: 3.9748307704925536, ETA in seconds: 1133507.529\n",
      "epoch: 827600, train loss: 3.897576928138733, val loss: 3.9810178756713865, ETA in seconds: 1132994.203\n",
      "epoch: 827700, train loss: 3.892194724082947, val loss: 3.9699103116989134, ETA in seconds: 1132476.084\n",
      "epoch: 827800, train loss: 3.888390564918518, val loss: 3.973748278617859, ETA in seconds: 1131960.356\n",
      "epoch: 827900, train loss: 3.88845055103302, val loss: 3.9695420980453493, ETA in seconds: 1131447.293\n",
      "epoch: 828000, train loss: 3.8955841302871703, val loss: 3.973563289642334, ETA in seconds: 1130914.368\n",
      "epoch: 828100, train loss: 3.9032432556152346, val loss: 3.979804849624634, ETA in seconds: 1130382.864\n",
      "epoch: 828200, train loss: 3.893820571899414, val loss: 3.975502634048462, ETA in seconds: 1129851.747\n",
      "epoch: 828300, train loss: 3.8977906465530396, val loss: 3.9678124666213987, ETA in seconds: 1129322.175\n",
      "epoch: 828400, train loss: 3.8837519884109497, val loss: 3.978623080253601, ETA in seconds: 1128793.877\n",
      "epoch: 828500, train loss: 3.8822932720184324, val loss: 3.9793387413024903, ETA in seconds: 1128259.264\n",
      "epoch: 828600, train loss: 3.887735366821289, val loss: 3.969469738006592, ETA in seconds: 1127721.735\n",
      "epoch: 828700, train loss: 3.8967195749282837, val loss: 3.9792410135269165, ETA in seconds: 1127184.641\n",
      "epoch: 828800, train loss: 3.8928017139434816, val loss: 3.977261996269226, ETA in seconds: 1126649.900\n",
      "epoch: 828900, train loss: 3.8974704265594484, val loss: 3.9661526918411254, ETA in seconds: 1126118.645\n",
      "epoch: 829000, train loss: 3.891997051239014, val loss: 3.959457182884216, ETA in seconds: 1125590.730\n",
      "epoch: 829100, train loss: 3.8909677982330324, val loss: 3.972383666038513, ETA in seconds: 1125079.979\n",
      "epoch: 829200, train loss: 3.8964251279830933, val loss: 3.9723582029342652, ETA in seconds: 1124564.297\n",
      "epoch: 829300, train loss: 3.8910537242889403, val loss: 3.96568922996521, ETA in seconds: 1124038.374\n",
      "epoch: 829400, train loss: 3.8906052350997924, val loss: 3.977291226387024, ETA in seconds: 1123505.701\n",
      "epoch: 829500, train loss: 3.900683069229126, val loss: 3.9701372623443603, ETA in seconds: 1122971.117\n",
      "epoch: 829600, train loss: 3.8971520900726317, val loss: 3.968721342086792, ETA in seconds: 1122441.477\n",
      "epoch: 829700, train loss: 3.890777087211609, val loss: 3.971334671974182, ETA in seconds: 1121917.534\n",
      "epoch: 829800, train loss: 3.900779294967651, val loss: 3.9771449089050295, ETA in seconds: 1121382.618\n",
      "epoch: 829900, train loss: 3.8973005533218386, val loss: 3.974847912788391, ETA in seconds: 1120850.543\n",
      "epoch: 830000, train loss: 3.899502396583557, val loss: 3.9605724811553955, ETA in seconds: 1120322.334\n",
      "epoch: 830100, train loss: 3.897183561325073, val loss: 3.9708269596099854, ETA in seconds: 1119791.513\n",
      "epoch: 830200, train loss: 3.904103636741638, val loss: 3.9713094234466553, ETA in seconds: 1119259.049\n",
      "epoch: 830300, train loss: 3.8916907787322996, val loss: 3.9778087615966795, ETA in seconds: 1118726.318\n",
      "epoch: 830400, train loss: 3.8960135221481322, val loss: 3.9692713022232056, ETA in seconds: 1118195.085\n",
      "epoch: 830500, train loss: 3.890089273452759, val loss: 3.977331447601318, ETA in seconds: 1117655.729\n",
      "epoch: 830600, train loss: 3.895918846130371, val loss: 3.9775766372680663, ETA in seconds: 1117120.495\n",
      "epoch: 830700, train loss: 3.884383535385132, val loss: 3.9657646656036376, ETA in seconds: 1116587.788\n",
      "epoch: 830800, train loss: 3.8868592977523804, val loss: 3.9584327220916746, ETA in seconds: 1116054.941\n",
      "epoch: 830900, train loss: 3.894754338264465, val loss: 3.9751354217529298, ETA in seconds: 1115522.936\n",
      "epoch: 831000, train loss: 3.8910552978515627, val loss: 3.9727030992507935, ETA in seconds: 1114986.247\n",
      "epoch: 831100, train loss: 3.882414364814758, val loss: 3.970822262763977, ETA in seconds: 1114451.220\n",
      "epoch: 831200, train loss: 3.89368941783905, val loss: 3.9827482223510744, ETA in seconds: 1113920.175\n",
      "epoch: 831300, train loss: 3.8987095832824705, val loss: 3.9719820976257325, ETA in seconds: 1113387.449\n",
      "epoch: 831400, train loss: 3.896504831314087, val loss: 3.973248338699341, ETA in seconds: 1112851.804\n",
      "epoch: 831500, train loss: 3.903868317604065, val loss: 3.967542219161987, ETA in seconds: 1112316.173\n",
      "epoch: 831600, train loss: 3.8988759756088256, val loss: 3.9710410833358765, ETA in seconds: 1111780.914\n",
      "epoch: 831700, train loss: 3.886146807670593, val loss: 3.95700364112854, ETA in seconds: 1111244.997\n",
      "epoch: 831800, train loss: 3.891200065612793, val loss: 3.9715193033218386, ETA in seconds: 1110711.138\n",
      "epoch: 831900, train loss: 3.8901183366775514, val loss: 3.9722101211547853, ETA in seconds: 1110179.135\n",
      "epoch: 832000, train loss: 3.8960993766784666, val loss: 3.987046957015991, ETA in seconds: 1109645.594\n",
      "epoch: 832100, train loss: 3.896571731567383, val loss: 3.970753788948059, ETA in seconds: 1109119.168\n",
      "epoch: 832200, train loss: 3.8938036441802977, val loss: 3.970651865005493, ETA in seconds: 1108587.873\n",
      "epoch: 832300, train loss: 3.8860284805297853, val loss: 3.9648751258850097, ETA in seconds: 1108062.659\n",
      "epoch: 832400, train loss: 3.893190932273865, val loss: 3.9771955490112303, ETA in seconds: 1107534.507\n",
      "epoch: 832500, train loss: 3.8927876710891725, val loss: 3.970875096321106, ETA in seconds: 1107020.901\n",
      "epoch: 832600, train loss: 3.8921523809432985, val loss: 3.9736644983291627, ETA in seconds: 1106510.307\n",
      "epoch: 832700, train loss: 3.889991807937622, val loss: 3.974263334274292, ETA in seconds: 1105990.585\n",
      "epoch: 832800, train loss: 3.8935543060302735, val loss: 3.971629428863525, ETA in seconds: 1105468.884\n",
      "epoch: 832900, train loss: 3.89933922290802, val loss: 3.981590151786804, ETA in seconds: 1104950.069\n",
      "epoch: 833000, train loss: 3.8998475313186645, val loss: 3.9707898139953612, ETA in seconds: 1104449.673\n",
      "epoch: 833100, train loss: 3.8975366353988647, val loss: 3.9732930660247803, ETA in seconds: 1103923.676\n",
      "epoch: 833200, train loss: 3.8870200634002687, val loss: 3.9769806623458863, ETA in seconds: 1103409.689\n",
      "epoch: 833300, train loss: 3.898847484588623, val loss: 3.9791744470596315, ETA in seconds: 1102899.877\n",
      "epoch: 833400, train loss: 3.8885687828063964, val loss: 3.957980155944824, ETA in seconds: 1102389.249\n",
      "epoch: 833500, train loss: 3.887998628616333, val loss: 3.968051052093506, ETA in seconds: 1101867.850\n",
      "epoch: 833600, train loss: 3.8835206031799316, val loss: 3.9764488458633425, ETA in seconds: 1101332.991\n",
      "epoch: 833700, train loss: 3.895537328720093, val loss: 3.9675359964370727, ETA in seconds: 1100800.866\n",
      "epoch: 833800, train loss: 3.898666000366211, val loss: 3.970026993751526, ETA in seconds: 1100281.836\n",
      "epoch: 833900, train loss: 3.8992546081542967, val loss: 3.9683114767074583, ETA in seconds: 1099772.695\n",
      "epoch: 834000, train loss: 3.8984397649765015, val loss: 3.976475954055786, ETA in seconds: 1099268.030\n",
      "epoch: 834100, train loss: 3.8936325550079345, val loss: 3.973996901512146, ETA in seconds: 1098769.656\n",
      "epoch: 834200, train loss: 3.8905022144317627, val loss: 3.9820201635360717, ETA in seconds: 1098268.322\n",
      "epoch: 834300, train loss: 3.8768805027008058, val loss: 3.9762153387069703, ETA in seconds: 1097771.247\n",
      "epoch: 834400, train loss: 3.8966196060180662, val loss: 3.9705992460250856, ETA in seconds: 1097252.160\n",
      "epoch: 834500, train loss: 3.878125047683716, val loss: 3.9799094915390016, ETA in seconds: 1096727.351\n",
      "epoch: 834600, train loss: 3.8947259426116942, val loss: 3.986580967903137, ETA in seconds: 1096181.160\n",
      "epoch: 834700, train loss: 3.8984330177307127, val loss: 3.970472574234009, ETA in seconds: 1095640.999\n",
      "epoch: 834800, train loss: 3.894047713279724, val loss: 3.9604081869125367, ETA in seconds: 1095108.145\n",
      "epoch: 834900, train loss: 3.9010458469390867, val loss: 3.9723382234573363, ETA in seconds: 1094598.862\n",
      "epoch: 835000, train loss: 3.9014070510864256, val loss: 3.982234811782837, ETA in seconds: 1094093.451\n",
      "epoch: 835100, train loss: 3.893996000289917, val loss: 3.9656362533569336, ETA in seconds: 1093585.806\n",
      "epoch: 835200, train loss: 3.893388199806213, val loss: 3.962533164024353, ETA in seconds: 1093070.323\n",
      "epoch: 835300, train loss: 3.8849243402481077, val loss: 3.973292660713196, ETA in seconds: 1092555.510\n",
      "epoch: 835400, train loss: 3.899952793121338, val loss: 3.973680877685547, ETA in seconds: 1092040.280\n",
      "epoch: 835500, train loss: 3.8883673191070556, val loss: 3.9703262090682983, ETA in seconds: 1091525.391\n",
      "epoch: 835600, train loss: 3.895246648788452, val loss: 3.97886266708374, ETA in seconds: 1091015.411\n",
      "epoch: 835700, train loss: 3.89498507976532, val loss: 3.9680498838424683, ETA in seconds: 1090498.764\n",
      "epoch: 835800, train loss: 3.8945969104766847, val loss: 3.97727792263031, ETA in seconds: 1089963.202\n",
      "epoch: 835900, train loss: 3.8998307943344117, val loss: 3.968109655380249, ETA in seconds: 1089426.352\n",
      "epoch: 836000, train loss: 3.8934780597686767, val loss: 3.9625243663787844, ETA in seconds: 1088889.576\n",
      "epoch: 836100, train loss: 3.8961344957351685, val loss: 3.970627999305725, ETA in seconds: 1088350.227\n",
      "epoch: 836200, train loss: 3.890284299850464, val loss: 3.965039920806885, ETA in seconds: 1087817.382\n",
      "epoch: 836300, train loss: 3.891001844406128, val loss: 3.9755513429641725, ETA in seconds: 1087282.661\n",
      "epoch: 836400, train loss: 3.8937642335891725, val loss: 3.9780213594436646, ETA in seconds: 1086747.808\n",
      "epoch: 836500, train loss: 3.895046758651733, val loss: 3.9705585479736327, ETA in seconds: 1086208.646\n",
      "epoch: 836600, train loss: 3.8888558626174925, val loss: 3.963659977912903, ETA in seconds: 1085671.091\n",
      "epoch: 836700, train loss: 3.8992290258407594, val loss: 3.9746632099151613, ETA in seconds: 1085142.152\n",
      "epoch: 836800, train loss: 3.8951995611190795, val loss: 3.9795835256576537, ETA in seconds: 1084630.613\n",
      "epoch: 836900, train loss: 3.8980432987213134, val loss: 3.9755878686904906, ETA in seconds: 1084120.247\n",
      "epoch: 837000, train loss: 3.8851786851882935, val loss: 3.9719404220581054, ETA in seconds: 1083594.575\n",
      "epoch: 837100, train loss: 3.8979122400283814, val loss: 3.9724396228790284, ETA in seconds: 1083065.525\n",
      "epoch: 837200, train loss: 3.897680234909058, val loss: 3.9672749519348143, ETA in seconds: 1082545.760\n",
      "epoch: 837300, train loss: 3.8893304586410524, val loss: 3.9795248985290526, ETA in seconds: 1082025.291\n",
      "epoch: 837400, train loss: 3.893577289581299, val loss: 3.980428862571716, ETA in seconds: 1081510.724\n",
      "epoch: 837500, train loss: 3.895732617378235, val loss: 3.9716869592666626, ETA in seconds: 1080992.707\n",
      "epoch: 837600, train loss: 3.894907999038696, val loss: 3.9722975492477417, ETA in seconds: 1080473.073\n",
      "epoch: 837700, train loss: 3.8969622373580934, val loss: 3.975206184387207, ETA in seconds: 1079953.350\n",
      "epoch: 837800, train loss: 3.887746477127075, val loss: 3.9710859775543215, ETA in seconds: 1079432.873\n",
      "epoch: 837900, train loss: 3.901973080635071, val loss: 3.9657088994979857, ETA in seconds: 1078912.356\n",
      "epoch: 838000, train loss: 3.8950909852981566, val loss: 3.979516053199768, ETA in seconds: 1078380.428\n",
      "epoch: 838100, train loss: 3.8918530225753782, val loss: 3.966917610168457, ETA in seconds: 1077843.568\n",
      "epoch: 838200, train loss: 3.8952419757843018, val loss: 3.9672000408172607, ETA in seconds: 1077325.163\n",
      "epoch: 838300, train loss: 3.8927753925323487, val loss: 3.9697242736816407, ETA in seconds: 1076812.462\n",
      "epoch: 838400, train loss: 3.889061760902405, val loss: 3.9668915271759033, ETA in seconds: 1076265.154\n",
      "epoch: 838500, train loss: 3.894011044502258, val loss: 3.9788084506988524, ETA in seconds: 1075716.572\n",
      "epoch: 838600, train loss: 3.889144945144653, val loss: 3.966805648803711, ETA in seconds: 1075177.973\n",
      "epoch: 838700, train loss: 3.9061721086502077, val loss: 3.9738640785217285, ETA in seconds: 1074643.536\n",
      "epoch: 838800, train loss: 3.8873180150985718, val loss: 3.968918561935425, ETA in seconds: 1074100.704\n",
      "epoch: 838900, train loss: 3.8944586515426636, val loss: 3.964521026611328, ETA in seconds: 1073551.718\n",
      "epoch: 839000, train loss: 3.890303897857666, val loss: 3.970841097831726, ETA in seconds: 1073002.236\n",
      "epoch: 839100, train loss: 3.8907323598861696, val loss: 3.970633125305176, ETA in seconds: 1072450.218\n",
      "epoch: 839200, train loss: 3.9075193881988524, val loss: 3.96642644405365, ETA in seconds: 1071899.757\n",
      "epoch: 839300, train loss: 3.8967481374740602, val loss: 3.969032406806946, ETA in seconds: 1071350.579\n",
      "epoch: 839400, train loss: 3.895188808441162, val loss: 3.9751933574676515, ETA in seconds: 1070799.656\n",
      "epoch: 839500, train loss: 3.886900234222412, val loss: 3.9657909154891966, ETA in seconds: 1070269.503\n",
      "epoch: 839600, train loss: 3.8899924755096436, val loss: 3.9728896856307983, ETA in seconds: 1069720.919\n",
      "epoch: 839700, train loss: 3.901634621620178, val loss: 3.958389472961426, ETA in seconds: 1069171.124\n",
      "epoch: 839800, train loss: 3.8938604593276978, val loss: 3.9699198722839357, ETA in seconds: 1068649.215\n",
      "epoch: 839900, train loss: 3.896550703048706, val loss: 3.959617781639099, ETA in seconds: 1068126.217\n",
      "epoch: 840000, train loss: 3.89515643119812, val loss: 3.963853359222412, ETA in seconds: 1067602.623\n",
      "epoch: 840100, train loss: 3.895799446105957, val loss: 3.9706489562988283, ETA in seconds: 1067080.380\n",
      "epoch: 840200, train loss: 3.886947274208069, val loss: 3.9648736715316772, ETA in seconds: 1066557.125\n",
      "epoch: 840300, train loss: 3.8914435625076296, val loss: 3.9726006746292115, ETA in seconds: 1066032.910\n",
      "epoch: 840400, train loss: 3.8946149826049803, val loss: 3.9531760215759277, ETA in seconds: 1065488.856\n",
      "epoch: 840500, train loss: 3.900265026092529, val loss: 3.9806065797805785, ETA in seconds: 1064968.183\n",
      "epoch: 840600, train loss: 3.8909295558929444, val loss: 3.9720653295516968, ETA in seconds: 1064411.751\n",
      "epoch: 840700, train loss: 3.893460273742676, val loss: 3.9690898180007936, ETA in seconds: 1063859.222\n",
      "epoch: 840800, train loss: 3.8941641807556153, val loss: 3.9692941188812254, ETA in seconds: 1063319.484\n",
      "epoch: 840900, train loss: 3.898035407066345, val loss: 3.97249813079834, ETA in seconds: 1062779.005\n",
      "epoch: 841000, train loss: 3.8866689443588256, val loss: 3.973347043991089, ETA in seconds: 1062239.043\n",
      "epoch: 841100, train loss: 3.900778961181641, val loss: 3.9703676462173463, ETA in seconds: 1061696.465\n",
      "epoch: 841200, train loss: 3.8995478630065916, val loss: 3.976706600189209, ETA in seconds: 1061144.428\n",
      "epoch: 841300, train loss: 3.8928020000457764, val loss: 3.964491772651672, ETA in seconds: 1060594.049\n",
      "epoch: 841400, train loss: 3.89823260307312, val loss: 3.9601551294326782, ETA in seconds: 1060052.241\n",
      "epoch: 841500, train loss: 3.8876708269119264, val loss: 3.973759984970093, ETA in seconds: 1059502.851\n",
      "epoch: 841600, train loss: 3.887710213661194, val loss: 3.964889907836914, ETA in seconds: 1058951.311\n",
      "epoch: 841700, train loss: 3.894919490814209, val loss: 3.961552119255066, ETA in seconds: 1058400.522\n",
      "epoch: 841800, train loss: 3.8934436321258543, val loss: 3.967268943786621, ETA in seconds: 1057850.107\n",
      "epoch: 841900, train loss: 3.8895208835601807, val loss: 3.9640912055969237, ETA in seconds: 1057301.238\n",
      "epoch: 842000, train loss: 3.897193932533264, val loss: 3.978748106956482, ETA in seconds: 1056751.768\n",
      "epoch: 842100, train loss: 3.888123321533203, val loss: 3.9735588312149046, ETA in seconds: 1056203.801\n",
      "epoch: 842200, train loss: 3.8884105920791625, val loss: 3.973294997215271, ETA in seconds: 1055663.817\n",
      "epoch: 842300, train loss: 3.8908596515655516, val loss: 3.968983840942383, ETA in seconds: 1055116.483\n",
      "epoch: 842400, train loss: 3.8960983753204346, val loss: 3.966335892677307, ETA in seconds: 1054563.882\n",
      "epoch: 842500, train loss: 3.8957971811294554, val loss: 3.973594546318054, ETA in seconds: 1054012.383\n",
      "epoch: 842600, train loss: 3.888783311843872, val loss: 3.970561909675598, ETA in seconds: 1053471.064\n",
      "epoch: 842700, train loss: 3.8881298780441282, val loss: 3.9849014043807984, ETA in seconds: 1052928.195\n",
      "epoch: 842800, train loss: 3.8881232738494873, val loss: 3.9699645757675173, ETA in seconds: 1052377.494\n",
      "epoch: 842900, train loss: 3.901264214515686, val loss: 3.9704980134963987, ETA in seconds: 1051828.702\n",
      "epoch: 843000, train loss: 3.885848808288574, val loss: 3.9678721189498902, ETA in seconds: 1051279.691\n",
      "epoch: 843100, train loss: 3.8923782825469972, val loss: 3.974831390380859, ETA in seconds: 1050731.639\n",
      "epoch: 843200, train loss: 3.8967353820800783, val loss: 3.9662530422210693, ETA in seconds: 1050187.994\n",
      "epoch: 843300, train loss: 3.896866798400879, val loss: 3.9720284223556517, ETA in seconds: 1049644.724\n",
      "epoch: 843400, train loss: 3.8999149322509767, val loss: 3.96240394115448, ETA in seconds: 1049101.904\n",
      "epoch: 843500, train loss: 3.9007414102554323, val loss: 3.9654425144195558, ETA in seconds: 1048557.518\n",
      "epoch: 843600, train loss: 3.8908676862716676, val loss: 3.9661669731140137, ETA in seconds: 1048007.864\n",
      "epoch: 843700, train loss: 3.893430233001709, val loss: 3.9652010917663576, ETA in seconds: 1047458.660\n",
      "epoch: 843800, train loss: 3.88923065662384, val loss: 3.968748617172241, ETA in seconds: 1046913.584\n",
      "epoch: 843900, train loss: 3.8961822032928466, val loss: 3.9679142951965334, ETA in seconds: 1046368.086\n",
      "epoch: 844000, train loss: 3.883361554145813, val loss: 3.9640618324279786, ETA in seconds: 1045819.889\n",
      "epoch: 844100, train loss: 3.895815396308899, val loss: 3.9742287397384644, ETA in seconds: 1045273.972\n",
      "epoch: 844200, train loss: 3.892866325378418, val loss: 3.97426974773407, ETA in seconds: 1044748.460\n",
      "epoch: 844300, train loss: 3.890583562850952, val loss: 3.9672213077545164, ETA in seconds: 1044228.147\n",
      "epoch: 844400, train loss: 3.902076244354248, val loss: 3.963415741920471, ETA in seconds: 1043680.073\n",
      "epoch: 844500, train loss: 3.898940181732178, val loss: 3.9743012189865112, ETA in seconds: 1043125.887\n",
      "epoch: 844600, train loss: 3.9033612489700316, val loss: 3.968240761756897, ETA in seconds: 1042572.920\n",
      "epoch: 844700, train loss: 3.8882351160049438, val loss: 3.969821500778198, ETA in seconds: 1042018.996\n",
      "epoch: 844800, train loss: 3.8917629718780518, val loss: 3.9610443353652953, ETA in seconds: 1041462.528\n",
      "epoch: 844900, train loss: 3.8914255619049074, val loss: 3.969769287109375, ETA in seconds: 1040921.938\n",
      "epoch: 845000, train loss: 3.8980220794677733, val loss: 3.9619264602661133, ETA in seconds: 1040361.898\n",
      "epoch: 845100, train loss: 3.8916874885559083, val loss: 3.9731791973114015, ETA in seconds: 1039801.920\n",
      "epoch: 845200, train loss: 3.892963480949402, val loss: 3.9778345346450807, ETA in seconds: 1039241.718\n",
      "epoch: 845300, train loss: 3.8974973917007447, val loss: 3.9672319889068604, ETA in seconds: 1038694.490\n",
      "epoch: 845400, train loss: 3.8893818140029905, val loss: 3.984449100494385, ETA in seconds: 1038146.448\n",
      "epoch: 845500, train loss: 3.8931675434112547, val loss: 3.970734119415283, ETA in seconds: 1037595.267\n",
      "epoch: 845600, train loss: 3.9029500484466553, val loss: 3.965355920791626, ETA in seconds: 1037042.990\n",
      "epoch: 845700, train loss: 3.8964175939559937, val loss: 3.9792540311813354, ETA in seconds: 1036486.268\n",
      "epoch: 845800, train loss: 3.898348641395569, val loss: 3.974668002128601, ETA in seconds: 1035929.151\n",
      "epoch: 845900, train loss: 3.8963423490524294, val loss: 3.9764523267745973, ETA in seconds: 1035372.630\n",
      "epoch: 846000, train loss: 3.8978670835494995, val loss: 3.9681643724441527, ETA in seconds: 1034815.971\n",
      "epoch: 846100, train loss: 3.900018000602722, val loss: 3.965203046798706, ETA in seconds: 1034257.181\n",
      "epoch: 846200, train loss: 3.897271418571472, val loss: 3.9688851118087767, ETA in seconds: 1033694.148\n",
      "epoch: 846300, train loss: 3.8869115114212036, val loss: 3.9546274185180663, ETA in seconds: 1033151.198\n",
      "epoch: 846400, train loss: 3.8993346214294435, val loss: 3.9693495512008665, ETA in seconds: 1032600.687\n",
      "epoch: 846500, train loss: 3.896084952354431, val loss: 3.9743834733963013, ETA in seconds: 1032064.006\n",
      "epoch: 846600, train loss: 3.893600082397461, val loss: 3.9681640625, ETA in seconds: 1031502.908\n",
      "epoch: 846700, train loss: 3.8970772981643678, val loss: 3.961177206039429, ETA in seconds: 1030939.601\n",
      "epoch: 846800, train loss: 3.896202564239502, val loss: 3.965788459777832, ETA in seconds: 1030379.944\n",
      "epoch: 846900, train loss: 3.8844636201858522, val loss: 3.9622164249420164, ETA in seconds: 1029830.337\n",
      "epoch: 847000, train loss: 3.891488552093506, val loss: 3.9615463972091676, ETA in seconds: 1029276.599\n",
      "epoch: 847100, train loss: 3.8938402414321898, val loss: 3.9646435260772703, ETA in seconds: 1028743.414\n",
      "epoch: 847200, train loss: 3.8932391166687013, val loss: 3.973209500312805, ETA in seconds: 1028185.671\n",
      "epoch: 847300, train loss: 3.897353267669678, val loss: 3.9694837093353272, ETA in seconds: 1027635.109\n",
      "epoch: 847400, train loss: 3.887405037879944, val loss: 3.9649839639663695, ETA in seconds: 1027090.364\n",
      "epoch: 847500, train loss: 3.8974411725997924, val loss: 3.9774633407592774, ETA in seconds: 1026548.928\n",
      "epoch: 847600, train loss: 3.89382483959198, val loss: 3.9797597169876098, ETA in seconds: 1026018.494\n",
      "epoch: 847700, train loss: 3.8936764001846313, val loss: 3.973001575469971, ETA in seconds: 1025469.281\n",
      "epoch: 847800, train loss: 3.8947742700576784, val loss: 3.974111533164978, ETA in seconds: 1024914.239\n",
      "epoch: 847900, train loss: 3.8901709079742433, val loss: 3.9702340602874755, ETA in seconds: 1024361.035\n",
      "epoch: 848000, train loss: 3.8905886888504027, val loss: 3.9729268550872803, ETA in seconds: 1023800.328\n",
      "epoch: 848100, train loss: 3.8928524971008303, val loss: 3.9638937950134276, ETA in seconds: 1023238.523\n",
      "epoch: 848200, train loss: 3.8969072103500366, val loss: 3.9725274562835695, ETA in seconds: 1022680.172\n",
      "epoch: 848300, train loss: 3.8923879146575926, val loss: 3.969978952407837, ETA in seconds: 1022121.399\n",
      "epoch: 848400, train loss: 3.8861228466033935, val loss: 3.970646119117737, ETA in seconds: 1021560.178\n",
      "epoch: 848500, train loss: 3.8905383348464966, val loss: 3.9717875957489013, ETA in seconds: 1021008.531\n",
      "epoch: 848600, train loss: 3.8887609958648683, val loss: 3.965484619140625, ETA in seconds: 1020481.021\n",
      "epoch: 848700, train loss: 3.888934302330017, val loss: 3.9858281135559084, ETA in seconds: 1019956.608\n",
      "epoch: 848800, train loss: 3.8924973964691163, val loss: 3.9677417516708373, ETA in seconds: 1019420.178\n",
      "epoch: 848900, train loss: 3.9031374931335447, val loss: 3.9803282260894775, ETA in seconds: 1018874.551\n",
      "epoch: 849000, train loss: 3.8902341604232786, val loss: 3.9774180173873903, ETA in seconds: 1018320.793\n",
      "epoch: 849100, train loss: 3.89068546295166, val loss: 3.9720394611358643, ETA in seconds: 1017764.253\n",
      "epoch: 849200, train loss: 3.8944960117340086, val loss: 3.967149567604065, ETA in seconds: 1017205.388\n",
      "epoch: 849300, train loss: 3.896076130867004, val loss: 3.958657455444336, ETA in seconds: 1016646.529\n",
      "epoch: 849400, train loss: 3.8941845417022707, val loss: 3.9808739185333253, ETA in seconds: 1016091.829\n",
      "epoch: 849500, train loss: 3.8881942510604857, val loss: 3.983594870567322, ETA in seconds: 1015539.482\n",
      "epoch: 849600, train loss: 3.8959852933883665, val loss: 3.9595620155334474, ETA in seconds: 1014999.951\n",
      "epoch: 849700, train loss: 3.8932010412216185, val loss: 3.978415513038635, ETA in seconds: 1014440.972\n",
      "epoch: 849800, train loss: 3.899256730079651, val loss: 3.9671286821365355, ETA in seconds: 1013880.989\n",
      "epoch: 849900, train loss: 3.8976246118545532, val loss: 3.9723907232284548, ETA in seconds: 1013320.975\n",
      "epoch: 850000, train loss: 3.8925638437271117, val loss: 3.9757051944732664, ETA in seconds: 1012763.740\n",
      "epoch: 850100, train loss: 3.892862582206726, val loss: 3.9714927196502687, ETA in seconds: 1012212.057\n",
      "epoch: 850200, train loss: 3.884122848510742, val loss: 3.9626297473907472, ETA in seconds: 1011658.204\n",
      "epoch: 850300, train loss: 3.897663688659668, val loss: 3.97162401676178, ETA in seconds: 1011102.637\n",
      "epoch: 850400, train loss: 3.8991421699523925, val loss: 3.9824002742767335, ETA in seconds: 1010538.787\n",
      "epoch: 850500, train loss: 3.8949743032455446, val loss: 3.9695730686187742, ETA in seconds: 1009976.327\n",
      "epoch: 850600, train loss: 3.8947898626327513, val loss: 3.969423031806946, ETA in seconds: 1009416.551\n",
      "epoch: 850700, train loss: 3.8884999990463256, val loss: 3.9739924907684325, ETA in seconds: 1008858.206\n",
      "epoch: 850800, train loss: 3.895063376426697, val loss: 3.976248097419739, ETA in seconds: 1008298.954\n",
      "epoch: 850900, train loss: 3.8948676586151123, val loss: 3.9773183822631837, ETA in seconds: 1007742.289\n",
      "epoch: 851000, train loss: 3.888942837715149, val loss: 3.9651484727859496, ETA in seconds: 1007184.034\n",
      "epoch: 851100, train loss: 3.89365234375, val loss: 3.9698139429092407, ETA in seconds: 1006644.139\n",
      "epoch: 851200, train loss: 3.8860172033309937, val loss: 3.96783447265625, ETA in seconds: 1006103.095\n",
      "epoch: 851300, train loss: 3.8952424049377443, val loss: 3.970207762718201, ETA in seconds: 1005538.390\n",
      "epoch: 851400, train loss: 3.892977976799011, val loss: 3.9733548879623415, ETA in seconds: 1004974.566\n",
      "epoch: 851500, train loss: 3.896082615852356, val loss: 3.9710355520248415, ETA in seconds: 1004411.438\n",
      "epoch: 851600, train loss: 3.9041815757751466, val loss: 3.9783337116241455, ETA in seconds: 1003854.508\n",
      "epoch: 851700, train loss: 3.8967366456985473, val loss: 3.9748895168304443, ETA in seconds: 1003313.731\n",
      "epoch: 851800, train loss: 3.8946916818618775, val loss: 3.970557355880737, ETA in seconds: 1002767.002\n",
      "epoch: 851900, train loss: 3.8861863136291506, val loss: 3.9741037845611573, ETA in seconds: 1002198.048\n",
      "epoch: 852000, train loss: 3.9001511573791503, val loss: 3.965816926956177, ETA in seconds: 1001633.613\n",
      "epoch: 852100, train loss: 3.886162614822388, val loss: 3.972202491760254, ETA in seconds: 1001080.733\n",
      "epoch: 852200, train loss: 3.8920081615448, val loss: 3.9735962390899657, ETA in seconds: 1000517.754\n",
      "epoch: 852300, train loss: 3.88573899269104, val loss: 3.973240280151367, ETA in seconds: 999953.837\n",
      "epoch: 852400, train loss: 3.888203978538513, val loss: 3.9823794603347777, ETA in seconds: 999395.862\n",
      "epoch: 852500, train loss: 3.8937325954437254, val loss: 3.972452473640442, ETA in seconds: 998824.831\n",
      "epoch: 852600, train loss: 3.889572548866272, val loss: 3.987931418418884, ETA in seconds: 998251.825\n",
      "epoch: 852700, train loss: 3.9013247966766356, val loss: 3.9752236366271974, ETA in seconds: 997682.688\n",
      "epoch: 852800, train loss: 3.890040874481201, val loss: 3.963887882232666, ETA in seconds: 997116.339\n",
      "epoch: 852900, train loss: 3.8823714733123778, val loss: 3.9719743728637695, ETA in seconds: 996549.182\n",
      "epoch: 853000, train loss: 3.8816559076309205, val loss: 3.979439616203308, ETA in seconds: 996002.120\n",
      "epoch: 853100, train loss: 3.8852203130722045, val loss: 3.9764969825744627, ETA in seconds: 995441.146\n",
      "epoch: 853200, train loss: 3.892372465133667, val loss: 3.9756564617156984, ETA in seconds: 994870.272\n",
      "epoch: 853300, train loss: 3.8852476358413695, val loss: 3.987683153152466, ETA in seconds: 994300.487\n",
      "epoch: 853400, train loss: 3.895745849609375, val loss: 3.965883731842041, ETA in seconds: 993738.059\n",
      "epoch: 853500, train loss: 3.893830680847168, val loss: 3.964324617385864, ETA in seconds: 993178.337\n",
      "epoch: 853600, train loss: 3.8894920349121094, val loss: 3.975159788131714, ETA in seconds: 992610.262\n",
      "epoch: 853700, train loss: 3.895168924331665, val loss: 3.9752994060516356, ETA in seconds: 992045.930\n",
      "epoch: 853800, train loss: 3.892329454421997, val loss: 3.9690580129623414, ETA in seconds: 991479.245\n",
      "epoch: 853900, train loss: 3.8850396156311033, val loss: 3.978074312210083, ETA in seconds: 990925.046\n",
      "epoch: 854000, train loss: 3.898523712158203, val loss: 3.972620368003845, ETA in seconds: 990378.882\n",
      "epoch: 854100, train loss: 3.8954907655715942, val loss: 3.9726707458496096, ETA in seconds: 989826.960\n",
      "epoch: 854200, train loss: 3.902947282791138, val loss: 3.9678916931152344, ETA in seconds: 989258.334\n",
      "epoch: 854300, train loss: 3.8886204957962036, val loss: 3.9608553886413573, ETA in seconds: 988694.163\n",
      "epoch: 854400, train loss: 3.89167582988739, val loss: 3.9687583684921264, ETA in seconds: 988128.676\n",
      "epoch: 854500, train loss: 3.8945761919021606, val loss: 3.9705820083618164, ETA in seconds: 987562.032\n",
      "epoch: 854600, train loss: 3.8858233451843263, val loss: 3.97480309009552, ETA in seconds: 987004.063\n",
      "epoch: 854700, train loss: 3.8997994899749755, val loss: 3.9678587675094605, ETA in seconds: 986469.141\n",
      "epoch: 854800, train loss: 3.8963445901870726, val loss: 3.96954083442688, ETA in seconds: 985934.448\n",
      "epoch: 854900, train loss: 3.8884001970291138, val loss: 3.969473433494568, ETA in seconds: 985381.103\n",
      "epoch: 855000, train loss: 3.8866238832473754, val loss: 3.9688878059387207, ETA in seconds: 984824.963\n",
      "epoch: 855100, train loss: 3.8880159854888916, val loss: 3.973871350288391, ETA in seconds: 984288.939\n",
      "epoch: 855200, train loss: 3.8875837326049805, val loss: 3.9752229928970335, ETA in seconds: 983745.621\n",
      "epoch: 855300, train loss: 3.8955392122268675, val loss: 3.97400848865509, ETA in seconds: 983199.903\n",
      "epoch: 855400, train loss: 3.8890498161315916, val loss: 3.96913743019104, ETA in seconds: 982664.170\n",
      "epoch: 855500, train loss: 3.8913251638412474, val loss: 3.975225830078125, ETA in seconds: 982115.515\n",
      "epoch: 855600, train loss: 3.901688551902771, val loss: 3.9719459772109986, ETA in seconds: 981575.370\n",
      "epoch: 855700, train loss: 3.9002092838287354, val loss: 3.9720492362976074, ETA in seconds: 981008.666\n",
      "epoch: 855800, train loss: 3.882682681083679, val loss: 3.963297891616821, ETA in seconds: 980444.700\n",
      "epoch: 855900, train loss: 3.895930314064026, val loss: 3.9790235280990602, ETA in seconds: 979881.305\n",
      "epoch: 856000, train loss: 3.890773057937622, val loss: 3.969377589225769, ETA in seconds: 979313.899\n",
      "epoch: 856100, train loss: 3.8915309429168703, val loss: 3.9681983947753907, ETA in seconds: 978747.533\n",
      "epoch: 856200, train loss: 3.8898560047149657, val loss: 3.9719985246658327, ETA in seconds: 978184.220\n",
      "epoch: 856300, train loss: 3.8997113943099975, val loss: 3.961186408996582, ETA in seconds: 977622.889\n",
      "epoch: 856400, train loss: 3.894953989982605, val loss: 3.9752915620803835, ETA in seconds: 977060.553\n",
      "epoch: 856500, train loss: 3.8992306709289553, val loss: 3.9638706922531126, ETA in seconds: 976496.801\n",
      "epoch: 856600, train loss: 3.888788032531738, val loss: 3.980241894721985, ETA in seconds: 975923.930\n",
      "epoch: 856700, train loss: 3.900461530685425, val loss: 3.9762066841125487, ETA in seconds: 975370.971\n",
      "epoch: 856800, train loss: 3.891525411605835, val loss: 3.98362021446228, ETA in seconds: 974833.514\n",
      "epoch: 856900, train loss: 3.901463484764099, val loss: 3.9798394680023192, ETA in seconds: 974291.500\n",
      "epoch: 857000, train loss: 3.8951701164245605, val loss: 3.9719552755355836, ETA in seconds: 973749.734\n",
      "epoch: 857100, train loss: 3.887825036048889, val loss: 3.9801530838012695, ETA in seconds: 973184.240\n",
      "epoch: 857200, train loss: 3.8971789598464968, val loss: 3.9748470306396486, ETA in seconds: 972620.475\n",
      "epoch: 857300, train loss: 3.8892934322357178, val loss: 3.972077465057373, ETA in seconds: 972056.112\n",
      "epoch: 857400, train loss: 3.8968008518218995, val loss: 3.97924222946167, ETA in seconds: 971501.104\n",
      "epoch: 857500, train loss: 3.899569511413574, val loss: 3.979823446273804, ETA in seconds: 970934.518\n",
      "epoch: 857600, train loss: 3.8869358777999876, val loss: 3.9759443759918214, ETA in seconds: 970369.474\n",
      "epoch: 857700, train loss: 3.8901510715484617, val loss: 3.9884368419647216, ETA in seconds: 969804.445\n",
      "epoch: 857800, train loss: 3.896513843536377, val loss: 3.9681664228439333, ETA in seconds: 969239.658\n",
      "epoch: 857900, train loss: 3.900488758087158, val loss: 3.9792667388916017, ETA in seconds: 968672.469\n",
      "epoch: 858000, train loss: 3.8912661552429197, val loss: 3.97113983631134, ETA in seconds: 968106.062\n",
      "epoch: 858100, train loss: 3.890294051170349, val loss: 3.9812883138656616, ETA in seconds: 967540.248\n",
      "epoch: 858200, train loss: 3.898511552810669, val loss: 3.957797980308533, ETA in seconds: 966960.411\n",
      "epoch: 858300, train loss: 3.889960503578186, val loss: 3.969444441795349, ETA in seconds: 966393.719\n",
      "epoch: 858400, train loss: 3.896318030357361, val loss: 3.9704170703887938, ETA in seconds: 965827.083\n",
      "epoch: 858500, train loss: 3.889399862289429, val loss: 3.9719491481781004, ETA in seconds: 965258.736\n",
      "epoch: 858600, train loss: 3.883854556083679, val loss: 3.972230553627014, ETA in seconds: 964686.021\n",
      "epoch: 858700, train loss: 3.8944881916046143, val loss: 3.9725730180740357, ETA in seconds: 964112.293\n",
      "epoch: 858800, train loss: 3.892203664779663, val loss: 3.972377300262451, ETA in seconds: 963562.837\n",
      "epoch: 858900, train loss: 3.893823432922363, val loss: 3.965031695365906, ETA in seconds: 962988.558\n",
      "epoch: 859000, train loss: 3.8907470703125, val loss: 3.965441370010376, ETA in seconds: 962425.476\n",
      "epoch: 859100, train loss: 3.895006847381592, val loss: 3.9787432670593263, ETA in seconds: 961871.026\n",
      "epoch: 859200, train loss: 3.89461612701416, val loss: 3.958357071876526, ETA in seconds: 961314.732\n",
      "epoch: 859300, train loss: 3.8933237075805662, val loss: 3.9612749338150026, ETA in seconds: 960756.931\n",
      "epoch: 859400, train loss: 3.898394775390625, val loss: 3.977534770965576, ETA in seconds: 960187.849\n",
      "epoch: 859500, train loss: 3.896392488479614, val loss: 3.964989113807678, ETA in seconds: 959616.655\n",
      "epoch: 859600, train loss: 3.8911205291748048, val loss: 3.980471396446228, ETA in seconds: 959045.420\n",
      "epoch: 859700, train loss: 3.887120079994202, val loss: 3.9574329376220705, ETA in seconds: 958475.923\n",
      "epoch: 859800, train loss: 3.8815757989883424, val loss: 3.9637773036956787, ETA in seconds: 957905.291\n",
      "epoch: 859900, train loss: 3.896873950958252, val loss: 3.9839988470077516, ETA in seconds: 957332.422\n",
      "epoch: 860000, train loss: 3.892114043235779, val loss: 3.974728155136108, ETA in seconds: 956755.456\n",
      "epoch: 860100, train loss: 3.8907716989517214, val loss: 3.964812994003296, ETA in seconds: 956179.149\n",
      "epoch: 860200, train loss: 3.8923730134963987, val loss: 3.9736217737197874, ETA in seconds: 955604.149\n",
      "epoch: 860300, train loss: 3.8987492084503175, val loss: 3.9697815418243407, ETA in seconds: 955033.539\n",
      "epoch: 860400, train loss: 3.90331768989563, val loss: 3.9785320520401, ETA in seconds: 954458.065\n",
      "epoch: 860500, train loss: 3.8921626806259155, val loss: 3.9714484453201293, ETA in seconds: 953883.999\n",
      "epoch: 860600, train loss: 3.8887519598007203, val loss: 3.952147197723389, ETA in seconds: 953322.248\n",
      "epoch: 860700, train loss: 3.9053789615631103, val loss: 3.9706414699554444, ETA in seconds: 952740.557\n",
      "epoch: 860800, train loss: 3.889856481552124, val loss: 3.9697789430618284, ETA in seconds: 952183.881\n",
      "epoch: 860900, train loss: 3.896290826797485, val loss: 3.967398929595947, ETA in seconds: 951599.870\n",
      "epoch: 861000, train loss: 3.890652084350586, val loss: 3.9587875843048095, ETA in seconds: 951017.070\n",
      "epoch: 861100, train loss: 3.8917829990386963, val loss: 3.972892236709595, ETA in seconds: 950437.073\n",
      "epoch: 861200, train loss: 3.897465944290161, val loss: 3.963830757141113, ETA in seconds: 949854.529\n",
      "epoch: 861300, train loss: 3.8919052124023437, val loss: 3.9729143619537353, ETA in seconds: 949270.225\n",
      "epoch: 861400, train loss: 3.9002758026123048, val loss: 3.972650909423828, ETA in seconds: 948695.880\n",
      "epoch: 861500, train loss: 3.8967084169387816, val loss: 3.9712239027023317, ETA in seconds: 948115.299\n",
      "epoch: 861600, train loss: 3.8951090812683105, val loss: 3.9706571817398073, ETA in seconds: 947533.118\n",
      "epoch: 861700, train loss: 3.895525050163269, val loss: 3.9719006538391115, ETA in seconds: 946956.341\n",
      "epoch: 861800, train loss: 3.893576645851135, val loss: 3.9639296531677246, ETA in seconds: 946394.635\n",
      "epoch: 861900, train loss: 3.8934279918670653, val loss: 3.976572799682617, ETA in seconds: 945832.339\n",
      "epoch: 862000, train loss: 3.89753053188324, val loss: 3.9718287706375124, ETA in seconds: 945270.320\n",
      "epoch: 862100, train loss: 3.904485249519348, val loss: 3.976963019371033, ETA in seconds: 944709.505\n",
      "epoch: 862200, train loss: 3.8989615201950074, val loss: 3.977627682685852, ETA in seconds: 944146.936\n",
      "epoch: 862300, train loss: 3.8978905200958254, val loss: 3.9765406370162966, ETA in seconds: 943584.953\n",
      "epoch: 862400, train loss: 3.89384183883667, val loss: 3.9710697174072265, ETA in seconds: 943022.787\n",
      "epoch: 862500, train loss: 3.8907726049423217, val loss: 3.964266061782837, ETA in seconds: 942442.448\n",
      "epoch: 862600, train loss: 3.895584988594055, val loss: 3.9669549465179443, ETA in seconds: 941858.921\n",
      "epoch: 862700, train loss: 3.895673370361328, val loss: 3.9666421175003053, ETA in seconds: 941280.786\n",
      "epoch: 862800, train loss: 3.890667986869812, val loss: 3.970825672149658, ETA in seconds: 940705.488\n",
      "epoch: 862900, train loss: 3.891923213005066, val loss: 3.9636823177337646, ETA in seconds: 940124.829\n",
      "epoch: 863000, train loss: 3.892876076698303, val loss: 3.967570924758911, ETA in seconds: 939543.811\n",
      "epoch: 863100, train loss: 3.8868160009384156, val loss: 3.976162314414978, ETA in seconds: 938963.633\n",
      "epoch: 863200, train loss: 3.8937098026275634, val loss: 3.970786118507385, ETA in seconds: 938402.345\n",
      "epoch: 863300, train loss: 3.898266887664795, val loss: 3.9780807733535766, ETA in seconds: 937826.004\n",
      "epoch: 863400, train loss: 3.896388554573059, val loss: 3.967901182174683, ETA in seconds: 937256.692\n",
      "epoch: 863500, train loss: 3.8976109981536866, val loss: 3.9630473852157593, ETA in seconds: 936683.248\n",
      "epoch: 863600, train loss: 3.894452428817749, val loss: 3.9619271993637084, ETA in seconds: 936109.135\n",
      "epoch: 863700, train loss: 3.88686466217041, val loss: 3.9820894479751585, ETA in seconds: 935532.540\n",
      "epoch: 863800, train loss: 3.9014626502990724, val loss: 3.9739423036575316, ETA in seconds: 934956.842\n",
      "epoch: 863900, train loss: 3.890670919418335, val loss: 3.969906544685364, ETA in seconds: 934378.644\n",
      "epoch: 864000, train loss: 3.898403024673462, val loss: 3.9736716747283936, ETA in seconds: 933808.598\n",
      "epoch: 864100, train loss: 3.8964894294738768, val loss: 3.981750249862671, ETA in seconds: 933244.191\n",
      "epoch: 864200, train loss: 3.893946123123169, val loss: 3.9742079257965086, ETA in seconds: 932662.758\n",
      "epoch: 864300, train loss: 3.888081502914429, val loss: 3.965768074989319, ETA in seconds: 932084.371\n",
      "epoch: 864400, train loss: 3.9044915676116942, val loss: 3.972757649421692, ETA in seconds: 931507.549\n",
      "epoch: 864500, train loss: 3.897481608390808, val loss: 3.9685998916625977, ETA in seconds: 930930.198\n",
      "epoch: 864600, train loss: 3.8940067052841187, val loss: 3.9714019298553467, ETA in seconds: 930352.051\n",
      "epoch: 864700, train loss: 3.895801568031311, val loss: 3.9698967933654785, ETA in seconds: 929771.154\n",
      "epoch: 864800, train loss: 3.897814702987671, val loss: 3.9833614110946653, ETA in seconds: 929183.728\n",
      "epoch: 864900, train loss: 3.890747547149658, val loss: 3.9631678819656373, ETA in seconds: 928603.798\n",
      "epoch: 865000, train loss: 3.8975073099136353, val loss: 3.9713721752166746, ETA in seconds: 928022.531\n",
      "epoch: 865100, train loss: 3.8933735609054567, val loss: 3.9587592840194703, ETA in seconds: 927434.372\n",
      "epoch: 865200, train loss: 3.89903666973114, val loss: 3.970166277885437, ETA in seconds: 926844.665\n",
      "epoch: 865300, train loss: 3.896925759315491, val loss: 3.966731905937195, ETA in seconds: 926254.520\n",
      "epoch: 865400, train loss: 3.8925365447998046, val loss: 3.9698910474777223, ETA in seconds: 925667.707\n",
      "epoch: 865500, train loss: 3.8945085287094114, val loss: 3.961362862586975, ETA in seconds: 925076.071\n",
      "epoch: 865600, train loss: 3.8912387132644652, val loss: 3.9628819704055784, ETA in seconds: 924486.228\n",
      "epoch: 865700, train loss: 3.890062117576599, val loss: 3.970043659210205, ETA in seconds: 923895.517\n",
      "epoch: 865800, train loss: 3.8979429960250855, val loss: 3.972331690788269, ETA in seconds: 923304.393\n",
      "epoch: 865900, train loss: 3.889164447784424, val loss: 3.9563981533050536, ETA in seconds: 922716.646\n",
      "epoch: 866000, train loss: 3.8939387798309326, val loss: 3.9715729475021364, ETA in seconds: 922149.418\n",
      "epoch: 866100, train loss: 3.8880312919616697, val loss: 3.9896169185638426, ETA in seconds: 921561.924\n",
      "epoch: 866200, train loss: 3.893846559524536, val loss: 3.9747764110565185, ETA in seconds: 920973.040\n",
      "epoch: 866300, train loss: 3.8878386974334718, val loss: 3.9690099716186524, ETA in seconds: 920383.691\n",
      "epoch: 866400, train loss: 3.8936053037643434, val loss: 3.9712278127670286, ETA in seconds: 919795.763\n",
      "epoch: 866500, train loss: 3.8799153327941895, val loss: 3.97042031288147, ETA in seconds: 919203.953\n",
      "epoch: 866600, train loss: 3.8915952920913695, val loss: 3.9672171831130982, ETA in seconds: 918613.573\n",
      "epoch: 866700, train loss: 3.891609263420105, val loss: 3.9699510097503663, ETA in seconds: 918023.514\n",
      "epoch: 866800, train loss: 3.8934195041656494, val loss: 3.9741952419281006, ETA in seconds: 917435.522\n",
      "epoch: 866900, train loss: 3.8912333965301515, val loss: 3.979731297492981, ETA in seconds: 916844.165\n",
      "epoch: 867000, train loss: 3.892049956321716, val loss: 3.975786328315735, ETA in seconds: 916253.026\n",
      "epoch: 867100, train loss: 3.8945982456207275, val loss: 3.988426113128662, ETA in seconds: 915665.456\n",
      "epoch: 867200, train loss: 3.8980813026428223, val loss: 3.98052237033844, ETA in seconds: 915071.123\n",
      "epoch: 867300, train loss: 3.8912955284118653, val loss: 3.9760578155517576, ETA in seconds: 914478.994\n",
      "epoch: 867400, train loss: 3.890635132789612, val loss: 3.9760552883148192, ETA in seconds: 913884.539\n",
      "epoch: 867500, train loss: 3.892882537841797, val loss: 3.974843668937683, ETA in seconds: 913289.339\n",
      "epoch: 867600, train loss: 3.8915621995925904, val loss: 3.97685706615448, ETA in seconds: 912697.887\n",
      "epoch: 867700, train loss: 3.9013886213302613, val loss: 3.971874475479126, ETA in seconds: 912110.748\n",
      "epoch: 867800, train loss: 3.8888315916061402, val loss: 3.9711974620819093, ETA in seconds: 911522.909\n",
      "epoch: 867900, train loss: 3.891678071022034, val loss: 3.9654523611068724, ETA in seconds: 910936.607\n",
      "epoch: 868000, train loss: 3.8909793138504027, val loss: 3.982415866851807, ETA in seconds: 910341.280\n",
      "epoch: 868100, train loss: 3.896987557411194, val loss: 3.9765937089920045, ETA in seconds: 909747.297\n",
      "epoch: 868200, train loss: 3.8905949115753176, val loss: 3.9816590309143067, ETA in seconds: 909156.341\n",
      "epoch: 868300, train loss: 3.8965330123901367, val loss: 3.9782251834869387, ETA in seconds: 908569.663\n",
      "epoch: 868400, train loss: 3.9007453680038453, val loss: 3.969138479232788, ETA in seconds: 908000.827\n",
      "epoch: 868500, train loss: 3.8993523836135866, val loss: 3.9793792963027954, ETA in seconds: 907406.093\n",
      "epoch: 868600, train loss: 3.8864010095596315, val loss: 3.9658825635910033, ETA in seconds: 906813.084\n",
      "epoch: 868700, train loss: 3.8823773145675657, val loss: 3.9829014778137206, ETA in seconds: 906219.023\n",
      "epoch: 868800, train loss: 3.8875999212265016, val loss: 3.9802268505096436, ETA in seconds: 905622.856\n",
      "epoch: 868900, train loss: 3.8940345525741575, val loss: 3.9864946365356446, ETA in seconds: 905025.899\n",
      "epoch: 869000, train loss: 3.8982863426208496, val loss: 3.9731013059616087, ETA in seconds: 904433.318\n",
      "epoch: 869100, train loss: 3.894532585144043, val loss: 3.967536759376526, ETA in seconds: 903840.774\n",
      "epoch: 869200, train loss: 3.8963159561157226, val loss: 3.9677658081054688, ETA in seconds: 903246.899\n",
      "epoch: 869300, train loss: 3.888468074798584, val loss: 3.959322118759155, ETA in seconds: 902651.904\n",
      "epoch: 869400, train loss: 3.8981727361679077, val loss: 3.9662622213363647, ETA in seconds: 902057.327\n",
      "epoch: 869500, train loss: 3.8927227020263673, val loss: 3.9732162237167357, ETA in seconds: 901462.250\n",
      "epoch: 869600, train loss: 3.883083438873291, val loss: 3.9817131757736206, ETA in seconds: 900872.166\n",
      "epoch: 869700, train loss: 3.890562891960144, val loss: 3.9712471961975098, ETA in seconds: 900274.821\n",
      "epoch: 869800, train loss: 3.8924141407012938, val loss: 3.9712923765182495, ETA in seconds: 899676.585\n",
      "epoch: 869900, train loss: 3.8985063791275025, val loss: 3.978812098503113, ETA in seconds: 899079.700\n",
      "epoch: 870000, train loss: 3.8821083307266235, val loss: 3.979701066017151, ETA in seconds: 898495.563\n",
      "epoch: 870100, train loss: 3.894474673271179, val loss: 3.965231466293335, ETA in seconds: 897895.294\n",
      "epoch: 870200, train loss: 3.8827287197113036, val loss: 3.9694392919540404, ETA in seconds: 897297.563\n",
      "epoch: 870300, train loss: 3.8973259210586546, val loss: 3.9661168098449706, ETA in seconds: 896697.556\n",
      "epoch: 870400, train loss: 3.895366883277893, val loss: 3.973301720619202, ETA in seconds: 896101.171\n",
      "epoch: 870500, train loss: 3.89032301902771, val loss: 3.9672752141952516, ETA in seconds: 895503.176\n",
      "epoch: 870600, train loss: 3.8952225923538206, val loss: 3.967887544631958, ETA in seconds: 894903.333\n",
      "epoch: 870700, train loss: 3.8898181915283203, val loss: 3.968543028831482, ETA in seconds: 894306.247\n",
      "epoch: 870800, train loss: 3.888175535202026, val loss: 3.9794737815856935, ETA in seconds: 893710.642\n",
      "epoch: 870900, train loss: 3.89059841632843, val loss: 3.9611210346221926, ETA in seconds: 893120.902\n",
      "epoch: 871000, train loss: 3.894553208351135, val loss: 3.9529333114624023, ETA in seconds: 892537.467\n",
      "epoch: 871100, train loss: 3.891546940803528, val loss: 3.975949788093567, ETA in seconds: 891960.581\n",
      "epoch: 871200, train loss: 3.893159103393555, val loss: 3.965851974487305, ETA in seconds: 891364.737\n",
      "epoch: 871300, train loss: 3.893540382385254, val loss: 3.973406171798706, ETA in seconds: 890779.385\n",
      "epoch: 871400, train loss: 3.897811841964722, val loss: 3.970173382759094, ETA in seconds: 890211.864\n",
      "epoch: 871500, train loss: 3.8890492916107178, val loss: 3.9676616907119753, ETA in seconds: 889626.419\n",
      "epoch: 871600, train loss: 3.8942421674728394, val loss: 3.9757797718048096, ETA in seconds: 889050.063\n",
      "epoch: 871700, train loss: 3.8903055667877195, val loss: 3.965854215621948, ETA in seconds: 888474.106\n",
      "epoch: 871800, train loss: 3.894665169715881, val loss: 3.9851670026779176, ETA in seconds: 887895.889\n",
      "epoch: 871900, train loss: 3.8951791763305663, val loss: 3.971509337425232, ETA in seconds: 887317.690\n",
      "epoch: 872000, train loss: 3.8906636238098145, val loss: 3.976265215873718, ETA in seconds: 886725.599\n",
      "epoch: 872100, train loss: 3.896975541114807, val loss: 3.975434589385986, ETA in seconds: 886123.134\n",
      "epoch: 872200, train loss: 3.895073580741882, val loss: 3.9829511404037476, ETA in seconds: 885523.428\n",
      "epoch: 872300, train loss: 3.8917285442352294, val loss: 3.977973699569702, ETA in seconds: 884944.244\n",
      "epoch: 872400, train loss: 3.889491057395935, val loss: 3.9683979749679565, ETA in seconds: 884365.854\n",
      "epoch: 872500, train loss: 3.890689253807068, val loss: 3.97455575466156, ETA in seconds: 883779.711\n",
      "epoch: 872600, train loss: 3.8954447507858276, val loss: 3.979262638092041, ETA in seconds: 883180.440\n",
      "epoch: 872700, train loss: 3.895734715461731, val loss: 3.970005178451538, ETA in seconds: 882578.690\n",
      "epoch: 872800, train loss: 3.8904932498931886, val loss: 3.9789981126785277, ETA in seconds: 881979.177\n",
      "epoch: 872900, train loss: 3.8930583000183105, val loss: 3.9780833244323732, ETA in seconds: 881381.224\n",
      "epoch: 873000, train loss: 3.895697999000549, val loss: 3.9711206197738647, ETA in seconds: 880779.339\n",
      "epoch: 873100, train loss: 3.894330072402954, val loss: 3.9772223472595214, ETA in seconds: 880180.264\n",
      "epoch: 873200, train loss: 3.8884502410888673, val loss: 3.9729483127593994, ETA in seconds: 879580.724\n",
      "epoch: 873300, train loss: 3.8919899702072143, val loss: 3.9684121131896974, ETA in seconds: 878988.935\n",
      "epoch: 873400, train loss: 3.892828178405762, val loss: 3.981001877784729, ETA in seconds: 878396.764\n",
      "epoch: 873500, train loss: 3.8933480978012085, val loss: 3.9778629779815673, ETA in seconds: 877804.337\n",
      "epoch: 873600, train loss: 3.887843155860901, val loss: 3.9770635604858398, ETA in seconds: 877211.408\n",
      "epoch: 873700, train loss: 3.896374797821045, val loss: 3.983950066566467, ETA in seconds: 876611.828\n",
      "epoch: 873800, train loss: 3.896825575828552, val loss: 3.974938654899597, ETA in seconds: 876016.928\n",
      "epoch: 873900, train loss: 3.88538920879364, val loss: 3.9728641748428344, ETA in seconds: 875419.136\n",
      "epoch: 874000, train loss: 3.8896509170532227, val loss: 3.967393898963928, ETA in seconds: 874830.179\n",
      "epoch: 874100, train loss: 3.8912400007247925, val loss: 3.968070387840271, ETA in seconds: 874250.499\n",
      "epoch: 874200, train loss: 3.896732449531555, val loss: 3.9695742607116697, ETA in seconds: 873657.227\n",
      "epoch: 874300, train loss: 3.890256905555725, val loss: 3.973795032501221, ETA in seconds: 873073.523\n",
      "epoch: 874400, train loss: 3.906325435638428, val loss: 3.9650466203689576, ETA in seconds: 872499.813\n",
      "epoch: 874500, train loss: 3.891550350189209, val loss: 3.9732328414916993, ETA in seconds: 871925.126\n",
      "epoch: 874600, train loss: 3.891765570640564, val loss: 3.9745867967605593, ETA in seconds: 871351.080\n",
      "epoch: 874700, train loss: 3.889749050140381, val loss: 3.9751429557800293, ETA in seconds: 870776.193\n",
      "epoch: 874800, train loss: 3.892240047454834, val loss: 3.9704525470733643, ETA in seconds: 870187.797\n",
      "epoch: 874900, train loss: 3.8935696840286256, val loss: 3.985108733177185, ETA in seconds: 869591.090\n",
      "epoch: 875000, train loss: 3.8959728956222532, val loss: 3.976488542556763, ETA in seconds: 868993.208\n",
      "epoch: 875100, train loss: 3.8936784267425537, val loss: 3.972502112388611, ETA in seconds: 868391.173\n",
      "epoch: 875200, train loss: 3.901544785499573, val loss: 3.970584273338318, ETA in seconds: 867789.403\n",
      "epoch: 875300, train loss: 3.894548010826111, val loss: 3.9727442979812624, ETA in seconds: 867186.413\n",
      "epoch: 875400, train loss: 3.898935556411743, val loss: 3.9933612823486326, ETA in seconds: 866588.739\n",
      "epoch: 875500, train loss: 3.894474816322327, val loss: 3.975400948524475, ETA in seconds: 865991.656\n",
      "epoch: 875600, train loss: 3.901727223396301, val loss: 3.9749228715896607, ETA in seconds: 865407.525\n",
      "epoch: 875700, train loss: 3.898397350311279, val loss: 3.974887418746948, ETA in seconds: 864824.130\n",
      "epoch: 875800, train loss: 3.89934356212616, val loss: 3.9695068359375, ETA in seconds: 864241.900\n",
      "epoch: 875900, train loss: 3.8941293478012087, val loss: 3.9679033041000364, ETA in seconds: 863658.428\n",
      "epoch: 876000, train loss: 3.8907541990280152, val loss: 3.976430296897888, ETA in seconds: 863073.304\n",
      "epoch: 876100, train loss: 3.893165183067322, val loss: 3.971956491470337, ETA in seconds: 862488.303\n",
      "epoch: 876200, train loss: 3.892220139503479, val loss: 3.9737660884857178, ETA in seconds: 861902.146\n",
      "epoch: 876300, train loss: 3.8996607780456545, val loss: 3.963124895095825, ETA in seconds: 861316.675\n",
      "epoch: 876400, train loss: 3.9020771265029905, val loss: 3.9751224517822266, ETA in seconds: 860731.002\n",
      "epoch: 876500, train loss: 3.896779441833496, val loss: 3.9696704626083372, ETA in seconds: 860145.782\n",
      "epoch: 876600, train loss: 3.900428318977356, val loss: 3.977030873298645, ETA in seconds: 859537.949\n",
      "epoch: 876700, train loss: 3.886240315437317, val loss: 3.974822497367859, ETA in seconds: 858931.654\n",
      "epoch: 876800, train loss: 3.8848824501037598, val loss: 3.9723478078842165, ETA in seconds: 858328.809\n",
      "epoch: 876900, train loss: 3.9012487888336183, val loss: 3.964398813247681, ETA in seconds: 857732.563\n",
      "epoch: 877000, train loss: 3.892803740501404, val loss: 3.9586543798446656, ETA in seconds: 857134.374\n",
      "epoch: 877100, train loss: 3.899479603767395, val loss: 3.9600873708724977, ETA in seconds: 856537.101\n",
      "epoch: 877200, train loss: 3.889546751976013, val loss: 3.972988820075989, ETA in seconds: 855933.678\n",
      "epoch: 877300, train loss: 3.8939787626266478, val loss: 3.9577260971069337, ETA in seconds: 855333.816\n",
      "epoch: 877400, train loss: 3.8917651176452637, val loss: 3.9664357185363768, ETA in seconds: 854736.254\n",
      "epoch: 877500, train loss: 3.8948989868164063, val loss: 3.9710341930389403, ETA in seconds: 854137.460\n",
      "epoch: 877600, train loss: 3.8892240285873414, val loss: 3.9707702875137327, ETA in seconds: 853540.829\n",
      "epoch: 877700, train loss: 3.899311399459839, val loss: 3.981032657623291, ETA in seconds: 852942.899\n",
      "epoch: 877800, train loss: 3.896772933006287, val loss: 3.9731064558029177, ETA in seconds: 852338.759\n",
      "epoch: 877900, train loss: 3.886545944213867, val loss: 3.960836672782898, ETA in seconds: 851734.114\n",
      "epoch: 878000, train loss: 3.8921502113342283, val loss: 3.977966809272766, ETA in seconds: 851129.937\n",
      "epoch: 878100, train loss: 3.8932111740112303, val loss: 3.9681877851486207, ETA in seconds: 850518.882\n",
      "epoch: 878200, train loss: 3.8972174644470217, val loss: 3.9701224088668825, ETA in seconds: 849911.379\n",
      "epoch: 878300, train loss: 3.886244702339172, val loss: 3.97200870513916, ETA in seconds: 849325.100\n",
      "epoch: 878400, train loss: 3.899754762649536, val loss: 3.967728924751282, ETA in seconds: 848737.660\n",
      "epoch: 878500, train loss: 3.8908291816711427, val loss: 3.9647102117538453, ETA in seconds: 848152.149\n",
      "epoch: 878600, train loss: 3.8927114486694334, val loss: 3.974343466758728, ETA in seconds: 847540.083\n",
      "epoch: 878700, train loss: 3.893312096595764, val loss: 3.978047752380371, ETA in seconds: 846930.881\n",
      "epoch: 878800, train loss: 3.8897324085235594, val loss: 3.9695107698440553, ETA in seconds: 846332.669\n",
      "epoch: 878900, train loss: 3.890792655944824, val loss: 3.964828133583069, ETA in seconds: 845741.509\n",
      "epoch: 879000, train loss: 3.8944639444351195, val loss: 3.9668511867523195, ETA in seconds: 845131.245\n",
      "epoch: 879100, train loss: 3.891848397254944, val loss: 3.983307933807373, ETA in seconds: 844524.243\n",
      "epoch: 879200, train loss: 3.8909122943878174, val loss: 3.9672592163085936, ETA in seconds: 843920.271\n",
      "epoch: 879300, train loss: 3.8840410709381104, val loss: 3.9509483337402345, ETA in seconds: 843313.712\n",
      "epoch: 879400, train loss: 3.883760619163513, val loss: 3.967396068572998, ETA in seconds: 842712.997\n",
      "epoch: 879500, train loss: 3.8908345460891725, val loss: 3.965265703201294, ETA in seconds: 842112.169\n",
      "epoch: 879600, train loss: 3.888678860664368, val loss: 3.9848854303359986, ETA in seconds: 841499.895\n",
      "epoch: 879700, train loss: 3.895726442337036, val loss: 3.9634014844894407, ETA in seconds: 840888.418\n",
      "epoch: 879800, train loss: 3.8997501850128176, val loss: 3.970622515678406, ETA in seconds: 840276.574\n",
      "epoch: 879900, train loss: 3.9073964595794677, val loss: 3.971748876571655, ETA in seconds: 839665.457\n",
      "epoch: 880000, train loss: 3.8937331199645997, val loss: 3.971670722961426, ETA in seconds: 839053.311\n",
      "epoch: 880100, train loss: 3.8942978382110596, val loss: 3.972391438484192, ETA in seconds: 838442.735\n",
      "epoch: 880200, train loss: 3.8901538133621214, val loss: 3.9806697607040404, ETA in seconds: 837837.258\n",
      "epoch: 880300, train loss: 3.896802544593811, val loss: 3.9621381998062133, ETA in seconds: 837253.115\n",
      "epoch: 880400, train loss: 3.88955180644989, val loss: 3.9740952253341675, ETA in seconds: 836660.880\n",
      "epoch: 880500, train loss: 3.8960909605026246, val loss: 3.9697001695632936, ETA in seconds: 836068.632\n",
      "epoch: 880600, train loss: 3.8923826932907106, val loss: 3.973841071128845, ETA in seconds: 835476.804\n",
      "epoch: 880700, train loss: 3.891352128982544, val loss: 3.9499325275421144, ETA in seconds: 834884.131\n",
      "epoch: 880800, train loss: 3.8916038513183593, val loss: 3.967282605171204, ETA in seconds: 834282.725\n",
      "epoch: 880900, train loss: 3.896897387504578, val loss: 3.9778677463531493, ETA in seconds: 833685.649\n",
      "epoch: 881000, train loss: 3.8911617279052733, val loss: 3.9595504999160767, ETA in seconds: 833082.670\n",
      "epoch: 881100, train loss: 3.8956536531448362, val loss: 3.968074369430542, ETA in seconds: 832493.262\n",
      "epoch: 881200, train loss: 3.8928884744644163, val loss: 3.96598162651062, ETA in seconds: 831893.840\n",
      "epoch: 881300, train loss: 3.887051486968994, val loss: 3.970546269416809, ETA in seconds: 831279.535\n",
      "epoch: 881400, train loss: 3.8952455043792726, val loss: 3.9633304119110107, ETA in seconds: 830666.343\n",
      "epoch: 881500, train loss: 3.896315836906433, val loss: 3.969630718231201, ETA in seconds: 830054.187\n",
      "epoch: 881600, train loss: 3.897852921485901, val loss: 3.972261834144592, ETA in seconds: 829444.765\n",
      "epoch: 881700, train loss: 3.8932964324951174, val loss: 3.9688889741897584, ETA in seconds: 828837.512\n",
      "epoch: 881800, train loss: 3.890967321395874, val loss: 3.980085587501526, ETA in seconds: 828231.913\n",
      "epoch: 881900, train loss: 3.8993438482284546, val loss: 3.9630930423736572, ETA in seconds: 827627.299\n",
      "epoch: 882000, train loss: 3.898035979270935, val loss: 3.966879367828369, ETA in seconds: 827022.552\n",
      "epoch: 882100, train loss: 3.9044333696365356, val loss: 3.9698069095611572, ETA in seconds: 826413.646\n",
      "epoch: 882200, train loss: 3.8956225872039796, val loss: 3.9738529205322264, ETA in seconds: 825804.345\n",
      "epoch: 882300, train loss: 3.886198306083679, val loss: 3.9610089540481566, ETA in seconds: 825194.317\n",
      "epoch: 882400, train loss: 3.89514696598053, val loss: 3.9767268657684327, ETA in seconds: 824587.536\n",
      "epoch: 882500, train loss: 3.891547703742981, val loss: 3.974939155578613, ETA in seconds: 823976.943\n",
      "epoch: 882600, train loss: 3.893813633918762, val loss: 3.968806028366089, ETA in seconds: 823364.742\n",
      "epoch: 882700, train loss: 3.88877911567688, val loss: 3.969931697845459, ETA in seconds: 822765.711\n",
      "epoch: 882800, train loss: 3.8965099573135378, val loss: 3.9822365999221803, ETA in seconds: 822170.059\n",
      "epoch: 882900, train loss: 3.886713409423828, val loss: 3.9709168910980224, ETA in seconds: 821576.947\n",
      "epoch: 883000, train loss: 3.8926234245300293, val loss: 3.981959915161133, ETA in seconds: 820963.340\n",
      "epoch: 883100, train loss: 3.889748787879944, val loss: 3.9655051708221434, ETA in seconds: 820348.427\n",
      "epoch: 883200, train loss: 3.8947285413742065, val loss: 3.9790610551834105, ETA in seconds: 819732.613\n",
      "epoch: 883300, train loss: 3.887442398071289, val loss: 3.9695884466171263, ETA in seconds: 819116.804\n",
      "epoch: 883400, train loss: 3.890182375907898, val loss: 3.9579330921173095, ETA in seconds: 818501.288\n",
      "epoch: 883500, train loss: 3.8860549926757812, val loss: 3.97140257358551, ETA in seconds: 817887.851\n",
      "epoch: 883600, train loss: 3.8887521028518677, val loss: 3.9744107723236084, ETA in seconds: 817277.878\n",
      "epoch: 883700, train loss: 3.888339304924011, val loss: 3.980082154273987, ETA in seconds: 816682.352\n",
      "epoch: 883800, train loss: 3.8968896150588987, val loss: 3.9725719451904298, ETA in seconds: 816085.700\n",
      "epoch: 883900, train loss: 3.8965703725814818, val loss: 3.9773795127868654, ETA in seconds: 815489.674\n",
      "epoch: 884000, train loss: 3.8972042560577393, val loss: 3.969856548309326, ETA in seconds: 814869.487\n",
      "epoch: 884100, train loss: 3.8897311210632326, val loss: 3.9695471048355104, ETA in seconds: 814249.767\n",
      "epoch: 884200, train loss: 3.8979072093963625, val loss: 3.9594053983688355, ETA in seconds: 813632.033\n",
      "epoch: 884300, train loss: 3.8872076511383056, val loss: 3.9748136520385744, ETA in seconds: 813019.538\n",
      "epoch: 884400, train loss: 3.8930367469787597, val loss: 3.9670682191848754, ETA in seconds: 812399.951\n",
      "epoch: 884500, train loss: 3.889902710914612, val loss: 3.970961809158325, ETA in seconds: 811783.340\n",
      "epoch: 884600, train loss: 3.895083522796631, val loss: 3.9732834339141845, ETA in seconds: 811166.045\n",
      "epoch: 884700, train loss: 3.8993839979171754, val loss: 3.966438889503479, ETA in seconds: 810547.874\n",
      "epoch: 884800, train loss: 3.9006041526794433, val loss: 3.966939377784729, ETA in seconds: 809932.591\n",
      "epoch: 884900, train loss: 3.8885216236114504, val loss: 3.9756747484207153, ETA in seconds: 809323.980\n",
      "epoch: 885000, train loss: 3.8913697481155394, val loss: 3.9678650856018067, ETA in seconds: 808718.607\n",
      "epoch: 885100, train loss: 3.8967510223388673, val loss: 3.983878254890442, ETA in seconds: 808106.652\n",
      "epoch: 885200, train loss: 3.8894443273544312, val loss: 3.980315589904785, ETA in seconds: 807495.987\n",
      "epoch: 885300, train loss: 3.9002376079559324, val loss: 3.9672572135925295, ETA in seconds: 806881.752\n",
      "epoch: 885400, train loss: 3.8938012599945067, val loss: 3.9776715517044066, ETA in seconds: 806263.356\n",
      "epoch: 885500, train loss: 3.9011059284210203, val loss: 3.968275618553162, ETA in seconds: 805645.217\n",
      "epoch: 885600, train loss: 3.8947964668273927, val loss: 3.974165201187134, ETA in seconds: 805032.248\n",
      "epoch: 885700, train loss: 3.8958463191986086, val loss: 3.979772210121155, ETA in seconds: 804417.688\n",
      "epoch: 885800, train loss: 3.8968456506729128, val loss: 3.9748077630996703, ETA in seconds: 803799.295\n",
      "epoch: 885900, train loss: 3.8839466094970705, val loss: 3.965853452682495, ETA in seconds: 803180.303\n",
      "epoch: 886000, train loss: 3.8950323343276976, val loss: 3.9767680168151855, ETA in seconds: 802562.594\n",
      "epoch: 886100, train loss: 3.90292649269104, val loss: 3.9714476108551025, ETA in seconds: 801949.396\n",
      "epoch: 886200, train loss: 3.884738874435425, val loss: 3.976649522781372, ETA in seconds: 801335.628\n",
      "epoch: 886300, train loss: 3.8972945690155028, val loss: 3.960607123374939, ETA in seconds: 800719.467\n",
      "epoch: 886400, train loss: 3.8843435525894163, val loss: 3.966799807548523, ETA in seconds: 800105.471\n",
      "epoch: 886500, train loss: 3.890475869178772, val loss: 3.9663324117660523, ETA in seconds: 799492.785\n",
      "epoch: 886600, train loss: 3.8895938634872436, val loss: 3.9792752027511598, ETA in seconds: 798879.249\n",
      "epoch: 886700, train loss: 3.89550199508667, val loss: 3.9689595222473146, ETA in seconds: 798262.247\n",
      "epoch: 886800, train loss: 3.8893510818481447, val loss: 3.974881172180176, ETA in seconds: 797645.505\n",
      "epoch: 886900, train loss: 3.895390748977661, val loss: 3.9664767026901244, ETA in seconds: 797029.312\n",
      "epoch: 887000, train loss: 3.894470953941345, val loss: 3.974769115447998, ETA in seconds: 796413.571\n",
      "epoch: 887100, train loss: 3.8923575639724732, val loss: 3.9675759792327883, ETA in seconds: 795807.603\n",
      "epoch: 887200, train loss: 3.892931914329529, val loss: 3.972916102409363, ETA in seconds: 795199.794\n",
      "epoch: 887300, train loss: 3.8936927318573, val loss: 3.9769059896469114, ETA in seconds: 794600.248\n",
      "epoch: 887400, train loss: 3.8927371501922607, val loss: 3.9712547063827515, ETA in seconds: 794002.480\n",
      "epoch: 887500, train loss: 3.89925639629364, val loss: 3.9737990856170655, ETA in seconds: 793404.285\n",
      "epoch: 887600, train loss: 3.8998703002929687, val loss: 3.9689184427261353, ETA in seconds: 792806.399\n",
      "epoch: 887700, train loss: 3.8894296646118165, val loss: 3.979328083992004, ETA in seconds: 792199.747\n",
      "epoch: 887800, train loss: 3.8894699811935425, val loss: 3.9757538557052614, ETA in seconds: 791592.654\n",
      "epoch: 887900, train loss: 3.8773648738861084, val loss: 3.9741859674453734, ETA in seconds: 790989.391\n",
      "epoch: 888000, train loss: 3.8989226818084717, val loss: 3.9728352546691896, ETA in seconds: 790386.340\n",
      "epoch: 888100, train loss: 3.8831811904907227, val loss: 3.975196123123169, ETA in seconds: 789782.881\n",
      "epoch: 888200, train loss: 3.886561703681946, val loss: 3.9730570316314697, ETA in seconds: 789177.020\n",
      "epoch: 888300, train loss: 3.8899545907974242, val loss: 3.962493085861206, ETA in seconds: 788560.680\n",
      "epoch: 888400, train loss: 3.8970263481140135, val loss: 3.9754820823669434, ETA in seconds: 787945.481\n",
      "epoch: 888500, train loss: 3.8980937719345095, val loss: 3.978024458885193, ETA in seconds: 787326.829\n",
      "epoch: 888600, train loss: 3.891684913635254, val loss: 3.971720552444458, ETA in seconds: 786706.283\n",
      "epoch: 888700, train loss: 3.9023629903793333, val loss: 3.955484485626221, ETA in seconds: 786085.859\n",
      "epoch: 888800, train loss: 3.9007312774658205, val loss: 3.9664988040924074, ETA in seconds: 785464.669\n",
      "epoch: 888900, train loss: 3.8896845817565917, val loss: 3.9594565868377685, ETA in seconds: 784846.797\n",
      "epoch: 889000, train loss: 3.8941223859786986, val loss: 3.9754900455474855, ETA in seconds: 784232.087\n",
      "epoch: 889100, train loss: 3.8980175733566282, val loss: 3.9761346578598022, ETA in seconds: 783611.551\n",
      "epoch: 889200, train loss: 3.8886292457580565, val loss: 3.966063952445984, ETA in seconds: 782991.661\n",
      "epoch: 889300, train loss: 3.8846623420715334, val loss: 3.9718494415283203, ETA in seconds: 782378.690\n",
      "epoch: 889400, train loss: 3.8956700563430786, val loss: 3.973922610282898, ETA in seconds: 781772.426\n",
      "epoch: 889500, train loss: 3.888753795623779, val loss: 3.970350241661072, ETA in seconds: 781165.135\n",
      "epoch: 889600, train loss: 3.8949532508850098, val loss: 3.9707067728042604, ETA in seconds: 780558.621\n",
      "epoch: 889700, train loss: 3.900816869735718, val loss: 3.973390293121338, ETA in seconds: 779950.846\n",
      "epoch: 889800, train loss: 3.8974010229110716, val loss: 3.9682175874710084, ETA in seconds: 779334.618\n",
      "epoch: 889900, train loss: 3.889032983779907, val loss: 3.9716511487960817, ETA in seconds: 778718.387\n",
      "epoch: 890000, train loss: 3.8984325647354128, val loss: 3.9815892219543456, ETA in seconds: 778097.047\n",
      "epoch: 890100, train loss: 3.895419645309448, val loss: 3.9732120513916014, ETA in seconds: 777473.570\n",
      "epoch: 890200, train loss: 3.895745062828064, val loss: 3.9717522144317625, ETA in seconds: 776861.198\n",
      "epoch: 890300, train loss: 3.8972644805908203, val loss: 3.9707864046096804, ETA in seconds: 776242.191\n",
      "epoch: 890400, train loss: 3.888793683052063, val loss: 3.9659165859222414, ETA in seconds: 775616.566\n",
      "epoch: 890500, train loss: 3.88319730758667, val loss: 3.9747999906539917, ETA in seconds: 774993.352\n",
      "epoch: 890600, train loss: 3.883178186416626, val loss: 3.9691610813140867, ETA in seconds: 774373.159\n",
      "epoch: 890700, train loss: 3.891434383392334, val loss: 3.972383904457092, ETA in seconds: 773749.361\n",
      "epoch: 890800, train loss: 3.901222991943359, val loss: 3.973987650871277, ETA in seconds: 773131.281\n",
      "epoch: 890900, train loss: 3.884583306312561, val loss: 3.9675971269607544, ETA in seconds: 772522.367\n",
      "epoch: 891000, train loss: 3.892168664932251, val loss: 3.9766302824020388, ETA in seconds: 771912.215\n",
      "epoch: 891100, train loss: 3.8899447202682493, val loss: 3.975415587425232, ETA in seconds: 771294.098\n",
      "epoch: 891200, train loss: 3.8941222429275513, val loss: 3.983381819725037, ETA in seconds: 770667.277\n",
      "epoch: 891300, train loss: 3.890700030326843, val loss: 3.971819257736206, ETA in seconds: 770039.864\n",
      "epoch: 891400, train loss: 3.897373628616333, val loss: 3.979074740409851, ETA in seconds: 769412.578\n",
      "epoch: 891500, train loss: 3.893690276145935, val loss: 3.9772724390029905, ETA in seconds: 768785.723\n",
      "epoch: 891600, train loss: 3.8973626613616945, val loss: 3.9718876838684083, ETA in seconds: 768159.984\n",
      "epoch: 891700, train loss: 3.884567070007324, val loss: 3.974644660949707, ETA in seconds: 767535.318\n",
      "epoch: 891800, train loss: 3.8952643632888795, val loss: 3.970110034942627, ETA in seconds: 766914.898\n",
      "epoch: 891900, train loss: 3.894601511955261, val loss: 3.966285824775696, ETA in seconds: 766294.601\n",
      "epoch: 892000, train loss: 3.892775869369507, val loss: 3.973975896835327, ETA in seconds: 765674.476\n",
      "epoch: 892100, train loss: 3.893452787399292, val loss: 3.973370885848999, ETA in seconds: 765053.360\n",
      "epoch: 892200, train loss: 3.8991593599319456, val loss: 3.978458380699158, ETA in seconds: 764426.088\n",
      "epoch: 892300, train loss: 3.893354368209839, val loss: 3.9764917612075807, ETA in seconds: 763803.244\n",
      "epoch: 892400, train loss: 3.89927818775177, val loss: 3.9715849637985228, ETA in seconds: 763182.019\n",
      "epoch: 892500, train loss: 3.8969757318496705, val loss: 3.970156025886536, ETA in seconds: 762556.223\n",
      "epoch: 892600, train loss: 3.88856520652771, val loss: 3.98396372795105, ETA in seconds: 761928.440\n",
      "epoch: 892700, train loss: 3.89107723236084, val loss: 3.9702713012695314, ETA in seconds: 761299.180\n",
      "epoch: 892800, train loss: 3.889690566062927, val loss: 3.9745840311050413, ETA in seconds: 760669.739\n",
      "epoch: 892900, train loss: 3.889125394821167, val loss: 3.9882691860198975, ETA in seconds: 760041.907\n",
      "epoch: 893000, train loss: 3.8941700220108033, val loss: 3.9721362352371217, ETA in seconds: 759412.374\n",
      "epoch: 893100, train loss: 3.888654112815857, val loss: 3.975269246101379, ETA in seconds: 758782.627\n",
      "epoch: 893200, train loss: 3.8963687658309936, val loss: 3.9660982131958007, ETA in seconds: 758152.920\n",
      "epoch: 893300, train loss: 3.8879854679107666, val loss: 3.9785724401474, ETA in seconds: 757525.471\n",
      "epoch: 893400, train loss: 3.891019320487976, val loss: 3.9599049568176268, ETA in seconds: 756896.193\n",
      "epoch: 893500, train loss: 3.8900336742401125, val loss: 3.9751290798187258, ETA in seconds: 756270.641\n",
      "epoch: 893600, train loss: 3.890066051483154, val loss: 3.9715885400772093, ETA in seconds: 755644.098\n",
      "epoch: 893700, train loss: 3.8960429430007935, val loss: 3.9781286239624025, ETA in seconds: 755013.647\n",
      "epoch: 893800, train loss: 3.8838987588882445, val loss: 3.9687442064285277, ETA in seconds: 754388.077\n",
      "epoch: 893900, train loss: 3.887874627113342, val loss: 3.9720958709716796, ETA in seconds: 753760.357\n",
      "epoch: 894000, train loss: 3.897030568122864, val loss: 3.9768694162368776, ETA in seconds: 753131.930\n",
      "epoch: 894100, train loss: 3.8890121221542358, val loss: 3.9625444412231445, ETA in seconds: 752510.361\n",
      "epoch: 894200, train loss: 3.8927053451538085, val loss: 3.9732537031173707, ETA in seconds: 751895.214\n",
      "epoch: 894300, train loss: 3.8836474657058715, val loss: 3.9601489543914794, ETA in seconds: 751268.948\n",
      "epoch: 894400, train loss: 3.895988440513611, val loss: 3.9709595680236816, ETA in seconds: 750641.581\n",
      "epoch: 894500, train loss: 3.9002243757247923, val loss: 3.9751689195632935, ETA in seconds: 750008.977\n",
      "epoch: 894600, train loss: 3.884877157211304, val loss: 3.969076466560364, ETA in seconds: 749382.038\n",
      "epoch: 894700, train loss: 3.8957937002182006, val loss: 3.979563021659851, ETA in seconds: 748753.356\n",
      "epoch: 894800, train loss: 3.898197364807129, val loss: 3.974589800834656, ETA in seconds: 748126.770\n",
      "epoch: 894900, train loss: 3.8936951875686647, val loss: 3.966030311584473, ETA in seconds: 747497.040\n",
      "epoch: 895000, train loss: 3.9023128032684324, val loss: 3.973125386238098, ETA in seconds: 746866.806\n",
      "epoch: 895100, train loss: 3.8996745347976685, val loss: 3.9743202447891237, ETA in seconds: 746235.915\n",
      "epoch: 895200, train loss: 3.8931674718856812, val loss: 3.961775708198547, ETA in seconds: 745605.073\n",
      "epoch: 895300, train loss: 3.8988301515579225, val loss: 3.979297065734863, ETA in seconds: 744975.641\n",
      "epoch: 895400, train loss: 3.8955858945846558, val loss: 3.970716857910156, ETA in seconds: 744349.955\n",
      "epoch: 895500, train loss: 3.891517233848572, val loss: 3.97842276096344, ETA in seconds: 743724.220\n",
      "epoch: 895600, train loss: 3.892762327194214, val loss: 3.9862746477127073, ETA in seconds: 743099.213\n",
      "epoch: 895700, train loss: 3.8952332735061646, val loss: 3.9673375844955445, ETA in seconds: 742473.148\n",
      "epoch: 895800, train loss: 3.898956274986267, val loss: 3.968413066864014, ETA in seconds: 741845.864\n",
      "epoch: 895900, train loss: 3.903075838088989, val loss: 3.9746922492980956, ETA in seconds: 741219.582\n",
      "epoch: 896000, train loss: 3.904222273826599, val loss: 3.979687452316284, ETA in seconds: 740588.603\n",
      "epoch: 896100, train loss: 3.8933053016662598, val loss: 3.9785666704177856, ETA in seconds: 739954.276\n",
      "epoch: 896200, train loss: 3.888128423690796, val loss: 3.9744705677032472, ETA in seconds: 739318.429\n",
      "epoch: 896300, train loss: 3.9003402471542357, val loss: 3.970223331451416, ETA in seconds: 738687.111\n",
      "epoch: 896400, train loss: 3.9019619941711428, val loss: 3.9762404680252077, ETA in seconds: 738052.153\n",
      "epoch: 896500, train loss: 3.894863176345825, val loss: 3.974526071548462, ETA in seconds: 737424.544\n",
      "epoch: 896600, train loss: 3.89478600025177, val loss: 3.9702260971069334, ETA in seconds: 736796.901\n",
      "epoch: 896700, train loss: 3.900106978416443, val loss: 3.970521831512451, ETA in seconds: 736168.560\n",
      "epoch: 896800, train loss: 3.8870872497558593, val loss: 3.985928845405579, ETA in seconds: 735541.671\n",
      "epoch: 896900, train loss: 3.8937402963638306, val loss: 3.973845386505127, ETA in seconds: 734913.836\n",
      "epoch: 897000, train loss: 3.8916056156158447, val loss: 3.985390043258667, ETA in seconds: 734286.223\n",
      "epoch: 897100, train loss: 3.891801428794861, val loss: 3.9750134229660032, ETA in seconds: 733669.125\n",
      "epoch: 897200, train loss: 3.893521761894226, val loss: 3.9750589609146116, ETA in seconds: 733041.302\n",
      "epoch: 897300, train loss: 3.8945539474487303, val loss: 3.981909680366516, ETA in seconds: 732411.601\n",
      "epoch: 897400, train loss: 3.8969036102294923, val loss: 3.978988456726074, ETA in seconds: 731781.100\n",
      "epoch: 897500, train loss: 3.8969447135925295, val loss: 3.9717327117919923, ETA in seconds: 731148.423\n",
      "epoch: 897600, train loss: 3.893822932243347, val loss: 3.9834762096405028, ETA in seconds: 730516.487\n",
      "epoch: 897700, train loss: 3.900801753997803, val loss: 3.9735917329788206, ETA in seconds: 729880.729\n",
      "epoch: 897800, train loss: 3.901740312576294, val loss: 3.97028534412384, ETA in seconds: 729246.527\n",
      "epoch: 897900, train loss: 3.8941402435302734, val loss: 3.974378514289856, ETA in seconds: 728611.997\n",
      "epoch: 898000, train loss: 3.8954554319381716, val loss: 3.9787158250808714, ETA in seconds: 727981.599\n",
      "epoch: 898100, train loss: 3.894062900543213, val loss: 3.9705731153488157, ETA in seconds: 727350.585\n",
      "epoch: 898200, train loss: 3.892989253997803, val loss: 3.970295524597168, ETA in seconds: 726719.535\n",
      "epoch: 898300, train loss: 3.8900121450424194, val loss: 3.9781108856201173, ETA in seconds: 726088.265\n",
      "epoch: 898400, train loss: 3.8934709072113036, val loss: 3.966845726966858, ETA in seconds: 725454.082\n",
      "epoch: 898500, train loss: 3.8954626083374024, val loss: 3.9682966470718384, ETA in seconds: 724818.377\n",
      "epoch: 898600, train loss: 3.8893229484558107, val loss: 3.9708098411560058, ETA in seconds: 724182.946\n",
      "epoch: 898700, train loss: 3.886429190635681, val loss: 3.976111388206482, ETA in seconds: 723547.373\n",
      "epoch: 898800, train loss: 3.893317627906799, val loss: 3.965309715270996, ETA in seconds: 722911.097\n",
      "epoch: 898900, train loss: 3.8931721448898315, val loss: 3.9737313270568846, ETA in seconds: 722277.179\n",
      "epoch: 899000, train loss: 3.8893140316009522, val loss: 3.966289234161377, ETA in seconds: 721650.786\n",
      "epoch: 899100, train loss: 3.8913979053497316, val loss: 3.976315808296204, ETA in seconds: 721011.575\n",
      "epoch: 899200, train loss: 3.893919491767883, val loss: 3.965173053741455, ETA in seconds: 720374.484\n",
      "epoch: 899300, train loss: 3.9002805233001707, val loss: 3.9721974611282347, ETA in seconds: 719738.673\n",
      "epoch: 899400, train loss: 3.8945161581039427, val loss: 3.959878087043762, ETA in seconds: 719103.141\n",
      "epoch: 899500, train loss: 3.888118362426758, val loss: 3.9723441123962404, ETA in seconds: 718462.603\n",
      "epoch: 899600, train loss: 3.8949584007263183, val loss: 3.966096210479736, ETA in seconds: 717822.979\n",
      "epoch: 899700, train loss: 3.8983142375946045, val loss: 3.9754123210906984, ETA in seconds: 717192.820\n",
      "epoch: 899800, train loss: 3.9000150203704833, val loss: 3.977522301673889, ETA in seconds: 716560.417\n",
      "epoch: 899900, train loss: 3.8859134197235106, val loss: 3.9749069452285766, ETA in seconds: 715927.925\n",
      "epoch: 900000, train loss: 3.8983470678329466, val loss: 3.977519464492798, ETA in seconds: 715299.551\n",
      "epoch: 900100, train loss: 3.8966659784317015, val loss: 3.9804683923721313, ETA in seconds: 714681.876\n",
      "epoch: 900200, train loss: 3.90252628326416, val loss: 3.97194619178772, ETA in seconds: 714063.829\n",
      "epoch: 900300, train loss: 3.8971146821975706, val loss: 3.9760871648788454, ETA in seconds: 713446.114\n",
      "epoch: 900400, train loss: 3.896078324317932, val loss: 3.9692445516586305, ETA in seconds: 712827.414\n",
      "epoch: 900500, train loss: 3.891451525688171, val loss: 3.964416241645813, ETA in seconds: 712208.050\n",
      "epoch: 900600, train loss: 3.898443007469177, val loss: 3.9682814836502076, ETA in seconds: 711589.383\n",
      "epoch: 900700, train loss: 3.8912359476089478, val loss: 3.963151526451111, ETA in seconds: 710970.923\n",
      "epoch: 900800, train loss: 3.8948228120803834, val loss: 3.9764821767807006, ETA in seconds: 710347.728\n",
      "epoch: 900900, train loss: 3.9011147260665893, val loss: 3.9669850349426268, ETA in seconds: 709706.788\n",
      "epoch: 901000, train loss: 3.892992639541626, val loss: 3.968182420730591, ETA in seconds: 709068.848\n",
      "epoch: 901100, train loss: 3.8928495407104493, val loss: 3.963787865638733, ETA in seconds: 708443.392\n",
      "epoch: 901200, train loss: 3.8971739292144774, val loss: 3.9839240312576294, ETA in seconds: 707811.936\n",
      "epoch: 901300, train loss: 3.892851233482361, val loss: 3.963358020782471, ETA in seconds: 707180.270\n",
      "epoch: 901400, train loss: 3.895512318611145, val loss: 3.9753233671188353, ETA in seconds: 706541.219\n",
      "epoch: 901500, train loss: 3.89886314868927, val loss: 3.9781863689422607, ETA in seconds: 705901.533\n",
      "epoch: 901600, train loss: 3.8914310932159424, val loss: 3.9782527923583983, ETA in seconds: 705261.108\n",
      "epoch: 901700, train loss: 3.894312834739685, val loss: 3.9684321165084837, ETA in seconds: 704620.834\n",
      "epoch: 901800, train loss: 3.8998790740966798, val loss: 3.9684290170669554, ETA in seconds: 703978.597\n",
      "epoch: 901900, train loss: 3.8838874816894533, val loss: 3.9693443059921263, ETA in seconds: 703337.707\n",
      "epoch: 902000, train loss: 3.889825868606567, val loss: 3.9789074897766112, ETA in seconds: 702698.744\n",
      "epoch: 902100, train loss: 3.8893377780914307, val loss: 3.9773193359375, ETA in seconds: 702052.523\n",
      "epoch: 902200, train loss: 3.8875680923461915, val loss: 3.9593871593475343, ETA in seconds: 701409.734\n",
      "epoch: 902300, train loss: 3.9026948690414427, val loss: 3.9766213417053224, ETA in seconds: 700765.538\n",
      "epoch: 902400, train loss: 3.8960562467575075, val loss: 3.9737393379211428, ETA in seconds: 700124.081\n",
      "epoch: 902500, train loss: 3.8981255769729612, val loss: 3.9712748765945434, ETA in seconds: 699482.694\n",
      "epoch: 902600, train loss: 3.897530198097229, val loss: 3.9682497501373293, ETA in seconds: 698845.675\n",
      "epoch: 902700, train loss: 3.887093997001648, val loss: 3.972652816772461, ETA in seconds: 698204.345\n",
      "epoch: 902800, train loss: 3.9083980321884155, val loss: 3.975269055366516, ETA in seconds: 697562.334\n",
      "epoch: 902900, train loss: 3.8910943984985353, val loss: 3.9627096176147463, ETA in seconds: 696922.327\n",
      "epoch: 903000, train loss: 3.892741298675537, val loss: 3.9804548978805543, ETA in seconds: 696283.876\n",
      "epoch: 903100, train loss: 3.8884592771530153, val loss: 3.9731255054473875, ETA in seconds: 695638.576\n",
      "epoch: 903200, train loss: 3.8923038959503176, val loss: 3.97729754447937, ETA in seconds: 694993.927\n",
      "epoch: 903300, train loss: 3.8942620038986204, val loss: 3.9787992238998413, ETA in seconds: 694349.013\n",
      "epoch: 903400, train loss: 3.893154740333557, val loss: 3.9653520345687867, ETA in seconds: 693707.468\n",
      "epoch: 903500, train loss: 3.895726537704468, val loss: 3.9640732526779177, ETA in seconds: 693069.596\n",
      "epoch: 903600, train loss: 3.8951664924621583, val loss: 3.9880048513412474, ETA in seconds: 692439.014\n",
      "epoch: 903700, train loss: 3.891320013999939, val loss: 3.9736099004745484, ETA in seconds: 691808.292\n",
      "epoch: 903800, train loss: 3.8793556690216064, val loss: 3.9774508237838746, ETA in seconds: 691177.216\n",
      "epoch: 903900, train loss: 3.891730785369873, val loss: 3.9765843391418456, ETA in seconds: 690534.698\n",
      "epoch: 904000, train loss: 3.9018943071365357, val loss: 3.9690145015716554, ETA in seconds: 689889.773\n",
      "epoch: 904100, train loss: 3.8983174085617067, val loss: 3.9633328676223756, ETA in seconds: 689245.188\n",
      "epoch: 904200, train loss: 3.90374436378479, val loss: 3.962733435630798, ETA in seconds: 688603.152\n",
      "epoch: 904300, train loss: 3.8890424013137816, val loss: 3.964912462234497, ETA in seconds: 687962.527\n",
      "epoch: 904400, train loss: 3.897012996673584, val loss: 3.9782347679138184, ETA in seconds: 687319.643\n",
      "epoch: 904500, train loss: 3.8914647579193113, val loss: 3.963411736488342, ETA in seconds: 686671.910\n",
      "epoch: 904600, train loss: 3.8883617401123045, val loss: 3.982654666900635, ETA in seconds: 686025.997\n",
      "epoch: 904700, train loss: 3.8900670766830445, val loss: 3.9752870321273805, ETA in seconds: 685399.556\n",
      "epoch: 904800, train loss: 3.8902522802352903, val loss: 3.97612407207489, ETA in seconds: 684774.395\n",
      "epoch: 904900, train loss: 3.8933281183242796, val loss: 3.979134702682495, ETA in seconds: 684146.517\n",
      "epoch: 905000, train loss: 3.8950244426727294, val loss: 3.9727548599243163, ETA in seconds: 683520.132\n",
      "epoch: 905100, train loss: 3.891422390937805, val loss: 3.975594472885132, ETA in seconds: 682892.826\n",
      "epoch: 905200, train loss: 3.896482825279236, val loss: 3.9669650077819822, ETA in seconds: 682260.004\n",
      "epoch: 905300, train loss: 3.8951829433441163, val loss: 3.975097966194153, ETA in seconds: 681623.871\n",
      "epoch: 905400, train loss: 3.887451434135437, val loss: 3.9657390832901003, ETA in seconds: 680990.172\n",
      "epoch: 905500, train loss: 3.8873284816741944, val loss: 3.977592206001282, ETA in seconds: 680354.436\n",
      "epoch: 905600, train loss: 3.897904801368713, val loss: 3.9769262790679933, ETA in seconds: 679717.556\n",
      "epoch: 905700, train loss: 3.8940360069274904, val loss: 3.967540168762207, ETA in seconds: 679063.482\n",
      "epoch: 905800, train loss: 3.8958993673324587, val loss: 3.9795090913772584, ETA in seconds: 678411.510\n",
      "epoch: 905900, train loss: 3.894606041908264, val loss: 3.9749817848205566, ETA in seconds: 677764.779\n",
      "epoch: 906000, train loss: 3.895277714729309, val loss: 3.970776081085205, ETA in seconds: 677120.418\n",
      "epoch: 906100, train loss: 3.887338733673096, val loss: 3.978133487701416, ETA in seconds: 676473.889\n",
      "epoch: 906200, train loss: 3.8783731937408445, val loss: 3.9657639503479003, ETA in seconds: 675821.866\n",
      "epoch: 906300, train loss: 3.896354866027832, val loss: 3.972459077835083, ETA in seconds: 675170.848\n",
      "epoch: 906400, train loss: 3.8826650381088257, val loss: 3.9647146701812743, ETA in seconds: 674520.229\n",
      "epoch: 906500, train loss: 3.8901116371154787, val loss: 3.9769572973251344, ETA in seconds: 673869.190\n",
      "epoch: 906600, train loss: 3.889180874824524, val loss: 3.965796375274658, ETA in seconds: 673218.402\n",
      "epoch: 906700, train loss: 3.8927976369857786, val loss: 3.96877281665802, ETA in seconds: 672571.681\n",
      "epoch: 906800, train loss: 3.901917290687561, val loss: 3.980542540550232, ETA in seconds: 671924.921\n",
      "epoch: 906900, train loss: 3.894900679588318, val loss: 3.978242206573486, ETA in seconds: 671283.654\n",
      "epoch: 907000, train loss: 3.893032693862915, val loss: 3.962437057495117, ETA in seconds: 670648.075\n",
      "epoch: 907100, train loss: 3.886606049537659, val loss: 3.9806476354599, ETA in seconds: 670011.843\n",
      "epoch: 907200, train loss: 3.887638258934021, val loss: 3.9642144203186036, ETA in seconds: 669374.506\n",
      "epoch: 907300, train loss: 3.8921049356460573, val loss: 3.9752543449401854, ETA in seconds: 668731.807\n",
      "epoch: 907400, train loss: 3.8904174327850343, val loss: 3.9716589212417603, ETA in seconds: 668082.624\n",
      "epoch: 907500, train loss: 3.8909686326980593, val loss: 3.968208074569702, ETA in seconds: 667435.710\n",
      "epoch: 907600, train loss: 3.8959080934524537, val loss: 3.9604485511779783, ETA in seconds: 666796.906\n",
      "epoch: 907700, train loss: 3.8916827201843263, val loss: 3.980245804786682, ETA in seconds: 666158.160\n",
      "epoch: 907800, train loss: 3.888999819755554, val loss: 3.9753026247024534, ETA in seconds: 665519.074\n",
      "epoch: 907900, train loss: 3.896524214744568, val loss: 3.9665088415145875, ETA in seconds: 664868.994\n",
      "epoch: 908000, train loss: 3.8849773645401, val loss: 3.9702327966690065, ETA in seconds: 664226.270\n",
      "epoch: 908100, train loss: 3.880947780609131, val loss: 3.9790805339813233, ETA in seconds: 663573.549\n",
      "epoch: 908200, train loss: 3.8960209131240844, val loss: 3.96707398891449, ETA in seconds: 662923.234\n",
      "epoch: 908300, train loss: 3.8955073595046996, val loss: 3.97128689289093, ETA in seconds: 662272.777\n",
      "epoch: 908400, train loss: 3.8886009216308595, val loss: 3.959536838531494, ETA in seconds: 661620.928\n",
      "epoch: 908500, train loss: 3.883807897567749, val loss: 3.9685978412628176, ETA in seconds: 660964.706\n",
      "epoch: 908600, train loss: 3.8959193229675293, val loss: 3.9698625564575196, ETA in seconds: 660307.934\n",
      "epoch: 908700, train loss: 3.8951324701309202, val loss: 3.9589993476867678, ETA in seconds: 659657.961\n",
      "epoch: 908800, train loss: 3.890349006652832, val loss: 3.9628278732299806, ETA in seconds: 659005.557\n",
      "epoch: 908900, train loss: 3.8892075300216673, val loss: 3.977069306373596, ETA in seconds: 658353.465\n",
      "epoch: 909000, train loss: 3.895088005065918, val loss: 3.966326928138733, ETA in seconds: 657699.573\n",
      "epoch: 909100, train loss: 3.8930399656295775, val loss: 3.9665457725524904, ETA in seconds: 657045.525\n",
      "epoch: 909200, train loss: 3.8876126527786257, val loss: 3.966874694824219, ETA in seconds: 656397.002\n",
      "epoch: 909300, train loss: 3.897670578956604, val loss: 3.9725562572479247, ETA in seconds: 655748.110\n",
      "epoch: 909400, train loss: 3.891201615333557, val loss: 3.960818290710449, ETA in seconds: 655096.827\n",
      "epoch: 909500, train loss: 3.88842830657959, val loss: 3.9673940658569338, ETA in seconds: 654441.903\n",
      "epoch: 909600, train loss: 3.887292766571045, val loss: 3.9685337066650392, ETA in seconds: 653785.907\n",
      "epoch: 909700, train loss: 3.89159095287323, val loss: 3.9672983407974245, ETA in seconds: 653129.806\n",
      "epoch: 909800, train loss: 3.891721248626709, val loss: 3.959667134284973, ETA in seconds: 652474.657\n",
      "epoch: 909900, train loss: 3.8958049535751345, val loss: 3.9691421508789064, ETA in seconds: 651817.217\n",
      "epoch: 910000, train loss: 3.894017744064331, val loss: 3.9590224742889406, ETA in seconds: 651160.638\n",
      "epoch: 910100, train loss: 3.890047311782837, val loss: 3.962253379821777, ETA in seconds: 650504.827\n",
      "epoch: 910200, train loss: 3.8859368801116942, val loss: 3.966180372238159, ETA in seconds: 649847.367\n",
      "epoch: 910300, train loss: 3.8896246910095216, val loss: 3.9607206106185915, ETA in seconds: 649195.673\n",
      "epoch: 910400, train loss: 3.8932462692260743, val loss: 3.9787715911865233, ETA in seconds: 648556.659\n",
      "epoch: 910500, train loss: 3.8905070066452025, val loss: 3.970987558364868, ETA in seconds: 647917.106\n",
      "epoch: 910600, train loss: 3.890956258773804, val loss: 3.9663859844207763, ETA in seconds: 647277.657\n",
      "epoch: 910700, train loss: 3.891997528076172, val loss: 3.981425738334656, ETA in seconds: 646638.680\n",
      "epoch: 910800, train loss: 3.8853816270828245, val loss: 3.9650068998336794, ETA in seconds: 645993.997\n",
      "epoch: 910900, train loss: 3.9042784929275514, val loss: 3.963558387756348, ETA in seconds: 645336.833\n",
      "epoch: 911000, train loss: 3.8892061948776244, val loss: 3.9704450607299804, ETA in seconds: 644681.680\n",
      "epoch: 911100, train loss: 3.892855501174927, val loss: 3.9671416997909548, ETA in seconds: 644024.752\n",
      "epoch: 911200, train loss: 3.8903334379196166, val loss: 3.9655571937561036, ETA in seconds: 643365.350\n",
      "epoch: 911300, train loss: 3.887541079521179, val loss: 3.9643147706985475, ETA in seconds: 642706.499\n",
      "epoch: 911400, train loss: 3.8962574005126953, val loss: 3.979679822921753, ETA in seconds: 642051.734\n",
      "epoch: 911500, train loss: 3.8791170358657836, val loss: 3.9669928312301637, ETA in seconds: 641394.225\n",
      "epoch: 911600, train loss: 3.8922312259674072, val loss: 3.961828374862671, ETA in seconds: 640735.487\n",
      "epoch: 911700, train loss: 3.8938960075378417, val loss: 3.974493384361267, ETA in seconds: 640076.479\n",
      "epoch: 911800, train loss: 3.895079827308655, val loss: 3.9645353078842165, ETA in seconds: 639417.517\n",
      "epoch: 911900, train loss: 3.8940353393554688, val loss: 3.9704375505447387, ETA in seconds: 638758.706\n",
      "epoch: 912000, train loss: 3.8966795921325685, val loss: 3.9730273962020872, ETA in seconds: 638099.194\n",
      "epoch: 912100, train loss: 3.8974826335906982, val loss: 3.967340183258057, ETA in seconds: 637440.204\n",
      "epoch: 912200, train loss: 3.8876349210739134, val loss: 3.973477339744568, ETA in seconds: 636782.744\n",
      "epoch: 912300, train loss: 3.9038857936859133, val loss: 3.9770556688308716, ETA in seconds: 636124.019\n",
      "epoch: 912400, train loss: 3.904443693161011, val loss: 3.973144054412842, ETA in seconds: 635466.027\n",
      "epoch: 912500, train loss: 3.8957361221313476, val loss: 3.9678462982177733, ETA in seconds: 634811.372\n",
      "epoch: 912600, train loss: 3.8949418306350707, val loss: 3.959073710441589, ETA in seconds: 634154.272\n",
      "epoch: 912700, train loss: 3.8881141424179075, val loss: 3.9812525033950807, ETA in seconds: 633497.390\n",
      "epoch: 912800, train loss: 3.8871321201324465, val loss: 3.967287278175354, ETA in seconds: 632839.912\n",
      "epoch: 912900, train loss: 3.9029434442520143, val loss: 3.9638916730880736, ETA in seconds: 632189.289\n",
      "epoch: 913000, train loss: 3.891863226890564, val loss: 3.9690093755722047, ETA in seconds: 631538.567\n",
      "epoch: 913100, train loss: 3.9003461837768554, val loss: 3.9809635162353514, ETA in seconds: 630887.668\n",
      "epoch: 913200, train loss: 3.8873264312744142, val loss: 3.978138279914856, ETA in seconds: 630235.793\n",
      "epoch: 913300, train loss: 3.886059355735779, val loss: 3.966174364089966, ETA in seconds: 629583.001\n",
      "epoch: 913400, train loss: 3.8941060304641724, val loss: 3.9673006772994994, ETA in seconds: 628930.277\n",
      "epoch: 913500, train loss: 3.8959182262420655, val loss: 3.9784688472747805, ETA in seconds: 628279.300\n",
      "epoch: 913600, train loss: 3.8902782678604124, val loss: 3.9662693977355956, ETA in seconds: 627627.073\n",
      "epoch: 913700, train loss: 3.891778016090393, val loss: 3.9779853582382203, ETA in seconds: 626973.119\n",
      "epoch: 913800, train loss: 3.8916236400604247, val loss: 3.9748910903930663, ETA in seconds: 626315.857\n",
      "epoch: 913900, train loss: 3.894759702682495, val loss: 3.9751511335372927, ETA in seconds: 625659.242\n",
      "epoch: 914000, train loss: 3.894481134414673, val loss: 3.969050097465515, ETA in seconds: 625000.436\n",
      "epoch: 914100, train loss: 3.883928680419922, val loss: 3.976477122306824, ETA in seconds: 624350.342\n",
      "epoch: 914200, train loss: 3.894417405128479, val loss: 3.9745956659317017, ETA in seconds: 623694.505\n",
      "epoch: 914300, train loss: 3.8928802490234373, val loss: 3.9745789527893067, ETA in seconds: 623048.675\n",
      "epoch: 914400, train loss: 3.892500162124634, val loss: 3.981615114212036, ETA in seconds: 622394.992\n",
      "epoch: 914500, train loss: 3.896458697319031, val loss: 3.953330636024475, ETA in seconds: 621738.997\n",
      "epoch: 914600, train loss: 3.890559458732605, val loss: 3.9710060358047485, ETA in seconds: 621080.534\n",
      "epoch: 914700, train loss: 3.8857905626297, val loss: 3.9772145986557006, ETA in seconds: 620423.647\n",
      "epoch: 914800, train loss: 3.8872546911239625, val loss: 3.9649475812911987, ETA in seconds: 619773.064\n",
      "epoch: 914900, train loss: 3.885679244995117, val loss: 3.9668925523757936, ETA in seconds: 619128.132\n",
      "epoch: 915000, train loss: 3.8941294670104982, val loss: 3.9776381254196167, ETA in seconds: 618483.092\n",
      "epoch: 915100, train loss: 3.8965478420257567, val loss: 3.9771795749664305, ETA in seconds: 617837.775\n",
      "epoch: 915200, train loss: 3.8950827598571776, val loss: 3.9871741056442263, ETA in seconds: 617191.977\n",
      "epoch: 915300, train loss: 3.8991564989089964, val loss: 3.9757461071014406, ETA in seconds: 616544.148\n",
      "epoch: 915400, train loss: 3.892452335357666, val loss: 3.9748149871826173, ETA in seconds: 615893.911\n",
      "epoch: 915500, train loss: 3.8928509950637817, val loss: 3.970650887489319, ETA in seconds: 615243.040\n",
      "epoch: 915600, train loss: 3.894417071342468, val loss: 3.9752914190292357, ETA in seconds: 614591.941\n",
      "epoch: 915700, train loss: 3.899977779388428, val loss: 3.962043023109436, ETA in seconds: 613941.163\n",
      "epoch: 915800, train loss: 3.8890767335891723, val loss: 3.9739914417266844, ETA in seconds: 613289.884\n",
      "epoch: 915900, train loss: 3.889235520362854, val loss: 3.9720791578292847, ETA in seconds: 612638.625\n",
      "epoch: 916000, train loss: 3.902341628074646, val loss: 3.981208920478821, ETA in seconds: 611987.203\n",
      "epoch: 916100, train loss: 3.894982123374939, val loss: 3.9916443109512327, ETA in seconds: 611335.179\n",
      "epoch: 916200, train loss: 3.8974470615386965, val loss: 3.970106601715088, ETA in seconds: 610682.808\n",
      "epoch: 916300, train loss: 3.8944143772125246, val loss: 3.9774417877197266, ETA in seconds: 610023.092\n",
      "epoch: 916400, train loss: 3.8846349239349367, val loss: 3.965338683128357, ETA in seconds: 609354.912\n",
      "epoch: 916500, train loss: 3.8884280920028687, val loss: 3.9735249280929565, ETA in seconds: 608688.312\n",
      "epoch: 916600, train loss: 3.885063910484314, val loss: 3.9646132707595827, ETA in seconds: 608020.337\n",
      "epoch: 916700, train loss: 3.8923727750778196, val loss: 3.966220808029175, ETA in seconds: 607351.469\n",
      "epoch: 916800, train loss: 3.8914255857467652, val loss: 3.9766966104507446, ETA in seconds: 606682.273\n",
      "epoch: 916900, train loss: 3.90021071434021, val loss: 3.977897214889526, ETA in seconds: 606016.841\n",
      "epoch: 917000, train loss: 3.8869727849960327, val loss: 3.9675753116607666, ETA in seconds: 605348.797\n",
      "epoch: 917100, train loss: 3.8856510639190676, val loss: 3.957236075401306, ETA in seconds: 604680.575\n",
      "epoch: 917200, train loss: 3.8885085821151733, val loss: 3.9842297554016115, ETA in seconds: 604012.226\n",
      "epoch: 917300, train loss: 3.897036981582642, val loss: 3.9746642827987673, ETA in seconds: 603343.874\n",
      "epoch: 917400, train loss: 3.893593740463257, val loss: 3.9690380334854125, ETA in seconds: 602674.975\n",
      "epoch: 917500, train loss: 3.886198043823242, val loss: 3.971163201332092, ETA in seconds: 602006.867\n",
      "epoch: 917600, train loss: 3.890341782569885, val loss: 3.977826976776123, ETA in seconds: 601337.176\n",
      "epoch: 917700, train loss: 3.890491771697998, val loss: 3.9819122791290282, ETA in seconds: 600667.052\n",
      "epoch: 917800, train loss: 3.8874160051345825, val loss: 3.973156452178955, ETA in seconds: 599997.080\n",
      "epoch: 917900, train loss: 3.9025098085403442, val loss: 3.969637060165405, ETA in seconds: 599327.421\n",
      "epoch: 918000, train loss: 3.890386462211609, val loss: 3.9642847299575807, ETA in seconds: 598656.467\n",
      "epoch: 918100, train loss: 3.893306016921997, val loss: 3.9760244607925417, ETA in seconds: 597986.258\n",
      "epoch: 918200, train loss: 3.882295513153076, val loss: 3.9666696071624754, ETA in seconds: 597329.194\n",
      "epoch: 918300, train loss: 3.897190570831299, val loss: 3.974699878692627, ETA in seconds: 596671.944\n",
      "epoch: 918400, train loss: 3.885332131385803, val loss: 3.969844126701355, ETA in seconds: 596014.576\n",
      "epoch: 918500, train loss: 3.8901198863983155, val loss: 3.9832010507583617, ETA in seconds: 595356.891\n",
      "epoch: 918600, train loss: 3.8952502727508547, val loss: 3.9710880756378173, ETA in seconds: 594699.436\n",
      "epoch: 918700, train loss: 3.887572431564331, val loss: 3.9704952001571656, ETA in seconds: 594041.619\n",
      "epoch: 918800, train loss: 3.8973381519317627, val loss: 3.981716442108154, ETA in seconds: 593383.501\n",
      "epoch: 918900, train loss: 3.8928646802902223, val loss: 3.9731529712677003, ETA in seconds: 592725.074\n",
      "epoch: 919000, train loss: 3.9049985647201537, val loss: 3.967517852783203, ETA in seconds: 592066.765\n",
      "epoch: 919100, train loss: 3.885851001739502, val loss: 3.972303795814514, ETA in seconds: 591396.260\n",
      "epoch: 919200, train loss: 3.889293360710144, val loss: 3.977970314025879, ETA in seconds: 590723.996\n",
      "epoch: 919300, train loss: 3.881474328041077, val loss: 3.9652069091796873, ETA in seconds: 590059.905\n",
      "epoch: 919400, train loss: 3.8958489179611204, val loss: 3.9727158546447754, ETA in seconds: 589387.971\n",
      "epoch: 919500, train loss: 3.8919981479644776, val loss: 3.974772238731384, ETA in seconds: 588716.305\n",
      "epoch: 919600, train loss: 3.8884074211120607, val loss: 3.9641455888748167, ETA in seconds: 588045.153\n",
      "epoch: 919700, train loss: 3.900699329376221, val loss: 3.9723094701766968, ETA in seconds: 587373.610\n",
      "epoch: 919800, train loss: 3.8867990016937255, val loss: 3.9709359645843505, ETA in seconds: 586702.327\n",
      "epoch: 919900, train loss: 3.9051228046417235, val loss: 3.9702878475189207, ETA in seconds: 586029.774\n",
      "epoch: 920000, train loss: 3.8878870487213133, val loss: 3.969862937927246, ETA in seconds: 585358.338\n",
      "epoch: 920100, train loss: 3.8932293891906737, val loss: 3.9731231451034548, ETA in seconds: 584684.336\n",
      "epoch: 920200, train loss: 3.8862286806106567, val loss: 3.978669023513794, ETA in seconds: 584009.435\n",
      "epoch: 920300, train loss: 3.890666127204895, val loss: 3.971654534339905, ETA in seconds: 583334.423\n",
      "epoch: 920400, train loss: 3.894457983970642, val loss: 3.9633236169815063, ETA in seconds: 582663.511\n",
      "epoch: 920500, train loss: 3.8894789457321166, val loss: 3.9642190456390383, ETA in seconds: 581989.257\n",
      "epoch: 920600, train loss: 3.8969882249832155, val loss: 3.9666759729385377, ETA in seconds: 581315.056\n",
      "epoch: 920700, train loss: 3.885082221031189, val loss: 3.976159596443176, ETA in seconds: 580640.717\n",
      "epoch: 920800, train loss: 3.888989210128784, val loss: 3.9857763290405273, ETA in seconds: 579964.968\n",
      "epoch: 920900, train loss: 3.888531732559204, val loss: 3.9723004579544066, ETA in seconds: 579288.877\n",
      "epoch: 921000, train loss: 3.894504499435425, val loss: 3.9611817598342896, ETA in seconds: 578612.051\n",
      "epoch: 921100, train loss: 3.8895352602005007, val loss: 3.9814152479171754, ETA in seconds: 577935.262\n",
      "epoch: 921200, train loss: 3.898594927787781, val loss: 3.97863085269928, ETA in seconds: 577258.607\n",
      "epoch: 921300, train loss: 3.8850640058517456, val loss: 3.978505754470825, ETA in seconds: 576581.230\n",
      "epoch: 921400, train loss: 3.886324334144592, val loss: 3.9774577140808107, ETA in seconds: 575905.944\n",
      "epoch: 921500, train loss: 3.894399547576904, val loss: 3.9742024898529054, ETA in seconds: 575233.557\n",
      "epoch: 921600, train loss: 3.8901772499084473, val loss: 3.966914176940918, ETA in seconds: 574559.982\n",
      "epoch: 921700, train loss: 3.9010670661926268, val loss: 3.9702622652053834, ETA in seconds: 573886.586\n",
      "epoch: 921800, train loss: 3.8948708295822145, val loss: 3.974148893356323, ETA in seconds: 573213.115\n",
      "epoch: 921900, train loss: 3.8915234327316286, val loss: 3.9756547689437864, ETA in seconds: 572540.230\n",
      "epoch: 922000, train loss: 3.893402886390686, val loss: 3.958816409111023, ETA in seconds: 571865.076\n",
      "epoch: 922100, train loss: 3.900823950767517, val loss: 3.971529412269592, ETA in seconds: 571191.137\n",
      "epoch: 922200, train loss: 3.8857354164123534, val loss: 3.966764950752258, ETA in seconds: 570522.384\n",
      "epoch: 922300, train loss: 3.8898691654205324, val loss: 3.969868803024292, ETA in seconds: 569846.642\n",
      "epoch: 922400, train loss: 3.894644784927368, val loss: 3.9696150541305544, ETA in seconds: 569171.264\n",
      "epoch: 922500, train loss: 3.8903709173202516, val loss: 3.9793950796127318, ETA in seconds: 568492.681\n",
      "epoch: 922600, train loss: 3.895431303977966, val loss: 3.9551915407180784, ETA in seconds: 567813.882\n",
      "epoch: 922700, train loss: 3.8930092573165895, val loss: 3.979664516448975, ETA in seconds: 567135.139\n",
      "epoch: 922800, train loss: 3.8877389192581178, val loss: 3.974625754356384, ETA in seconds: 566460.312\n",
      "epoch: 922900, train loss: 3.89907329082489, val loss: 3.9762099742889405, ETA in seconds: 565785.081\n",
      "epoch: 923000, train loss: 3.8899158954620363, val loss: 3.963401031494141, ETA in seconds: 565105.566\n",
      "epoch: 923100, train loss: 3.900856041908264, val loss: 3.9779202461242678, ETA in seconds: 564426.113\n",
      "epoch: 923200, train loss: 3.899249863624573, val loss: 3.9697923183441164, ETA in seconds: 563749.432\n",
      "epoch: 923300, train loss: 3.896442174911499, val loss: 3.96755473613739, ETA in seconds: 563082.999\n",
      "epoch: 923400, train loss: 3.8901994466781615, val loss: 3.9816643238067626, ETA in seconds: 562417.602\n",
      "epoch: 923500, train loss: 3.8895875215530396, val loss: 3.9695050954818725, ETA in seconds: 561752.369\n",
      "epoch: 923600, train loss: 3.886481237411499, val loss: 3.977105140686035, ETA in seconds: 561084.474\n",
      "epoch: 923700, train loss: 3.896374750137329, val loss: 3.968047857284546, ETA in seconds: 560404.516\n",
      "epoch: 923800, train loss: 3.9001169204711914, val loss: 3.9699734449386597, ETA in seconds: 559724.959\n",
      "epoch: 923900, train loss: 3.8866456747055054, val loss: 3.95639910697937, ETA in seconds: 559048.265\n",
      "epoch: 924000, train loss: 3.904260993003845, val loss: 3.961735153198242, ETA in seconds: 558374.065\n",
      "epoch: 924100, train loss: 3.8939107179641725, val loss: 3.9712973833084106, ETA in seconds: 557697.581\n",
      "epoch: 924200, train loss: 3.9043349027633667, val loss: 3.9659731149673463, ETA in seconds: 557017.551\n",
      "epoch: 924300, train loss: 3.8832640171051027, val loss: 3.971160340309143, ETA in seconds: 556340.564\n",
      "epoch: 924400, train loss: 3.889073395729065, val loss: 3.9662994146347046, ETA in seconds: 555661.299\n",
      "epoch: 924500, train loss: 3.895631790161133, val loss: 3.9775440454483033, ETA in seconds: 554983.931\n",
      "epoch: 924600, train loss: 3.905547547340393, val loss: 3.961824083328247, ETA in seconds: 554306.288\n",
      "epoch: 924700, train loss: 3.894561505317688, val loss: 3.9762980699539185, ETA in seconds: 553628.379\n",
      "epoch: 924800, train loss: 3.891601061820984, val loss: 3.972754788398743, ETA in seconds: 552950.892\n",
      "epoch: 924900, train loss: 3.9023486614227294, val loss: 3.9738662481307983, ETA in seconds: 552271.636\n",
      "epoch: 925000, train loss: 3.8926578044891356, val loss: 3.965818977355957, ETA in seconds: 551596.225\n",
      "epoch: 925100, train loss: 3.881427526473999, val loss: 3.9713985681533814, ETA in seconds: 550916.438\n",
      "epoch: 925200, train loss: 3.890672779083252, val loss: 3.9691267728805544, ETA in seconds: 550237.883\n",
      "epoch: 925300, train loss: 3.88363778591156, val loss: 3.973624610900879, ETA in seconds: 549559.076\n",
      "epoch: 925400, train loss: 3.902283716201782, val loss: 3.977748489379883, ETA in seconds: 548878.907\n",
      "epoch: 925500, train loss: 3.892305326461792, val loss: 3.964943027496338, ETA in seconds: 548199.163\n",
      "epoch: 925600, train loss: 3.901671028137207, val loss: 3.9729326725006104, ETA in seconds: 547519.099\n",
      "epoch: 925700, train loss: 3.8913098573684692, val loss: 3.970161247253418, ETA in seconds: 546841.379\n",
      "epoch: 925800, train loss: 3.8884021997451783, val loss: 3.960167670249939, ETA in seconds: 546164.965\n",
      "epoch: 925900, train loss: 3.896249461174011, val loss: 3.968307065963745, ETA in seconds: 545486.333\n",
      "epoch: 926000, train loss: 3.896833825111389, val loss: 3.983733820915222, ETA in seconds: 544805.772\n",
      "epoch: 926100, train loss: 3.8941864252090452, val loss: 3.968856954574585, ETA in seconds: 544125.108\n",
      "epoch: 926200, train loss: 3.8986929416656495, val loss: 3.9748196840286254, ETA in seconds: 543445.731\n",
      "epoch: 926300, train loss: 3.889781665802002, val loss: 3.976158690452576, ETA in seconds: 542769.519\n",
      "epoch: 926400, train loss: 3.8917468786239624, val loss: 3.979064440727234, ETA in seconds: 542093.597\n",
      "epoch: 926500, train loss: 3.893456816673279, val loss: 3.974457550048828, ETA in seconds: 541412.319\n",
      "epoch: 926600, train loss: 3.9000303983688354, val loss: 3.9836795806884764, ETA in seconds: 540731.445\n",
      "epoch: 926700, train loss: 3.895200824737549, val loss: 3.9789846658706667, ETA in seconds: 540049.173\n",
      "epoch: 926800, train loss: 3.890638303756714, val loss: 3.9657060384750364, ETA in seconds: 539367.162\n",
      "epoch: 926900, train loss: 3.888507771492004, val loss: 3.971807527542114, ETA in seconds: 538685.509\n",
      "epoch: 927000, train loss: 3.893437623977661, val loss: 3.9778786659240724, ETA in seconds: 538002.894\n",
      "epoch: 927100, train loss: 3.8961199522018433, val loss: 3.972724103927612, ETA in seconds: 537321.987\n",
      "epoch: 927200, train loss: 3.892330551147461, val loss: 3.9625205755233766, ETA in seconds: 536637.505\n",
      "epoch: 927300, train loss: 3.8940942764282225, val loss: 3.967767429351807, ETA in seconds: 535953.156\n",
      "epoch: 927400, train loss: 3.889327120780945, val loss: 3.973273015022278, ETA in seconds: 535270.117\n",
      "epoch: 927500, train loss: 3.8918861627578734, val loss: 3.9686981439590454, ETA in seconds: 534586.742\n",
      "epoch: 927600, train loss: 3.895024561882019, val loss: 3.9676802158355713, ETA in seconds: 533904.461\n",
      "epoch: 927700, train loss: 3.901838207244873, val loss: 3.976083207130432, ETA in seconds: 533221.582\n",
      "epoch: 927800, train loss: 3.8860261917114256, val loss: 3.977825379371643, ETA in seconds: 532538.852\n",
      "epoch: 927900, train loss: 3.8873040676116943, val loss: 3.9737854957580567, ETA in seconds: 531855.226\n",
      "epoch: 928000, train loss: 3.8910401344299315, val loss: 3.9725716590881346, ETA in seconds: 531172.574\n",
      "epoch: 928100, train loss: 3.8911352872848513, val loss: 3.9750091791152955, ETA in seconds: 530491.259\n",
      "epoch: 928200, train loss: 3.8800734519958495, val loss: 3.970462608337402, ETA in seconds: 529810.476\n",
      "epoch: 928300, train loss: 3.882724928855896, val loss: 3.9760868549346924, ETA in seconds: 529128.756\n",
      "epoch: 928400, train loss: 3.897394633293152, val loss: 3.9769089460372924, ETA in seconds: 528444.616\n",
      "epoch: 928500, train loss: 3.897571873664856, val loss: 3.965723967552185, ETA in seconds: 527762.145\n",
      "epoch: 928600, train loss: 3.8968162298202516, val loss: 3.9642483234405517, ETA in seconds: 527077.096\n",
      "epoch: 928700, train loss: 3.8927560806274415, val loss: 3.972341012954712, ETA in seconds: 526390.956\n",
      "epoch: 928800, train loss: 3.890004277229309, val loss: 3.9578013896942137, ETA in seconds: 525715.096\n",
      "epoch: 928900, train loss: 3.8890000104904177, val loss: 3.9709362506866457, ETA in seconds: 525040.814\n",
      "epoch: 929000, train loss: 3.8880959033966063, val loss: 3.9619516849517824, ETA in seconds: 524365.991\n",
      "epoch: 929100, train loss: 3.8988845348358154, val loss: 3.98446946144104, ETA in seconds: 523691.227\n",
      "epoch: 929200, train loss: 3.90156409740448, val loss: 3.971812295913696, ETA in seconds: 523016.156\n",
      "epoch: 929300, train loss: 3.9015382528305054, val loss: 3.9595036029815676, ETA in seconds: 522327.671\n",
      "epoch: 929400, train loss: 3.8827929973602293, val loss: 3.9752477645874023, ETA in seconds: 521639.609\n",
      "epoch: 929500, train loss: 3.8993547201156615, val loss: 3.9621191024780273, ETA in seconds: 520953.259\n",
      "epoch: 929600, train loss: 3.8965965509414673, val loss: 3.97207293510437, ETA in seconds: 520277.250\n",
      "epoch: 929700, train loss: 3.8936089992523195, val loss: 3.972225379943848, ETA in seconds: 519600.817\n",
      "epoch: 929800, train loss: 3.8924217224121094, val loss: 3.967189645767212, ETA in seconds: 518925.161\n",
      "epoch: 929900, train loss: 3.8929283142089846, val loss: 3.9770932674407957, ETA in seconds: 518248.659\n",
      "epoch: 930000, train loss: 3.892363977432251, val loss: 3.9649273633956907, ETA in seconds: 517571.889\n",
      "epoch: 930100, train loss: 3.8982603311538697, val loss: 3.976231050491333, ETA in seconds: 516894.923\n",
      "epoch: 930200, train loss: 3.890879583358765, val loss: 3.972640562057495, ETA in seconds: 516208.998\n",
      "epoch: 930300, train loss: 3.892005205154419, val loss: 3.977365732192993, ETA in seconds: 515520.769\n",
      "epoch: 930400, train loss: 3.9028328895568847, val loss: 3.9793617725372314, ETA in seconds: 514836.030\n",
      "epoch: 930500, train loss: 3.8857662200927736, val loss: 3.982639217376709, ETA in seconds: 514151.602\n",
      "epoch: 930600, train loss: 3.8942730903625487, val loss: 3.9735361099243165, ETA in seconds: 513466.902\n",
      "epoch: 930700, train loss: 3.893141031265259, val loss: 3.977176856994629, ETA in seconds: 512782.866\n",
      "epoch: 930800, train loss: 3.8986064434051513, val loss: 3.9674713611602783, ETA in seconds: 512097.758\n",
      "epoch: 930900, train loss: 3.89719672203064, val loss: 3.974789023399353, ETA in seconds: 511410.051\n",
      "epoch: 931000, train loss: 3.891492462158203, val loss: 3.9867536306381224, ETA in seconds: 510721.502\n",
      "epoch: 931100, train loss: 3.886569857597351, val loss: 3.976699876785278, ETA in seconds: 510032.835\n",
      "epoch: 931200, train loss: 3.8958086490631105, val loss: 3.965941047668457, ETA in seconds: 509343.144\n",
      "epoch: 931300, train loss: 3.900439238548279, val loss: 3.969935178756714, ETA in seconds: 508654.686\n",
      "epoch: 931400, train loss: 3.885015439987183, val loss: 3.9628787279129027, ETA in seconds: 507968.854\n",
      "epoch: 931500, train loss: 3.8899105310440065, val loss: 3.965838646888733, ETA in seconds: 507283.011\n",
      "epoch: 931600, train loss: 3.900133562088013, val loss: 3.9694378852844237, ETA in seconds: 506597.680\n",
      "epoch: 931700, train loss: 3.898961043357849, val loss: 3.9810667514801024, ETA in seconds: 505910.775\n",
      "epoch: 931800, train loss: 3.9040271520614622, val loss: 3.970099687576294, ETA in seconds: 505221.834\n",
      "epoch: 931900, train loss: 3.888738822937012, val loss: 3.9671530723571777, ETA in seconds: 504531.785\n",
      "epoch: 932000, train loss: 3.8967377662658693, val loss: 3.9740979194641115, ETA in seconds: 503845.345\n",
      "epoch: 932100, train loss: 3.8850608110427856, val loss: 3.977213191986084, ETA in seconds: 503158.553\n",
      "epoch: 932200, train loss: 3.900037431716919, val loss: 3.968093466758728, ETA in seconds: 502472.004\n",
      "epoch: 932300, train loss: 3.896393632888794, val loss: 3.9679205656051635, ETA in seconds: 501785.579\n",
      "epoch: 932400, train loss: 3.902853798866272, val loss: 3.97781445980072, ETA in seconds: 501098.898\n",
      "epoch: 932500, train loss: 3.9028865814208986, val loss: 3.9742908239364625, ETA in seconds: 500409.820\n",
      "epoch: 932600, train loss: 3.8996644735336305, val loss: 3.9629063844680785, ETA in seconds: 499721.468\n",
      "epoch: 932700, train loss: 3.893157696723938, val loss: 3.9641310930252076, ETA in seconds: 499034.194\n",
      "epoch: 932800, train loss: 3.8894572019577027, val loss: 3.9690223932266235, ETA in seconds: 498343.576\n",
      "epoch: 932900, train loss: 3.8951151609420775, val loss: 3.9736823081970214, ETA in seconds: 497650.873\n",
      "epoch: 933000, train loss: 3.8821971893310545, val loss: 3.96782124042511, ETA in seconds: 496958.079\n",
      "epoch: 933100, train loss: 3.9012664318084718, val loss: 3.9712264776229858, ETA in seconds: 496266.634\n",
      "epoch: 933200, train loss: 3.889834976196289, val loss: 3.9622359037399293, ETA in seconds: 495574.581\n",
      "epoch: 933300, train loss: 3.891537618637085, val loss: 3.958720302581787, ETA in seconds: 494881.184\n",
      "epoch: 933400, train loss: 3.8859066009521483, val loss: 3.9648611307144166, ETA in seconds: 494188.456\n",
      "epoch: 933500, train loss: 3.89027259349823, val loss: 3.962862801551819, ETA in seconds: 493506.670\n",
      "epoch: 933600, train loss: 3.8941712617874145, val loss: 3.973804545402527, ETA in seconds: 492818.663\n",
      "epoch: 933700, train loss: 3.8856714248657225, val loss: 3.968012547492981, ETA in seconds: 492124.093\n",
      "epoch: 933800, train loss: 3.890138125419617, val loss: 3.9549980640411375, ETA in seconds: 491429.162\n",
      "epoch: 933900, train loss: 3.8913530349731444, val loss: 3.975900435447693, ETA in seconds: 490735.349\n",
      "epoch: 934000, train loss: 3.889066386222839, val loss: 3.966848921775818, ETA in seconds: 490042.653\n",
      "epoch: 934100, train loss: 3.8856361627578737, val loss: 3.975449299812317, ETA in seconds: 489353.377\n",
      "epoch: 934200, train loss: 3.893317723274231, val loss: 3.9572194814682007, ETA in seconds: 488658.521\n",
      "epoch: 934300, train loss: 3.889505934715271, val loss: 3.9659764766693115, ETA in seconds: 487966.776\n",
      "epoch: 934400, train loss: 3.8987249374389648, val loss: 3.964274859428406, ETA in seconds: 487274.488\n",
      "epoch: 934500, train loss: 3.8904881954193113, val loss: 3.9658649921417237, ETA in seconds: 486589.157\n",
      "epoch: 934600, train loss: 3.898180055618286, val loss: 3.969960880279541, ETA in seconds: 485907.843\n",
      "epoch: 934700, train loss: 3.8919492244720457, val loss: 3.9685848236083983, ETA in seconds: 485227.942\n",
      "epoch: 934800, train loss: 3.9006088972091675, val loss: 3.960456132888794, ETA in seconds: 484546.618\n",
      "epoch: 934900, train loss: 3.8955010890960695, val loss: 3.968751049041748, ETA in seconds: 483854.813\n",
      "epoch: 935000, train loss: 3.8887727737426756, val loss: 3.9674799919128416, ETA in seconds: 483164.062\n",
      "epoch: 935100, train loss: 3.8965497255325316, val loss: 3.9650354862213133, ETA in seconds: 482472.924\n",
      "epoch: 935200, train loss: 3.890976405143738, val loss: 3.9704393625259398, ETA in seconds: 481779.591\n",
      "epoch: 935300, train loss: 3.8948463916778566, val loss: 3.968533968925476, ETA in seconds: 481085.209\n",
      "epoch: 935400, train loss: 3.8897916793823244, val loss: 3.9731974601745605, ETA in seconds: 480394.281\n",
      "epoch: 935500, train loss: 3.8969857454299928, val loss: 3.9743701696395872, ETA in seconds: 479705.815\n",
      "epoch: 935600, train loss: 3.8854111194610597, val loss: 3.974040913581848, ETA in seconds: 479019.561\n",
      "epoch: 935700, train loss: 3.887729358673096, val loss: 3.971331071853638, ETA in seconds: 478333.154\n",
      "epoch: 935800, train loss: 3.8851929187774656, val loss: 3.9698827505111693, ETA in seconds: 477647.388\n",
      "epoch: 935900, train loss: 3.8918665647506714, val loss: 3.9662269830703734, ETA in seconds: 476960.849\n",
      "epoch: 936000, train loss: 3.893319344520569, val loss: 3.9717349290847777, ETA in seconds: 476265.361\n",
      "epoch: 936100, train loss: 3.8973275661468505, val loss: 3.9664026260375977, ETA in seconds: 475569.549\n",
      "epoch: 936200, train loss: 3.8913509845733643, val loss: 3.961861228942871, ETA in seconds: 474877.119\n",
      "epoch: 936300, train loss: 3.8948672294616697, val loss: 3.976297473907471, ETA in seconds: 474184.728\n",
      "epoch: 936400, train loss: 3.899251842498779, val loss: 3.9634825229644775, ETA in seconds: 473492.238\n",
      "epoch: 936500, train loss: 3.890924668312073, val loss: 3.9765511989593505, ETA in seconds: 472798.804\n",
      "epoch: 936600, train loss: 3.8955332040786743, val loss: 3.960153841972351, ETA in seconds: 472105.401\n",
      "epoch: 936700, train loss: 3.8901482582092286, val loss: 3.9642462015151976, ETA in seconds: 471411.495\n",
      "epoch: 936800, train loss: 3.889280605316162, val loss: 3.9753994941711426, ETA in seconds: 470718.128\n",
      "epoch: 936900, train loss: 3.8950734376907348, val loss: 3.9661399364471435, ETA in seconds: 470024.041\n",
      "epoch: 937000, train loss: 3.904190254211426, val loss: 3.962260675430298, ETA in seconds: 469329.444\n",
      "epoch: 937100, train loss: 3.889165735244751, val loss: 3.9645222425460815, ETA in seconds: 468635.935\n",
      "epoch: 937200, train loss: 3.892827773094177, val loss: 3.9633451461791993, ETA in seconds: 467939.939\n",
      "epoch: 937300, train loss: 3.896378684043884, val loss: 3.957514214515686, ETA in seconds: 467244.688\n",
      "epoch: 937400, train loss: 3.891716742515564, val loss: 3.972842502593994, ETA in seconds: 466560.696\n",
      "epoch: 937500, train loss: 3.899479532241821, val loss: 3.9637452125549317, ETA in seconds: 465868.303\n",
      "epoch: 937600, train loss: 3.901320743560791, val loss: 3.9721405267715455, ETA in seconds: 465167.279\n",
      "epoch: 937700, train loss: 3.898151659965515, val loss: 3.9657960653305055, ETA in seconds: 464469.685\n",
      "epoch: 937800, train loss: 3.88696084022522, val loss: 3.967995452880859, ETA in seconds: 463770.841\n",
      "epoch: 937900, train loss: 3.8980759143829347, val loss: 3.9803351163864136, ETA in seconds: 463076.528\n",
      "epoch: 938000, train loss: 3.8891186237335207, val loss: 3.9709089517593386, ETA in seconds: 462381.391\n",
      "epoch: 938100, train loss: 3.8899824619293213, val loss: 3.9691804885864257, ETA in seconds: 461685.107\n",
      "epoch: 938200, train loss: 3.8920321464538574, val loss: 3.9673680782318117, ETA in seconds: 460988.590\n",
      "epoch: 938300, train loss: 3.9007076263427733, val loss: 3.9686139106750487, ETA in seconds: 460288.867\n",
      "epoch: 938400, train loss: 3.8872251510620117, val loss: 3.9581047773361204, ETA in seconds: 459589.549\n",
      "epoch: 938500, train loss: 3.890380311012268, val loss: 3.974724841117859, ETA in seconds: 458892.882\n",
      "epoch: 938600, train loss: 3.889032578468323, val loss: 3.967209815979004, ETA in seconds: 458194.322\n",
      "epoch: 938700, train loss: 3.8887646198272705, val loss: 3.9759952545166017, ETA in seconds: 457494.494\n",
      "epoch: 938800, train loss: 3.8879360437393187, val loss: 3.967668867111206, ETA in seconds: 456798.088\n",
      "epoch: 938900, train loss: 3.8915921211242677, val loss: 3.9667187213897703, ETA in seconds: 456107.501\n",
      "epoch: 939000, train loss: 3.899380850791931, val loss: 3.962296199798584, ETA in seconds: 455418.643\n",
      "epoch: 939100, train loss: 3.900666618347168, val loss: 3.967651605606079, ETA in seconds: 454722.896\n",
      "epoch: 939200, train loss: 3.89806969165802, val loss: 3.9693843603134153, ETA in seconds: 454021.832\n",
      "epoch: 939300, train loss: 3.8972549438476562, val loss: 3.9679085493087767, ETA in seconds: 453321.554\n",
      "epoch: 939400, train loss: 3.895704460144043, val loss: 3.9651960372924804, ETA in seconds: 452623.866\n",
      "epoch: 939500, train loss: 3.885185384750366, val loss: 3.9619476795196533, ETA in seconds: 451932.038\n",
      "epoch: 939600, train loss: 3.8827088594436647, val loss: 3.980748677253723, ETA in seconds: 451239.779\n",
      "epoch: 939700, train loss: 3.8988997459411623, val loss: 3.972931241989136, ETA in seconds: 450544.849\n",
      "epoch: 939800, train loss: 3.890907645225525, val loss: 3.9764450788497925, ETA in seconds: 449846.233\n",
      "epoch: 939900, train loss: 3.891484498977661, val loss: 3.9704665899276734, ETA in seconds: 449147.355\n",
      "epoch: 940000, train loss: 3.8897072792053224, val loss: 3.971089720726013, ETA in seconds: 448447.093\n",
      "epoch: 940100, train loss: 3.8888195991516112, val loss: 3.9602583169937136, ETA in seconds: 447743.650\n",
      "epoch: 940200, train loss: 3.895492601394653, val loss: 3.968070650100708, ETA in seconds: 447042.272\n",
      "epoch: 940300, train loss: 3.8895721197128297, val loss: 3.964805555343628, ETA in seconds: 446348.391\n",
      "epoch: 940400, train loss: 3.8778566837310793, val loss: 3.976205587387085, ETA in seconds: 445647.625\n",
      "epoch: 940500, train loss: 3.890320563316345, val loss: 3.96552574634552, ETA in seconds: 444944.189\n",
      "epoch: 940600, train loss: 3.895467758178711, val loss: 3.9629271030426025, ETA in seconds: 444250.320\n",
      "epoch: 940700, train loss: 3.8949987649917603, val loss: 3.960595178604126, ETA in seconds: 443556.811\n",
      "epoch: 940800, train loss: 3.8898826122283934, val loss: 3.9667578458786013, ETA in seconds: 442858.330\n",
      "epoch: 940900, train loss: 3.9053559064865113, val loss: 3.968342924118042, ETA in seconds: 442163.461\n",
      "epoch: 941000, train loss: 3.890134263038635, val loss: 3.9641854047775267, ETA in seconds: 441468.204\n",
      "epoch: 941100, train loss: 3.8946107149124147, val loss: 3.9733319997787477, ETA in seconds: 440767.061\n",
      "epoch: 941200, train loss: 3.89723117351532, val loss: 3.970285677909851, ETA in seconds: 440066.412\n",
      "epoch: 941300, train loss: 3.8934553146362303, val loss: 3.9705604553222655, ETA in seconds: 439364.458\n",
      "epoch: 941400, train loss: 3.8914463043212892, val loss: 3.9744014739990234, ETA in seconds: 438663.482\n",
      "epoch: 941500, train loss: 3.896273303031921, val loss: 3.9752756118774415, ETA in seconds: 437965.302\n",
      "epoch: 941600, train loss: 3.8962432861328127, val loss: 3.9639468669891356, ETA in seconds: 437271.353\n",
      "epoch: 941700, train loss: 3.8985267877578735, val loss: 3.9703426122665406, ETA in seconds: 436579.975\n",
      "epoch: 941800, train loss: 3.8949745655059815, val loss: 3.985178804397583, ETA in seconds: 435887.614\n",
      "epoch: 941900, train loss: 3.8944113731384276, val loss: 3.9692675113677978, ETA in seconds: 435196.015\n",
      "epoch: 942000, train loss: 3.8915786027908323, val loss: 3.976951742172241, ETA in seconds: 434491.154\n",
      "epoch: 942100, train loss: 3.8877502918243407, val loss: 3.9723394393920897, ETA in seconds: 433785.481\n",
      "epoch: 942200, train loss: 3.8948843717575072, val loss: 3.9623602867126464, ETA in seconds: 433082.879\n",
      "epoch: 942300, train loss: 3.8864328861236572, val loss: 3.97114679813385, ETA in seconds: 432381.010\n",
      "epoch: 942400, train loss: 3.8927337169647216, val loss: 3.9691497564315794, ETA in seconds: 431675.661\n",
      "epoch: 942500, train loss: 3.898142600059509, val loss: 3.9719258546829224, ETA in seconds: 430969.412\n",
      "epoch: 942600, train loss: 3.901230883598328, val loss: 3.9686556339263914, ETA in seconds: 430265.231\n",
      "epoch: 942700, train loss: 3.8885568618774413, val loss: 3.9483354091644287, ETA in seconds: 429559.365\n",
      "epoch: 942800, train loss: 3.8964014768600466, val loss: 3.9714855909347535, ETA in seconds: 428853.021\n",
      "epoch: 942900, train loss: 3.905768632888794, val loss: 3.9626200437545775, ETA in seconds: 428145.056\n",
      "epoch: 943000, train loss: 3.909162759780884, val loss: 3.975422239303589, ETA in seconds: 427444.655\n",
      "epoch: 943100, train loss: 3.895266032218933, val loss: 3.984236717224121, ETA in seconds: 426738.192\n",
      "epoch: 943200, train loss: 3.8932984352111815, val loss: 3.9583077669143676, ETA in seconds: 426030.534\n",
      "epoch: 943300, train loss: 3.8997365951538088, val loss: 3.9556181907653807, ETA in seconds: 425321.806\n",
      "epoch: 943400, train loss: 3.890023684501648, val loss: 3.9649595260620116, ETA in seconds: 424612.985\n",
      "epoch: 943500, train loss: 3.8883214950561524, val loss: 3.9736530780792236, ETA in seconds: 423903.088\n",
      "epoch: 943600, train loss: 3.897857332229614, val loss: 3.976644492149353, ETA in seconds: 423193.733\n",
      "epoch: 943700, train loss: 3.8913126945495606, val loss: 3.969173526763916, ETA in seconds: 422488.836\n",
      "epoch: 943800, train loss: 3.8930471658706667, val loss: 3.9681097745895384, ETA in seconds: 421779.591\n",
      "epoch: 943900, train loss: 3.8847952842712403, val loss: 3.982973027229309, ETA in seconds: 421069.523\n",
      "epoch: 944000, train loss: 3.8888195514678956, val loss: 3.967943024635315, ETA in seconds: 420360.592\n",
      "epoch: 944100, train loss: 3.895045018196106, val loss: 3.988018584251404, ETA in seconds: 419652.105\n",
      "epoch: 944200, train loss: 3.8997382879257203, val loss: 3.974084234237671, ETA in seconds: 418942.518\n",
      "epoch: 944300, train loss: 3.8874907732009887, val loss: 3.9676481246948243, ETA in seconds: 418233.762\n",
      "epoch: 944400, train loss: 3.8962155103683473, val loss: 3.972351241111755, ETA in seconds: 417532.970\n",
      "epoch: 944500, train loss: 3.900443124771118, val loss: 3.9726130247116087, ETA in seconds: 416832.808\n",
      "epoch: 944600, train loss: 3.895832347869873, val loss: 3.9823206424713136, ETA in seconds: 416132.918\n",
      "epoch: 944700, train loss: 3.902199387550354, val loss: 3.9774877786636353, ETA in seconds: 415427.253\n",
      "epoch: 944800, train loss: 3.893901252746582, val loss: 3.9767220973968507, ETA in seconds: 414719.292\n",
      "epoch: 944900, train loss: 3.895918846130371, val loss: 3.972180700302124, ETA in seconds: 414011.116\n",
      "epoch: 945000, train loss: 3.8930619955062866, val loss: 3.970953178405762, ETA in seconds: 413306.470\n",
      "epoch: 945100, train loss: 3.888120698928833, val loss: 3.967596912384033, ETA in seconds: 412600.012\n",
      "epoch: 945200, train loss: 3.88201367855072, val loss: 3.9567334413528443, ETA in seconds: 411891.234\n",
      "epoch: 945300, train loss: 3.8879462003707888, val loss: 3.956774926185608, ETA in seconds: 411182.108\n",
      "epoch: 945400, train loss: 3.881600189208984, val loss: 3.9737508296966553, ETA in seconds: 410473.253\n",
      "epoch: 945500, train loss: 3.905194902420044, val loss: 3.971659278869629, ETA in seconds: 409763.964\n",
      "epoch: 945600, train loss: 3.8836175441741942, val loss: 3.970779299736023, ETA in seconds: 409054.334\n",
      "epoch: 945700, train loss: 3.8854193925857543, val loss: 3.971875214576721, ETA in seconds: 408345.411\n",
      "epoch: 945800, train loss: 3.8892412185668945, val loss: 3.9617731094360353, ETA in seconds: 407634.965\n",
      "epoch: 945900, train loss: 3.902139902114868, val loss: 3.975805950164795, ETA in seconds: 406925.549\n",
      "epoch: 946000, train loss: 3.8826190710067747, val loss: 3.966233658790588, ETA in seconds: 406214.507\n",
      "epoch: 946100, train loss: 3.893837904930115, val loss: 3.9850895404815674, ETA in seconds: 405503.446\n",
      "epoch: 946200, train loss: 3.8917790174484255, val loss: 3.979316163063049, ETA in seconds: 404791.228\n",
      "epoch: 946300, train loss: 3.889342188835144, val loss: 3.9740142822265625, ETA in seconds: 404079.620\n",
      "epoch: 946400, train loss: 3.887348461151123, val loss: 3.9617098569869995, ETA in seconds: 403378.498\n",
      "epoch: 946500, train loss: 3.884326386451721, val loss: 3.978920650482178, ETA in seconds: 402665.349\n",
      "epoch: 946600, train loss: 3.8970972299575806, val loss: 3.956449365615845, ETA in seconds: 401955.362\n",
      "epoch: 946700, train loss: 3.893378233909607, val loss: 3.977506971359253, ETA in seconds: 401240.793\n",
      "epoch: 946800, train loss: 3.890054988861084, val loss: 3.9596757650375367, ETA in seconds: 400526.485\n",
      "epoch: 946900, train loss: 3.8986593008041384, val loss: 3.97502658367157, ETA in seconds: 399814.056\n",
      "epoch: 947000, train loss: 3.8972286939620973, val loss: 3.9744070291519167, ETA in seconds: 399107.524\n",
      "epoch: 947100, train loss: 3.897170615196228, val loss: 3.969293165206909, ETA in seconds: 398404.746\n",
      "epoch: 947200, train loss: 3.8923497438430785, val loss: 3.979089689254761, ETA in seconds: 397696.547\n",
      "epoch: 947300, train loss: 3.895432710647583, val loss: 3.9708108425140383, ETA in seconds: 396986.808\n",
      "epoch: 947400, train loss: 3.9021592855453493, val loss: 3.9735657930374146, ETA in seconds: 396270.837\n",
      "epoch: 947500, train loss: 3.8963843107223513, val loss: 3.9711928367614746, ETA in seconds: 395555.906\n",
      "epoch: 947600, train loss: 3.895197105407715, val loss: 3.980285978317261, ETA in seconds: 394840.003\n",
      "epoch: 947700, train loss: 3.892328882217407, val loss: 3.9673173666000365, ETA in seconds: 394125.001\n",
      "epoch: 947800, train loss: 3.900853395462036, val loss: 3.97583167552948, ETA in seconds: 393411.035\n",
      "epoch: 947900, train loss: 3.9002788066864014, val loss: 3.9687188386917116, ETA in seconds: 392697.137\n",
      "epoch: 948000, train loss: 3.894641470909119, val loss: 3.9695880889892576, ETA in seconds: 391980.561\n",
      "epoch: 948100, train loss: 3.892931342124939, val loss: 3.963250517845154, ETA in seconds: 391265.227\n",
      "epoch: 948200, train loss: 3.9010448455810547, val loss: 3.970836853981018, ETA in seconds: 390548.519\n",
      "epoch: 948300, train loss: 3.8929795742034914, val loss: 3.972455620765686, ETA in seconds: 389830.970\n",
      "epoch: 948400, train loss: 3.8897495031356812, val loss: 3.9697534799575807, ETA in seconds: 389117.169\n",
      "epoch: 948500, train loss: 3.885909819602966, val loss: 3.970848250389099, ETA in seconds: 388403.918\n",
      "epoch: 948600, train loss: 3.8899876356124876, val loss: 3.9687941551208494, ETA in seconds: 387688.942\n",
      "epoch: 948700, train loss: 3.8925962448120117, val loss: 3.962307667732239, ETA in seconds: 386974.277\n",
      "epoch: 948800, train loss: 3.898582100868225, val loss: 3.9646952629089354, ETA in seconds: 386259.703\n",
      "epoch: 948900, train loss: 3.8875866889953614, val loss: 3.976863169670105, ETA in seconds: 385543.741\n",
      "epoch: 949000, train loss: 3.89561128616333, val loss: 3.9754379749298097, ETA in seconds: 384830.040\n",
      "epoch: 949100, train loss: 3.892602801322937, val loss: 3.96725537776947, ETA in seconds: 384113.989\n",
      "epoch: 949200, train loss: 3.881266379356384, val loss: 3.9750602722167967, ETA in seconds: 383398.232\n",
      "epoch: 949300, train loss: 3.8995497941970827, val loss: 3.984133815765381, ETA in seconds: 382683.350\n",
      "epoch: 949400, train loss: 3.8871005296707155, val loss: 3.9758861541748045, ETA in seconds: 381964.857\n",
      "epoch: 949500, train loss: 3.8941858053207397, val loss: 3.9622071266174315, ETA in seconds: 381245.455\n",
      "epoch: 949600, train loss: 3.8876673221588134, val loss: 3.9825114965438844, ETA in seconds: 380525.685\n",
      "epoch: 949700, train loss: 3.882229042053223, val loss: 3.977814197540283, ETA in seconds: 379808.768\n",
      "epoch: 949800, train loss: 3.9029176950454714, val loss: 3.9717575788497923, ETA in seconds: 379093.740\n",
      "epoch: 949900, train loss: 3.8977046966552735, val loss: 3.9747026681900026, ETA in seconds: 378383.995\n",
      "epoch: 950000, train loss: 3.8935311317443846, val loss: 3.9796661138534546, ETA in seconds: 377667.047\n",
      "epoch: 950100, train loss: 3.888577675819397, val loss: 3.973341393470764, ETA in seconds: 376949.932\n",
      "epoch: 950200, train loss: 3.894584822654724, val loss: 3.972003126144409, ETA in seconds: 376236.161\n",
      "epoch: 950300, train loss: 3.8905351400375365, val loss: 3.9777284383773805, ETA in seconds: 375522.745\n",
      "epoch: 950400, train loss: 3.897885274887085, val loss: 3.97400643825531, ETA in seconds: 374806.779\n",
      "epoch: 950500, train loss: 3.8812840938568116, val loss: 3.97368586063385, ETA in seconds: 374092.533\n",
      "epoch: 950600, train loss: 3.8891316652297974, val loss: 3.9791811227798464, ETA in seconds: 373381.757\n",
      "epoch: 950700, train loss: 3.8853121995925903, val loss: 3.965221619606018, ETA in seconds: 372669.689\n",
      "epoch: 950800, train loss: 3.894234371185303, val loss: 3.9678717851638794, ETA in seconds: 371950.851\n",
      "epoch: 950900, train loss: 3.8906699657440185, val loss: 3.963732290267944, ETA in seconds: 371233.025\n",
      "epoch: 951000, train loss: 3.898373508453369, val loss: 3.983698105812073, ETA in seconds: 370515.043\n",
      "epoch: 951100, train loss: 3.891729235649109, val loss: 3.9739200115203857, ETA in seconds: 369795.103\n",
      "epoch: 951200, train loss: 3.8952235698699953, val loss: 3.9763522386550902, ETA in seconds: 369075.175\n",
      "epoch: 951300, train loss: 3.881297063827515, val loss: 3.972981667518616, ETA in seconds: 368354.705\n",
      "epoch: 951400, train loss: 3.8837050676345823, val loss: 3.975841784477234, ETA in seconds: 367635.387\n",
      "epoch: 951500, train loss: 3.889553594589233, val loss: 3.9679747819900513, ETA in seconds: 366915.241\n",
      "epoch: 951600, train loss: 3.898740553855896, val loss: 3.982831025123596, ETA in seconds: 366195.120\n",
      "epoch: 951700, train loss: 3.886763095855713, val loss: 3.985859990119934, ETA in seconds: 365474.369\n",
      "epoch: 951800, train loss: 3.9011507511138914, val loss: 3.976454520225525, ETA in seconds: 364752.989\n",
      "epoch: 951900, train loss: 3.9002447366714477, val loss: 3.9623281478881838, ETA in seconds: 364030.688\n",
      "epoch: 952000, train loss: 3.889811635017395, val loss: 3.9742106437683105, ETA in seconds: 363309.969\n",
      "epoch: 952100, train loss: 3.8855601072311403, val loss: 3.98085503578186, ETA in seconds: 362590.556\n",
      "epoch: 952200, train loss: 3.8875252962112428, val loss: 3.977003741264343, ETA in seconds: 361870.436\n",
      "epoch: 952300, train loss: 3.892690086364746, val loss: 3.9682012796401978, ETA in seconds: 361149.767\n",
      "epoch: 952400, train loss: 3.89064416885376, val loss: 3.9710965394973754, ETA in seconds: 360429.626\n",
      "epoch: 952500, train loss: 3.8865546226501464, val loss: 3.9696088075637816, ETA in seconds: 359718.421\n",
      "epoch: 952600, train loss: 3.886846089363098, val loss: 3.9844648122787474, ETA in seconds: 358998.652\n",
      "epoch: 952700, train loss: 3.892972707748413, val loss: 3.971951460838318, ETA in seconds: 358282.297\n",
      "epoch: 952800, train loss: 3.8825932264328005, val loss: 3.9684772729873656, ETA in seconds: 357569.644\n",
      "epoch: 952900, train loss: 3.893415904045105, val loss: 3.9749775409698485, ETA in seconds: 356861.596\n",
      "epoch: 953000, train loss: 3.8883167266845704, val loss: 3.982565498352051, ETA in seconds: 356140.910\n",
      "epoch: 953100, train loss: 3.893852710723877, val loss: 3.9792247772216798, ETA in seconds: 355421.487\n",
      "epoch: 953200, train loss: 3.8924800872802736, val loss: 3.9661921501159667, ETA in seconds: 354702.245\n",
      "epoch: 953300, train loss: 3.8968976974487304, val loss: 3.967416548728943, ETA in seconds: 353986.634\n",
      "epoch: 953400, train loss: 3.8828248500823976, val loss: 3.972380828857422, ETA in seconds: 353270.529\n",
      "epoch: 953500, train loss: 3.8995402336120604, val loss: 3.970958423614502, ETA in seconds: 352552.204\n",
      "epoch: 953600, train loss: 3.901108169555664, val loss: 3.9738319396972654, ETA in seconds: 351828.282\n",
      "epoch: 953700, train loss: 3.8920461893081666, val loss: 3.965342807769775, ETA in seconds: 351104.366\n",
      "epoch: 953800, train loss: 3.8933153629302977, val loss: 3.973720359802246, ETA in seconds: 350380.745\n",
      "epoch: 953900, train loss: 3.899423623085022, val loss: 3.9712474584579467, ETA in seconds: 349657.799\n",
      "epoch: 954000, train loss: 3.8953811883926392, val loss: 3.97714250087738, ETA in seconds: 348933.397\n",
      "epoch: 954100, train loss: 3.8949658632278443, val loss: 3.9687397480010986, ETA in seconds: 348208.882\n",
      "epoch: 954200, train loss: 3.8842617988586428, val loss: 3.978530693054199, ETA in seconds: 347485.956\n",
      "epoch: 954300, train loss: 3.893716812133789, val loss: 3.96705801486969, ETA in seconds: 346760.690\n",
      "epoch: 954400, train loss: 3.899385595321655, val loss: 3.974445915222168, ETA in seconds: 346037.776\n",
      "epoch: 954500, train loss: 3.8982505559921266, val loss: 3.9632644176483156, ETA in seconds: 345317.391\n",
      "epoch: 954600, train loss: 3.894959259033203, val loss: 3.973368740081787, ETA in seconds: 344592.078\n",
      "epoch: 954700, train loss: 3.8923207759857177, val loss: 3.964255380630493, ETA in seconds: 343866.035\n",
      "epoch: 954800, train loss: 3.8899783849716187, val loss: 3.9680415391921997, ETA in seconds: 343139.650\n",
      "epoch: 954900, train loss: 3.887999415397644, val loss: 3.963881802558899, ETA in seconds: 342414.273\n",
      "epoch: 955000, train loss: 3.908070611953735, val loss: 3.9645686626434324, ETA in seconds: 341688.273\n",
      "epoch: 955100, train loss: 3.898129200935364, val loss: 3.965904474258423, ETA in seconds: 340961.887\n",
      "epoch: 955200, train loss: 3.8934170246124267, val loss: 3.9634457349777223, ETA in seconds: 340235.474\n",
      "epoch: 955300, train loss: 3.8938014030456545, val loss: 3.9579312562942506, ETA in seconds: 339509.544\n",
      "epoch: 955400, train loss: 3.897431182861328, val loss: 3.968922805786133, ETA in seconds: 338782.577\n",
      "epoch: 955500, train loss: 3.8888277292251585, val loss: 3.9889842748641966, ETA in seconds: 338055.333\n",
      "epoch: 955600, train loss: 3.895212769508362, val loss: 3.972476673126221, ETA in seconds: 337327.272\n",
      "epoch: 955700, train loss: 3.8854849100112916, val loss: 3.9635851860046385, ETA in seconds: 336599.454\n",
      "epoch: 955800, train loss: 3.8873674869537354, val loss: 3.9702996492385862, ETA in seconds: 335873.156\n",
      "epoch: 955900, train loss: 3.8899070262908935, val loss: 3.973471975326538, ETA in seconds: 335145.873\n",
      "epoch: 956000, train loss: 3.8934914827346803, val loss: 3.972448801994324, ETA in seconds: 334421.537\n",
      "epoch: 956100, train loss: 3.891944980621338, val loss: 3.960115885734558, ETA in seconds: 333694.199\n",
      "epoch: 956200, train loss: 3.893285536766052, val loss: 3.9837708950042723, ETA in seconds: 332969.044\n",
      "epoch: 956300, train loss: 3.8909207344055177, val loss: 3.9787076711654663, ETA in seconds: 332248.100\n",
      "epoch: 956400, train loss: 3.8952567338943482, val loss: 3.97026641368866, ETA in seconds: 331526.891\n",
      "epoch: 956500, train loss: 3.8898595809936523, val loss: 3.9790975570678713, ETA in seconds: 330805.565\n",
      "epoch: 956600, train loss: 3.885794973373413, val loss: 3.962199068069458, ETA in seconds: 330079.784\n",
      "epoch: 956700, train loss: 3.904359769821167, val loss: 3.9804508686065674, ETA in seconds: 329351.815\n",
      "epoch: 956800, train loss: 3.889181351661682, val loss: 3.9691773653030396, ETA in seconds: 328623.231\n",
      "epoch: 956900, train loss: 3.8902386665344237, val loss: 3.964669919013977, ETA in seconds: 327897.226\n",
      "epoch: 957000, train loss: 3.891700530052185, val loss: 3.963950276374817, ETA in seconds: 327167.652\n",
      "epoch: 957100, train loss: 3.8966655492782594, val loss: 3.970298504829407, ETA in seconds: 326438.073\n",
      "epoch: 957200, train loss: 3.889716124534607, val loss: 3.974159860610962, ETA in seconds: 325708.615\n",
      "epoch: 957300, train loss: 3.8917712688446047, val loss: 3.973069667816162, ETA in seconds: 324978.814\n",
      "epoch: 957400, train loss: 3.8979061126708983, val loss: 3.9638490676879883, ETA in seconds: 324249.245\n",
      "epoch: 957500, train loss: 3.8856242418289186, val loss: 3.980818581581116, ETA in seconds: 323519.267\n",
      "epoch: 957600, train loss: 3.900606608390808, val loss: 3.974640202522278, ETA in seconds: 322789.632\n",
      "epoch: 957700, train loss: 3.890890431404114, val loss: 3.9687880754470823, ETA in seconds: 322059.980\n",
      "epoch: 957800, train loss: 3.8970953226089478, val loss: 3.9713431358337403, ETA in seconds: 321329.845\n",
      "epoch: 957900, train loss: 3.8980544567108155, val loss: 3.9755117654800416, ETA in seconds: 320598.840\n",
      "epoch: 958000, train loss: 3.8935799837112426, val loss: 3.9640325784683226, ETA in seconds: 319868.497\n",
      "epoch: 958100, train loss: 3.9024280548095702, val loss: 3.970358204841614, ETA in seconds: 319138.978\n",
      "epoch: 958200, train loss: 3.888214373588562, val loss: 3.9724347829818725, ETA in seconds: 318408.114\n",
      "epoch: 958300, train loss: 3.889940595626831, val loss: 3.9674679040908813, ETA in seconds: 317677.608\n",
      "epoch: 958400, train loss: 3.8988396406173704, val loss: 3.967481327056885, ETA in seconds: 316947.085\n",
      "epoch: 958500, train loss: 3.890167808532715, val loss: 3.9736591577529907, ETA in seconds: 316216.722\n",
      "epoch: 958600, train loss: 3.899737286567688, val loss: 3.963482666015625, ETA in seconds: 315486.344\n",
      "epoch: 958700, train loss: 3.8800957679748533, val loss: 3.974137020111084, ETA in seconds: 314753.133\n",
      "epoch: 958800, train loss: 3.894391965866089, val loss: 3.9759685516357424, ETA in seconds: 314025.266\n",
      "epoch: 958900, train loss: 3.8919684410095217, val loss: 3.970481538772583, ETA in seconds: 313294.529\n",
      "epoch: 959000, train loss: 3.8917510747909545, val loss: 3.9686166524887083, ETA in seconds: 312560.972\n",
      "epoch: 959100, train loss: 3.898956537246704, val loss: 3.982844376564026, ETA in seconds: 311828.350\n",
      "epoch: 959200, train loss: 3.882174348831177, val loss: 3.9804453372955324, ETA in seconds: 311096.459\n",
      "epoch: 959300, train loss: 3.8928974866867065, val loss: 3.9680718898773195, ETA in seconds: 310364.717\n",
      "epoch: 959400, train loss: 3.8956945180892943, val loss: 3.9635557889938355, ETA in seconds: 309632.005\n",
      "epoch: 959500, train loss: 3.897741961479187, val loss: 3.958163595199585, ETA in seconds: 308899.545\n",
      "epoch: 959600, train loss: 3.892104887962341, val loss: 3.972501039505005, ETA in seconds: 308166.799\n",
      "epoch: 959700, train loss: 3.8903659105300905, val loss: 3.9605728149414063, ETA in seconds: 307433.698\n",
      "epoch: 959800, train loss: 3.8914990425109863, val loss: 3.980465626716614, ETA in seconds: 306700.464\n",
      "epoch: 959900, train loss: 3.8952262878417967, val loss: 3.974134016036987, ETA in seconds: 305966.930\n",
      "epoch: 960000, train loss: 3.8843472003936768, val loss: 3.9696009159088135, ETA in seconds: 305233.336\n",
      "epoch: 960100, train loss: 3.890135717391968, val loss: 3.9834505796432493, ETA in seconds: 304499.600\n",
      "epoch: 960200, train loss: 3.9016483783721925, val loss: 3.971479034423828, ETA in seconds: 303765.919\n",
      "epoch: 960300, train loss: 3.8940176963806152, val loss: 3.963988208770752, ETA in seconds: 303032.363\n",
      "epoch: 960400, train loss: 3.889721488952637, val loss: 3.9683475494384766, ETA in seconds: 302300.258\n",
      "epoch: 960500, train loss: 3.8895242691040037, val loss: 3.965988278388977, ETA in seconds: 301572.128\n",
      "epoch: 960600, train loss: 3.8933740854263306, val loss: 3.9643109798431397, ETA in seconds: 300841.937\n",
      "epoch: 960700, train loss: 3.894730734825134, val loss: 3.9846439123153687, ETA in seconds: 300110.892\n",
      "epoch: 960800, train loss: 3.8993001222610473, val loss: 3.9697516441345213, ETA in seconds: 299383.012\n",
      "epoch: 960900, train loss: 3.890554332733154, val loss: 3.966056966781616, ETA in seconds: 298650.499\n",
      "epoch: 961000, train loss: 3.8993640184402465, val loss: 3.9669418334960938, ETA in seconds: 297919.526\n",
      "epoch: 961100, train loss: 3.8951206684112547, val loss: 3.9640670299530028, ETA in seconds: 297190.410\n",
      "epoch: 961200, train loss: 3.891131353378296, val loss: 3.9567126989364625, ETA in seconds: 296461.047\n",
      "epoch: 961300, train loss: 3.886551356315613, val loss: 3.968159890174866, ETA in seconds: 295731.590\n",
      "epoch: 961400, train loss: 3.8891233921051027, val loss: 3.972011351585388, ETA in seconds: 295001.495\n",
      "epoch: 961500, train loss: 3.8855703830718995, val loss: 3.9734012126922607, ETA in seconds: 294269.498\n",
      "epoch: 961600, train loss: 3.901846694946289, val loss: 3.9645545959472654, ETA in seconds: 293533.624\n",
      "epoch: 961700, train loss: 3.8972058057785035, val loss: 3.984131336212158, ETA in seconds: 292797.528\n",
      "epoch: 961800, train loss: 3.88787043094635, val loss: 3.9631147384643555, ETA in seconds: 292065.264\n",
      "epoch: 961900, train loss: 3.898118829727173, val loss: 3.9713449716567992, ETA in seconds: 291332.969\n",
      "epoch: 962000, train loss: 3.8965884685516357, val loss: 3.9815680265426634, ETA in seconds: 290598.122\n",
      "epoch: 962100, train loss: 3.895786738395691, val loss: 3.974332022666931, ETA in seconds: 289861.668\n",
      "epoch: 962200, train loss: 3.8833969593048097, val loss: 3.9675035953521727, ETA in seconds: 289127.290\n",
      "epoch: 962300, train loss: 3.887022519111633, val loss: 3.973216152191162, ETA in seconds: 288396.331\n",
      "epoch: 962400, train loss: 3.8911927700042725, val loss: 3.9658998966217043, ETA in seconds: 287666.112\n",
      "epoch: 962500, train loss: 3.894449496269226, val loss: 3.9641255855560305, ETA in seconds: 286930.132\n",
      "epoch: 962600, train loss: 3.903506565093994, val loss: 3.9769522190093993, ETA in seconds: 286192.662\n",
      "epoch: 962700, train loss: 3.8952491760253904, val loss: 3.971713638305664, ETA in seconds: 285454.669\n",
      "epoch: 962800, train loss: 3.9022993087768554, val loss: 3.9695857048034666, ETA in seconds: 284718.749\n",
      "epoch: 962900, train loss: 3.8920193195343016, val loss: 3.9788577556610107, ETA in seconds: 283985.743\n",
      "epoch: 963000, train loss: 3.889108943939209, val loss: 3.9745534658432007, ETA in seconds: 283248.561\n",
      "epoch: 963100, train loss: 3.8954137325286866, val loss: 3.9698463439941407, ETA in seconds: 282510.298\n",
      "epoch: 963200, train loss: 3.8887799978256226, val loss: 3.9739603757858277, ETA in seconds: 281772.507\n",
      "epoch: 963300, train loss: 3.88493549823761, val loss: 3.973965620994568, ETA in seconds: 281034.829\n",
      "epoch: 963400, train loss: 3.8984600067138673, val loss: 3.954640102386475, ETA in seconds: 280298.229\n",
      "epoch: 963500, train loss: 3.884680461883545, val loss: 3.97122483253479, ETA in seconds: 279565.642\n",
      "epoch: 963600, train loss: 3.8960089683532715, val loss: 3.9754093408584597, ETA in seconds: 278832.557\n",
      "epoch: 963700, train loss: 3.902291417121887, val loss: 3.969236731529236, ETA in seconds: 278099.078\n",
      "epoch: 963800, train loss: 3.8927181482315065, val loss: 3.954640793800354, ETA in seconds: 277359.843\n",
      "epoch: 963900, train loss: 3.889394497871399, val loss: 3.9818995237350463, ETA in seconds: 276619.757\n",
      "epoch: 964000, train loss: 3.8926040649414064, val loss: 3.972486138343811, ETA in seconds: 275881.804\n",
      "epoch: 964100, train loss: 3.8907886266708376, val loss: 3.966536450386047, ETA in seconds: 275147.726\n",
      "epoch: 964200, train loss: 3.8944448471069335, val loss: 3.9706310510635374, ETA in seconds: 274413.606\n",
      "epoch: 964300, train loss: 3.8978442907333375, val loss: 3.9720550060272215, ETA in seconds: 273679.319\n",
      "epoch: 964400, train loss: 3.892455530166626, val loss: 3.970802879333496, ETA in seconds: 272942.183\n",
      "epoch: 964500, train loss: 3.889198422431946, val loss: 3.9663645029067993, ETA in seconds: 272200.475\n",
      "epoch: 964600, train loss: 3.893612003326416, val loss: 3.9729986429214477, ETA in seconds: 271459.890\n",
      "epoch: 964700, train loss: 3.897566056251526, val loss: 3.973749375343323, ETA in seconds: 270718.474\n",
      "epoch: 964800, train loss: 3.896195960044861, val loss: 3.9647014856338503, ETA in seconds: 269977.648\n",
      "epoch: 964900, train loss: 3.894068884849548, val loss: 3.969176173210144, ETA in seconds: 269237.426\n",
      "epoch: 965000, train loss: 3.8866116523742678, val loss: 3.9690947771072387, ETA in seconds: 268498.444\n",
      "epoch: 965100, train loss: 3.899487614631653, val loss: 3.9818745374679567, ETA in seconds: 267758.620\n",
      "epoch: 965200, train loss: 3.891581392288208, val loss: 3.9754138946533204, ETA in seconds: 267019.749\n",
      "epoch: 965300, train loss: 3.8936794281005858, val loss: 3.9750133752822876, ETA in seconds: 266278.215\n",
      "epoch: 965400, train loss: 3.8943636417388916, val loss: 3.974585509300232, ETA in seconds: 265536.948\n",
      "epoch: 965500, train loss: 3.8936330795288088, val loss: 3.980442762374878, ETA in seconds: 264794.031\n",
      "epoch: 965600, train loss: 3.887592148780823, val loss: 3.9827767610549927, ETA in seconds: 264051.211\n",
      "epoch: 965700, train loss: 3.8904291152954102, val loss: 3.9741546392440794, ETA in seconds: 263307.665\n",
      "epoch: 965800, train loss: 3.896891951560974, val loss: 3.9815914630889893, ETA in seconds: 262564.086\n",
      "epoch: 965900, train loss: 3.896842336654663, val loss: 3.974201464653015, ETA in seconds: 261820.392\n",
      "epoch: 966000, train loss: 3.888013482093811, val loss: 3.9737552404403687, ETA in seconds: 261076.565\n",
      "epoch: 966100, train loss: 3.899881887435913, val loss: 3.971161150932312, ETA in seconds: 260332.521\n",
      "epoch: 966200, train loss: 3.89818434715271, val loss: 3.9656134605407716, ETA in seconds: 259588.276\n",
      "epoch: 966300, train loss: 3.8874601125717163, val loss: 3.9653926849365235, ETA in seconds: 258844.306\n",
      "epoch: 966400, train loss: 3.8892526626586914, val loss: 3.967290163040161, ETA in seconds: 258100.043\n",
      "epoch: 966500, train loss: 3.883710503578186, val loss: 3.978508186340332, ETA in seconds: 257355.684\n",
      "epoch: 966600, train loss: 3.885044240951538, val loss: 3.967372751235962, ETA in seconds: 256610.903\n",
      "epoch: 966700, train loss: 3.894599270820618, val loss: 3.969552779197693, ETA in seconds: 255866.460\n",
      "epoch: 966800, train loss: 3.903071665763855, val loss: 3.969743180274963, ETA in seconds: 255121.517\n",
      "epoch: 966900, train loss: 3.895204496383667, val loss: 3.969022583961487, ETA in seconds: 254376.224\n",
      "epoch: 967000, train loss: 3.898398923873901, val loss: 3.9859875679016112, ETA in seconds: 253631.644\n",
      "epoch: 967100, train loss: 3.891013765335083, val loss: 3.965919327735901, ETA in seconds: 252890.702\n",
      "epoch: 967200, train loss: 3.885745668411255, val loss: 3.9663750886917115, ETA in seconds: 252145.261\n",
      "epoch: 967300, train loss: 3.888852858543396, val loss: 3.969465732574463, ETA in seconds: 251399.897\n",
      "epoch: 967400, train loss: 3.8915894746780397, val loss: 3.9681726455688477, ETA in seconds: 250660.474\n",
      "epoch: 967500, train loss: 3.8808625459671022, val loss: 3.9728062391281127, ETA in seconds: 249914.356\n",
      "epoch: 967600, train loss: 3.888706088066101, val loss: 3.967503476142883, ETA in seconds: 249169.473\n",
      "epoch: 967700, train loss: 3.8976855993270876, val loss: 3.9669272899627686, ETA in seconds: 248423.259\n",
      "epoch: 967800, train loss: 3.9017616271972657, val loss: 3.9746996402740478, ETA in seconds: 247678.575\n",
      "epoch: 967900, train loss: 3.89548020362854, val loss: 3.984211802482605, ETA in seconds: 246933.068\n",
      "epoch: 968000, train loss: 3.8922375202178956, val loss: 3.970421576499939, ETA in seconds: 246187.526\n",
      "epoch: 968100, train loss: 3.89147207736969, val loss: 3.973226618766785, ETA in seconds: 245442.081\n",
      "epoch: 968200, train loss: 3.8882675647735594, val loss: 3.9662919521331785, ETA in seconds: 244696.575\n",
      "epoch: 968300, train loss: 3.8936616659164427, val loss: 3.964679217338562, ETA in seconds: 243950.742\n",
      "epoch: 968400, train loss: 3.8925448417663575, val loss: 3.967818522453308, ETA in seconds: 243204.986\n",
      "epoch: 968500, train loss: 3.9019144773483276, val loss: 3.9720465660095217, ETA in seconds: 242459.705\n",
      "epoch: 968600, train loss: 3.8930187225341797, val loss: 3.9709985971450807, ETA in seconds: 241713.611\n",
      "epoch: 968700, train loss: 3.8953988552093506, val loss: 3.973185133934021, ETA in seconds: 240967.943\n",
      "epoch: 968800, train loss: 3.8963625907897947, val loss: 3.963276672363281, ETA in seconds: 240222.210\n",
      "epoch: 968900, train loss: 3.8873321771621705, val loss: 3.973938155174255, ETA in seconds: 239476.125\n",
      "epoch: 969000, train loss: 3.8913556337356567, val loss: 3.973070240020752, ETA in seconds: 238729.269\n",
      "epoch: 969100, train loss: 3.9048690795898438, val loss: 3.9664799451828, ETA in seconds: 237981.901\n",
      "epoch: 969200, train loss: 3.8771654605865478, val loss: 3.9725573778152468, ETA in seconds: 237235.368\n",
      "epoch: 969300, train loss: 3.9008660078048707, val loss: 3.963144564628601, ETA in seconds: 236487.449\n",
      "epoch: 969400, train loss: 3.890522575378418, val loss: 3.9629687547683714, ETA in seconds: 235740.340\n",
      "epoch: 969500, train loss: 3.8932804346084593, val loss: 3.969422149658203, ETA in seconds: 234997.424\n",
      "epoch: 969600, train loss: 3.89371497631073, val loss: 3.9622023820877077, ETA in seconds: 234251.940\n",
      "epoch: 969700, train loss: 3.8969452381134033, val loss: 3.9776856184005736, ETA in seconds: 233504.802\n",
      "epoch: 969800, train loss: 3.8902862071990967, val loss: 3.974933075904846, ETA in seconds: 232756.300\n",
      "epoch: 969900, train loss: 3.894618201255798, val loss: 3.97197585105896, ETA in seconds: 232010.934\n",
      "epoch: 970000, train loss: 3.88922324180603, val loss: 3.967648649215698, ETA in seconds: 231263.500\n",
      "epoch: 970100, train loss: 3.8934874057769777, val loss: 3.97134747505188, ETA in seconds: 230521.598\n",
      "epoch: 970200, train loss: 3.9005308866500856, val loss: 3.9502937316894533, ETA in seconds: 229774.796\n",
      "epoch: 970300, train loss: 3.8902369022369383, val loss: 3.983121871948242, ETA in seconds: 229028.380\n",
      "epoch: 970400, train loss: 3.8924020528793335, val loss: 3.966118025779724, ETA in seconds: 228284.107\n",
      "epoch: 970500, train loss: 3.901319408416748, val loss: 3.975056838989258, ETA in seconds: 227538.050\n",
      "epoch: 970600, train loss: 3.9021697759628298, val loss: 3.970248889923096, ETA in seconds: 226791.274\n",
      "epoch: 970700, train loss: 3.887983536720276, val loss: 3.9757824897766114, ETA in seconds: 226046.157\n",
      "epoch: 970800, train loss: 3.8874937295913696, val loss: 3.990096926689148, ETA in seconds: 225299.093\n",
      "epoch: 970900, train loss: 3.890921139717102, val loss: 3.9667895555496218, ETA in seconds: 224549.856\n",
      "epoch: 971000, train loss: 3.894385027885437, val loss: 3.9826233625411986, ETA in seconds: 223799.877\n",
      "epoch: 971100, train loss: 3.8894798278808596, val loss: 3.9712878465652466, ETA in seconds: 223049.611\n",
      "epoch: 971200, train loss: 3.888200283050537, val loss: 3.9846659898757935, ETA in seconds: 222303.357\n",
      "epoch: 971300, train loss: 3.8864808559417723, val loss: 3.9764909982681274, ETA in seconds: 221553.347\n",
      "epoch: 971400, train loss: 3.9005546808242797, val loss: 3.9621002197265627, ETA in seconds: 220804.659\n",
      "epoch: 971500, train loss: 3.8900508165359495, val loss: 3.9754422903060913, ETA in seconds: 220055.590\n",
      "epoch: 971600, train loss: 3.8963630199432373, val loss: 3.969449496269226, ETA in seconds: 219305.390\n",
      "epoch: 971700, train loss: 3.8883792638778685, val loss: 3.97146258354187, ETA in seconds: 218554.503\n",
      "epoch: 971800, train loss: 3.8930562257766725, val loss: 3.979049038887024, ETA in seconds: 217803.729\n",
      "epoch: 971900, train loss: 3.89192852973938, val loss: 3.971261477470398, ETA in seconds: 217053.052\n",
      "epoch: 972000, train loss: 3.897640061378479, val loss: 3.9736212491989136, ETA in seconds: 216301.474\n",
      "epoch: 972100, train loss: 3.890799331665039, val loss: 3.9716975688934326, ETA in seconds: 215553.478\n",
      "epoch: 972200, train loss: 3.8838505506515504, val loss: 3.972973871231079, ETA in seconds: 214805.852\n",
      "epoch: 972300, train loss: 3.8984489679336547, val loss: 3.9751636266708372, ETA in seconds: 214058.696\n",
      "epoch: 972400, train loss: 3.8951729774475097, val loss: 3.961018991470337, ETA in seconds: 213306.620\n",
      "epoch: 972500, train loss: 3.8918365955352785, val loss: 3.9732797384262084, ETA in seconds: 212554.463\n",
      "epoch: 972600, train loss: 3.8930511236190797, val loss: 3.9735189199447634, ETA in seconds: 211801.247\n",
      "epoch: 972700, train loss: 3.8972734451293944, val loss: 3.9777695178985595, ETA in seconds: 211048.333\n",
      "epoch: 972800, train loss: 3.8889864921569823, val loss: 3.9754008531570433, ETA in seconds: 210295.257\n",
      "epoch: 972900, train loss: 3.8942660808563234, val loss: 3.9736955165863037, ETA in seconds: 209541.834\n",
      "epoch: 973000, train loss: 3.8905205965042113, val loss: 3.974283456802368, ETA in seconds: 208788.828\n",
      "epoch: 973100, train loss: 3.8899276494979858, val loss: 3.9655789375305175, ETA in seconds: 208035.464\n",
      "epoch: 973200, train loss: 3.8933937549591064, val loss: 3.969692850112915, ETA in seconds: 207282.180\n",
      "epoch: 973300, train loss: 3.887824010848999, val loss: 3.9763415575027468, ETA in seconds: 206528.704\n",
      "epoch: 973400, train loss: 3.8894839763641356, val loss: 3.980886769294739, ETA in seconds: 205775.085\n",
      "epoch: 973500, train loss: 3.893522310256958, val loss: 3.973051333427429, ETA in seconds: 205021.394\n",
      "epoch: 973600, train loss: 3.889535665512085, val loss: 3.9697102546691894, ETA in seconds: 204267.992\n",
      "epoch: 973700, train loss: 3.8931993007659913, val loss: 3.983348846435547, ETA in seconds: 203515.502\n",
      "epoch: 973800, train loss: 3.892979860305786, val loss: 3.9794809341430666, ETA in seconds: 202761.580\n",
      "epoch: 973900, train loss: 3.8989995956420898, val loss: 3.975634527206421, ETA in seconds: 202007.529\n",
      "epoch: 974000, train loss: 3.8934438943862917, val loss: 3.9726154804229736, ETA in seconds: 201253.816\n",
      "epoch: 974100, train loss: 3.8847575426101684, val loss: 3.966855192184448, ETA in seconds: 200499.255\n",
      "epoch: 974200, train loss: 3.894221806526184, val loss: 3.9720885515213014, ETA in seconds: 199744.930\n",
      "epoch: 974300, train loss: 3.8867416381835938, val loss: 3.9798726081848144, ETA in seconds: 198990.551\n",
      "epoch: 974400, train loss: 3.8929171800613402, val loss: 3.9776139497756957, ETA in seconds: 198236.183\n",
      "epoch: 974500, train loss: 3.8983815193176268, val loss: 3.97690896987915, ETA in seconds: 197481.288\n",
      "epoch: 974600, train loss: 3.8987529039382935, val loss: 3.977366638183594, ETA in seconds: 196725.519\n",
      "epoch: 974700, train loss: 3.8865909337997437, val loss: 3.954980802536011, ETA in seconds: 195971.598\n",
      "epoch: 974800, train loss: 3.8938531398773195, val loss: 3.9715449810028076, ETA in seconds: 195215.809\n",
      "epoch: 974900, train loss: 3.8944213151931764, val loss: 3.988438677787781, ETA in seconds: 194459.583\n",
      "epoch: 975000, train loss: 3.8959447383880614, val loss: 3.9617549180984497, ETA in seconds: 193703.577\n",
      "epoch: 975100, train loss: 3.888249659538269, val loss: 3.96464147567749, ETA in seconds: 192946.560\n",
      "epoch: 975200, train loss: 3.8889755487442015, val loss: 3.9724099159240724, ETA in seconds: 192189.294\n",
      "epoch: 975300, train loss: 3.899098205566406, val loss: 3.9787626028060914, ETA in seconds: 191432.145\n",
      "epoch: 975400, train loss: 3.888450765609741, val loss: 3.9787704229354857, ETA in seconds: 190675.356\n",
      "epoch: 975500, train loss: 3.903596591949463, val loss: 3.9742753744125365, ETA in seconds: 189921.324\n",
      "epoch: 975600, train loss: 3.8848790407180784, val loss: 3.9681374549865724, ETA in seconds: 189163.482\n",
      "epoch: 975700, train loss: 3.8955649375915526, val loss: 3.982154297828674, ETA in seconds: 188406.157\n",
      "epoch: 975800, train loss: 3.891297626495361, val loss: 3.97811119556427, ETA in seconds: 187649.263\n",
      "epoch: 975900, train loss: 3.8768280267715456, val loss: 3.972066044807434, ETA in seconds: 186891.629\n",
      "epoch: 976000, train loss: 3.893693232536316, val loss: 3.987696385383606, ETA in seconds: 186133.975\n",
      "epoch: 976100, train loss: 3.899942135810852, val loss: 3.965668034553528, ETA in seconds: 185378.178\n",
      "epoch: 976200, train loss: 3.8930975437164306, val loss: 3.9783106327056883, ETA in seconds: 184619.994\n",
      "epoch: 976300, train loss: 3.895099902153015, val loss: 3.973465156555176, ETA in seconds: 183863.515\n",
      "epoch: 976400, train loss: 3.8952481746673584, val loss: 3.97291362285614, ETA in seconds: 183106.728\n",
      "epoch: 976500, train loss: 3.891154336929321, val loss: 3.9626685619354247, ETA in seconds: 182348.613\n",
      "epoch: 976600, train loss: 3.893686819076538, val loss: 3.9684626340866087, ETA in seconds: 181589.611\n",
      "epoch: 976700, train loss: 3.8847023248672485, val loss: 3.9785555362701417, ETA in seconds: 180831.322\n",
      "epoch: 976800, train loss: 3.908456802368164, val loss: 3.9661309242248537, ETA in seconds: 180072.572\n",
      "epoch: 976900, train loss: 3.8902838945388796, val loss: 3.9617687463760376, ETA in seconds: 179316.125\n",
      "epoch: 977000, train loss: 3.894700455665588, val loss: 3.9704777717590334, ETA in seconds: 178560.104\n",
      "epoch: 977100, train loss: 3.904869723320007, val loss: 3.9753259897232054, ETA in seconds: 177803.107\n",
      "epoch: 977200, train loss: 3.8796935796737673, val loss: 3.9735121011734007, ETA in seconds: 177044.379\n",
      "epoch: 977300, train loss: 3.8941150426864626, val loss: 3.9753548860549928, ETA in seconds: 176284.396\n",
      "epoch: 977400, train loss: 3.8909526824951173, val loss: 3.9838243961334228, ETA in seconds: 175526.695\n",
      "epoch: 977500, train loss: 3.8979997396469117, val loss: 3.983735370635986, ETA in seconds: 174772.185\n",
      "epoch: 977600, train loss: 3.889261984825134, val loss: 3.9808661222457884, ETA in seconds: 174014.497\n",
      "epoch: 977700, train loss: 3.8930012464523314, val loss: 3.9793786287307737, ETA in seconds: 173253.963\n",
      "epoch: 977800, train loss: 3.8871665477752684, val loss: 3.981098484992981, ETA in seconds: 172493.279\n",
      "epoch: 977900, train loss: 3.8938437938690185, val loss: 3.9718279361724855, ETA in seconds: 171732.748\n",
      "epoch: 978000, train loss: 3.9017263650894165, val loss: 3.9732691049575806, ETA in seconds: 170971.362\n",
      "epoch: 978100, train loss: 3.8976550579071043, val loss: 3.974759888648987, ETA in seconds: 170210.459\n",
      "epoch: 978200, train loss: 3.8911774158477783, val loss: 3.972648859024048, ETA in seconds: 169449.370\n",
      "epoch: 978300, train loss: 3.892413520812988, val loss: 3.9754082441329954, ETA in seconds: 168688.199\n",
      "epoch: 978400, train loss: 3.897074055671692, val loss: 3.9712928533554077, ETA in seconds: 167926.990\n",
      "epoch: 978500, train loss: 3.888670802116394, val loss: 3.968514108657837, ETA in seconds: 167166.007\n",
      "epoch: 978600, train loss: 3.8925010204315185, val loss: 3.9801496028900147, ETA in seconds: 166404.694\n",
      "epoch: 978700, train loss: 3.89660439491272, val loss: 3.9740344285964966, ETA in seconds: 165643.218\n",
      "epoch: 978800, train loss: 3.8989317178726197, val loss: 3.9771186113357544, ETA in seconds: 164884.321\n",
      "epoch: 978900, train loss: 3.8932919979095457, val loss: 3.979061722755432, ETA in seconds: 164124.087\n",
      "epoch: 979000, train loss: 3.900066876411438, val loss: 3.9723403692245483, ETA in seconds: 163364.073\n",
      "epoch: 979100, train loss: 3.898321270942688, val loss: 3.979966711997986, ETA in seconds: 162603.899\n",
      "epoch: 979200, train loss: 3.882253909111023, val loss: 3.9768260717391968, ETA in seconds: 161842.825\n",
      "epoch: 979300, train loss: 3.897239351272583, val loss: 3.9761025667190553, ETA in seconds: 161080.393\n",
      "epoch: 979400, train loss: 3.8901705980300902, val loss: 3.9733410835266114, ETA in seconds: 160318.167\n",
      "epoch: 979500, train loss: 3.8961124420166016, val loss: 3.965996837615967, ETA in seconds: 159555.789\n",
      "epoch: 979600, train loss: 3.8928523302078246, val loss: 3.982476258277893, ETA in seconds: 158793.171\n",
      "epoch: 979700, train loss: 3.8943042516708375, val loss: 3.9631827592849733, ETA in seconds: 158030.463\n",
      "epoch: 979800, train loss: 3.892562675476074, val loss: 3.973853039741516, ETA in seconds: 157267.368\n",
      "epoch: 979900, train loss: 3.8868873357772826, val loss: 3.975029969215393, ETA in seconds: 156506.759\n",
      "epoch: 980000, train loss: 3.8928592443466186, val loss: 3.9769877672195433, ETA in seconds: 155745.286\n",
      "epoch: 980100, train loss: 3.886251163482666, val loss: 3.966703176498413, ETA in seconds: 154983.623\n",
      "epoch: 980200, train loss: 3.8873655557632447, val loss: 3.960912561416626, ETA in seconds: 154219.988\n",
      "epoch: 980300, train loss: 3.8904354572296143, val loss: 3.9630581378936767, ETA in seconds: 153455.839\n",
      "epoch: 980400, train loss: 3.8943095207214355, val loss: 3.9625039100646973, ETA in seconds: 152691.395\n",
      "epoch: 980500, train loss: 3.8923166513442995, val loss: 3.9776048183441164, ETA in seconds: 151926.619\n",
      "epoch: 980600, train loss: 3.8889520883560182, val loss: 3.9617022275924683, ETA in seconds: 151162.140\n",
      "epoch: 980700, train loss: 3.894616222381592, val loss: 3.961313796043396, ETA in seconds: 150397.166\n",
      "epoch: 980800, train loss: 3.895028567314148, val loss: 3.962566947937012, ETA in seconds: 149633.922\n",
      "epoch: 980900, train loss: 3.899682950973511, val loss: 3.9615434646606444, ETA in seconds: 148869.194\n",
      "epoch: 981000, train loss: 3.8962921142578124, val loss: 3.9645036458969116, ETA in seconds: 148104.002\n",
      "epoch: 981100, train loss: 3.9044337034225465, val loss: 3.973950958251953, ETA in seconds: 147339.151\n",
      "epoch: 981200, train loss: 3.8896007537841797, val loss: 3.9618191480636598, ETA in seconds: 146575.878\n",
      "epoch: 981300, train loss: 3.900345516204834, val loss: 3.967508840560913, ETA in seconds: 145810.116\n",
      "epoch: 981400, train loss: 3.9009928703308105, val loss: 3.969446134567261, ETA in seconds: 145044.331\n",
      "epoch: 981500, train loss: 3.894511842727661, val loss: 3.9720282554626465, ETA in seconds: 144279.201\n",
      "epoch: 981600, train loss: 3.899109411239624, val loss: 3.9701341867446898, ETA in seconds: 143513.057\n",
      "epoch: 981700, train loss: 3.897483968734741, val loss: 3.962424612045288, ETA in seconds: 142746.881\n",
      "epoch: 981800, train loss: 3.8942556142807008, val loss: 3.9629337549209596, ETA in seconds: 141980.354\n",
      "epoch: 981900, train loss: 3.8967616319656373, val loss: 3.9731408596038817, ETA in seconds: 141213.693\n",
      "epoch: 982000, train loss: 3.895000529289246, val loss: 3.967886734008789, ETA in seconds: 140446.954\n",
      "epoch: 982100, train loss: 3.895968198776245, val loss: 3.9698578834533693, ETA in seconds: 139679.987\n",
      "epoch: 982200, train loss: 3.8981744766235353, val loss: 3.981070303916931, ETA in seconds: 138912.904\n",
      "epoch: 982300, train loss: 3.888020396232605, val loss: 3.9773979425430297, ETA in seconds: 138146.102\n",
      "epoch: 982400, train loss: 3.892769932746887, val loss: 3.9619778633117675, ETA in seconds: 137378.584\n",
      "epoch: 982500, train loss: 3.8936735153198243, val loss: 3.9793848037719726, ETA in seconds: 136610.645\n",
      "epoch: 982600, train loss: 3.8992404460906984, val loss: 3.9687437534332277, ETA in seconds: 135842.739\n",
      "epoch: 982700, train loss: 3.8892240047454836, val loss: 3.959233212471008, ETA in seconds: 135075.870\n",
      "epoch: 982800, train loss: 3.8921608924865723, val loss: 3.9800737142562865, ETA in seconds: 134308.844\n",
      "epoch: 982900, train loss: 3.8936384439468386, val loss: 3.9714232683181763, ETA in seconds: 133541.719\n",
      "epoch: 983000, train loss: 3.890293025970459, val loss: 3.9690284013748167, ETA in seconds: 132773.231\n",
      "epoch: 983100, train loss: 3.893687868118286, val loss: 3.963470721244812, ETA in seconds: 132004.404\n",
      "epoch: 983200, train loss: 3.8943493604660033, val loss: 3.963041067123413, ETA in seconds: 131235.232\n",
      "epoch: 983300, train loss: 3.894524669647217, val loss: 3.974832272529602, ETA in seconds: 130466.198\n",
      "epoch: 983400, train loss: 3.890827441215515, val loss: 3.975579118728638, ETA in seconds: 129696.826\n",
      "epoch: 983500, train loss: 3.8995872020721434, val loss: 3.9610472440719606, ETA in seconds: 128930.510\n",
      "epoch: 983600, train loss: 3.888697552680969, val loss: 3.96172034740448, ETA in seconds: 128161.689\n",
      "epoch: 983700, train loss: 3.902113652229309, val loss: 3.975272798538208, ETA in seconds: 127392.257\n",
      "epoch: 983800, train loss: 3.900536632537842, val loss: 3.9698693990707397, ETA in seconds: 126623.176\n",
      "epoch: 983900, train loss: 3.897368860244751, val loss: 3.978038763999939, ETA in seconds: 125854.113\n",
      "epoch: 984000, train loss: 3.887761354446411, val loss: 3.9704257249832153, ETA in seconds: 125083.964\n",
      "epoch: 984100, train loss: 3.8938151359558106, val loss: 3.9751670122146607, ETA in seconds: 124314.251\n",
      "epoch: 984200, train loss: 3.8991151809692384, val loss: 3.9801140785217286, ETA in seconds: 123544.185\n",
      "epoch: 984300, train loss: 3.8977108716964723, val loss: 3.9702919483184815, ETA in seconds: 122773.542\n",
      "epoch: 984400, train loss: 3.8940173387527466, val loss: 3.972096872329712, ETA in seconds: 122003.115\n",
      "epoch: 984500, train loss: 3.891551160812378, val loss: 3.9656728982925413, ETA in seconds: 121232.283\n",
      "epoch: 984600, train loss: 3.9047929286956786, val loss: 3.9707984209060667, ETA in seconds: 120461.207\n",
      "epoch: 984700, train loss: 3.887349510192871, val loss: 3.972641921043396, ETA in seconds: 119690.200\n",
      "epoch: 984800, train loss: 3.8982919692993163, val loss: 3.966485381126404, ETA in seconds: 118919.229\n",
      "epoch: 984900, train loss: 3.899662232398987, val loss: 3.9685678482055664, ETA in seconds: 118148.061\n",
      "epoch: 985000, train loss: 3.8866603136062623, val loss: 3.9743063926696776, ETA in seconds: 117376.831\n",
      "epoch: 985100, train loss: 3.8965585231781006, val loss: 3.980859327316284, ETA in seconds: 116606.094\n",
      "epoch: 985200, train loss: 3.8918285369873047, val loss: 3.972344756126404, ETA in seconds: 115834.321\n",
      "epoch: 985300, train loss: 3.8925078868865968, val loss: 3.9645089387893675, ETA in seconds: 115062.769\n",
      "epoch: 985400, train loss: 3.8944277286529543, val loss: 3.964045453071594, ETA in seconds: 114290.724\n",
      "epoch: 985500, train loss: 3.894297742843628, val loss: 3.9696640491485597, ETA in seconds: 113518.777\n",
      "epoch: 985600, train loss: 3.887038469314575, val loss: 3.98601438999176, ETA in seconds: 112747.608\n",
      "epoch: 985700, train loss: 3.8987513303756716, val loss: 3.976193881034851, ETA in seconds: 111977.346\n",
      "epoch: 985800, train loss: 3.8983455657958985, val loss: 3.9612632036209106, ETA in seconds: 111207.109\n",
      "epoch: 985900, train loss: 3.8964279174804686, val loss: 3.9819500923156737, ETA in seconds: 110436.752\n",
      "epoch: 986000, train loss: 3.8949416637420655, val loss: 3.9759527444839478, ETA in seconds: 109666.297\n",
      "epoch: 986100, train loss: 3.889584851264954, val loss: 3.9802865982055664, ETA in seconds: 108895.617\n",
      "epoch: 986200, train loss: 3.9045247793197633, val loss: 3.989086651802063, ETA in seconds: 108124.974\n",
      "epoch: 986300, train loss: 3.8979158878326414, val loss: 3.9870906591415407, ETA in seconds: 107354.087\n",
      "epoch: 986400, train loss: 3.8964319467544555, val loss: 3.9863395929336547, ETA in seconds: 106583.075\n",
      "epoch: 986500, train loss: 3.892150330543518, val loss: 3.971318507194519, ETA in seconds: 105811.716\n",
      "epoch: 986600, train loss: 3.889097809791565, val loss: 3.964396071434021, ETA in seconds: 105040.016\n",
      "epoch: 986700, train loss: 3.8900877237319946, val loss: 3.983582043647766, ETA in seconds: 104266.752\n",
      "epoch: 986800, train loss: 3.8905901432037355, val loss: 3.9793291807174684, ETA in seconds: 103493.612\n",
      "epoch: 986900, train loss: 3.885499930381775, val loss: 3.9707873582839968, ETA in seconds: 102719.225\n",
      "epoch: 987000, train loss: 3.8947140693664553, val loss: 3.965828776359558, ETA in seconds: 101944.772\n",
      "epoch: 987100, train loss: 3.8939181566238403, val loss: 3.9813524961471556, ETA in seconds: 101169.964\n",
      "epoch: 987200, train loss: 3.8903149127960206, val loss: 3.969132399559021, ETA in seconds: 100394.789\n",
      "epoch: 987300, train loss: 3.8952821493148804, val loss: 3.969834804534912, ETA in seconds: 99620.051\n",
      "epoch: 987400, train loss: 3.8948250293731688, val loss: 3.9690313816070555, ETA in seconds: 98845.096\n",
      "epoch: 987500, train loss: 3.883229398727417, val loss: 3.9619289875030517, ETA in seconds: 98069.865\n",
      "epoch: 987600, train loss: 3.891830396652222, val loss: 3.97292046546936, ETA in seconds: 97294.650\n",
      "epoch: 987700, train loss: 3.893204188346863, val loss: 3.964642882347107, ETA in seconds: 96519.480\n",
      "epoch: 987800, train loss: 3.894094157218933, val loss: 3.9752370595932005, ETA in seconds: 95744.282\n",
      "epoch: 987900, train loss: 3.8956005334854127, val loss: 3.965324354171753, ETA in seconds: 94969.559\n",
      "epoch: 988000, train loss: 3.8953853845596313, val loss: 3.976645755767822, ETA in seconds: 94194.506\n",
      "epoch: 988100, train loss: 3.8858029365539553, val loss: 3.975173735618591, ETA in seconds: 93418.633\n",
      "epoch: 988200, train loss: 3.8922534465789793, val loss: 3.970189619064331, ETA in seconds: 92642.456\n",
      "epoch: 988300, train loss: 3.8893991708755493, val loss: 3.9725984573364257, ETA in seconds: 91866.132\n",
      "epoch: 988400, train loss: 3.8927073001861574, val loss: 3.985045146942139, ETA in seconds: 91089.880\n",
      "epoch: 988500, train loss: 3.897413635253906, val loss: 3.9681610345840452, ETA in seconds: 90313.150\n",
      "epoch: 988600, train loss: 3.8985618352890015, val loss: 3.974943017959595, ETA in seconds: 89536.293\n",
      "epoch: 988700, train loss: 3.8923218488693236, val loss: 3.974879813194275, ETA in seconds: 88759.297\n",
      "epoch: 988800, train loss: 3.8869704246520995, val loss: 3.9682647943496705, ETA in seconds: 87982.805\n",
      "epoch: 988900, train loss: 3.899963140487671, val loss: 3.9606319665908813, ETA in seconds: 87206.548\n",
      "epoch: 989000, train loss: 3.8877561330795287, val loss: 3.964763641357422, ETA in seconds: 86429.536\n",
      "epoch: 989100, train loss: 3.8998555898666383, val loss: 3.9775411605834963, ETA in seconds: 85652.140\n",
      "epoch: 989200, train loss: 3.894225072860718, val loss: 3.962182116508484, ETA in seconds: 84875.363\n",
      "epoch: 989300, train loss: 3.8980857133865356, val loss: 3.972012972831726, ETA in seconds: 84099.287\n",
      "epoch: 989400, train loss: 3.8857909440994263, val loss: 3.9850719213485717, ETA in seconds: 83323.080\n",
      "epoch: 989500, train loss: 3.889637589454651, val loss: 3.973631262779236, ETA in seconds: 82546.716\n",
      "epoch: 989600, train loss: 3.8906680822372435, val loss: 3.9786508798599245, ETA in seconds: 81769.913\n",
      "epoch: 989700, train loss: 3.883881378173828, val loss: 3.9840739965438843, ETA in seconds: 80992.169\n",
      "epoch: 989800, train loss: 3.9001831293106077, val loss: 3.970106291770935, ETA in seconds: 80213.322\n",
      "epoch: 989900, train loss: 3.8915317058563232, val loss: 3.9690998792648315, ETA in seconds: 79434.354\n",
      "epoch: 990000, train loss: 3.8936474323272705, val loss: 3.971755838394165, ETA in seconds: 78655.237\n",
      "epoch: 990100, train loss: 3.883475661277771, val loss: 3.9711979150772097, ETA in seconds: 77876.051\n",
      "epoch: 990200, train loss: 3.8945471048355103, val loss: 3.9741703510284423, ETA in seconds: 77096.654\n",
      "epoch: 990300, train loss: 3.887930083274841, val loss: 3.9639463663101195, ETA in seconds: 76317.093\n",
      "epoch: 990400, train loss: 3.8924590349197388, val loss: 3.9600144386291505, ETA in seconds: 75537.496\n",
      "epoch: 990500, train loss: 3.8910393953323363, val loss: 3.9646236419677736, ETA in seconds: 74757.711\n",
      "epoch: 990600, train loss: 3.8983293771743774, val loss: 3.974844455718994, ETA in seconds: 73977.851\n",
      "epoch: 990700, train loss: 3.8914072036743166, val loss: 3.9758384227752686, ETA in seconds: 73198.054\n",
      "epoch: 990800, train loss: 3.8903584241867066, val loss: 3.9765027523040772, ETA in seconds: 72419.064\n",
      "epoch: 990900, train loss: 3.8940858840942383, val loss: 3.9790473222732543, ETA in seconds: 71640.516\n",
      "epoch: 991000, train loss: 3.8957722187042236, val loss: 3.970301556587219, ETA in seconds: 70861.262\n",
      "epoch: 991100, train loss: 3.8903892755508425, val loss: 3.9686256885528564, ETA in seconds: 70080.824\n",
      "epoch: 991200, train loss: 3.90181987285614, val loss: 3.9788863897323608, ETA in seconds: 69300.060\n",
      "epoch: 991300, train loss: 3.8874494552612306, val loss: 3.9744565725326537, ETA in seconds: 68520.432\n",
      "epoch: 991400, train loss: 3.900295114517212, val loss: 3.961199402809143, ETA in seconds: 67740.035\n",
      "epoch: 991500, train loss: 3.9003648281097414, val loss: 3.9769894599914553, ETA in seconds: 66960.718\n",
      "epoch: 991600, train loss: 3.8968512296676634, val loss: 3.9736769914627077, ETA in seconds: 66180.982\n",
      "epoch: 991700, train loss: 3.891719126701355, val loss: 3.9822739362716675, ETA in seconds: 65399.627\n",
      "epoch: 991800, train loss: 3.8960750341415404, val loss: 3.9656689167022705, ETA in seconds: 64617.465\n",
      "epoch: 991900, train loss: 3.8953099489212035, val loss: 3.961291766166687, ETA in seconds: 63835.537\n",
      "epoch: 992000, train loss: 3.8965025663375856, val loss: 3.968779730796814, ETA in seconds: 63053.493\n",
      "epoch: 992100, train loss: 3.8937074184417724, val loss: 3.9718497514724733, ETA in seconds: 62271.281\n",
      "epoch: 992200, train loss: 3.8922237396240233, val loss: 3.981375050544739, ETA in seconds: 61488.952\n",
      "epoch: 992300, train loss: 3.9073512077331545, val loss: 3.980810761451721, ETA in seconds: 60706.440\n",
      "epoch: 992400, train loss: 3.887630486488342, val loss: 3.961559772491455, ETA in seconds: 59924.933\n",
      "epoch: 992500, train loss: 3.8908535718917845, val loss: 3.974549651145935, ETA in seconds: 59142.021\n",
      "epoch: 992600, train loss: 3.8859954833984376, val loss: 3.9703927993774415, ETA in seconds: 58359.082\n",
      "epoch: 992700, train loss: 3.894973874092102, val loss: 3.970676398277283, ETA in seconds: 57576.274\n",
      "epoch: 992800, train loss: 3.90144362449646, val loss: 3.971437096595764, ETA in seconds: 56793.135\n",
      "epoch: 992900, train loss: 3.8899190425872803, val loss: 3.9695135831832884, ETA in seconds: 56009.625\n",
      "epoch: 993000, train loss: 3.8925112009048464, val loss: 3.963496708869934, ETA in seconds: 55226.996\n",
      "epoch: 993100, train loss: 3.890912890434265, val loss: 3.9634894132614136, ETA in seconds: 54444.762\n",
      "epoch: 993200, train loss: 3.887673830986023, val loss: 3.9623929262161255, ETA in seconds: 53661.836\n",
      "epoch: 993300, train loss: 3.8921826124191283, val loss: 3.9665934085845946, ETA in seconds: 52878.608\n",
      "epoch: 993400, train loss: 3.8961206912994384, val loss: 3.9626606464385987, ETA in seconds: 52095.244\n",
      "epoch: 993500, train loss: 3.8921621084213256, val loss: 3.967267608642578, ETA in seconds: 51311.678\n",
      "epoch: 993600, train loss: 3.905645656585693, val loss: 3.950561261177063, ETA in seconds: 50528.225\n",
      "epoch: 993700, train loss: 3.904648470878601, val loss: 3.9700188398361207, ETA in seconds: 49744.014\n",
      "epoch: 993800, train loss: 3.892858457565308, val loss: 3.970025730133057, ETA in seconds: 48959.559\n",
      "epoch: 993900, train loss: 3.89633731842041, val loss: 3.9531109809875487, ETA in seconds: 48174.425\n",
      "epoch: 994000, train loss: 3.8896474123001097, val loss: 3.9721821784973144, ETA in seconds: 47389.389\n",
      "epoch: 994100, train loss: 3.9002124309539794, val loss: 3.965281677246094, ETA in seconds: 46604.104\n",
      "epoch: 994200, train loss: 3.887544322013855, val loss: 3.9717309951782225, ETA in seconds: 45818.409\n",
      "epoch: 994300, train loss: 3.8876822233200072, val loss: 3.972823977470398, ETA in seconds: 45032.714\n",
      "epoch: 994400, train loss: 3.889461898803711, val loss: 3.979119801521301, ETA in seconds: 44246.923\n",
      "epoch: 994500, train loss: 3.895495343208313, val loss: 3.966350555419922, ETA in seconds: 43460.925\n",
      "epoch: 994600, train loss: 3.8884102582931517, val loss: 3.961788368225098, ETA in seconds: 42674.780\n",
      "epoch: 994700, train loss: 3.889317274093628, val loss: 3.970247721672058, ETA in seconds: 41888.471\n",
      "epoch: 994800, train loss: 3.8968257904052734, val loss: 3.9689202070236207, ETA in seconds: 41102.010\n",
      "epoch: 994900, train loss: 3.896402382850647, val loss: 3.9848279476165773, ETA in seconds: 40315.418\n",
      "epoch: 995000, train loss: 3.8928433418273927, val loss: 3.9675243139266967, ETA in seconds: 39528.671\n",
      "epoch: 995100, train loss: 3.897295522689819, val loss: 3.9637118339538575, ETA in seconds: 38741.800\n",
      "epoch: 995200, train loss: 3.886393332481384, val loss: 3.9600316524505614, ETA in seconds: 37954.744\n",
      "epoch: 995300, train loss: 3.896768355369568, val loss: 3.971222233772278, ETA in seconds: 37167.555\n",
      "epoch: 995400, train loss: 3.896855616569519, val loss: 3.9713043689727785, ETA in seconds: 36380.231\n",
      "epoch: 995500, train loss: 3.8944734811782835, val loss: 3.9707469940185547, ETA in seconds: 35592.751\n",
      "epoch: 995600, train loss: 3.896902894973755, val loss: 3.9649675130844115, ETA in seconds: 34805.106\n",
      "epoch: 995700, train loss: 3.8959428548812864, val loss: 3.9692596673965452, ETA in seconds: 34017.407\n",
      "epoch: 995800, train loss: 3.88826265335083, val loss: 3.9776987552642824, ETA in seconds: 33229.781\n",
      "epoch: 995900, train loss: 3.893466067314148, val loss: 3.9683262586593626, ETA in seconds: 32442.105\n",
      "epoch: 996000, train loss: 3.890869069099426, val loss: 3.9695852518081667, ETA in seconds: 31653.697\n",
      "epoch: 996100, train loss: 3.8969595670700072, val loss: 3.970677614212036, ETA in seconds: 30865.180\n",
      "epoch: 996200, train loss: 3.899507522583008, val loss: 3.962356209754944, ETA in seconds: 30076.512\n",
      "epoch: 996300, train loss: 3.886751317977905, val loss: 3.967547130584717, ETA in seconds: 29287.744\n",
      "epoch: 996400, train loss: 3.8873026847839354, val loss: 3.9809443235397337, ETA in seconds: 28498.810\n",
      "epoch: 996500, train loss: 3.8919329404830934, val loss: 3.960028624534607, ETA in seconds: 27709.782\n",
      "epoch: 996600, train loss: 3.8925901889801025, val loss: 3.9756083488464355, ETA in seconds: 26920.497\n",
      "epoch: 996700, train loss: 3.8975909471511843, val loss: 3.977914333343506, ETA in seconds: 26131.048\n",
      "epoch: 996800, train loss: 3.890173649787903, val loss: 3.9642037630081175, ETA in seconds: 25341.444\n",
      "epoch: 996900, train loss: 3.8924291849136354, val loss: 3.9728081703186033, ETA in seconds: 24551.732\n",
      "epoch: 997000, train loss: 3.893835687637329, val loss: 3.9821809053421022, ETA in seconds: 23761.909\n",
      "epoch: 997100, train loss: 3.888692116737366, val loss: 3.968447184562683, ETA in seconds: 22971.941\n",
      "epoch: 997200, train loss: 3.901391053199768, val loss: 3.972021746635437, ETA in seconds: 22182.149\n",
      "epoch: 997300, train loss: 3.8905933141708373, val loss: 3.9661564111709593, ETA in seconds: 21391.872\n",
      "epoch: 997400, train loss: 3.8866464614868166, val loss: 3.9755712032318113, ETA in seconds: 20601.418\n",
      "epoch: 997500, train loss: 3.901087236404419, val loss: 3.974114274978638, ETA in seconds: 19810.828\n",
      "epoch: 997600, train loss: 3.8818896770477296, val loss: 3.973709774017334, ETA in seconds: 19020.142\n",
      "epoch: 997700, train loss: 3.8893548250198364, val loss: 3.957663369178772, ETA in seconds: 18229.264\n",
      "epoch: 997800, train loss: 3.8900819063186645, val loss: 3.974856448173523, ETA in seconds: 17438.223\n",
      "epoch: 997900, train loss: 3.8933259010314942, val loss: 3.9735755205154417, ETA in seconds: 16647.055\n",
      "epoch: 998000, train loss: 3.880630111694336, val loss: 3.9616374492645265, ETA in seconds: 15855.783\n",
      "epoch: 998100, train loss: 3.8925410747528075, val loss: 3.9662217140197753, ETA in seconds: 15064.418\n",
      "epoch: 998200, train loss: 3.89358594417572, val loss: 3.9710925102233885, ETA in seconds: 14272.988\n",
      "epoch: 998300, train loss: 3.891966128349304, val loss: 3.978833532333374, ETA in seconds: 13481.673\n",
      "epoch: 998400, train loss: 3.897591805458069, val loss: 3.978075695037842, ETA in seconds: 12689.890\n",
      "epoch: 998500, train loss: 3.8858985185623167, val loss: 3.9780616760253906, ETA in seconds: 11897.831\n",
      "epoch: 998600, train loss: 3.8831791162490843, val loss: 3.9791750431060793, ETA in seconds: 11105.630\n",
      "epoch: 998700, train loss: 3.9008260488510134, val loss: 3.956865644454956, ETA in seconds: 10313.306\n",
      "epoch: 998800, train loss: 3.8911858558654786, val loss: 3.9862548828125, ETA in seconds: 9520.837\n",
      "epoch: 998900, train loss: 3.8932849168777466, val loss: 3.9702494144439697, ETA in seconds: 8728.208\n",
      "epoch: 999000, train loss: 3.9010794878005983, val loss: 3.9768630266189575, ETA in seconds: 7935.434\n",
      "epoch: 999100, train loss: 3.8965424060821534, val loss: 3.961182689666748, ETA in seconds: 7142.525\n",
      "epoch: 999200, train loss: 3.8935122728347777, val loss: 3.967478036880493, ETA in seconds: 6349.472\n",
      "epoch: 999300, train loss: 3.8956215381622314, val loss: 3.9597736835479735, ETA in seconds: 5556.370\n",
      "epoch: 999400, train loss: 3.905449938774109, val loss: 3.969739532470703, ETA in seconds: 4763.125\n",
      "epoch: 999500, train loss: 3.9025205612182616, val loss: 3.970490574836731, ETA in seconds: 3969.712\n",
      "epoch: 999600, train loss: 3.890209102630615, val loss: 3.979090166091919, ETA in seconds: 3176.127\n",
      "epoch: 999700, train loss: 3.88124737739563, val loss: 3.9738786935806276, ETA in seconds: 2382.333\n",
      "epoch: 999800, train loss: 3.898102879524231, val loss: 3.976711130142212, ETA in seconds: 1588.370\n",
      "epoch: 999900, train loss: 3.8994508266448973, val loss: 3.967593789100647, ETA in seconds: 794.263\n",
      "validation loss:  3.967593789100647\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, scheduler=None, print_logs=False, use_cuda=False):\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(EPOCH):\n",
    "        xs,ys = get_batches(dataset,\"train\",batch_size=BATCH_SIZE,context_window=CONTEXT_SIZE)\n",
    "        if use_cuda:\n",
    "            xs = xs.cuda()\n",
    "            ys = ys.cuda()\n",
    "        logits, loss = model(xs,targets=ys)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % LOG_INTERVAL == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model,use_cuda=use_cuda)\n",
    "            losses.append(x)\n",
    "            if print_logs:\n",
    "                print(f\"epoch: {epoch}, train loss: {x['train']}, val loss: {x['val']}, ETA in seconds: {batch_time*(EPOCH-epoch)/LOG_INTERVAL:.3f}\")\n",
    "\n",
    "    print(\"validation loss: \",losses[-1][\"val\"])\n",
    "    return losses\n",
    "\n",
    "model = SimpleModel(len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "losses = train(model,optimizer,print_logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ay as well\\nStrike at the heaven ',\n",
       "  'y as well\\nStrike at the heaven w',\n",
       "  't tt te ll\\ne  t  tt t e te te :t'),\n",
       " ('icians of you. For your wants,\\nY',\n",
       "  'cians of you. For your wants,\\nYo',\n",
       "  'tott: tuot us\\ntiu t us tet:   \\no'),\n",
       " (' charitable care\\nHave the patric',\n",
       "  'charitable care\\nHave the patrici',\n",
       "  'toet t tel tot  \\nete t e tet  to'),\n",
       " ('ll\\nStrike at the heaven with you',\n",
       "  'l\\nStrike at the heaven with your',\n",
       "  'll\\ne  t  tt t e te te :tet et us'),\n",
       " ('t them\\nAgainst the Roman state, ',\n",
       "  ' them\\nAgainst the Roman state, w',\n",
       "  ' t e e\\nlott:  t e teuet:t  t   t'),\n",
       " ('f you. For your wants,\\nYour suff',\n",
       "  ' you. For your wants,\\nYour suffe',\n",
       "  'ot us\\ntiu t us tet:   \\nous t soo'),\n",
       " ('hose course will on\\nThe way it t',\n",
       "  'ose course will on\\nThe way it ta',\n",
       "  'eu  tous   tetlltu:\\nhe tet tt t '),\n",
       " ('st the Roman state, whose course',\n",
       "  't the Roman state, whose course ',\n",
       "  '  t e teuet:t  t   teeu  tous   '),\n",
       " ('ants,\\nYour suffering in this dea',\n",
       "  'nts,\\nYour suffering in this dear',\n",
       "  't:   \\nous t soo  t:ott:t et t  t'),\n",
       " ('s\\nOf more strong link asunder th',\n",
       "  '\\nOf more strong link asunder tha',\n",
       "  ' \\nuoteu  t   u:otlt: tt s:   t e'),\n",
       " (' on\\nThe way it takes, cracking t',\n",
       "  'on\\nThe way it takes, cracking te',\n",
       "  'tu:\\nhe tet tt t t    to to t:ot '),\n",
       " ('suffering in this dearth, you ma',\n",
       "  'uffering in this dearth, you may',\n",
       "  ' soo  t:ott:t et t  t  e t ustet'),\n",
       " ('s\\nOf more strong link asunder th',\n",
       "  '\\nOf more strong link asunder tha',\n",
       "  ' \\nuoteu  t   u:otlt: tt s:   t e'),\n",
       " ('nst the Roman state, whose cours',\n",
       "  'st the Roman state, whose course',\n",
       "  ':  t e teuet:t  t   teeu  tous  '),\n",
       " ('he patricians of you. For your w',\n",
       "  'e patricians of you. For your wa',\n",
       "  'e tet  tott: tuot us\\ntiu t us te'),\n",
       " (' patricians of you. For your wan',\n",
       "  'patricians of you. For your want',\n",
       "  'tet  tott: tuot us\\ntiu t us tet:')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs,ys = get_batches(dataset,\"test\",batch_size=BATCH_SIZE,context_window=CONTEXT_SIZE)\n",
    "evaluate_text(model,xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it be done: away, away!\\n\\nSecond ',\n",
       "  't be done: away, away!\\n\\nSecond C',\n",
       "  't te t u: \\nttet  ttet \\n\\n\\ne ou: t'),\n",
       " ('tizen:\\nConsider you what service',\n",
       "  'izen:\\nConsider you what services',\n",
       "  ' te :\\n\\niu: t   t usteet t   eto '),\n",
       " ('cians good.\\nWhat authority surfe',\n",
       "  'ians good.\\nWhat authority surfei',\n",
       "  'ott: touu \\n\\nhet tts eu t  t s o '),\n",
       " ('n no way say he is covetous.\\n\\nFi',\n",
       "  ' no way say he is covetous.\\n\\nFir',\n",
       "  ':t:utet t t te tt toue  us \\n\\n\\nit'),\n",
       " ('er, hear me speak.\\n\\nAll:\\nSpeak, ',\n",
       "  'r, hear me speak.\\n\\nAll:\\nSpeak, s',\n",
       "  '   te t te t e t \\n\\n\\nlll\\n\\nee t  t'),\n",
       " (' till the altitude of his virtue',\n",
       "  'till the altitude of his virtue.',\n",
       "  't tllt e ttl t s  tuotet tet  s '),\n",
       " (' loved\\nthe people.\\n\\nFirst Citize',\n",
       "  'loved\\nthe people.\\n\\nFirst Citizen',\n",
       "  'tlue  \\n e te uel \\n\\n\\nit   tit te '),\n",
       " ('himself with being proud.\\n\\nSecon',\n",
       "  'imself with being proud.\\n\\nSecond',\n",
       "  'ete  lotet ete t:ote us \\n\\n\\ne ou:'),\n",
       " ('e? to the Capitol!\\n\\nAll:\\nCome, c',\n",
       "  '? to the Capitol!\\n\\nAll:\\nCome, co',\n",
       "  ' \\nt ut e titet ul\\n\\n\\nlll\\n\\niue  to'),\n",
       " (' he did\\nit to that end: though s',\n",
       "  'he did\\nit to that end: though so',\n",
       "  'te t t \\nt t ut et t : \\nt eusoet '),\n",
       " ('nd: though soft-conscienced men ',\n",
       "  'd: though soft-conscienced men c',\n",
       "  ': \\nt eusoet uo cou: ot :o  te :t'),\n",
       " ('to particularise their abundance',\n",
       "  'o particularise their abundance;',\n",
       "  ' utet  toslt t  t e t ttes: t:o '),\n",
       " ('is in hunger for bread, not in t',\n",
       "  's in hunger for bread, not in th',\n",
       "  't tt:tes:o  tou te  t  t:u tt:t '),\n",
       " ('s that\\nafflicts us, the object o',\n",
       "  ' that\\nafflicts us, the object of',\n",
       "  ' t et \\ntoolto  ts  t e tuee o tu'),\n",
       " ('covetous.\\n\\nFirst Citizen:\\nIf I m',\n",
       "  'ovetous.\\n\\nFirst Citizen:\\nIf I mu',\n",
       "  'oue  us \\n\\n\\nit   tit te :\\n\\n ot te'),\n",
       " ('First Citizen:\\nSoft! who comes h',\n",
       "  'irst Citizen:\\nSoft! who comes he',\n",
       "  'it   tit te :\\n\\neuo \\nteeutoue  te')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs,ys = get_batches(dataset,\"train\",batch_size=BATCH_SIZE,context_window=CONTEXT_SIZE)\n",
    "evaluate_text(model,xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d rather to die than to famish?\n",
      "\n",
      "d rather to die than to famish?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train,_ = get_batches(dataset,\"train\",batch_size=1,context_window=CONTEXT_SIZE)\n",
    "x_val,_ = get_batches(dataset,\"val\",batch_size=1,context_window=CONTEXT_SIZE)\n",
    "\n",
    "print(decode(x_train[0].tolist()))\n",
    "#print(decode(x_val[0].tolist()))\n",
    "pred_sentence = model.generate(x_train,max_new_tokens=100)\n",
    "print(decode(pred_sentence.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import PositionalEmbedding, GPTBlocks\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, vocab_size: int, num_blocks: int,\n",
    "                 embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len (int): 入力系列の最大長\n",
    "            vocab_size (int): 語彙数\n",
    "            embed_dim (int): 埋め込み次元数\n",
    "            num_blocks (int): TransformerBlockの数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            ff_hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super(GPT,self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(max_seq_len,embed_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlocks(embed_dim=embed_dim,num_heads=num_heads,ff_hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.head = nn.Linear(embed_dim,vocab_size) # embeddingの逆行列を使う方法もある\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,x,targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): 入力トークン (batch_size,seq_len)\n",
    "            target (torch.Tensor): 教師トークン (batch_size,seq_len)\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: GPTの出力 (batch_size,seq_len,vocab_size), 損失 (1,)\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = nn.functional.cross_entropy(x.view(-1,x.size(-1)),targets.view(-1))\n",
    "            return x,loss\n",
    "        else:\n",
    "            return x,None\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self,idx,max_new_tokens,temperature=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (list[int]): 入力トークンのリスト\n",
    "            max_new_tokens (int): 生成するトークン数\n",
    "            temperature (float): softmaxの温度\n",
    "        Returns:\n",
    "            list[int]: 生成されたトークンのリスト\n",
    "        TODO:\n",
    "            ちゃんと実装する\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            if idx.size(1) > self.max_seq_len:\n",
    "                idx = idx[:,-self.max_seq_len:]\n",
    "            x = idx.clone().detach().view(1,-1)\n",
    "            y,_ = self(x)\n",
    "            idx_next = y.argmax(-1)[:,-1]\n",
    "            idx_next = idx_next.unsqueeze(-1).clone().detach()\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 4.04306697845459, val loss: 4.06845440864563, ETA in seconds: 58.410\n",
      "epoch: 100, train loss: 4.0549702644348145, val loss: 4.0676655769348145, ETA in seconds: 5539.733\n",
      "epoch: 200, train loss: 4.051649951934815, val loss: 4.071181201934815, ETA in seconds: 10301.415\n",
      "epoch: 300, train loss: 4.055556201934815, val loss: 4.071767139434814, ETA in seconds: 15021.506\n",
      "epoch: 400, train loss: 4.052235889434814, val loss: 4.065321826934815, ETA in seconds: 20034.012\n",
      "epoch: 500, train loss: 4.051845264434815, val loss: 4.072939014434814, ETA in seconds: 25643.219\n",
      "epoch: 600, train loss: 4.046767139434815, val loss: 4.070009326934814, ETA in seconds: 30995.998\n",
      "epoch: 700, train loss: 4.053603076934815, val loss: 4.0686421394348145, ETA in seconds: 37018.503\n",
      "epoch: 800, train loss: 4.0598530769348145, val loss: 4.062001514434814, ETA in seconds: 42383.409\n",
      "epoch: 900, train loss: 4.0500874519348145, val loss: 4.068056201934814, ETA in seconds: 47831.771\n",
      "epoch: 1000, train loss: 4.048329639434814, val loss: 4.070985889434814, ETA in seconds: 53192.044\n",
      "epoch: 1100, train loss: 4.051845264434815, val loss: 4.067079639434814, ETA in seconds: 58656.251\n",
      "epoch: 1200, train loss: 4.0471577644348145, val loss: 4.063173389434814, ETA in seconds: 64045.206\n",
      "epoch: 1300, train loss: 4.052431201934814, val loss: 4.059657764434815, ETA in seconds: 69401.314\n",
      "epoch: 1400, train loss: 4.053407764434814, val loss: 4.0715718269348145, ETA in seconds: 74819.460\n",
      "epoch: 1500, train loss: 4.056532764434815, val loss: 4.064149951934814, ETA in seconds: 80452.172\n",
      "epoch: 1600, train loss: 4.0481343269348145, val loss: 4.068251514434815, ETA in seconds: 85590.058\n",
      "epoch: 1700, train loss: 4.048524951934814, val loss: 4.062392139434815, ETA in seconds: 90395.699\n",
      "epoch: 1800, train loss: 4.051649951934815, val loss: 4.0696187019348145, ETA in seconds: 95128.510\n",
      "epoch: 1900, train loss: 4.0549702644348145, val loss: 4.064931201934814, ETA in seconds: 99661.369\n",
      "epoch: 2000, train loss: 4.056532764434815, val loss: 4.068446826934815, ETA in seconds: 104291.767\n",
      "epoch: 2100, train loss: 4.047743701934815, val loss: 4.067470264434815, ETA in seconds: 108901.456\n",
      "epoch: 2200, train loss: 4.046962451934815, val loss: 4.070985889434814, ETA in seconds: 113614.945\n",
      "epoch: 2300, train loss: 4.0549702644348145, val loss: 4.064149951934814, ETA in seconds: 118098.683\n",
      "epoch: 2400, train loss: 4.0491108894348145, val loss: 4.069032764434814, ETA in seconds: 122677.423\n",
      "epoch: 2500, train loss: 4.0569233894348145, val loss: 4.066884326934814, ETA in seconds: 127297.036\n",
      "epoch: 2600, train loss: 4.044814014434815, val loss: 4.063173389434814, ETA in seconds: 132060.906\n",
      "epoch: 2700, train loss: 4.048329639434814, val loss: 4.064149951934814, ETA in seconds: 137090.349\n",
      "epoch: 2800, train loss: 4.049696826934815, val loss: 4.062196826934814, ETA in seconds: 142107.374\n",
      "epoch: 2900, train loss: 4.047548389434814, val loss: 4.066298389434815, ETA in seconds: 147005.948\n",
      "epoch: 3000, train loss: 4.052235889434814, val loss: 4.062587451934815, ETA in seconds: 152124.966\n",
      "epoch: 3100, train loss: 4.051259326934814, val loss: 4.066493701934815, ETA in seconds: 156723.623\n",
      "epoch: 3200, train loss: 4.0559468269348145, val loss: 4.067860889434814, ETA in seconds: 161604.801\n",
      "epoch: 3300, train loss: 4.052235889434814, val loss: 4.0666890144348145, ETA in seconds: 166686.342\n",
      "epoch: 3400, train loss: 4.050478076934814, val loss: 4.065907764434814, ETA in seconds: 172055.214\n",
      "epoch: 3500, train loss: 4.045595264434814, val loss: 4.065907764434814, ETA in seconds: 177331.840\n",
      "epoch: 3600, train loss: 4.051454639434814, val loss: 4.069423389434815, ETA in seconds: 182645.631\n",
      "epoch: 3700, train loss: 4.052821826934815, val loss: 4.068251514434815, ETA in seconds: 187989.445\n",
      "epoch: 3800, train loss: 4.0559468269348145, val loss: 4.065907764434814, ETA in seconds: 193615.317\n",
      "epoch: 3900, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 198843.908\n",
      "epoch: 4000, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 204119.853\n",
      "epoch: 4100, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 209773.613\n",
      "epoch: 4200, train loss: 4.048524951934814, val loss: 4.068837451934814, ETA in seconds: 215315.506\n",
      "epoch: 4300, train loss: 4.051259326934814, val loss: 4.071376514434815, ETA in seconds: 220968.316\n",
      "epoch: 4400, train loss: 4.0520405769348145, val loss: 4.064345264434815, ETA in seconds: 226735.444\n",
      "epoch: 4500, train loss: 4.048524951934814, val loss: 4.0647358894348145, ETA in seconds: 232322.727\n",
      "epoch: 4600, train loss: 4.0481343269348145, val loss: 4.066493701934815, ETA in seconds: 237905.865\n",
      "epoch: 4700, train loss: 4.0481343269348145, val loss: 4.056532764434815, ETA in seconds: 243286.056\n",
      "epoch: 4800, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 248936.917\n",
      "epoch: 4900, train loss: 4.0510640144348145, val loss: 4.074696826934814, ETA in seconds: 254796.311\n",
      "epoch: 5000, train loss: 4.050673389434815, val loss: 4.065126514434814, ETA in seconds: 260749.264\n",
      "epoch: 5100, train loss: 4.055360889434814, val loss: 4.065126514434814, ETA in seconds: 266524.101\n",
      "epoch: 5200, train loss: 4.0539937019348145, val loss: 4.067274951934815, ETA in seconds: 271986.891\n",
      "epoch: 5300, train loss: 4.056337451934814, val loss: 4.061415576934815, ETA in seconds: 277249.510\n",
      "epoch: 5400, train loss: 4.045595264434814, val loss: 4.066493701934815, ETA in seconds: 282576.442\n",
      "epoch: 5500, train loss: 4.053407764434814, val loss: 4.0637593269348145, ETA in seconds: 287947.547\n",
      "epoch: 5600, train loss: 4.049306201934814, val loss: 4.064345264434815, ETA in seconds: 293487.578\n",
      "epoch: 5700, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 299064.481\n",
      "epoch: 5800, train loss: 4.048329639434814, val loss: 4.062978076934814, ETA in seconds: 304644.759\n",
      "epoch: 5900, train loss: 4.0598530769348145, val loss: 4.072353076934815, ETA in seconds: 310141.875\n",
      "epoch: 6000, train loss: 4.0500874519348145, val loss: 4.0627827644348145, ETA in seconds: 315671.623\n",
      "epoch: 6100, train loss: 4.044814014434815, val loss: 4.063173389434814, ETA in seconds: 321308.860\n",
      "epoch: 6200, train loss: 4.051649951934815, val loss: 4.0627827644348145, ETA in seconds: 326818.930\n",
      "epoch: 6300, train loss: 4.0471577644348145, val loss: 4.071767139434814, ETA in seconds: 332177.760\n",
      "epoch: 6400, train loss: 4.0510640144348145, val loss: 4.061024951934814, ETA in seconds: 337807.306\n",
      "epoch: 6500, train loss: 4.053798389434815, val loss: 4.0637593269348145, ETA in seconds: 343286.608\n",
      "epoch: 6600, train loss: 4.0530171394348145, val loss: 4.069032764434814, ETA in seconds: 348757.359\n",
      "epoch: 6700, train loss: 4.0559468269348145, val loss: 4.0657124519348145, ETA in seconds: 354980.902\n",
      "epoch: 6800, train loss: 4.051259326934814, val loss: 4.065321826934815, ETA in seconds: 360485.003\n",
      "epoch: 6900, train loss: 4.054774951934815, val loss: 4.068446826934815, ETA in seconds: 365847.616\n",
      "epoch: 7000, train loss: 4.046767139434815, val loss: 4.066884326934814, ETA in seconds: 371274.756\n",
      "epoch: 7100, train loss: 4.050282764434814, val loss: 4.065321826934815, ETA in seconds: 376743.952\n",
      "epoch: 7200, train loss: 4.050673389434815, val loss: 4.071767139434814, ETA in seconds: 382547.910\n",
      "epoch: 7300, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 388641.871\n",
      "epoch: 7400, train loss: 4.0500874519348145, val loss: 4.069814014434814, ETA in seconds: 393785.019\n",
      "epoch: 7500, train loss: 4.0461812019348145, val loss: 4.070204639434815, ETA in seconds: 399155.876\n",
      "epoch: 7600, train loss: 4.054189014434814, val loss: 4.069032764434814, ETA in seconds: 404679.245\n",
      "epoch: 7700, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 410077.145\n",
      "epoch: 7800, train loss: 4.050673389434815, val loss: 4.062978076934814, ETA in seconds: 415409.513\n",
      "epoch: 7900, train loss: 4.046962451934815, val loss: 4.0696187019348145, ETA in seconds: 420818.816\n",
      "epoch: 8000, train loss: 4.051259326934814, val loss: 4.063368701934815, ETA in seconds: 426243.224\n",
      "epoch: 8100, train loss: 4.053798389434815, val loss: 4.0637593269348145, ETA in seconds: 431669.249\n",
      "epoch: 8200, train loss: 4.055751514434815, val loss: 4.0618062019348145, ETA in seconds: 437026.947\n",
      "epoch: 8300, train loss: 4.049501514434814, val loss: 4.0686421394348145, ETA in seconds: 442372.585\n",
      "epoch: 8400, train loss: 4.055556201934815, val loss: 4.064345264434815, ETA in seconds: 447737.655\n",
      "epoch: 8500, train loss: 4.0559468269348145, val loss: 4.066298389434815, ETA in seconds: 452965.427\n",
      "epoch: 8600, train loss: 4.053407764434814, val loss: 4.066103076934814, ETA in seconds: 458226.175\n",
      "epoch: 8700, train loss: 4.0539937019348145, val loss: 4.069423389434815, ETA in seconds: 463243.821\n",
      "epoch: 8800, train loss: 4.047353076934814, val loss: 4.065126514434814, ETA in seconds: 468479.125\n",
      "epoch: 8900, train loss: 4.051259326934814, val loss: 4.0676655769348145, ETA in seconds: 473859.711\n",
      "epoch: 9000, train loss: 4.0500874519348145, val loss: 4.0627827644348145, ETA in seconds: 479214.941\n",
      "epoch: 9100, train loss: 4.052821826934815, val loss: 4.0686421394348145, ETA in seconds: 484691.612\n",
      "epoch: 9200, train loss: 4.057509326934815, val loss: 4.062196826934814, ETA in seconds: 490140.859\n",
      "epoch: 9300, train loss: 4.050478076934814, val loss: 4.065126514434814, ETA in seconds: 495591.431\n",
      "epoch: 9400, train loss: 4.053798389434815, val loss: 4.072743701934814, ETA in seconds: 501020.000\n",
      "epoch: 9500, train loss: 4.057314014434814, val loss: 4.0618062019348145, ETA in seconds: 506321.145\n",
      "epoch: 9600, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 511743.171\n",
      "epoch: 9700, train loss: 4.046571826934814, val loss: 4.065321826934815, ETA in seconds: 516903.168\n",
      "epoch: 9800, train loss: 4.054189014434814, val loss: 4.062978076934814, ETA in seconds: 522115.741\n",
      "epoch: 9900, train loss: 4.053603076934815, val loss: 4.063368701934815, ETA in seconds: 527338.876\n",
      "epoch: 10000, train loss: 4.045009326934815, val loss: 4.060439014434815, ETA in seconds: 532530.906\n",
      "epoch: 10100, train loss: 4.044618701934814, val loss: 4.071376514434815, ETA in seconds: 537757.927\n",
      "epoch: 10200, train loss: 4.054189014434814, val loss: 4.0696187019348145, ETA in seconds: 542996.539\n",
      "epoch: 10300, train loss: 4.055751514434815, val loss: 4.066103076934814, ETA in seconds: 548317.907\n",
      "epoch: 10400, train loss: 4.050282764434814, val loss: 4.067079639434814, ETA in seconds: 553516.593\n",
      "epoch: 10500, train loss: 4.046571826934814, val loss: 4.070204639434815, ETA in seconds: 558634.051\n",
      "epoch: 10600, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 564064.054\n",
      "epoch: 10700, train loss: 4.053798389434815, val loss: 4.068446826934815, ETA in seconds: 569220.050\n",
      "epoch: 10800, train loss: 4.0471577644348145, val loss: 4.0676655769348145, ETA in seconds: 574243.698\n",
      "epoch: 10900, train loss: 4.057509326934815, val loss: 4.065321826934815, ETA in seconds: 579546.579\n",
      "epoch: 11000, train loss: 4.055360889434814, val loss: 4.067274951934815, ETA in seconds: 584900.314\n",
      "epoch: 11100, train loss: 4.056337451934814, val loss: 4.067470264434815, ETA in seconds: 590053.270\n",
      "epoch: 11200, train loss: 4.052626514434815, val loss: 4.071962451934814, ETA in seconds: 595004.690\n",
      "epoch: 11300, train loss: 4.0539937019348145, val loss: 4.065517139434815, ETA in seconds: 600120.612\n",
      "epoch: 11400, train loss: 4.057314014434814, val loss: 4.065321826934815, ETA in seconds: 604757.976\n",
      "epoch: 11500, train loss: 4.047939014434815, val loss: 4.067079639434814, ETA in seconds: 609567.051\n",
      "epoch: 11600, train loss: 4.056337451934814, val loss: 4.069228076934815, ETA in seconds: 614562.997\n",
      "epoch: 11700, train loss: 4.054579639434815, val loss: 4.063368701934815, ETA in seconds: 619990.397\n",
      "epoch: 11800, train loss: 4.050478076934814, val loss: 4.066493701934815, ETA in seconds: 625304.371\n",
      "epoch: 11900, train loss: 4.056532764434815, val loss: 4.0705952644348145, ETA in seconds: 630618.586\n",
      "epoch: 12000, train loss: 4.0598530769348145, val loss: 4.064931201934814, ETA in seconds: 635940.465\n",
      "epoch: 12100, train loss: 4.053407764434814, val loss: 4.067470264434815, ETA in seconds: 641435.620\n",
      "epoch: 12200, train loss: 4.049892139434815, val loss: 4.067274951934815, ETA in seconds: 646605.317\n",
      "epoch: 12300, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 652086.009\n",
      "epoch: 12400, train loss: 4.0500874519348145, val loss: 4.064540576934815, ETA in seconds: 657440.205\n",
      "epoch: 12500, train loss: 4.048329639434814, val loss: 4.063368701934815, ETA in seconds: 662780.268\n",
      "epoch: 12600, train loss: 4.053603076934815, val loss: 4.0725483894348145, ETA in seconds: 667854.307\n",
      "epoch: 12700, train loss: 4.058485889434815, val loss: 4.070204639434815, ETA in seconds: 673088.750\n",
      "epoch: 12800, train loss: 4.057314014434814, val loss: 4.066884326934814, ETA in seconds: 678339.531\n",
      "epoch: 12900, train loss: 4.052235889434814, val loss: 4.0705952644348145, ETA in seconds: 683643.013\n",
      "epoch: 13000, train loss: 4.054189014434814, val loss: 4.070204639434815, ETA in seconds: 688980.779\n",
      "epoch: 13100, train loss: 4.054384326934814, val loss: 4.065126514434814, ETA in seconds: 694225.833\n",
      "epoch: 13200, train loss: 4.053212451934814, val loss: 4.068446826934815, ETA in seconds: 699663.321\n",
      "epoch: 13300, train loss: 4.054384326934814, val loss: 4.0647358894348145, ETA in seconds: 705148.601\n",
      "epoch: 13400, train loss: 4.0539937019348145, val loss: 4.072743701934814, ETA in seconds: 710576.422\n",
      "epoch: 13500, train loss: 4.0530171394348145, val loss: 4.069032764434814, ETA in seconds: 716371.003\n",
      "epoch: 13600, train loss: 4.052431201934814, val loss: 4.064931201934814, ETA in seconds: 722094.884\n",
      "epoch: 13700, train loss: 4.050868701934815, val loss: 4.067860889434814, ETA in seconds: 727723.380\n",
      "epoch: 13800, train loss: 4.0500874519348145, val loss: 4.070009326934814, ETA in seconds: 733275.711\n",
      "epoch: 13900, train loss: 4.051649951934815, val loss: 4.067079639434814, ETA in seconds: 738606.332\n",
      "epoch: 14000, train loss: 4.052821826934815, val loss: 4.0676655769348145, ETA in seconds: 743481.983\n",
      "epoch: 14100, train loss: 4.0471577644348145, val loss: 4.065907764434814, ETA in seconds: 748903.389\n",
      "epoch: 14200, train loss: 4.054579639434815, val loss: 4.065907764434814, ETA in seconds: 754292.390\n",
      "epoch: 14300, train loss: 4.053212451934814, val loss: 4.072157764434815, ETA in seconds: 759609.455\n",
      "epoch: 14400, train loss: 4.056142139434814, val loss: 4.070009326934814, ETA in seconds: 764854.552\n",
      "epoch: 14500, train loss: 4.039735889434814, val loss: 4.070204639434815, ETA in seconds: 770090.967\n",
      "epoch: 14600, train loss: 4.044032764434815, val loss: 4.070985889434814, ETA in seconds: 775325.419\n",
      "epoch: 14700, train loss: 4.055360889434814, val loss: 4.065321826934815, ETA in seconds: 780567.820\n",
      "epoch: 14800, train loss: 4.049696826934815, val loss: 4.0647358894348145, ETA in seconds: 785999.682\n",
      "epoch: 14900, train loss: 4.053212451934814, val loss: 4.069423389434815, ETA in seconds: 791263.087\n",
      "epoch: 15000, train loss: 4.047353076934814, val loss: 4.070790576934814, ETA in seconds: 796570.173\n",
      "epoch: 15100, train loss: 4.047548389434814, val loss: 4.069032764434814, ETA in seconds: 801899.386\n",
      "epoch: 15200, train loss: 4.0481343269348145, val loss: 4.069423389434815, ETA in seconds: 807097.228\n",
      "epoch: 15300, train loss: 4.0461812019348145, val loss: 4.065907764434814, ETA in seconds: 812392.861\n",
      "epoch: 15400, train loss: 4.054384326934814, val loss: 4.067274951934815, ETA in seconds: 817888.717\n",
      "epoch: 15500, train loss: 4.054189014434814, val loss: 4.0705952644348145, ETA in seconds: 823410.047\n",
      "epoch: 15600, train loss: 4.053212451934814, val loss: 4.0745015144348145, ETA in seconds: 828821.299\n",
      "epoch: 15700, train loss: 4.046571826934814, val loss: 4.064540576934815, ETA in seconds: 834123.467\n",
      "epoch: 15800, train loss: 4.046962451934815, val loss: 4.0618062019348145, ETA in seconds: 839257.772\n",
      "epoch: 15900, train loss: 4.055360889434814, val loss: 4.066298389434815, ETA in seconds: 844438.523\n",
      "epoch: 16000, train loss: 4.052235889434814, val loss: 4.064931201934814, ETA in seconds: 849572.642\n",
      "epoch: 16100, train loss: 4.056728076934815, val loss: 4.062001514434814, ETA in seconds: 854829.509\n",
      "epoch: 16200, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 860209.766\n",
      "epoch: 16300, train loss: 4.050868701934815, val loss: 4.0647358894348145, ETA in seconds: 865551.582\n",
      "epoch: 16400, train loss: 4.0471577644348145, val loss: 4.063954639434814, ETA in seconds: 870761.308\n",
      "epoch: 16500, train loss: 4.054579639434815, val loss: 4.071181201934815, ETA in seconds: 875952.186\n",
      "epoch: 16600, train loss: 4.052235889434814, val loss: 4.063954639434814, ETA in seconds: 881243.625\n",
      "epoch: 16700, train loss: 4.0549702644348145, val loss: 4.0666890144348145, ETA in seconds: 886451.872\n",
      "epoch: 16800, train loss: 4.0491108894348145, val loss: 4.065907764434814, ETA in seconds: 891646.211\n",
      "epoch: 16900, train loss: 4.050478076934814, val loss: 4.067079639434814, ETA in seconds: 896838.238\n",
      "epoch: 17000, train loss: 4.052821826934815, val loss: 4.068251514434815, ETA in seconds: 902092.432\n",
      "epoch: 17100, train loss: 4.0520405769348145, val loss: 4.0686421394348145, ETA in seconds: 907186.798\n",
      "epoch: 17200, train loss: 4.046962451934815, val loss: 4.065907764434814, ETA in seconds: 912401.871\n",
      "epoch: 17300, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 917539.388\n",
      "epoch: 17400, train loss: 4.048524951934814, val loss: 4.066493701934815, ETA in seconds: 922681.389\n",
      "epoch: 17500, train loss: 4.049696826934815, val loss: 4.067274951934815, ETA in seconds: 928009.583\n",
      "epoch: 17600, train loss: 4.053212451934814, val loss: 4.0637593269348145, ETA in seconds: 933330.306\n",
      "epoch: 17700, train loss: 4.055360889434814, val loss: 4.064931201934814, ETA in seconds: 938626.583\n",
      "epoch: 17800, train loss: 4.051649951934815, val loss: 4.063954639434814, ETA in seconds: 943910.891\n",
      "epoch: 17900, train loss: 4.049892139434815, val loss: 4.065517139434815, ETA in seconds: 949152.020\n",
      "epoch: 18000, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 954271.721\n",
      "epoch: 18100, train loss: 4.0471577644348145, val loss: 4.065907764434814, ETA in seconds: 959685.495\n",
      "epoch: 18200, train loss: 4.0461812019348145, val loss: 4.070399951934815, ETA in seconds: 965000.848\n",
      "epoch: 18300, train loss: 4.052235889434814, val loss: 4.0696187019348145, ETA in seconds: 970067.005\n",
      "epoch: 18400, train loss: 4.051259326934814, val loss: 4.0637593269348145, ETA in seconds: 975119.623\n",
      "epoch: 18500, train loss: 4.0578999519348145, val loss: 4.066298389434815, ETA in seconds: 980060.852\n",
      "epoch: 18600, train loss: 4.0520405769348145, val loss: 4.069423389434815, ETA in seconds: 985241.353\n",
      "epoch: 18700, train loss: 4.055360889434814, val loss: 4.063173389434814, ETA in seconds: 990382.310\n",
      "epoch: 18800, train loss: 4.0500874519348145, val loss: 4.064149951934814, ETA in seconds: 995691.657\n",
      "epoch: 18900, train loss: 4.050868701934815, val loss: 4.062978076934814, ETA in seconds: 1000993.334\n",
      "epoch: 19000, train loss: 4.053212451934814, val loss: 4.067860889434814, ETA in seconds: 1006244.799\n",
      "epoch: 19100, train loss: 4.051845264434815, val loss: 4.066493701934815, ETA in seconds: 1011354.087\n",
      "epoch: 19200, train loss: 4.051259326934814, val loss: 4.065907764434814, ETA in seconds: 1016597.400\n",
      "epoch: 19300, train loss: 4.056728076934815, val loss: 4.071376514434815, ETA in seconds: 1021856.014\n",
      "epoch: 19400, train loss: 4.042470264434814, val loss: 4.068056201934814, ETA in seconds: 1027393.340\n",
      "epoch: 19500, train loss: 4.057314014434814, val loss: 4.066298389434815, ETA in seconds: 1032828.730\n",
      "epoch: 19600, train loss: 4.056142139434814, val loss: 4.064540576934815, ETA in seconds: 1037921.081\n",
      "epoch: 19700, train loss: 4.050282764434814, val loss: 4.066493701934815, ETA in seconds: 1043044.069\n",
      "epoch: 19800, train loss: 4.045009326934815, val loss: 4.0696187019348145, ETA in seconds: 1048939.400\n",
      "epoch: 19900, train loss: 4.051649951934815, val loss: 4.069814014434814, ETA in seconds: 1054835.815\n",
      "epoch: 20000, train loss: 4.0500874519348145, val loss: 4.066884326934814, ETA in seconds: 1059981.679\n",
      "epoch: 20100, train loss: 4.046767139434815, val loss: 4.0666890144348145, ETA in seconds: 1065245.256\n",
      "epoch: 20200, train loss: 4.0569233894348145, val loss: 4.073134326934815, ETA in seconds: 1070441.054\n",
      "epoch: 20300, train loss: 4.051649951934815, val loss: 4.0686421394348145, ETA in seconds: 1075536.964\n",
      "epoch: 20400, train loss: 4.054384326934814, val loss: 4.070399951934815, ETA in seconds: 1080848.025\n",
      "epoch: 20500, train loss: 4.0549702644348145, val loss: 4.064931201934814, ETA in seconds: 1085941.918\n",
      "epoch: 20600, train loss: 4.049696826934815, val loss: 4.065517139434815, ETA in seconds: 1091094.351\n",
      "epoch: 20700, train loss: 4.053212451934814, val loss: 4.075868701934814, ETA in seconds: 1096465.712\n",
      "epoch: 20800, train loss: 4.050868701934815, val loss: 4.0696187019348145, ETA in seconds: 1101504.954\n",
      "epoch: 20900, train loss: 4.050282764434814, val loss: 4.066298389434815, ETA in seconds: 1106908.886\n",
      "epoch: 21000, train loss: 4.052235889434814, val loss: 4.059071826934814, ETA in seconds: 1111947.994\n",
      "epoch: 21100, train loss: 4.052626514434815, val loss: 4.069423389434815, ETA in seconds: 1117055.386\n",
      "epoch: 21200, train loss: 4.048524951934814, val loss: 4.069032764434814, ETA in seconds: 1122445.403\n",
      "epoch: 21300, train loss: 4.048524951934814, val loss: 4.064149951934814, ETA in seconds: 1127860.295\n",
      "epoch: 21400, train loss: 4.0549702644348145, val loss: 4.063368701934815, ETA in seconds: 1133169.836\n",
      "epoch: 21500, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 1138224.189\n",
      "epoch: 21600, train loss: 4.055360889434814, val loss: 4.068446826934815, ETA in seconds: 1143398.613\n",
      "epoch: 21700, train loss: 4.053407764434814, val loss: 4.065517139434815, ETA in seconds: 1148452.033\n",
      "epoch: 21800, train loss: 4.048329639434814, val loss: 4.0676655769348145, ETA in seconds: 1153770.071\n",
      "epoch: 21900, train loss: 4.053407764434814, val loss: 4.065126514434814, ETA in seconds: 1159195.076\n",
      "epoch: 22000, train loss: 4.052431201934814, val loss: 4.0647358894348145, ETA in seconds: 1164407.203\n",
      "epoch: 22100, train loss: 4.055751514434815, val loss: 4.068056201934814, ETA in seconds: 1169723.691\n",
      "epoch: 22200, train loss: 4.055556201934815, val loss: 4.073720264434814, ETA in seconds: 1175580.580\n",
      "epoch: 22300, train loss: 4.0578999519348145, val loss: 4.071181201934815, ETA in seconds: 1180938.296\n",
      "epoch: 22400, train loss: 4.049696826934815, val loss: 4.065907764434814, ETA in seconds: 1186165.001\n",
      "epoch: 22500, train loss: 4.046376514434814, val loss: 4.066298389434815, ETA in seconds: 1191388.945\n",
      "epoch: 22600, train loss: 4.056337451934814, val loss: 4.070790576934814, ETA in seconds: 1196697.554\n",
      "epoch: 22700, train loss: 4.054579639434815, val loss: 4.068837451934814, ETA in seconds: 1201888.603\n",
      "epoch: 22800, train loss: 4.055165576934814, val loss: 4.062001514434814, ETA in seconds: 1207152.612\n",
      "epoch: 22900, train loss: 4.049501514434814, val loss: 4.067470264434815, ETA in seconds: 1212307.249\n",
      "epoch: 23000, train loss: 4.047548389434814, val loss: 4.071181201934815, ETA in seconds: 1217381.251\n",
      "epoch: 23100, train loss: 4.0549702644348145, val loss: 4.067470264434815, ETA in seconds: 1222657.290\n",
      "epoch: 23200, train loss: 4.050282764434814, val loss: 4.066884326934814, ETA in seconds: 1227753.472\n",
      "epoch: 23300, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 1232879.356\n",
      "epoch: 23400, train loss: 4.045790576934815, val loss: 4.068251514434815, ETA in seconds: 1238067.940\n",
      "epoch: 23500, train loss: 4.053603076934815, val loss: 4.069814014434814, ETA in seconds: 1243088.618\n",
      "epoch: 23600, train loss: 4.047743701934815, val loss: 4.069032764434814, ETA in seconds: 1248050.821\n",
      "epoch: 23700, train loss: 4.042470264434814, val loss: 4.067470264434815, ETA in seconds: 1253378.354\n",
      "epoch: 23800, train loss: 4.048915576934815, val loss: 4.069032764434814, ETA in seconds: 1258554.194\n",
      "epoch: 23900, train loss: 4.049696826934815, val loss: 4.0657124519348145, ETA in seconds: 1263629.775\n",
      "epoch: 24000, train loss: 4.047939014434815, val loss: 4.0627827644348145, ETA in seconds: 1268638.941\n",
      "epoch: 24100, train loss: 4.054384326934814, val loss: 4.065517139434815, ETA in seconds: 1273613.329\n",
      "epoch: 24200, train loss: 4.050868701934815, val loss: 4.075282764434815, ETA in seconds: 1278814.441\n",
      "epoch: 24300, train loss: 4.055360889434814, val loss: 4.062196826934814, ETA in seconds: 1284012.501\n",
      "epoch: 24400, train loss: 4.049501514434814, val loss: 4.067860889434814, ETA in seconds: 1289461.241\n",
      "epoch: 24500, train loss: 4.0481343269348145, val loss: 4.0666890144348145, ETA in seconds: 1294744.126\n",
      "epoch: 24600, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 1300319.002\n",
      "epoch: 24700, train loss: 4.0452046394348145, val loss: 4.065126514434814, ETA in seconds: 1305664.401\n",
      "epoch: 24800, train loss: 4.0588765144348145, val loss: 4.0696187019348145, ETA in seconds: 1310784.614\n",
      "epoch: 24900, train loss: 4.051259326934814, val loss: 4.0647358894348145, ETA in seconds: 1315895.602\n",
      "epoch: 25000, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 1321066.579\n",
      "epoch: 25100, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 1326329.068\n",
      "epoch: 25200, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 1331534.954\n",
      "epoch: 25300, train loss: 4.049501514434814, val loss: 4.068056201934814, ETA in seconds: 1337068.688\n",
      "epoch: 25400, train loss: 4.0500874519348145, val loss: 4.068251514434815, ETA in seconds: 1342270.584\n",
      "epoch: 25500, train loss: 4.047939014434815, val loss: 4.067079639434814, ETA in seconds: 1347546.585\n",
      "epoch: 25600, train loss: 4.053212451934814, val loss: 4.0647358894348145, ETA in seconds: 1352595.904\n",
      "epoch: 25700, train loss: 4.052431201934814, val loss: 4.065126514434814, ETA in seconds: 1357748.532\n",
      "epoch: 25800, train loss: 4.054189014434814, val loss: 4.068446826934815, ETA in seconds: 1362979.708\n",
      "epoch: 25900, train loss: 4.055556201934815, val loss: 4.0696187019348145, ETA in seconds: 1368191.851\n",
      "epoch: 26000, train loss: 4.048915576934815, val loss: 4.066103076934814, ETA in seconds: 1373452.750\n",
      "epoch: 26100, train loss: 4.0510640144348145, val loss: 4.061220264434814, ETA in seconds: 1379016.560\n",
      "epoch: 26200, train loss: 4.047548389434814, val loss: 4.062001514434814, ETA in seconds: 1384554.898\n",
      "epoch: 26300, train loss: 4.0471577644348145, val loss: 4.070204639434815, ETA in seconds: 1390300.373\n",
      "epoch: 26400, train loss: 4.052626514434815, val loss: 4.064149951934814, ETA in seconds: 1396254.777\n",
      "epoch: 26500, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 1402339.694\n",
      "epoch: 26600, train loss: 4.050868701934815, val loss: 4.062392139434815, ETA in seconds: 1407426.197\n",
      "epoch: 26700, train loss: 4.055360889434814, val loss: 4.066103076934814, ETA in seconds: 1412723.001\n",
      "epoch: 26800, train loss: 4.051259326934814, val loss: 4.062196826934814, ETA in seconds: 1418170.930\n",
      "epoch: 26900, train loss: 4.048329639434814, val loss: 4.0676655769348145, ETA in seconds: 1423670.813\n",
      "epoch: 27000, train loss: 4.052431201934814, val loss: 4.061024951934814, ETA in seconds: 1429108.956\n",
      "epoch: 27100, train loss: 4.049892139434815, val loss: 4.0676655769348145, ETA in seconds: 1434372.278\n",
      "epoch: 27200, train loss: 4.050478076934814, val loss: 4.069814014434814, ETA in seconds: 1439480.920\n",
      "epoch: 27300, train loss: 4.053407764434814, val loss: 4.066493701934815, ETA in seconds: 1444577.264\n",
      "epoch: 27400, train loss: 4.056728076934815, val loss: 4.066884326934814, ETA in seconds: 1449709.844\n",
      "epoch: 27500, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 1454695.588\n",
      "epoch: 27600, train loss: 4.051845264434815, val loss: 4.070399951934815, ETA in seconds: 1459681.597\n",
      "epoch: 27700, train loss: 4.051649951934815, val loss: 4.066884326934814, ETA in seconds: 1464784.342\n",
      "epoch: 27800, train loss: 4.057704639434815, val loss: 4.0715718269348145, ETA in seconds: 1469895.899\n",
      "epoch: 27900, train loss: 4.045985889434815, val loss: 4.065907764434814, ETA in seconds: 1474997.080\n",
      "epoch: 28000, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 1479771.340\n",
      "epoch: 28100, train loss: 4.057704639434815, val loss: 4.0676655769348145, ETA in seconds: 1484607.119\n",
      "epoch: 28200, train loss: 4.0471577644348145, val loss: 4.068056201934814, ETA in seconds: 1489574.398\n",
      "epoch: 28300, train loss: 4.053407764434814, val loss: 4.070399951934815, ETA in seconds: 1494729.480\n",
      "epoch: 28400, train loss: 4.052626514434815, val loss: 4.070009326934814, ETA in seconds: 1500014.201\n",
      "epoch: 28500, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 1505486.860\n",
      "epoch: 28600, train loss: 4.0520405769348145, val loss: 4.068251514434815, ETA in seconds: 1510896.318\n",
      "epoch: 28700, train loss: 4.051454639434814, val loss: 4.0647358894348145, ETA in seconds: 1515961.299\n",
      "epoch: 28800, train loss: 4.056337451934814, val loss: 4.065517139434815, ETA in seconds: 1520971.497\n",
      "epoch: 28900, train loss: 4.060243701934814, val loss: 4.065517139434815, ETA in seconds: 1526091.615\n",
      "epoch: 29000, train loss: 4.049696826934815, val loss: 4.0676655769348145, ETA in seconds: 1531363.658\n",
      "epoch: 29100, train loss: 4.055165576934814, val loss: 4.0696187019348145, ETA in seconds: 1536150.385\n",
      "epoch: 29200, train loss: 4.054774951934815, val loss: 4.0715718269348145, ETA in seconds: 1540388.356\n",
      "epoch: 29300, train loss: 4.050673389434815, val loss: 4.0686421394348145, ETA in seconds: 1545044.241\n",
      "epoch: 29400, train loss: 4.049501514434814, val loss: 4.064149951934814, ETA in seconds: 1550335.608\n",
      "epoch: 29500, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 1555498.364\n",
      "epoch: 29600, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 1560646.150\n",
      "epoch: 29700, train loss: 4.048720264434815, val loss: 4.0676655769348145, ETA in seconds: 1565759.273\n",
      "epoch: 29800, train loss: 4.055751514434815, val loss: 4.063368701934815, ETA in seconds: 1570805.173\n",
      "epoch: 29900, train loss: 4.048524951934814, val loss: 4.067470264434815, ETA in seconds: 1575955.959\n",
      "epoch: 30000, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 1581074.996\n",
      "epoch: 30100, train loss: 4.056728076934815, val loss: 4.071181201934815, ETA in seconds: 1585982.640\n",
      "epoch: 30200, train loss: 4.057118701934814, val loss: 4.066298389434815, ETA in seconds: 1591004.272\n",
      "epoch: 30300, train loss: 4.060243701934814, val loss: 4.061415576934815, ETA in seconds: 1596164.966\n",
      "epoch: 30400, train loss: 4.053798389434815, val loss: 4.066103076934814, ETA in seconds: 1601365.627\n",
      "epoch: 30500, train loss: 4.054774951934815, val loss: 4.067860889434814, ETA in seconds: 1606439.246\n",
      "epoch: 30600, train loss: 4.050282764434814, val loss: 4.066493701934815, ETA in seconds: 1611657.338\n",
      "epoch: 30700, train loss: 4.054579639434815, val loss: 4.072353076934815, ETA in seconds: 1617025.119\n",
      "epoch: 30800, train loss: 4.054774951934815, val loss: 4.068251514434815, ETA in seconds: 1622296.453\n",
      "epoch: 30900, train loss: 4.050673389434815, val loss: 4.066884326934814, ETA in seconds: 1627582.515\n",
      "epoch: 31000, train loss: 4.047548389434814, val loss: 4.0686421394348145, ETA in seconds: 1632967.727\n",
      "epoch: 31100, train loss: 4.048329639434814, val loss: 4.066493701934815, ETA in seconds: 1638090.423\n",
      "epoch: 31200, train loss: 4.057118701934814, val loss: 4.069423389434815, ETA in seconds: 1643088.619\n",
      "epoch: 31300, train loss: 4.0559468269348145, val loss: 4.0676655769348145, ETA in seconds: 1648192.936\n",
      "epoch: 31400, train loss: 4.0491108894348145, val loss: 4.066884326934814, ETA in seconds: 1653330.124\n",
      "epoch: 31500, train loss: 4.055165576934814, val loss: 4.068446826934815, ETA in seconds: 1658547.986\n",
      "epoch: 31600, train loss: 4.0539937019348145, val loss: 4.071376514434815, ETA in seconds: 1663682.080\n",
      "epoch: 31700, train loss: 4.058485889434815, val loss: 4.068056201934814, ETA in seconds: 1668892.706\n",
      "epoch: 31800, train loss: 4.052821826934815, val loss: 4.064540576934815, ETA in seconds: 1674066.466\n",
      "epoch: 31900, train loss: 4.0500874519348145, val loss: 4.065321826934815, ETA in seconds: 1679283.858\n",
      "epoch: 32000, train loss: 4.049501514434814, val loss: 4.061610889434815, ETA in seconds: 1684371.563\n",
      "epoch: 32100, train loss: 4.052821826934815, val loss: 4.0637593269348145, ETA in seconds: 1689537.567\n",
      "epoch: 32200, train loss: 4.059462451934815, val loss: 4.066298389434815, ETA in seconds: 1694563.990\n",
      "epoch: 32300, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 1699725.568\n",
      "epoch: 32400, train loss: 4.0569233894348145, val loss: 4.0676655769348145, ETA in seconds: 1704674.077\n",
      "epoch: 32500, train loss: 4.0520405769348145, val loss: 4.071181201934815, ETA in seconds: 1710141.448\n",
      "epoch: 32600, train loss: 4.050868701934815, val loss: 4.060439014434815, ETA in seconds: 1715402.845\n",
      "epoch: 32700, train loss: 4.0500874519348145, val loss: 4.070985889434814, ETA in seconds: 1720490.691\n",
      "epoch: 32800, train loss: 4.056532764434815, val loss: 4.066298389434815, ETA in seconds: 1725563.677\n",
      "epoch: 32900, train loss: 4.045985889434815, val loss: 4.061610889434815, ETA in seconds: 1730476.827\n",
      "epoch: 33000, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 1735546.523\n",
      "epoch: 33100, train loss: 4.0510640144348145, val loss: 4.066103076934814, ETA in seconds: 1740751.512\n",
      "epoch: 33200, train loss: 4.057118701934814, val loss: 4.0666890144348145, ETA in seconds: 1745798.812\n",
      "epoch: 33300, train loss: 4.054189014434814, val loss: 4.0686421394348145, ETA in seconds: 1750982.095\n",
      "epoch: 33400, train loss: 4.0520405769348145, val loss: 4.070985889434814, ETA in seconds: 1756253.904\n",
      "epoch: 33500, train loss: 4.055165576934814, val loss: 4.063368701934815, ETA in seconds: 1761292.713\n",
      "epoch: 33600, train loss: 4.053603076934815, val loss: 4.072353076934815, ETA in seconds: 1766523.349\n",
      "epoch: 33700, train loss: 4.052626514434815, val loss: 4.067860889434814, ETA in seconds: 1771447.340\n",
      "epoch: 33800, train loss: 4.050478076934814, val loss: 4.0715718269348145, ETA in seconds: 1776327.306\n",
      "epoch: 33900, train loss: 4.049306201934814, val loss: 4.068837451934814, ETA in seconds: 1781513.782\n",
      "epoch: 34000, train loss: 4.053407764434814, val loss: 4.062196826934814, ETA in seconds: 1786644.051\n",
      "epoch: 34100, train loss: 4.0491108894348145, val loss: 4.0754780769348145, ETA in seconds: 1791669.974\n",
      "epoch: 34200, train loss: 4.056728076934815, val loss: 4.066103076934814, ETA in seconds: 1796622.773\n",
      "epoch: 34300, train loss: 4.049892139434815, val loss: 4.064345264434815, ETA in seconds: 1801541.370\n",
      "epoch: 34400, train loss: 4.047353076934814, val loss: 4.063368701934815, ETA in seconds: 1806614.637\n",
      "epoch: 34500, train loss: 4.049892139434815, val loss: 4.0676655769348145, ETA in seconds: 1811926.930\n",
      "epoch: 34600, train loss: 4.0461812019348145, val loss: 4.066298389434815, ETA in seconds: 1817094.941\n",
      "epoch: 34700, train loss: 4.054384326934814, val loss: 4.068837451934814, ETA in seconds: 1822191.961\n",
      "epoch: 34800, train loss: 4.053212451934814, val loss: 4.068446826934815, ETA in seconds: 1827325.492\n",
      "epoch: 34900, train loss: 4.047353076934814, val loss: 4.066493701934815, ETA in seconds: 1832385.854\n",
      "epoch: 35000, train loss: 4.046962451934815, val loss: 4.071376514434815, ETA in seconds: 1837561.725\n",
      "epoch: 35100, train loss: 4.063173389434814, val loss: 4.069423389434815, ETA in seconds: 1842439.818\n",
      "epoch: 35200, train loss: 4.051259326934814, val loss: 4.0676655769348145, ETA in seconds: 1847963.635\n",
      "epoch: 35300, train loss: 4.058485889434815, val loss: 4.070985889434814, ETA in seconds: 1853321.682\n",
      "epoch: 35400, train loss: 4.053798389434815, val loss: 4.0696187019348145, ETA in seconds: 1858370.163\n",
      "epoch: 35500, train loss: 4.0491108894348145, val loss: 4.069814014434814, ETA in seconds: 1863235.303\n",
      "epoch: 35600, train loss: 4.058681201934815, val loss: 4.0686421394348145, ETA in seconds: 1868187.234\n",
      "epoch: 35700, train loss: 4.056142139434814, val loss: 4.067860889434814, ETA in seconds: 1873159.569\n",
      "epoch: 35800, train loss: 4.0530171394348145, val loss: 4.065321826934815, ETA in seconds: 1878144.869\n",
      "epoch: 35900, train loss: 4.0510640144348145, val loss: 4.0686421394348145, ETA in seconds: 1883253.036\n",
      "epoch: 36000, train loss: 4.050282764434814, val loss: 4.065907764434814, ETA in seconds: 1888334.882\n",
      "epoch: 36100, train loss: 4.043642139434814, val loss: 4.070985889434814, ETA in seconds: 1893218.871\n",
      "epoch: 36200, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 1898216.134\n",
      "epoch: 36300, train loss: 4.0471577644348145, val loss: 4.064931201934814, ETA in seconds: 1903102.698\n",
      "epoch: 36400, train loss: 4.046767139434815, val loss: 4.072939014434814, ETA in seconds: 1908088.917\n",
      "epoch: 36500, train loss: 4.054384326934814, val loss: 4.069228076934815, ETA in seconds: 1913075.083\n",
      "epoch: 36600, train loss: 4.0578999519348145, val loss: 4.066103076934814, ETA in seconds: 1918054.184\n",
      "epoch: 36700, train loss: 4.051845264434815, val loss: 4.069228076934815, ETA in seconds: 1923104.205\n",
      "epoch: 36800, train loss: 4.052821826934815, val loss: 4.064931201934814, ETA in seconds: 1928253.693\n",
      "epoch: 36900, train loss: 4.052626514434815, val loss: 4.063954639434814, ETA in seconds: 1933186.063\n",
      "epoch: 37000, train loss: 4.058681201934815, val loss: 4.064149951934814, ETA in seconds: 1938169.408\n",
      "epoch: 37100, train loss: 4.0500874519348145, val loss: 4.066103076934814, ETA in seconds: 1943149.271\n",
      "epoch: 37200, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 1948280.338\n",
      "epoch: 37300, train loss: 4.047939014434815, val loss: 4.061024951934814, ETA in seconds: 1953397.506\n",
      "epoch: 37400, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 1958526.163\n",
      "epoch: 37500, train loss: 4.0481343269348145, val loss: 4.065321826934815, ETA in seconds: 1963138.129\n",
      "epoch: 37600, train loss: 4.054384326934814, val loss: 4.062978076934814, ETA in seconds: 1968104.079\n",
      "epoch: 37700, train loss: 4.0559468269348145, val loss: 4.0637593269348145, ETA in seconds: 1972998.204\n",
      "epoch: 37800, train loss: 4.0549702644348145, val loss: 4.066103076934814, ETA in seconds: 1977644.266\n",
      "epoch: 37900, train loss: 4.050868701934815, val loss: 4.066493701934815, ETA in seconds: 1982536.281\n",
      "epoch: 38000, train loss: 4.053798389434815, val loss: 4.065517139434815, ETA in seconds: 1987709.046\n",
      "epoch: 38100, train loss: 4.057704639434815, val loss: 4.0705952644348145, ETA in seconds: 1992858.598\n",
      "epoch: 38200, train loss: 4.050478076934814, val loss: 4.070204639434815, ETA in seconds: 1997719.262\n",
      "epoch: 38300, train loss: 4.055360889434814, val loss: 4.065126514434814, ETA in seconds: 2002546.321\n",
      "epoch: 38400, train loss: 4.062001514434814, val loss: 4.0657124519348145, ETA in seconds: 2007459.396\n",
      "epoch: 38500, train loss: 4.048524951934814, val loss: 4.063368701934815, ETA in seconds: 2012395.487\n",
      "epoch: 38600, train loss: 4.050478076934814, val loss: 4.070204639434815, ETA in seconds: 2017526.393\n",
      "epoch: 38700, train loss: 4.045399951934814, val loss: 4.064540576934815, ETA in seconds: 2022476.605\n",
      "epoch: 38800, train loss: 4.056337451934814, val loss: 4.069032764434814, ETA in seconds: 2027383.026\n",
      "epoch: 38900, train loss: 4.0539937019348145, val loss: 4.066298389434815, ETA in seconds: 2032242.536\n",
      "epoch: 39000, train loss: 4.050673389434815, val loss: 4.065907764434814, ETA in seconds: 2037029.842\n",
      "epoch: 39100, train loss: 4.055165576934814, val loss: 4.066493701934815, ETA in seconds: 2041942.649\n",
      "epoch: 39200, train loss: 4.057704639434815, val loss: 4.064345264434815, ETA in seconds: 2047274.270\n",
      "epoch: 39300, train loss: 4.048720264434815, val loss: 4.070985889434814, ETA in seconds: 2052460.004\n",
      "epoch: 39400, train loss: 4.051259326934814, val loss: 4.067079639434814, ETA in seconds: 2057547.035\n",
      "epoch: 39500, train loss: 4.056728076934815, val loss: 4.0666890144348145, ETA in seconds: 2062658.098\n",
      "epoch: 39600, train loss: 4.047353076934814, val loss: 4.066884326934814, ETA in seconds: 2067653.909\n",
      "epoch: 39700, train loss: 4.0520405769348145, val loss: 4.072353076934815, ETA in seconds: 2072528.814\n",
      "epoch: 39800, train loss: 4.054774951934815, val loss: 4.063954639434814, ETA in seconds: 2077609.076\n",
      "epoch: 39900, train loss: 4.0539937019348145, val loss: 4.070204639434815, ETA in seconds: 2082650.294\n",
      "epoch: 40000, train loss: 4.059071826934814, val loss: 4.067274951934815, ETA in seconds: 2087628.076\n",
      "epoch: 40100, train loss: 4.047939014434815, val loss: 4.064149951934814, ETA in seconds: 2092562.817\n",
      "epoch: 40200, train loss: 4.058681201934815, val loss: 4.0686421394348145, ETA in seconds: 2097555.533\n",
      "epoch: 40300, train loss: 4.045009326934815, val loss: 4.067274951934815, ETA in seconds: 2102343.705\n",
      "epoch: 40400, train loss: 4.055165576934814, val loss: 4.069228076934815, ETA in seconds: 2107222.197\n",
      "epoch: 40500, train loss: 4.054774951934815, val loss: 4.067274951934815, ETA in seconds: 2112207.248\n",
      "epoch: 40600, train loss: 4.054579639434815, val loss: 4.072939014434814, ETA in seconds: 2117117.837\n",
      "epoch: 40700, train loss: 4.052235889434814, val loss: 4.069032764434814, ETA in seconds: 2122026.478\n",
      "epoch: 40800, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 2126937.172\n",
      "epoch: 40900, train loss: 4.055360889434814, val loss: 4.064345264434815, ETA in seconds: 2131782.444\n",
      "epoch: 41000, train loss: 4.049892139434815, val loss: 4.070399951934815, ETA in seconds: 2136664.815\n",
      "epoch: 41100, train loss: 4.048915576934815, val loss: 4.062392139434815, ETA in seconds: 2141541.769\n",
      "epoch: 41200, train loss: 4.053603076934815, val loss: 4.065907764434814, ETA in seconds: 2146716.934\n",
      "epoch: 41300, train loss: 4.047353076934814, val loss: 4.067470264434815, ETA in seconds: 2151920.359\n",
      "epoch: 41400, train loss: 4.054579639434815, val loss: 4.069423389434815, ETA in seconds: 2157016.392\n",
      "epoch: 41500, train loss: 4.054774951934815, val loss: 4.0735249519348145, ETA in seconds: 2162126.346\n",
      "epoch: 41600, train loss: 4.0549702644348145, val loss: 4.068251514434815, ETA in seconds: 2167088.253\n",
      "epoch: 41700, train loss: 4.050673389434815, val loss: 4.064931201934814, ETA in seconds: 2171945.222\n",
      "epoch: 41800, train loss: 4.045009326934815, val loss: 4.065126514434814, ETA in seconds: 2176876.366\n",
      "epoch: 41900, train loss: 4.0510640144348145, val loss: 4.065321826934815, ETA in seconds: 2181727.443\n",
      "epoch: 42000, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 2186526.996\n",
      "epoch: 42100, train loss: 4.0422749519348145, val loss: 4.068056201934814, ETA in seconds: 2191425.348\n",
      "epoch: 42200, train loss: 4.052626514434815, val loss: 4.068837451934814, ETA in seconds: 2196444.436\n",
      "epoch: 42300, train loss: 4.050868701934815, val loss: 4.065517139434815, ETA in seconds: 2201345.527\n",
      "epoch: 42400, train loss: 4.059071826934814, val loss: 4.068056201934814, ETA in seconds: 2206321.848\n",
      "epoch: 42500, train loss: 4.044032764434815, val loss: 4.066884326934814, ETA in seconds: 2211160.490\n",
      "epoch: 42600, train loss: 4.057704639434815, val loss: 4.067079639434814, ETA in seconds: 2215802.016\n",
      "epoch: 42700, train loss: 4.0442280769348145, val loss: 4.0745015144348145, ETA in seconds: 2220217.388\n",
      "epoch: 42800, train loss: 4.0491108894348145, val loss: 4.067470264434815, ETA in seconds: 2225425.598\n",
      "epoch: 42900, train loss: 4.0442280769348145, val loss: 4.071181201934815, ETA in seconds: 2230374.191\n",
      "epoch: 43000, train loss: 4.051454639434814, val loss: 4.061220264434814, ETA in seconds: 2235329.320\n",
      "epoch: 43100, train loss: 4.046962451934815, val loss: 4.065907764434814, ETA in seconds: 2240507.280\n",
      "epoch: 43200, train loss: 4.054189014434814, val loss: 4.069032764434814, ETA in seconds: 2245629.609\n",
      "epoch: 43300, train loss: 4.0549702644348145, val loss: 4.069814014434814, ETA in seconds: 2250817.796\n",
      "epoch: 43400, train loss: 4.049306201934814, val loss: 4.064540576934815, ETA in seconds: 2255946.673\n",
      "epoch: 43500, train loss: 4.050282764434814, val loss: 4.059657764434815, ETA in seconds: 2261242.415\n",
      "epoch: 43600, train loss: 4.0471577644348145, val loss: 4.0686421394348145, ETA in seconds: 2266320.488\n",
      "epoch: 43700, train loss: 4.057118701934814, val loss: 4.070399951934815, ETA in seconds: 2271224.853\n",
      "epoch: 43800, train loss: 4.053212451934814, val loss: 4.0657124519348145, ETA in seconds: 2276207.540\n",
      "epoch: 43900, train loss: 4.0422749519348145, val loss: 4.070204639434815, ETA in seconds: 2281200.235\n",
      "epoch: 44000, train loss: 4.051649951934815, val loss: 4.063173389434814, ETA in seconds: 2285955.904\n",
      "epoch: 44100, train loss: 4.053798389434815, val loss: 4.062587451934815, ETA in seconds: 2290837.782\n",
      "epoch: 44200, train loss: 4.058095264434814, val loss: 4.062392139434815, ETA in seconds: 2295572.500\n",
      "epoch: 44300, train loss: 4.055165576934814, val loss: 4.067274951934815, ETA in seconds: 2300374.549\n",
      "epoch: 44400, train loss: 4.052431201934814, val loss: 4.067860889434814, ETA in seconds: 2305312.688\n",
      "epoch: 44500, train loss: 4.048915576934815, val loss: 4.069423389434815, ETA in seconds: 2310407.481\n",
      "epoch: 44600, train loss: 4.055751514434815, val loss: 4.072157764434815, ETA in seconds: 2315518.997\n",
      "epoch: 44700, train loss: 4.051454639434814, val loss: 4.066493701934815, ETA in seconds: 2320718.009\n",
      "epoch: 44800, train loss: 4.051845264434815, val loss: 4.070204639434815, ETA in seconds: 2325848.712\n",
      "epoch: 44900, train loss: 4.0491108894348145, val loss: 4.070204639434815, ETA in seconds: 2330813.764\n",
      "epoch: 45000, train loss: 4.048915576934815, val loss: 4.073329639434815, ETA in seconds: 2335756.352\n",
      "epoch: 45100, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 2340878.288\n",
      "epoch: 45200, train loss: 4.054189014434814, val loss: 4.069228076934815, ETA in seconds: 2345953.193\n",
      "epoch: 45300, train loss: 4.050478076934814, val loss: 4.0745015144348145, ETA in seconds: 2351103.237\n",
      "epoch: 45400, train loss: 4.0520405769348145, val loss: 4.0647358894348145, ETA in seconds: 2356094.482\n",
      "epoch: 45500, train loss: 4.0471577644348145, val loss: 4.066493701934815, ETA in seconds: 2361318.076\n",
      "epoch: 45600, train loss: 4.047939014434815, val loss: 4.068056201934814, ETA in seconds: 2366511.625\n",
      "epoch: 45700, train loss: 4.050282764434814, val loss: 4.070985889434814, ETA in seconds: 2371299.621\n",
      "epoch: 45800, train loss: 4.056337451934814, val loss: 4.068446826934815, ETA in seconds: 2376163.623\n",
      "epoch: 45900, train loss: 4.049696826934815, val loss: 4.0696187019348145, ETA in seconds: 2380964.921\n",
      "epoch: 46000, train loss: 4.048915576934815, val loss: 4.058095264434814, ETA in seconds: 2385853.818\n",
      "epoch: 46100, train loss: 4.051454639434814, val loss: 4.058290576934814, ETA in seconds: 2390524.093\n",
      "epoch: 46200, train loss: 4.054384326934814, val loss: 4.063173389434814, ETA in seconds: 2395269.302\n",
      "epoch: 46300, train loss: 4.050478076934814, val loss: 4.064345264434815, ETA in seconds: 2400088.542\n",
      "epoch: 46400, train loss: 4.050282764434814, val loss: 4.068446826934815, ETA in seconds: 2404954.767\n",
      "epoch: 46500, train loss: 4.053407764434814, val loss: 4.065517139434815, ETA in seconds: 2410015.321\n",
      "epoch: 46600, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 2415011.159\n",
      "epoch: 46700, train loss: 4.049696826934815, val loss: 4.069228076934815, ETA in seconds: 2420015.623\n",
      "epoch: 46800, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 2424817.203\n",
      "epoch: 46900, train loss: 4.051649951934815, val loss: 4.061024951934814, ETA in seconds: 2429793.767\n",
      "epoch: 47000, train loss: 4.051259326934814, val loss: 4.0657124519348145, ETA in seconds: 2434656.245\n",
      "epoch: 47100, train loss: 4.045595264434814, val loss: 4.067079639434814, ETA in seconds: 2439526.760\n",
      "epoch: 47200, train loss: 4.045985889434815, val loss: 4.066298389434815, ETA in seconds: 2444431.759\n",
      "epoch: 47300, train loss: 4.0539937019348145, val loss: 4.065321826934815, ETA in seconds: 2449511.918\n",
      "epoch: 47400, train loss: 4.052431201934814, val loss: 4.071376514434815, ETA in seconds: 2454473.138\n",
      "epoch: 47500, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 2459437.049\n",
      "epoch: 47600, train loss: 4.0491108894348145, val loss: 4.069423389434815, ETA in seconds: 2464135.492\n",
      "epoch: 47700, train loss: 4.048720264434815, val loss: 4.071181201934815, ETA in seconds: 2469062.056\n",
      "epoch: 47800, train loss: 4.048720264434815, val loss: 4.0647358894348145, ETA in seconds: 2473926.729\n",
      "epoch: 47900, train loss: 4.052235889434814, val loss: 4.069423389434815, ETA in seconds: 2478755.269\n",
      "epoch: 48000, train loss: 4.055751514434815, val loss: 4.069032764434814, ETA in seconds: 2483528.336\n",
      "epoch: 48100, train loss: 4.0510640144348145, val loss: 4.0715718269348145, ETA in seconds: 2487833.618\n",
      "epoch: 48200, train loss: 4.0491108894348145, val loss: 4.070399951934815, ETA in seconds: 2492738.622\n",
      "epoch: 48300, train loss: 4.053212451934814, val loss: 4.066298389434815, ETA in seconds: 2497605.775\n",
      "epoch: 48400, train loss: 4.056337451934814, val loss: 4.071376514434815, ETA in seconds: 2502600.786\n",
      "epoch: 48500, train loss: 4.054189014434814, val loss: 4.068446826934815, ETA in seconds: 2507446.211\n",
      "epoch: 48600, train loss: 4.050478076934814, val loss: 4.068837451934814, ETA in seconds: 2512434.962\n",
      "epoch: 48700, train loss: 4.054774951934815, val loss: 4.066103076934814, ETA in seconds: 2517541.586\n",
      "epoch: 48800, train loss: 4.055360889434814, val loss: 4.0657124519348145, ETA in seconds: 2522557.797\n",
      "epoch: 48900, train loss: 4.0549702644348145, val loss: 4.063564014434815, ETA in seconds: 2527541.495\n",
      "epoch: 49000, train loss: 4.050673389434815, val loss: 4.068251514434815, ETA in seconds: 2532612.525\n",
      "epoch: 49100, train loss: 4.050868701934815, val loss: 4.0705952644348145, ETA in seconds: 2537734.725\n",
      "epoch: 49200, train loss: 4.049892139434815, val loss: 4.067860889434814, ETA in seconds: 2542799.665\n",
      "epoch: 49300, train loss: 4.053212451934814, val loss: 4.0657124519348145, ETA in seconds: 2547688.174\n",
      "epoch: 49400, train loss: 4.054384326934814, val loss: 4.066493701934815, ETA in seconds: 2552439.653\n",
      "epoch: 49500, train loss: 4.052431201934814, val loss: 4.069814014434814, ETA in seconds: 2557573.254\n",
      "epoch: 49600, train loss: 4.049696826934815, val loss: 4.067470264434815, ETA in seconds: 2562513.945\n",
      "epoch: 49700, train loss: 4.0520405769348145, val loss: 4.066103076934814, ETA in seconds: 2567445.034\n",
      "epoch: 49800, train loss: 4.049306201934814, val loss: 4.065321826934815, ETA in seconds: 2572662.406\n",
      "epoch: 49900, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 2577702.745\n",
      "epoch: 50000, train loss: 4.050478076934814, val loss: 4.071376514434815, ETA in seconds: 2582454.579\n",
      "epoch: 50100, train loss: 4.052235889434814, val loss: 4.066103076934814, ETA in seconds: 2587557.307\n",
      "epoch: 50200, train loss: 4.052821826934815, val loss: 4.058485889434815, ETA in seconds: 2592629.835\n",
      "epoch: 50300, train loss: 4.049501514434814, val loss: 4.068251514434815, ETA in seconds: 2597463.539\n",
      "epoch: 50400, train loss: 4.056728076934815, val loss: 4.068837451934814, ETA in seconds: 2602208.224\n",
      "epoch: 50500, train loss: 4.049306201934814, val loss: 4.065907764434814, ETA in seconds: 2606928.532\n",
      "epoch: 50600, train loss: 4.050868701934815, val loss: 4.072353076934815, ETA in seconds: 2611627.585\n",
      "epoch: 50700, train loss: 4.0559468269348145, val loss: 4.068837451934814, ETA in seconds: 2616651.008\n",
      "epoch: 50800, train loss: 4.051259326934814, val loss: 4.070985889434814, ETA in seconds: 2621398.614\n",
      "epoch: 50900, train loss: 4.056337451934814, val loss: 4.070985889434814, ETA in seconds: 2626208.342\n",
      "epoch: 51000, train loss: 4.049696826934815, val loss: 4.066103076934814, ETA in seconds: 2631048.295\n",
      "epoch: 51100, train loss: 4.046767139434815, val loss: 4.066493701934815, ETA in seconds: 2635806.946\n",
      "epoch: 51200, train loss: 4.0520405769348145, val loss: 4.063954639434814, ETA in seconds: 2640594.575\n",
      "epoch: 51300, train loss: 4.046571826934814, val loss: 4.071376514434815, ETA in seconds: 2645711.488\n",
      "epoch: 51400, train loss: 4.052235889434814, val loss: 4.069814014434814, ETA in seconds: 2650607.377\n",
      "epoch: 51500, train loss: 4.044814014434815, val loss: 4.0686421394348145, ETA in seconds: 2655521.063\n",
      "epoch: 51600, train loss: 4.0520405769348145, val loss: 4.074696826934814, ETA in seconds: 2660404.862\n",
      "epoch: 51700, train loss: 4.053798389434815, val loss: 4.068837451934814, ETA in seconds: 2665413.344\n",
      "epoch: 51800, train loss: 4.053212451934814, val loss: 4.063954639434814, ETA in seconds: 2670505.279\n",
      "epoch: 51900, train loss: 4.050868701934815, val loss: 4.061220264434814, ETA in seconds: 2675419.416\n",
      "epoch: 52000, train loss: 4.050478076934814, val loss: 4.063368701934815, ETA in seconds: 2680182.689\n",
      "epoch: 52100, train loss: 4.051454639434814, val loss: 4.070204639434815, ETA in seconds: 2685138.868\n",
      "epoch: 52200, train loss: 4.051259326934814, val loss: 4.071767139434814, ETA in seconds: 2690022.867\n",
      "epoch: 52300, train loss: 4.0549702644348145, val loss: 4.066103076934814, ETA in seconds: 2695090.909\n",
      "epoch: 52400, train loss: 4.046962451934815, val loss: 4.071962451934814, ETA in seconds: 2699953.861\n",
      "epoch: 52500, train loss: 4.050868701934815, val loss: 4.064149951934814, ETA in seconds: 2704780.811\n",
      "epoch: 52600, train loss: 4.052626514434815, val loss: 4.062978076934814, ETA in seconds: 2709799.837\n",
      "epoch: 52700, train loss: 4.050868701934815, val loss: 4.072743701934814, ETA in seconds: 2714895.177\n",
      "epoch: 52800, train loss: 4.0500874519348145, val loss: 4.064149951934814, ETA in seconds: 2719709.552\n",
      "epoch: 52900, train loss: 4.045985889434815, val loss: 4.064540576934815, ETA in seconds: 2724814.906\n",
      "epoch: 53000, train loss: 4.054579639434815, val loss: 4.066103076934814, ETA in seconds: 2729644.868\n",
      "epoch: 53100, train loss: 4.053798389434815, val loss: 4.0666890144348145, ETA in seconds: 2734339.868\n",
      "epoch: 53200, train loss: 4.044618701934814, val loss: 4.0686421394348145, ETA in seconds: 2739126.002\n",
      "epoch: 53300, train loss: 4.055751514434815, val loss: 4.0627827644348145, ETA in seconds: 2743934.187\n",
      "epoch: 53400, train loss: 4.053798389434815, val loss: 4.063173389434814, ETA in seconds: 2748749.451\n",
      "epoch: 53500, train loss: 4.058095264434814, val loss: 4.067470264434815, ETA in seconds: 2753552.635\n",
      "epoch: 53600, train loss: 4.054384326934814, val loss: 4.064149951934814, ETA in seconds: 2758242.212\n",
      "epoch: 53700, train loss: 4.048524951934814, val loss: 4.0686421394348145, ETA in seconds: 2762988.222\n",
      "epoch: 53800, train loss: 4.055751514434815, val loss: 4.066298389434815, ETA in seconds: 2768195.929\n",
      "epoch: 53900, train loss: 4.046571826934814, val loss: 4.068251514434815, ETA in seconds: 2773240.900\n",
      "epoch: 54000, train loss: 4.052821826934815, val loss: 4.065126514434814, ETA in seconds: 2778130.515\n",
      "epoch: 54100, train loss: 4.049306201934814, val loss: 4.067470264434815, ETA in seconds: 2783074.209\n",
      "epoch: 54200, train loss: 4.059462451934815, val loss: 4.063954639434814, ETA in seconds: 2787933.045\n",
      "epoch: 54300, train loss: 4.049306201934814, val loss: 4.070204639434815, ETA in seconds: 2792696.696\n",
      "epoch: 54400, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 2797510.501\n",
      "epoch: 54500, train loss: 4.052431201934814, val loss: 4.061415576934815, ETA in seconds: 2802714.084\n",
      "epoch: 54600, train loss: 4.047939014434815, val loss: 4.070009326934814, ETA in seconds: 2807726.041\n",
      "epoch: 54700, train loss: 4.050282764434814, val loss: 4.0696187019348145, ETA in seconds: 2812468.962\n",
      "epoch: 54800, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 2817353.112\n",
      "epoch: 54900, train loss: 4.051454639434814, val loss: 4.070790576934814, ETA in seconds: 2822121.281\n",
      "epoch: 55000, train loss: 4.051259326934814, val loss: 4.065126514434814, ETA in seconds: 2826946.638\n",
      "epoch: 55100, train loss: 4.058290576934814, val loss: 4.073915576934814, ETA in seconds: 2832031.543\n",
      "epoch: 55200, train loss: 4.049892139434815, val loss: 4.070399951934815, ETA in seconds: 2836924.053\n",
      "epoch: 55300, train loss: 4.0559468269348145, val loss: 4.073329639434815, ETA in seconds: 2841826.671\n",
      "epoch: 55400, train loss: 4.055360889434814, val loss: 4.071181201934815, ETA in seconds: 2846620.540\n",
      "epoch: 55500, train loss: 4.053212451934814, val loss: 4.067860889434814, ETA in seconds: 2851327.330\n",
      "epoch: 55600, train loss: 4.056728076934815, val loss: 4.064345264434815, ETA in seconds: 2856032.451\n",
      "epoch: 55700, train loss: 4.049501514434814, val loss: 4.070009326934814, ETA in seconds: 2860757.922\n",
      "epoch: 55800, train loss: 4.051259326934814, val loss: 4.0666890144348145, ETA in seconds: 2865452.557\n",
      "epoch: 55900, train loss: 4.0539937019348145, val loss: 4.067860889434814, ETA in seconds: 2870300.026\n",
      "epoch: 56000, train loss: 4.053798389434815, val loss: 4.0598530769348145, ETA in seconds: 2875176.502\n",
      "epoch: 56100, train loss: 4.0559468269348145, val loss: 4.0715718269348145, ETA in seconds: 2879995.041\n",
      "epoch: 56200, train loss: 4.053798389434815, val loss: 4.0725483894348145, ETA in seconds: 2885035.277\n",
      "epoch: 56300, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 2889965.013\n",
      "epoch: 56400, train loss: 4.0481343269348145, val loss: 4.0647358894348145, ETA in seconds: 2894968.595\n",
      "epoch: 56500, train loss: 4.048915576934815, val loss: 4.070204639434815, ETA in seconds: 2899825.755\n",
      "epoch: 56600, train loss: 4.0481343269348145, val loss: 4.063368701934815, ETA in seconds: 2904733.034\n",
      "epoch: 56700, train loss: 4.054579639434815, val loss: 4.071767139434814, ETA in seconds: 2909731.374\n",
      "epoch: 56800, train loss: 4.046571826934814, val loss: 4.067470264434815, ETA in seconds: 2914456.009\n",
      "epoch: 56900, train loss: 4.0510640144348145, val loss: 4.064345264434815, ETA in seconds: 2919802.646\n",
      "epoch: 57000, train loss: 4.0520405769348145, val loss: 4.063173389434814, ETA in seconds: 2925199.205\n",
      "epoch: 57100, train loss: 4.050478076934814, val loss: 4.069032764434814, ETA in seconds: 2930459.542\n",
      "epoch: 57200, train loss: 4.058095264434814, val loss: 4.0686421394348145, ETA in seconds: 2935266.960\n",
      "epoch: 57300, train loss: 4.0578999519348145, val loss: 4.0686421394348145, ETA in seconds: 2939944.558\n",
      "epoch: 57400, train loss: 4.056728076934815, val loss: 4.066884326934814, ETA in seconds: 2944747.556\n",
      "epoch: 57500, train loss: 4.0510640144348145, val loss: 4.065321826934815, ETA in seconds: 2949707.634\n",
      "epoch: 57600, train loss: 4.053603076934815, val loss: 4.0705952644348145, ETA in seconds: 2954501.665\n",
      "epoch: 57700, train loss: 4.0481343269348145, val loss: 4.076064014434815, ETA in seconds: 2959352.591\n",
      "epoch: 57800, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 2964068.320\n",
      "epoch: 57900, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 2968834.904\n",
      "epoch: 58000, train loss: 4.050478076934814, val loss: 4.0637593269348145, ETA in seconds: 2973631.144\n",
      "epoch: 58100, train loss: 4.057118701934814, val loss: 4.066884326934814, ETA in seconds: 2978319.428\n",
      "epoch: 58200, train loss: 4.055751514434815, val loss: 4.0666890144348145, ETA in seconds: 2983187.483\n",
      "epoch: 58300, train loss: 4.048720264434815, val loss: 4.069423389434815, ETA in seconds: 2988094.331\n",
      "epoch: 58400, train loss: 4.0530171394348145, val loss: 4.061220264434814, ETA in seconds: 2992825.377\n",
      "epoch: 58500, train loss: 4.052626514434815, val loss: 4.0705952644348145, ETA in seconds: 2997607.828\n",
      "epoch: 58600, train loss: 4.053407764434814, val loss: 4.065321826934815, ETA in seconds: 3002386.998\n",
      "epoch: 58700, train loss: 4.057118701934814, val loss: 4.0666890144348145, ETA in seconds: 3007208.293\n",
      "epoch: 58800, train loss: 4.054384326934814, val loss: 4.0705952644348145, ETA in seconds: 3012126.489\n",
      "epoch: 58900, train loss: 4.0491108894348145, val loss: 4.0627827644348145, ETA in seconds: 3017009.532\n",
      "epoch: 59000, train loss: 4.0530171394348145, val loss: 4.065907764434814, ETA in seconds: 3021761.448\n",
      "epoch: 59100, train loss: 4.045595264434814, val loss: 4.072743701934814, ETA in seconds: 3026521.401\n",
      "epoch: 59200, train loss: 4.054579639434815, val loss: 4.065126514434814, ETA in seconds: 3031484.948\n",
      "epoch: 59300, train loss: 4.053603076934815, val loss: 4.065126514434814, ETA in seconds: 3036291.583\n",
      "epoch: 59400, train loss: 4.0530171394348145, val loss: 4.074110889434815, ETA in seconds: 3040941.054\n",
      "epoch: 59500, train loss: 4.049892139434815, val loss: 4.067079639434814, ETA in seconds: 3045529.541\n",
      "epoch: 59600, train loss: 4.046767139434815, val loss: 4.066298389434815, ETA in seconds: 3050176.554\n",
      "epoch: 59700, train loss: 4.048329639434814, val loss: 4.0696187019348145, ETA in seconds: 3054890.971\n",
      "epoch: 59800, train loss: 4.052626514434815, val loss: 4.070399951934815, ETA in seconds: 3059949.054\n",
      "epoch: 59900, train loss: 4.058485889434815, val loss: 4.069228076934815, ETA in seconds: 3064821.220\n",
      "epoch: 60000, train loss: 4.0520405769348145, val loss: 4.066103076934814, ETA in seconds: 3069672.729\n",
      "epoch: 60100, train loss: 4.050478076934814, val loss: 4.061415576934815, ETA in seconds: 3074893.302\n",
      "epoch: 60200, train loss: 4.048720264434815, val loss: 4.073915576934814, ETA in seconds: 3079763.908\n",
      "epoch: 60300, train loss: 4.048720264434815, val loss: 4.067860889434814, ETA in seconds: 3084607.749\n",
      "epoch: 60400, train loss: 4.045790576934815, val loss: 4.0657124519348145, ETA in seconds: 3089671.298\n",
      "epoch: 60500, train loss: 4.0481343269348145, val loss: 4.071767139434814, ETA in seconds: 3094601.626\n",
      "epoch: 60600, train loss: 4.0491108894348145, val loss: 4.071767139434814, ETA in seconds: 3099406.693\n",
      "epoch: 60700, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 3104216.236\n",
      "epoch: 60800, train loss: 4.0530171394348145, val loss: 4.062978076934814, ETA in seconds: 3109208.778\n",
      "epoch: 60900, train loss: 4.057118701934814, val loss: 4.068446826934815, ETA in seconds: 3113766.512\n",
      "epoch: 61000, train loss: 4.0627827644348145, val loss: 4.063173389434814, ETA in seconds: 3118421.621\n",
      "epoch: 61100, train loss: 4.055165576934814, val loss: 4.064540576934815, ETA in seconds: 3123239.755\n",
      "epoch: 61200, train loss: 4.055556201934815, val loss: 4.066884326934814, ETA in seconds: 3128100.113\n",
      "epoch: 61300, train loss: 4.050478076934814, val loss: 4.064540576934815, ETA in seconds: 3133405.773\n",
      "epoch: 61400, train loss: 4.051259326934814, val loss: 4.067470264434815, ETA in seconds: 3138592.537\n",
      "epoch: 61500, train loss: 4.053798389434815, val loss: 4.074892139434814, ETA in seconds: 3143485.756\n",
      "epoch: 61600, train loss: 4.0530171394348145, val loss: 4.0696187019348145, ETA in seconds: 3148342.097\n",
      "epoch: 61700, train loss: 4.051454639434814, val loss: 4.067470264434815, ETA in seconds: 3153288.055\n",
      "epoch: 61800, train loss: 4.0588765144348145, val loss: 4.066884326934814, ETA in seconds: 3158088.778\n",
      "epoch: 61900, train loss: 4.060048389434814, val loss: 4.067274951934815, ETA in seconds: 3163097.951\n",
      "epoch: 62000, train loss: 4.055751514434815, val loss: 4.074696826934814, ETA in seconds: 3168356.185\n",
      "epoch: 62100, train loss: 4.056142139434814, val loss: 4.070204639434815, ETA in seconds: 3173487.160\n",
      "epoch: 62200, train loss: 4.047548389434814, val loss: 4.065321826934815, ETA in seconds: 3178553.637\n",
      "epoch: 62300, train loss: 4.045790576934815, val loss: 4.070790576934814, ETA in seconds: 3183425.609\n",
      "epoch: 62400, train loss: 4.0491108894348145, val loss: 4.067470264434815, ETA in seconds: 3188301.823\n",
      "epoch: 62500, train loss: 4.0578999519348145, val loss: 4.066298389434815, ETA in seconds: 3193393.727\n",
      "epoch: 62600, train loss: 4.057704639434815, val loss: 4.072157764434815, ETA in seconds: 3198402.060\n",
      "epoch: 62700, train loss: 4.051845264434815, val loss: 4.067860889434814, ETA in seconds: 3203403.439\n",
      "epoch: 62800, train loss: 4.0500874519348145, val loss: 4.070985889434814, ETA in seconds: 3207967.873\n",
      "epoch: 62900, train loss: 4.0491108894348145, val loss: 4.066884326934814, ETA in seconds: 3212551.856\n",
      "epoch: 63000, train loss: 4.053212451934814, val loss: 4.0725483894348145, ETA in seconds: 3217379.117\n",
      "epoch: 63100, train loss: 4.052431201934814, val loss: 4.065907764434814, ETA in seconds: 3222224.899\n",
      "epoch: 63200, train loss: 4.054384326934814, val loss: 4.067274951934815, ETA in seconds: 3227210.699\n",
      "epoch: 63300, train loss: 4.054189014434814, val loss: 4.0657124519348145, ETA in seconds: 3232409.998\n",
      "epoch: 63400, train loss: 4.052626514434815, val loss: 4.073720264434814, ETA in seconds: 3237461.482\n",
      "epoch: 63500, train loss: 4.055165576934814, val loss: 4.0657124519348145, ETA in seconds: 3242686.851\n",
      "epoch: 63600, train loss: 4.050868701934815, val loss: 4.065517139434815, ETA in seconds: 3247851.617\n",
      "epoch: 63700, train loss: 4.057704639434815, val loss: 4.068251514434815, ETA in seconds: 3252695.294\n",
      "epoch: 63800, train loss: 4.058290576934814, val loss: 4.0676655769348145, ETA in seconds: 3257422.296\n",
      "epoch: 63900, train loss: 4.051649951934815, val loss: 4.068251514434815, ETA in seconds: 3262063.107\n",
      "epoch: 64000, train loss: 4.047353076934814, val loss: 4.066298389434815, ETA in seconds: 3266869.616\n",
      "epoch: 64100, train loss: 4.0559468269348145, val loss: 4.068056201934814, ETA in seconds: 3271797.872\n",
      "epoch: 64200, train loss: 4.054384326934814, val loss: 4.070009326934814, ETA in seconds: 3276511.621\n",
      "epoch: 64300, train loss: 4.049892139434815, val loss: 4.067470264434815, ETA in seconds: 3281330.090\n",
      "epoch: 64400, train loss: 4.056142139434814, val loss: 4.070790576934814, ETA in seconds: 3286077.978\n",
      "epoch: 64500, train loss: 4.052626514434815, val loss: 4.065321826934815, ETA in seconds: 3290558.480\n",
      "epoch: 64600, train loss: 4.0539937019348145, val loss: 4.065321826934815, ETA in seconds: 3295113.439\n",
      "epoch: 64700, train loss: 4.051259326934814, val loss: 4.062392139434815, ETA in seconds: 3299804.004\n",
      "epoch: 64800, train loss: 4.051845264434815, val loss: 4.069032764434814, ETA in seconds: 3304499.243\n",
      "epoch: 64900, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 3309245.964\n",
      "epoch: 65000, train loss: 4.063564014434815, val loss: 4.069228076934815, ETA in seconds: 3313887.620\n",
      "epoch: 65100, train loss: 4.050868701934815, val loss: 4.069814014434814, ETA in seconds: 3318683.446\n",
      "epoch: 65200, train loss: 4.058290576934814, val loss: 4.063368701934815, ETA in seconds: 3323664.880\n",
      "epoch: 65300, train loss: 4.050282764434814, val loss: 4.0715718269348145, ETA in seconds: 3328451.949\n",
      "epoch: 65400, train loss: 4.0539937019348145, val loss: 4.062978076934814, ETA in seconds: 3333263.143\n",
      "epoch: 65500, train loss: 4.048720264434815, val loss: 4.0686421394348145, ETA in seconds: 3338165.879\n",
      "epoch: 65600, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 3343076.486\n",
      "epoch: 65700, train loss: 4.047548389434814, val loss: 4.0588765144348145, ETA in seconds: 3348016.491\n",
      "epoch: 65800, train loss: 4.049696826934815, val loss: 4.063564014434815, ETA in seconds: 3352877.075\n",
      "epoch: 65900, train loss: 4.045790576934815, val loss: 4.065126514434814, ETA in seconds: 3357817.673\n",
      "epoch: 66000, train loss: 4.058681201934815, val loss: 4.068251514434815, ETA in seconds: 3362694.485\n",
      "epoch: 66100, train loss: 4.0422749519348145, val loss: 4.066493701934815, ETA in seconds: 3367508.922\n",
      "epoch: 66200, train loss: 4.0520405769348145, val loss: 4.0637593269348145, ETA in seconds: 3372187.908\n",
      "epoch: 66300, train loss: 4.054774951934815, val loss: 4.067079639434814, ETA in seconds: 3376991.635\n",
      "epoch: 66400, train loss: 4.054774951934815, val loss: 4.0676655769348145, ETA in seconds: 3381747.103\n",
      "epoch: 66500, train loss: 4.048524951934814, val loss: 4.068251514434815, ETA in seconds: 3386457.095\n",
      "epoch: 66600, train loss: 4.0530171394348145, val loss: 4.0647358894348145, ETA in seconds: 3391183.686\n",
      "epoch: 66700, train loss: 4.053798389434815, val loss: 4.0618062019348145, ETA in seconds: 3396004.339\n",
      "epoch: 66800, train loss: 4.057118701934814, val loss: 4.069423389434815, ETA in seconds: 3400703.864\n",
      "epoch: 66900, train loss: 4.0559468269348145, val loss: 4.068837451934814, ETA in seconds: 3405415.449\n",
      "epoch: 67000, train loss: 4.0491108894348145, val loss: 4.0657124519348145, ETA in seconds: 3410245.192\n",
      "epoch: 67100, train loss: 4.045985889434815, val loss: 4.064931201934814, ETA in seconds: 3415101.093\n",
      "epoch: 67200, train loss: 4.0549702644348145, val loss: 4.0696187019348145, ETA in seconds: 3419760.259\n",
      "epoch: 67300, train loss: 4.0500874519348145, val loss: 4.070009326934814, ETA in seconds: 3424277.107\n",
      "epoch: 67400, train loss: 4.054189014434814, val loss: 4.0686421394348145, ETA in seconds: 3429022.349\n",
      "epoch: 67500, train loss: 4.051649951934815, val loss: 4.064540576934815, ETA in seconds: 3434103.928\n",
      "epoch: 67600, train loss: 4.058485889434815, val loss: 4.063173389434814, ETA in seconds: 3439307.857\n",
      "epoch: 67700, train loss: 4.049306201934814, val loss: 4.067860889434814, ETA in seconds: 3444035.206\n",
      "epoch: 67800, train loss: 4.055165576934814, val loss: 4.069228076934815, ETA in seconds: 3448933.244\n",
      "epoch: 67900, train loss: 4.051845264434815, val loss: 4.0657124519348145, ETA in seconds: 3453699.072\n",
      "epoch: 68000, train loss: 4.050282764434814, val loss: 4.065517139434815, ETA in seconds: 3458445.174\n",
      "epoch: 68100, train loss: 4.053407764434814, val loss: 4.066493701934815, ETA in seconds: 3463273.277\n",
      "epoch: 68200, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 3467931.162\n",
      "epoch: 68300, train loss: 4.060439014434815, val loss: 4.0666890144348145, ETA in seconds: 3472690.015\n",
      "epoch: 68400, train loss: 4.047548389434814, val loss: 4.073915576934814, ETA in seconds: 3477555.604\n",
      "epoch: 68500, train loss: 4.0471577644348145, val loss: 4.065907764434814, ETA in seconds: 3482025.951\n",
      "epoch: 68600, train loss: 4.0491108894348145, val loss: 4.066884326934814, ETA in seconds: 3486168.386\n",
      "epoch: 68700, train loss: 4.0510640144348145, val loss: 4.072157764434815, ETA in seconds: 3490170.195\n",
      "epoch: 68800, train loss: 4.055165576934814, val loss: 4.066884326934814, ETA in seconds: 3494396.483\n",
      "epoch: 68900, train loss: 4.048524951934814, val loss: 4.065126514434814, ETA in seconds: 3499455.845\n",
      "epoch: 69000, train loss: 4.054384326934814, val loss: 4.069228076934815, ETA in seconds: 3504097.499\n",
      "epoch: 69100, train loss: 4.053603076934815, val loss: 4.068251514434815, ETA in seconds: 3508748.305\n",
      "epoch: 69200, train loss: 4.048524951934814, val loss: 4.0686421394348145, ETA in seconds: 3513509.570\n",
      "epoch: 69300, train loss: 4.051649951934815, val loss: 4.068056201934814, ETA in seconds: 3518329.446\n",
      "epoch: 69400, train loss: 4.055556201934815, val loss: 4.068837451934814, ETA in seconds: 3523027.346\n",
      "epoch: 69500, train loss: 4.047353076934814, val loss: 4.068056201934814, ETA in seconds: 3527605.713\n",
      "epoch: 69600, train loss: 4.0530171394348145, val loss: 4.067079639434814, ETA in seconds: 3532349.882\n",
      "epoch: 69700, train loss: 4.049892139434815, val loss: 4.0666890144348145, ETA in seconds: 3537102.986\n",
      "epoch: 69800, train loss: 4.053407764434814, val loss: 4.0637593269348145, ETA in seconds: 3541791.498\n",
      "epoch: 69900, train loss: 4.048915576934815, val loss: 4.065517139434815, ETA in seconds: 3545889.333\n",
      "epoch: 70000, train loss: 4.052431201934814, val loss: 4.064931201934814, ETA in seconds: 3550879.483\n",
      "epoch: 70100, train loss: 4.046571826934814, val loss: 4.066493701934815, ETA in seconds: 3555838.062\n",
      "epoch: 70200, train loss: 4.0549702644348145, val loss: 4.065126514434814, ETA in seconds: 3560670.652\n",
      "epoch: 70300, train loss: 4.058485889434815, val loss: 4.0764546394348145, ETA in seconds: 3565546.277\n",
      "epoch: 70400, train loss: 4.0461812019348145, val loss: 4.067079639434814, ETA in seconds: 3570424.896\n",
      "epoch: 70500, train loss: 4.048524951934814, val loss: 4.065907764434814, ETA in seconds: 3575034.323\n",
      "epoch: 70600, train loss: 4.047743701934815, val loss: 4.066103076934814, ETA in seconds: 3579707.128\n",
      "epoch: 70700, train loss: 4.048720264434815, val loss: 4.063954639434814, ETA in seconds: 3584412.212\n",
      "epoch: 70800, train loss: 4.048524951934814, val loss: 4.066298389434815, ETA in seconds: 3589147.599\n",
      "epoch: 70900, train loss: 4.0549702644348145, val loss: 4.069228076934815, ETA in seconds: 3593884.071\n",
      "epoch: 71000, train loss: 4.051454639434814, val loss: 4.069228076934815, ETA in seconds: 3598632.620\n",
      "epoch: 71100, train loss: 4.049501514434814, val loss: 4.061415576934815, ETA in seconds: 3603318.979\n",
      "epoch: 71200, train loss: 4.047548389434814, val loss: 4.059267139434814, ETA in seconds: 3607913.779\n",
      "epoch: 71300, train loss: 4.046962451934815, val loss: 4.072353076934815, ETA in seconds: 3612519.720\n",
      "epoch: 71400, train loss: 4.054774951934815, val loss: 4.067860889434814, ETA in seconds: 3617217.714\n",
      "epoch: 71500, train loss: 4.048720264434815, val loss: 4.071767139434814, ETA in seconds: 3621895.170\n",
      "epoch: 71600, train loss: 4.048720264434815, val loss: 4.065321826934815, ETA in seconds: 3626649.930\n",
      "epoch: 71700, train loss: 4.0559468269348145, val loss: 4.064931201934814, ETA in seconds: 3631521.570\n",
      "epoch: 71800, train loss: 4.054774951934815, val loss: 4.065907764434814, ETA in seconds: 3636208.536\n",
      "epoch: 71900, train loss: 4.053407764434814, val loss: 4.064345264434815, ETA in seconds: 3640840.682\n",
      "epoch: 72000, train loss: 4.055751514434815, val loss: 4.064931201934814, ETA in seconds: 3645764.612\n",
      "epoch: 72100, train loss: 4.053798389434815, val loss: 4.075282764434815, ETA in seconds: 3650667.022\n",
      "epoch: 72200, train loss: 4.051845264434815, val loss: 4.0647358894348145, ETA in seconds: 3655343.837\n",
      "epoch: 72300, train loss: 4.050868701934815, val loss: 4.0725483894348145, ETA in seconds: 3660163.352\n",
      "epoch: 72400, train loss: 4.0618062019348145, val loss: 4.067860889434814, ETA in seconds: 3664774.146\n",
      "epoch: 72500, train loss: 4.046767139434815, val loss: 4.063954639434814, ETA in seconds: 3669409.794\n",
      "epoch: 72600, train loss: 4.049892139434815, val loss: 4.065907764434814, ETA in seconds: 3673992.692\n",
      "epoch: 72700, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 3678500.370\n",
      "epoch: 72800, train loss: 4.052431201934814, val loss: 4.0657124519348145, ETA in seconds: 3682959.609\n",
      "epoch: 72900, train loss: 4.041493701934814, val loss: 4.068056201934814, ETA in seconds: 3687566.644\n",
      "epoch: 73000, train loss: 4.057509326934815, val loss: 4.068446826934815, ETA in seconds: 3692140.748\n",
      "epoch: 73100, train loss: 4.057118701934814, val loss: 4.067274951934815, ETA in seconds: 3696753.234\n",
      "epoch: 73200, train loss: 4.043642139434814, val loss: 4.065907764434814, ETA in seconds: 3701230.798\n",
      "epoch: 73300, train loss: 4.046767139434815, val loss: 4.063954639434814, ETA in seconds: 3705684.908\n",
      "epoch: 73400, train loss: 4.047743701934815, val loss: 4.068056201934814, ETA in seconds: 3710256.102\n",
      "epoch: 73500, train loss: 4.0588765144348145, val loss: 4.061024951934814, ETA in seconds: 3714824.423\n",
      "epoch: 73600, train loss: 4.053212451934814, val loss: 4.0686421394348145, ETA in seconds: 3719488.515\n",
      "epoch: 73700, train loss: 4.051649951934815, val loss: 4.067079639434814, ETA in seconds: 3724492.976\n",
      "epoch: 73800, train loss: 4.0520405769348145, val loss: 4.063564014434815, ETA in seconds: 3729408.090\n",
      "epoch: 73900, train loss: 4.047353076934814, val loss: 4.068446826934815, ETA in seconds: 3734517.307\n",
      "epoch: 74000, train loss: 4.047939014434815, val loss: 4.064540576934815, ETA in seconds: 3739426.473\n",
      "epoch: 74100, train loss: 4.055165576934814, val loss: 4.066298389434815, ETA in seconds: 3743987.343\n",
      "epoch: 74200, train loss: 4.052626514434815, val loss: 4.066493701934815, ETA in seconds: 3748564.837\n",
      "epoch: 74300, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 3753313.143\n",
      "epoch: 74400, train loss: 4.049892139434815, val loss: 4.0696187019348145, ETA in seconds: 3757914.389\n",
      "epoch: 74500, train loss: 4.050282764434814, val loss: 4.068251514434815, ETA in seconds: 3762588.104\n",
      "epoch: 74600, train loss: 4.050868701934815, val loss: 4.061024951934814, ETA in seconds: 3767257.492\n",
      "epoch: 74700, train loss: 4.049306201934814, val loss: 4.070009326934814, ETA in seconds: 3772110.596\n",
      "epoch: 74800, train loss: 4.047743701934815, val loss: 4.070009326934814, ETA in seconds: 3776890.823\n",
      "epoch: 74900, train loss: 4.048524951934814, val loss: 4.064345264434815, ETA in seconds: 3781555.298\n",
      "epoch: 75000, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 3786265.570\n",
      "epoch: 75100, train loss: 4.050282764434814, val loss: 4.0627827644348145, ETA in seconds: 3790994.066\n",
      "epoch: 75200, train loss: 4.059267139434814, val loss: 4.065321826934815, ETA in seconds: 3795522.982\n",
      "epoch: 75300, train loss: 4.045595264434814, val loss: 4.066884326934814, ETA in seconds: 3800266.024\n",
      "epoch: 75400, train loss: 4.0461812019348145, val loss: 4.068251514434815, ETA in seconds: 3804939.864\n",
      "epoch: 75500, train loss: 4.054384326934814, val loss: 4.0657124519348145, ETA in seconds: 3809783.097\n",
      "epoch: 75600, train loss: 4.060048389434814, val loss: 4.069814014434814, ETA in seconds: 3814471.256\n",
      "epoch: 75700, train loss: 4.052821826934815, val loss: 4.067079639434814, ETA in seconds: 3819066.343\n",
      "epoch: 75800, train loss: 4.050673389434815, val loss: 4.073329639434815, ETA in seconds: 3823534.308\n",
      "epoch: 75900, train loss: 4.042860889434815, val loss: 4.0657124519348145, ETA in seconds: 3828213.170\n",
      "epoch: 76000, train loss: 4.057314014434814, val loss: 4.0637593269348145, ETA in seconds: 3832717.377\n",
      "epoch: 76100, train loss: 4.0539937019348145, val loss: 4.069228076934815, ETA in seconds: 3837122.512\n",
      "epoch: 76200, train loss: 4.049501514434814, val loss: 4.066884326934814, ETA in seconds: 3841370.059\n",
      "epoch: 76300, train loss: 4.050868701934815, val loss: 4.0657124519348145, ETA in seconds: 3845551.779\n",
      "epoch: 76400, train loss: 4.055165576934814, val loss: 4.069032764434814, ETA in seconds: 3849667.938\n",
      "epoch: 76500, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 3854031.113\n",
      "epoch: 76600, train loss: 4.0452046394348145, val loss: 4.067079639434814, ETA in seconds: 3858351.985\n",
      "epoch: 76700, train loss: 4.0471577644348145, val loss: 4.0666890144348145, ETA in seconds: 3862828.449\n",
      "epoch: 76800, train loss: 4.049306201934814, val loss: 4.070204639434815, ETA in seconds: 3867166.713\n",
      "epoch: 76900, train loss: 4.056532764434815, val loss: 4.065517139434815, ETA in seconds: 3871487.449\n",
      "epoch: 77000, train loss: 4.053212451934814, val loss: 4.062001514434814, ETA in seconds: 3875702.986\n",
      "epoch: 77100, train loss: 4.0520405769348145, val loss: 4.070985889434814, ETA in seconds: 3879859.981\n",
      "epoch: 77200, train loss: 4.046571826934814, val loss: 4.072157764434815, ETA in seconds: 3884226.756\n",
      "epoch: 77300, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 3888702.345\n",
      "epoch: 77400, train loss: 4.0559468269348145, val loss: 4.067274951934815, ETA in seconds: 3893225.134\n",
      "epoch: 77500, train loss: 4.054189014434814, val loss: 4.067860889434814, ETA in seconds: 3897639.886\n",
      "epoch: 77600, train loss: 4.047939014434815, val loss: 4.072157764434815, ETA in seconds: 3902268.742\n",
      "epoch: 77700, train loss: 4.040126514434815, val loss: 4.066493701934815, ETA in seconds: 3906694.526\n",
      "epoch: 77800, train loss: 4.052821826934815, val loss: 4.0676655769348145, ETA in seconds: 3911006.073\n",
      "epoch: 77900, train loss: 4.050868701934815, val loss: 4.070204639434815, ETA in seconds: 3915792.974\n",
      "epoch: 78000, train loss: 4.049501514434814, val loss: 4.061415576934815, ETA in seconds: 3920612.302\n",
      "epoch: 78100, train loss: 4.0491108894348145, val loss: 4.067274951934815, ETA in seconds: 3925439.087\n",
      "epoch: 78200, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 3930232.003\n",
      "epoch: 78300, train loss: 4.052235889434814, val loss: 4.066884326934814, ETA in seconds: 3934896.292\n",
      "epoch: 78400, train loss: 4.0559468269348145, val loss: 4.069228076934815, ETA in seconds: 3939534.062\n",
      "epoch: 78500, train loss: 4.054579639434815, val loss: 4.0657124519348145, ETA in seconds: 3944148.188\n",
      "epoch: 78600, train loss: 4.049892139434815, val loss: 4.0725483894348145, ETA in seconds: 3948871.870\n",
      "epoch: 78700, train loss: 4.050673389434815, val loss: 4.0745015144348145, ETA in seconds: 3953495.433\n",
      "epoch: 78800, train loss: 4.0530171394348145, val loss: 4.068837451934814, ETA in seconds: 3958021.253\n",
      "epoch: 78900, train loss: 4.056142139434814, val loss: 4.064931201934814, ETA in seconds: 3962515.348\n",
      "epoch: 79000, train loss: 4.051454639434814, val loss: 4.069814014434814, ETA in seconds: 3967030.988\n",
      "epoch: 79100, train loss: 4.047743701934815, val loss: 4.068837451934814, ETA in seconds: 3971675.945\n",
      "epoch: 79200, train loss: 4.0471577644348145, val loss: 4.067274951934815, ETA in seconds: 3976293.239\n",
      "epoch: 79300, train loss: 4.052626514434815, val loss: 4.068251514434815, ETA in seconds: 3980876.041\n",
      "epoch: 79400, train loss: 4.048329639434814, val loss: 4.062001514434814, ETA in seconds: 3985342.470\n",
      "epoch: 79500, train loss: 4.052431201934814, val loss: 4.0657124519348145, ETA in seconds: 3989944.627\n",
      "epoch: 79600, train loss: 4.053407764434814, val loss: 4.0696187019348145, ETA in seconds: 3994424.041\n",
      "epoch: 79700, train loss: 4.059462451934815, val loss: 4.0686421394348145, ETA in seconds: 3998936.836\n",
      "epoch: 79800, train loss: 4.057509326934815, val loss: 4.070399951934815, ETA in seconds: 4003318.417\n",
      "epoch: 79900, train loss: 4.0559468269348145, val loss: 4.067079639434814, ETA in seconds: 4007722.825\n",
      "epoch: 80000, train loss: 4.050478076934814, val loss: 4.062978076934814, ETA in seconds: 4012288.364\n",
      "epoch: 80100, train loss: 4.056728076934815, val loss: 4.069032764434814, ETA in seconds: 4016820.168\n",
      "epoch: 80200, train loss: 4.056532764434815, val loss: 4.0637593269348145, ETA in seconds: 4021354.747\n",
      "epoch: 80300, train loss: 4.050868701934815, val loss: 4.068837451934814, ETA in seconds: 4025761.359\n",
      "epoch: 80400, train loss: 4.055165576934814, val loss: 4.067470264434815, ETA in seconds: 4030142.240\n",
      "epoch: 80500, train loss: 4.051845264434815, val loss: 4.0647358894348145, ETA in seconds: 4034671.515\n",
      "epoch: 80600, train loss: 4.048915576934815, val loss: 4.072939014434814, ETA in seconds: 4039205.637\n",
      "epoch: 80700, train loss: 4.056532764434815, val loss: 4.066298389434815, ETA in seconds: 4043797.768\n",
      "epoch: 80800, train loss: 4.046376514434814, val loss: 4.069814014434814, ETA in seconds: 4048531.174\n",
      "epoch: 80900, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 4053150.117\n",
      "epoch: 81000, train loss: 4.0471577644348145, val loss: 4.067079639434814, ETA in seconds: 4058022.969\n",
      "epoch: 81100, train loss: 4.057118701934814, val loss: 4.066493701934815, ETA in seconds: 4062735.965\n",
      "epoch: 81200, train loss: 4.0539937019348145, val loss: 4.072939014434814, ETA in seconds: 4067344.513\n",
      "epoch: 81300, train loss: 4.0461812019348145, val loss: 4.0647358894348145, ETA in seconds: 4071859.935\n",
      "epoch: 81400, train loss: 4.048329639434814, val loss: 4.062392139434815, ETA in seconds: 4076386.526\n",
      "epoch: 81500, train loss: 4.046962451934815, val loss: 4.072353076934815, ETA in seconds: 4080782.419\n",
      "epoch: 81600, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 4085256.457\n",
      "epoch: 81700, train loss: 4.051454639434814, val loss: 4.067860889434814, ETA in seconds: 4089685.404\n",
      "epoch: 81800, train loss: 4.049306201934814, val loss: 4.065517139434815, ETA in seconds: 4094152.250\n",
      "epoch: 81900, train loss: 4.049501514434814, val loss: 4.065907764434814, ETA in seconds: 4098678.616\n",
      "epoch: 82000, train loss: 4.053407764434814, val loss: 4.0637593269348145, ETA in seconds: 4103299.605\n",
      "epoch: 82100, train loss: 4.052431201934814, val loss: 4.067470264434815, ETA in seconds: 4107763.088\n",
      "epoch: 82200, train loss: 4.048524951934814, val loss: 4.069032764434814, ETA in seconds: 4111839.745\n",
      "epoch: 82300, train loss: 4.053212451934814, val loss: 4.070985889434814, ETA in seconds: 4115982.147\n",
      "epoch: 82400, train loss: 4.052235889434814, val loss: 4.066884326934814, ETA in seconds: 4120302.535\n",
      "epoch: 82500, train loss: 4.050868701934815, val loss: 4.064149951934814, ETA in seconds: 4124799.836\n",
      "epoch: 82600, train loss: 4.041103076934815, val loss: 4.063173389434814, ETA in seconds: 4129359.078\n",
      "epoch: 82700, train loss: 4.048915576934815, val loss: 4.070399951934815, ETA in seconds: 4133751.547\n",
      "epoch: 82800, train loss: 4.052821826934815, val loss: 4.0627827644348145, ETA in seconds: 4138210.649\n",
      "epoch: 82900, train loss: 4.054774951934815, val loss: 4.060634326934815, ETA in seconds: 4142772.008\n",
      "epoch: 83000, train loss: 4.051259326934814, val loss: 4.064931201934814, ETA in seconds: 4147219.467\n",
      "epoch: 83100, train loss: 4.052431201934814, val loss: 4.066103076934814, ETA in seconds: 4151803.303\n",
      "epoch: 83200, train loss: 4.055360889434814, val loss: 4.064931201934814, ETA in seconds: 4156554.634\n",
      "epoch: 83300, train loss: 4.046767139434815, val loss: 4.069423389434815, ETA in seconds: 4161138.072\n",
      "epoch: 83400, train loss: 4.055165576934814, val loss: 4.0715718269348145, ETA in seconds: 4165778.093\n",
      "epoch: 83500, train loss: 4.0500874519348145, val loss: 4.065321826934815, ETA in seconds: 4170188.243\n",
      "epoch: 83600, train loss: 4.058290576934814, val loss: 4.067470264434815, ETA in seconds: 4174491.045\n",
      "epoch: 83700, train loss: 4.054579639434815, val loss: 4.064540576934815, ETA in seconds: 4178108.609\n",
      "epoch: 83800, train loss: 4.055165576934814, val loss: 4.0657124519348145, ETA in seconds: 4181630.896\n",
      "epoch: 83900, train loss: 4.051649951934815, val loss: 4.065321826934815, ETA in seconds: 4185127.765\n",
      "epoch: 84000, train loss: 4.054189014434814, val loss: 4.070985889434814, ETA in seconds: 4188804.469\n",
      "epoch: 84100, train loss: 4.050673389434815, val loss: 4.063954639434814, ETA in seconds: 4193496.146\n",
      "epoch: 84200, train loss: 4.049892139434815, val loss: 4.069423389434815, ETA in seconds: 4198028.559\n",
      "epoch: 84300, train loss: 4.0432515144348145, val loss: 4.065907764434814, ETA in seconds: 4202435.450\n",
      "epoch: 84400, train loss: 4.057118701934814, val loss: 4.0666890144348145, ETA in seconds: 4206832.288\n",
      "epoch: 84500, train loss: 4.051649951934815, val loss: 4.065126514434814, ETA in seconds: 4211360.832\n",
      "epoch: 84600, train loss: 4.059657764434815, val loss: 4.069032764434814, ETA in seconds: 4215807.942\n",
      "epoch: 84700, train loss: 4.0588765144348145, val loss: 4.0686421394348145, ETA in seconds: 4220373.304\n",
      "epoch: 84800, train loss: 4.0657124519348145, val loss: 4.0627827644348145, ETA in seconds: 4224907.627\n",
      "epoch: 84900, train loss: 4.052235889434814, val loss: 4.070204639434815, ETA in seconds: 4229319.172\n",
      "epoch: 85000, train loss: 4.044618701934814, val loss: 4.069423389434815, ETA in seconds: 4233284.840\n",
      "epoch: 85100, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 4238167.613\n",
      "epoch: 85200, train loss: 4.0598530769348145, val loss: 4.062587451934815, ETA in seconds: 4242822.476\n",
      "epoch: 85300, train loss: 4.0520405769348145, val loss: 4.068056201934814, ETA in seconds: 4247425.310\n",
      "epoch: 85400, train loss: 4.0578999519348145, val loss: 4.071181201934815, ETA in seconds: 4252068.051\n",
      "epoch: 85500, train loss: 4.0588765144348145, val loss: 4.071376514434815, ETA in seconds: 4256725.213\n",
      "epoch: 85600, train loss: 4.0549702644348145, val loss: 4.063368701934815, ETA in seconds: 4261310.811\n",
      "epoch: 85700, train loss: 4.047353076934814, val loss: 4.066884326934814, ETA in seconds: 4265941.604\n",
      "epoch: 85800, train loss: 4.050868701934815, val loss: 4.071376514434815, ETA in seconds: 4270463.760\n",
      "epoch: 85900, train loss: 4.054579639434815, val loss: 4.066103076934814, ETA in seconds: 4275006.513\n",
      "epoch: 86000, train loss: 4.059071826934814, val loss: 4.063564014434815, ETA in seconds: 4279705.221\n",
      "epoch: 86100, train loss: 4.0491108894348145, val loss: 4.065321826934815, ETA in seconds: 4284455.720\n",
      "epoch: 86200, train loss: 4.054579639434815, val loss: 4.069032764434814, ETA in seconds: 4288888.501\n",
      "epoch: 86300, train loss: 4.0530171394348145, val loss: 4.0705952644348145, ETA in seconds: 4293353.296\n",
      "epoch: 86400, train loss: 4.0539937019348145, val loss: 4.070399951934815, ETA in seconds: 4297904.820\n",
      "epoch: 86500, train loss: 4.050673389434815, val loss: 4.0657124519348145, ETA in seconds: 4302769.906\n",
      "epoch: 86600, train loss: 4.053407764434814, val loss: 4.064931201934814, ETA in seconds: 4307702.024\n",
      "epoch: 86700, train loss: 4.0491108894348145, val loss: 4.065907764434814, ETA in seconds: 4312428.132\n",
      "epoch: 86800, train loss: 4.0471577644348145, val loss: 4.0735249519348145, ETA in seconds: 4317146.183\n",
      "epoch: 86900, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 4321830.636\n",
      "epoch: 87000, train loss: 4.052235889434814, val loss: 4.064345264434815, ETA in seconds: 4326468.813\n",
      "epoch: 87100, train loss: 4.049501514434814, val loss: 4.0696187019348145, ETA in seconds: 4331012.321\n",
      "epoch: 87200, train loss: 4.0559468269348145, val loss: 4.070985889434814, ETA in seconds: 4335598.585\n",
      "epoch: 87300, train loss: 4.048915576934815, val loss: 4.0657124519348145, ETA in seconds: 4340319.194\n",
      "epoch: 87400, train loss: 4.055165576934814, val loss: 4.070790576934814, ETA in seconds: 4344818.619\n",
      "epoch: 87500, train loss: 4.053603076934815, val loss: 4.069423389434815, ETA in seconds: 4349429.088\n",
      "epoch: 87600, train loss: 4.051845264434815, val loss: 4.064931201934814, ETA in seconds: 4353911.278\n",
      "epoch: 87700, train loss: 4.052431201934814, val loss: 4.065321826934815, ETA in seconds: 4358507.443\n",
      "epoch: 87800, train loss: 4.050282764434814, val loss: 4.065321826934815, ETA in seconds: 4363241.821\n",
      "epoch: 87900, train loss: 4.050282764434814, val loss: 4.067274951934815, ETA in seconds: 4367860.704\n",
      "epoch: 88000, train loss: 4.054384326934814, val loss: 4.069423389434815, ETA in seconds: 4372461.398\n",
      "epoch: 88100, train loss: 4.0500874519348145, val loss: 4.0696187019348145, ETA in seconds: 4377447.004\n",
      "epoch: 88200, train loss: 4.051259326934814, val loss: 4.069032764434814, ETA in seconds: 4382100.058\n",
      "epoch: 88300, train loss: 4.047353076934814, val loss: 4.0637593269348145, ETA in seconds: 4386653.411\n",
      "epoch: 88400, train loss: 4.047353076934814, val loss: 4.0686421394348145, ETA in seconds: 4391190.195\n",
      "epoch: 88500, train loss: 4.0549702644348145, val loss: 4.062196826934814, ETA in seconds: 4395698.469\n",
      "epoch: 88600, train loss: 4.0530171394348145, val loss: 4.0686421394348145, ETA in seconds: 4399935.011\n",
      "epoch: 88700, train loss: 4.0500874519348145, val loss: 4.070790576934814, ETA in seconds: 4404347.845\n",
      "epoch: 88800, train loss: 4.044032764434815, val loss: 4.068446826934815, ETA in seconds: 4408945.840\n",
      "epoch: 88900, train loss: 4.0481343269348145, val loss: 4.070399951934815, ETA in seconds: 4413605.963\n",
      "epoch: 89000, train loss: 4.0432515144348145, val loss: 4.063954639434814, ETA in seconds: 4418093.416\n",
      "epoch: 89100, train loss: 4.054189014434814, val loss: 4.063954639434814, ETA in seconds: 4422365.695\n",
      "epoch: 89200, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 4426698.272\n",
      "epoch: 89300, train loss: 4.0549702644348145, val loss: 4.060048389434814, ETA in seconds: 4431056.583\n",
      "epoch: 89400, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 4435683.836\n",
      "epoch: 89500, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 4440194.789\n",
      "epoch: 89600, train loss: 4.043056201934815, val loss: 4.065321826934815, ETA in seconds: 4444566.167\n",
      "epoch: 89700, train loss: 4.054384326934814, val loss: 4.065907764434814, ETA in seconds: 4449015.870\n",
      "epoch: 89800, train loss: 4.050478076934814, val loss: 4.073915576934814, ETA in seconds: 4453591.313\n",
      "epoch: 89900, train loss: 4.055360889434814, val loss: 4.0637593269348145, ETA in seconds: 4457995.593\n",
      "epoch: 90000, train loss: 4.0539937019348145, val loss: 4.0647358894348145, ETA in seconds: 4462421.233\n",
      "epoch: 90100, train loss: 4.051845264434815, val loss: 4.0637593269348145, ETA in seconds: 4466797.111\n",
      "epoch: 90200, train loss: 4.055360889434814, val loss: 4.064540576934815, ETA in seconds: 4471256.988\n",
      "epoch: 90300, train loss: 4.054579639434815, val loss: 4.071767139434814, ETA in seconds: 4475696.746\n",
      "epoch: 90400, train loss: 4.0578999519348145, val loss: 4.0705952644348145, ETA in seconds: 4480105.968\n",
      "epoch: 90500, train loss: 4.045399951934814, val loss: 4.070985889434814, ETA in seconds: 4484732.107\n",
      "epoch: 90600, train loss: 4.050868701934815, val loss: 4.066493701934815, ETA in seconds: 4489426.612\n",
      "epoch: 90700, train loss: 4.055165576934814, val loss: 4.069814014434814, ETA in seconds: 4493864.891\n",
      "epoch: 90800, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 4498295.705\n",
      "epoch: 90900, train loss: 4.047548389434814, val loss: 4.067274951934815, ETA in seconds: 4502893.361\n",
      "epoch: 91000, train loss: 4.0510640144348145, val loss: 4.069423389434815, ETA in seconds: 4507233.731\n",
      "epoch: 91100, train loss: 4.050868701934815, val loss: 4.069228076934815, ETA in seconds: 4511654.912\n",
      "epoch: 91200, train loss: 4.055165576934814, val loss: 4.070399951934815, ETA in seconds: 4516141.481\n",
      "epoch: 91300, train loss: 4.052431201934814, val loss: 4.068446826934815, ETA in seconds: 4520736.884\n",
      "epoch: 91400, train loss: 4.052626514434815, val loss: 4.071767139434814, ETA in seconds: 4525156.231\n",
      "epoch: 91500, train loss: 4.052235889434814, val loss: 4.065321826934815, ETA in seconds: 4529548.796\n",
      "epoch: 91600, train loss: 4.0559468269348145, val loss: 4.070204639434815, ETA in seconds: 4533807.798\n",
      "epoch: 91700, train loss: 4.055556201934815, val loss: 4.063564014434815, ETA in seconds: 4538226.671\n",
      "epoch: 91800, train loss: 4.0578999519348145, val loss: 4.0647358894348145, ETA in seconds: 4542608.045\n",
      "epoch: 91900, train loss: 4.049892139434815, val loss: 4.0647358894348145, ETA in seconds: 4547041.206\n",
      "epoch: 92000, train loss: 4.0471577644348145, val loss: 4.070009326934814, ETA in seconds: 4551228.667\n",
      "epoch: 92100, train loss: 4.0500874519348145, val loss: 4.066884326934814, ETA in seconds: 4555688.703\n",
      "epoch: 92200, train loss: 4.051845264434815, val loss: 4.0657124519348145, ETA in seconds: 4560236.942\n",
      "epoch: 92300, train loss: 4.056728076934815, val loss: 4.067274951934815, ETA in seconds: 4564744.033\n",
      "epoch: 92400, train loss: 4.055360889434814, val loss: 4.070009326934814, ETA in seconds: 4569318.082\n",
      "epoch: 92500, train loss: 4.048720264434815, val loss: 4.066298389434815, ETA in seconds: 4573774.242\n",
      "epoch: 92600, train loss: 4.056337451934814, val loss: 4.069228076934815, ETA in seconds: 4578211.253\n",
      "epoch: 92700, train loss: 4.051454639434814, val loss: 4.073134326934815, ETA in seconds: 4582625.485\n",
      "epoch: 92800, train loss: 4.053212451934814, val loss: 4.064345264434815, ETA in seconds: 4587220.632\n",
      "epoch: 92900, train loss: 4.052626514434815, val loss: 4.069814014434814, ETA in seconds: 4591798.838\n",
      "epoch: 93000, train loss: 4.046767139434815, val loss: 4.070204639434815, ETA in seconds: 4596443.021\n",
      "epoch: 93100, train loss: 4.052626514434815, val loss: 4.069228076934815, ETA in seconds: 4601286.616\n",
      "epoch: 93200, train loss: 4.0530171394348145, val loss: 4.065126514434814, ETA in seconds: 4606442.742\n",
      "epoch: 93300, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 4611650.003\n",
      "epoch: 93400, train loss: 4.052821826934815, val loss: 4.065321826934815, ETA in seconds: 4616311.439\n",
      "epoch: 93500, train loss: 4.051845264434815, val loss: 4.066884326934814, ETA in seconds: 4621038.713\n",
      "epoch: 93600, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 4626174.792\n",
      "epoch: 93700, train loss: 4.055165576934814, val loss: 4.072157764434815, ETA in seconds: 4630628.983\n",
      "epoch: 93800, train loss: 4.053798389434815, val loss: 4.0686421394348145, ETA in seconds: 4635004.008\n",
      "epoch: 93900, train loss: 4.0549702644348145, val loss: 4.0657124519348145, ETA in seconds: 4639461.058\n",
      "epoch: 94000, train loss: 4.056142139434814, val loss: 4.065907764434814, ETA in seconds: 4643801.194\n",
      "epoch: 94100, train loss: 4.052431201934814, val loss: 4.0666890144348145, ETA in seconds: 4648155.091\n",
      "epoch: 94200, train loss: 4.046571826934814, val loss: 4.068056201934814, ETA in seconds: 4652698.630\n",
      "epoch: 94300, train loss: 4.050673389434815, val loss: 4.071181201934815, ETA in seconds: 4657061.322\n",
      "epoch: 94400, train loss: 4.055751514434815, val loss: 4.063954639434814, ETA in seconds: 4661588.350\n",
      "epoch: 94500, train loss: 4.054579639434815, val loss: 4.069423389434815, ETA in seconds: 4666026.424\n",
      "epoch: 94600, train loss: 4.0471577644348145, val loss: 4.069032764434814, ETA in seconds: 4670600.914\n",
      "epoch: 94700, train loss: 4.054189014434814, val loss: 4.063954639434814, ETA in seconds: 4675051.954\n",
      "epoch: 94800, train loss: 4.052235889434814, val loss: 4.072353076934815, ETA in seconds: 4679282.835\n",
      "epoch: 94900, train loss: 4.051649951934815, val loss: 4.065321826934815, ETA in seconds: 4683570.278\n",
      "epoch: 95000, train loss: 4.058681201934815, val loss: 4.0618062019348145, ETA in seconds: 4688041.575\n",
      "epoch: 95100, train loss: 4.047939014434815, val loss: 4.067860889434814, ETA in seconds: 4692498.955\n",
      "epoch: 95200, train loss: 4.0530171394348145, val loss: 4.063564014434815, ETA in seconds: 4696963.766\n",
      "epoch: 95300, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 4701405.178\n",
      "epoch: 95400, train loss: 4.052431201934814, val loss: 4.064149951934814, ETA in seconds: 4705791.304\n",
      "epoch: 95500, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 4710223.813\n",
      "epoch: 95600, train loss: 4.057118701934814, val loss: 4.0598530769348145, ETA in seconds: 4714561.398\n",
      "epoch: 95700, train loss: 4.048915576934815, val loss: 4.068251514434815, ETA in seconds: 4718934.978\n",
      "epoch: 95800, train loss: 4.056532764434815, val loss: 4.064540576934815, ETA in seconds: 4723280.706\n",
      "epoch: 95900, train loss: 4.0491108894348145, val loss: 4.070985889434814, ETA in seconds: 4727617.438\n",
      "epoch: 96000, train loss: 4.0539937019348145, val loss: 4.069423389434815, ETA in seconds: 4731834.633\n",
      "epoch: 96100, train loss: 4.059071826934814, val loss: 4.063954639434814, ETA in seconds: 4736229.394\n",
      "epoch: 96200, train loss: 4.044032764434815, val loss: 4.067079639434814, ETA in seconds: 4740540.900\n",
      "epoch: 96300, train loss: 4.050282764434814, val loss: 4.063954639434814, ETA in seconds: 4744950.315\n",
      "epoch: 96400, train loss: 4.047743701934815, val loss: 4.071767139434814, ETA in seconds: 4749302.195\n",
      "epoch: 96500, train loss: 4.0471577644348145, val loss: 4.064931201934814, ETA in seconds: 4753785.028\n",
      "epoch: 96600, train loss: 4.049501514434814, val loss: 4.076845264434814, ETA in seconds: 4758111.850\n",
      "epoch: 96700, train loss: 4.052626514434815, val loss: 4.070009326934814, ETA in seconds: 4762513.674\n",
      "epoch: 96800, train loss: 4.0520405769348145, val loss: 4.059657764434815, ETA in seconds: 4766949.846\n",
      "epoch: 96900, train loss: 4.058681201934815, val loss: 4.062978076934814, ETA in seconds: 4771326.465\n",
      "epoch: 97000, train loss: 4.0539937019348145, val loss: 4.069423389434815, ETA in seconds: 4775645.895\n",
      "epoch: 97100, train loss: 4.050868701934815, val loss: 4.0627827644348145, ETA in seconds: 4779925.210\n",
      "epoch: 97200, train loss: 4.051649951934815, val loss: 4.069032764434814, ETA in seconds: 4784374.874\n",
      "epoch: 97300, train loss: 4.0442280769348145, val loss: 4.067274951934815, ETA in seconds: 4788758.615\n",
      "epoch: 97400, train loss: 4.054189014434814, val loss: 4.070009326934814, ETA in seconds: 4793164.283\n",
      "epoch: 97500, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 4797453.924\n",
      "epoch: 97600, train loss: 4.051845264434815, val loss: 4.063173389434814, ETA in seconds: 4801940.646\n",
      "epoch: 97700, train loss: 4.057704639434815, val loss: 4.070399951934815, ETA in seconds: 4806503.996\n",
      "epoch: 97800, train loss: 4.049892139434815, val loss: 4.067470264434815, ETA in seconds: 4810896.970\n",
      "epoch: 97900, train loss: 4.056142139434814, val loss: 4.066103076934814, ETA in seconds: 4815350.522\n",
      "epoch: 98000, train loss: 4.055360889434814, val loss: 4.066103076934814, ETA in seconds: 4819870.840\n",
      "epoch: 98100, train loss: 4.0530171394348145, val loss: 4.068837451934814, ETA in seconds: 4824180.631\n",
      "epoch: 98200, train loss: 4.047548389434814, val loss: 4.063564014434815, ETA in seconds: 4828659.311\n",
      "epoch: 98300, train loss: 4.045009326934815, val loss: 4.066103076934814, ETA in seconds: 4833160.919\n",
      "epoch: 98400, train loss: 4.0520405769348145, val loss: 4.069814014434814, ETA in seconds: 4837590.480\n",
      "epoch: 98500, train loss: 4.0549702644348145, val loss: 4.065517139434815, ETA in seconds: 4841880.294\n",
      "epoch: 98600, train loss: 4.047548389434814, val loss: 4.066493701934815, ETA in seconds: 4846416.255\n",
      "epoch: 98700, train loss: 4.053212451934814, val loss: 4.067470264434815, ETA in seconds: 4850891.677\n",
      "epoch: 98800, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 4855005.773\n",
      "epoch: 98900, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 4859252.026\n",
      "epoch: 99000, train loss: 4.052235889434814, val loss: 4.064540576934815, ETA in seconds: 4863709.010\n",
      "epoch: 99100, train loss: 4.055165576934814, val loss: 4.065321826934815, ETA in seconds: 4868044.212\n",
      "epoch: 99200, train loss: 4.053603076934815, val loss: 4.069032764434814, ETA in seconds: 4872330.364\n",
      "epoch: 99300, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 4876689.817\n",
      "epoch: 99400, train loss: 4.047939014434815, val loss: 4.062978076934814, ETA in seconds: 4880890.968\n",
      "epoch: 99500, train loss: 4.046571826934814, val loss: 4.067470264434815, ETA in seconds: 4885328.975\n",
      "epoch: 99600, train loss: 4.056337451934814, val loss: 4.067860889434814, ETA in seconds: 4889569.096\n",
      "epoch: 99700, train loss: 4.0500874519348145, val loss: 4.072743701934814, ETA in seconds: 4894051.560\n",
      "epoch: 99800, train loss: 4.0539937019348145, val loss: 4.068446826934815, ETA in seconds: 4898553.292\n",
      "epoch: 99900, train loss: 4.053798389434815, val loss: 4.068837451934814, ETA in seconds: 4902845.975\n",
      "epoch: 100000, train loss: 4.051454639434814, val loss: 4.064149951934814, ETA in seconds: 4907057.347\n",
      "epoch: 100100, train loss: 4.0530171394348145, val loss: 4.070204639434815, ETA in seconds: 4911549.802\n",
      "epoch: 100200, train loss: 4.0481343269348145, val loss: 4.066103076934814, ETA in seconds: 4916044.983\n",
      "epoch: 100300, train loss: 4.049306201934814, val loss: 4.0696187019348145, ETA in seconds: 4920289.042\n",
      "epoch: 100400, train loss: 4.047939014434815, val loss: 4.0715718269348145, ETA in seconds: 4924434.378\n",
      "epoch: 100500, train loss: 4.049306201934814, val loss: 4.068837451934814, ETA in seconds: 4928786.394\n",
      "epoch: 100600, train loss: 4.054384326934814, val loss: 4.0696187019348145, ETA in seconds: 4933147.428\n",
      "epoch: 100700, train loss: 4.0510640144348145, val loss: 4.065321826934815, ETA in seconds: 4937337.524\n",
      "epoch: 100800, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 4941663.869\n",
      "epoch: 100900, train loss: 4.053603076934815, val loss: 4.070399951934815, ETA in seconds: 4946154.224\n",
      "epoch: 101000, train loss: 4.052821826934815, val loss: 4.069814014434814, ETA in seconds: 4950846.806\n",
      "epoch: 101100, train loss: 4.051259326934814, val loss: 4.068446826934815, ETA in seconds: 4955337.822\n",
      "epoch: 101200, train loss: 4.051454639434814, val loss: 4.062978076934814, ETA in seconds: 4959735.037\n",
      "epoch: 101300, train loss: 4.0461812019348145, val loss: 4.062196826934814, ETA in seconds: 4964331.680\n",
      "epoch: 101400, train loss: 4.053603076934815, val loss: 4.066103076934814, ETA in seconds: 4968739.194\n",
      "epoch: 101500, train loss: 4.055360889434814, val loss: 4.0676655769348145, ETA in seconds: 4973155.382\n",
      "epoch: 101600, train loss: 4.055751514434815, val loss: 4.070204639434815, ETA in seconds: 4977417.219\n",
      "epoch: 101700, train loss: 4.050282764434814, val loss: 4.0676655769348145, ETA in seconds: 4981744.118\n",
      "epoch: 101800, train loss: 4.052431201934814, val loss: 4.066298389434815, ETA in seconds: 4986125.516\n",
      "epoch: 101900, train loss: 4.048329639434814, val loss: 4.066298389434815, ETA in seconds: 4990397.556\n",
      "epoch: 102000, train loss: 4.046962451934815, val loss: 4.064540576934815, ETA in seconds: 4994577.889\n",
      "epoch: 102100, train loss: 4.050282764434814, val loss: 4.065517139434815, ETA in seconds: 4998852.951\n",
      "epoch: 102200, train loss: 4.052431201934814, val loss: 4.067860889434814, ETA in seconds: 5003119.638\n",
      "epoch: 102300, train loss: 4.051454639434814, val loss: 4.069032764434814, ETA in seconds: 5007404.521\n",
      "epoch: 102400, train loss: 4.051454639434814, val loss: 4.064931201934814, ETA in seconds: 5011650.913\n",
      "epoch: 102500, train loss: 4.046571826934814, val loss: 4.070009326934814, ETA in seconds: 5016008.070\n",
      "epoch: 102600, train loss: 4.054189014434814, val loss: 4.064149951934814, ETA in seconds: 5020240.870\n",
      "epoch: 102700, train loss: 4.0520405769348145, val loss: 4.070204639434815, ETA in seconds: 5024320.803\n",
      "epoch: 102800, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 5028566.696\n",
      "epoch: 102900, train loss: 4.052626514434815, val loss: 4.061610889434815, ETA in seconds: 5032701.159\n",
      "epoch: 103000, train loss: 4.050282764434814, val loss: 4.0666890144348145, ETA in seconds: 5037034.031\n",
      "epoch: 103100, train loss: 4.0500874519348145, val loss: 4.073134326934815, ETA in seconds: 5041341.918\n",
      "epoch: 103200, train loss: 4.043642139434814, val loss: 4.070985889434814, ETA in seconds: 5045662.559\n",
      "epoch: 103300, train loss: 4.048524951934814, val loss: 4.066884326934814, ETA in seconds: 5049997.164\n",
      "epoch: 103400, train loss: 4.0491108894348145, val loss: 4.070204639434815, ETA in seconds: 5054390.547\n",
      "epoch: 103500, train loss: 4.0500874519348145, val loss: 4.064149951934814, ETA in seconds: 5059211.283\n",
      "epoch: 103600, train loss: 4.049501514434814, val loss: 4.063564014434815, ETA in seconds: 5064157.347\n",
      "epoch: 103700, train loss: 4.050282764434814, val loss: 4.0735249519348145, ETA in seconds: 5068455.604\n",
      "epoch: 103800, train loss: 4.047939014434815, val loss: 4.069032764434814, ETA in seconds: 5072833.256\n",
      "epoch: 103900, train loss: 4.052431201934814, val loss: 4.062587451934815, ETA in seconds: 5077232.927\n",
      "epoch: 104000, train loss: 4.0520405769348145, val loss: 4.0696187019348145, ETA in seconds: 5081569.099\n",
      "epoch: 104100, train loss: 4.048915576934815, val loss: 4.0657124519348145, ETA in seconds: 5085885.397\n",
      "epoch: 104200, train loss: 4.047353076934814, val loss: 4.068056201934814, ETA in seconds: 5089983.960\n",
      "epoch: 104300, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 5094186.378\n",
      "epoch: 104400, train loss: 4.0520405769348145, val loss: 4.066103076934814, ETA in seconds: 5098367.478\n",
      "epoch: 104500, train loss: 4.049696826934815, val loss: 4.066493701934815, ETA in seconds: 5102610.804\n",
      "epoch: 104600, train loss: 4.046767139434815, val loss: 4.0657124519348145, ETA in seconds: 5106937.322\n",
      "epoch: 104700, train loss: 4.056142139434814, val loss: 4.068446826934815, ETA in seconds: 5111349.984\n",
      "epoch: 104800, train loss: 4.056532764434815, val loss: 4.0618062019348145, ETA in seconds: 5115405.491\n",
      "epoch: 104900, train loss: 4.052821826934815, val loss: 4.067470264434815, ETA in seconds: 5119593.851\n",
      "epoch: 105000, train loss: 4.054189014434814, val loss: 4.073720264434814, ETA in seconds: 5123854.357\n",
      "epoch: 105100, train loss: 4.052235889434814, val loss: 4.0647358894348145, ETA in seconds: 5128123.496\n",
      "epoch: 105200, train loss: 4.0491108894348145, val loss: 4.0705952644348145, ETA in seconds: 5132255.243\n",
      "epoch: 105300, train loss: 4.052431201934814, val loss: 4.0657124519348145, ETA in seconds: 5136487.657\n",
      "epoch: 105400, train loss: 4.0461812019348145, val loss: 4.065321826934815, ETA in seconds: 5140703.509\n",
      "epoch: 105500, train loss: 4.0539937019348145, val loss: 4.062978076934814, ETA in seconds: 5144872.840\n",
      "epoch: 105600, train loss: 4.051454639434814, val loss: 4.0705952644348145, ETA in seconds: 5149198.808\n",
      "epoch: 105700, train loss: 4.051259326934814, val loss: 4.067860889434814, ETA in seconds: 5153376.085\n",
      "epoch: 105800, train loss: 4.061220264434814, val loss: 4.064931201934814, ETA in seconds: 5157646.486\n",
      "epoch: 105900, train loss: 4.046376514434814, val loss: 4.063173389434814, ETA in seconds: 5161879.201\n",
      "epoch: 106000, train loss: 4.051454639434814, val loss: 4.070985889434814, ETA in seconds: 5166068.689\n",
      "epoch: 106100, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 5170216.027\n",
      "epoch: 106200, train loss: 4.055360889434814, val loss: 4.065907764434814, ETA in seconds: 5174335.704\n",
      "epoch: 106300, train loss: 4.049306201934814, val loss: 4.070399951934815, ETA in seconds: 5178440.485\n",
      "epoch: 106400, train loss: 4.050868701934815, val loss: 4.063564014434815, ETA in seconds: 5182637.156\n",
      "epoch: 106500, train loss: 4.051259326934814, val loss: 4.063954639434814, ETA in seconds: 5186903.554\n",
      "epoch: 106600, train loss: 4.056142139434814, val loss: 4.067860889434814, ETA in seconds: 5191163.893\n",
      "epoch: 106700, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 5195284.974\n",
      "epoch: 106800, train loss: 4.052235889434814, val loss: 4.068056201934814, ETA in seconds: 5199513.872\n",
      "epoch: 106900, train loss: 4.049696826934815, val loss: 4.069423389434815, ETA in seconds: 5203732.651\n",
      "epoch: 107000, train loss: 4.055751514434815, val loss: 4.075282764434815, ETA in seconds: 5207940.960\n",
      "epoch: 107100, train loss: 4.0481343269348145, val loss: 4.068251514434815, ETA in seconds: 5212245.013\n",
      "epoch: 107200, train loss: 4.055751514434815, val loss: 4.070985889434814, ETA in seconds: 5216529.029\n",
      "epoch: 107300, train loss: 4.053212451934814, val loss: 4.064931201934814, ETA in seconds: 5220710.681\n",
      "epoch: 107400, train loss: 4.0569233894348145, val loss: 4.069814014434814, ETA in seconds: 5224896.968\n",
      "epoch: 107500, train loss: 4.0520405769348145, val loss: 4.065321826934815, ETA in seconds: 5229023.370\n",
      "epoch: 107600, train loss: 4.049501514434814, val loss: 4.065517139434815, ETA in seconds: 5233173.445\n",
      "epoch: 107700, train loss: 4.055165576934814, val loss: 4.0735249519348145, ETA in seconds: 5237370.251\n",
      "epoch: 107800, train loss: 4.0549702644348145, val loss: 4.065907764434814, ETA in seconds: 5241582.004\n",
      "epoch: 107900, train loss: 4.051454639434814, val loss: 4.062978076934814, ETA in seconds: 5245712.574\n",
      "epoch: 108000, train loss: 4.0559468269348145, val loss: 4.0666890144348145, ETA in seconds: 5249868.586\n",
      "epoch: 108100, train loss: 4.052431201934814, val loss: 4.0705952644348145, ETA in seconds: 5253974.328\n",
      "epoch: 108200, train loss: 4.052626514434815, val loss: 4.070985889434814, ETA in seconds: 5258163.048\n",
      "epoch: 108300, train loss: 4.0520405769348145, val loss: 4.067470264434815, ETA in seconds: 5262465.025\n",
      "epoch: 108400, train loss: 4.054774951934815, val loss: 4.0657124519348145, ETA in seconds: 5266775.199\n",
      "epoch: 108500, train loss: 4.048329639434814, val loss: 4.067470264434815, ETA in seconds: 5270950.683\n",
      "epoch: 108600, train loss: 4.048524951934814, val loss: 4.070009326934814, ETA in seconds: 5275058.580\n",
      "epoch: 108700, train loss: 4.0530171394348145, val loss: 4.065126514434814, ETA in seconds: 5279366.184\n",
      "epoch: 108800, train loss: 4.043056201934815, val loss: 4.070009326934814, ETA in seconds: 5283560.381\n",
      "epoch: 108900, train loss: 4.052431201934814, val loss: 4.070399951934815, ETA in seconds: 5287784.435\n",
      "epoch: 109000, train loss: 4.059071826934814, val loss: 4.068837451934814, ETA in seconds: 5291961.491\n",
      "epoch: 109100, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 5296257.076\n",
      "epoch: 109200, train loss: 4.056142139434814, val loss: 4.0647358894348145, ETA in seconds: 5300520.114\n",
      "epoch: 109300, train loss: 4.051259326934814, val loss: 4.066103076934814, ETA in seconds: 5304883.729\n",
      "epoch: 109400, train loss: 4.042860889434815, val loss: 4.065321826934815, ETA in seconds: 5309099.378\n",
      "epoch: 109500, train loss: 4.051259326934814, val loss: 4.0637593269348145, ETA in seconds: 5313379.482\n",
      "epoch: 109600, train loss: 4.050673389434815, val loss: 4.0627827644348145, ETA in seconds: 5317701.657\n",
      "epoch: 109700, train loss: 4.047939014434815, val loss: 4.066298389434815, ETA in seconds: 5322015.858\n",
      "epoch: 109800, train loss: 4.049501514434814, val loss: 4.0666890144348145, ETA in seconds: 5326184.136\n",
      "epoch: 109900, train loss: 4.048524951934814, val loss: 4.066493701934815, ETA in seconds: 5330443.870\n",
      "epoch: 110000, train loss: 4.056337451934814, val loss: 4.0666890144348145, ETA in seconds: 5334615.629\n",
      "epoch: 110100, train loss: 4.0539937019348145, val loss: 4.067470264434815, ETA in seconds: 5338879.026\n",
      "epoch: 110200, train loss: 4.053603076934815, val loss: 4.064540576934815, ETA in seconds: 5343038.395\n",
      "epoch: 110300, train loss: 4.049501514434814, val loss: 4.067274951934815, ETA in seconds: 5347232.465\n",
      "epoch: 110400, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 5351497.088\n",
      "epoch: 110500, train loss: 4.051845264434815, val loss: 4.0657124519348145, ETA in seconds: 5355886.648\n",
      "epoch: 110600, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 5360706.382\n",
      "epoch: 110700, train loss: 4.055360889434814, val loss: 4.068837451934814, ETA in seconds: 5365389.060\n",
      "epoch: 110800, train loss: 4.046376514434814, val loss: 4.062392139434815, ETA in seconds: 5369576.815\n",
      "epoch: 110900, train loss: 4.055165576934814, val loss: 4.074306201934815, ETA in seconds: 5373948.837\n",
      "epoch: 111000, train loss: 4.052431201934814, val loss: 4.059657764434815, ETA in seconds: 5378301.950\n",
      "epoch: 111100, train loss: 4.060048389434814, val loss: 4.071767139434814, ETA in seconds: 5382698.886\n",
      "epoch: 111200, train loss: 4.053798389434815, val loss: 4.066493701934815, ETA in seconds: 5386887.110\n",
      "epoch: 111300, train loss: 4.0520405769348145, val loss: 4.067079639434814, ETA in seconds: 5391114.264\n",
      "epoch: 111400, train loss: 4.055165576934814, val loss: 4.061415576934815, ETA in seconds: 5395529.246\n",
      "epoch: 111500, train loss: 4.054384326934814, val loss: 4.0676655769348145, ETA in seconds: 5399785.639\n",
      "epoch: 111600, train loss: 4.058681201934815, val loss: 4.070204639434815, ETA in seconds: 5404121.096\n",
      "epoch: 111700, train loss: 4.0500874519348145, val loss: 4.0657124519348145, ETA in seconds: 5408430.918\n",
      "epoch: 111800, train loss: 4.051259326934814, val loss: 4.0725483894348145, ETA in seconds: 5412769.721\n",
      "epoch: 111900, train loss: 4.057704639434815, val loss: 4.070009326934814, ETA in seconds: 5417040.286\n",
      "epoch: 112000, train loss: 4.0491108894348145, val loss: 4.067079639434814, ETA in seconds: 5421452.514\n",
      "epoch: 112100, train loss: 4.049696826934815, val loss: 4.068251514434815, ETA in seconds: 5425826.103\n",
      "epoch: 112200, train loss: 4.0520405769348145, val loss: 4.067470264434815, ETA in seconds: 5430392.145\n",
      "epoch: 112300, train loss: 4.051259326934814, val loss: 4.070985889434814, ETA in seconds: 5435030.299\n",
      "epoch: 112400, train loss: 4.058485889434815, val loss: 4.068251514434815, ETA in seconds: 5439837.359\n",
      "epoch: 112500, train loss: 4.0549702644348145, val loss: 4.062392139434815, ETA in seconds: 5444324.906\n",
      "epoch: 112600, train loss: 4.051649951934815, val loss: 4.070204639434815, ETA in seconds: 5448568.320\n",
      "epoch: 112700, train loss: 4.052821826934815, val loss: 4.064931201934814, ETA in seconds: 5452929.725\n",
      "epoch: 112800, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 5457238.593\n",
      "epoch: 112900, train loss: 4.054189014434814, val loss: 4.065907764434814, ETA in seconds: 5461571.886\n",
      "epoch: 113000, train loss: 4.049892139434815, val loss: 4.069423389434815, ETA in seconds: 5465834.248\n",
      "epoch: 113100, train loss: 4.051845264434815, val loss: 4.0745015144348145, ETA in seconds: 5469987.009\n",
      "epoch: 113200, train loss: 4.0530171394348145, val loss: 4.070009326934814, ETA in seconds: 5474227.320\n",
      "epoch: 113300, train loss: 4.050478076934814, val loss: 4.0666890144348145, ETA in seconds: 5478375.529\n",
      "epoch: 113400, train loss: 4.0549702644348145, val loss: 4.0657124519348145, ETA in seconds: 5482615.807\n",
      "epoch: 113500, train loss: 4.0500874519348145, val loss: 4.059462451934815, ETA in seconds: 5486707.123\n",
      "epoch: 113600, train loss: 4.055165576934814, val loss: 4.067274951934815, ETA in seconds: 5491017.238\n",
      "epoch: 113700, train loss: 4.052431201934814, val loss: 4.068446826934815, ETA in seconds: 5495230.916\n",
      "epoch: 113800, train loss: 4.0520405769348145, val loss: 4.064540576934815, ETA in seconds: 5499325.638\n",
      "epoch: 113900, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 5503544.112\n",
      "epoch: 114000, train loss: 4.049892139434815, val loss: 4.067079639434814, ETA in seconds: 5507596.778\n",
      "epoch: 114100, train loss: 4.0549702644348145, val loss: 4.067470264434815, ETA in seconds: 5511428.237\n",
      "epoch: 114200, train loss: 4.0569233894348145, val loss: 4.066103076934814, ETA in seconds: 5515389.106\n",
      "epoch: 114300, train loss: 4.051259326934814, val loss: 4.070009326934814, ETA in seconds: 5519572.228\n",
      "epoch: 114400, train loss: 4.048915576934815, val loss: 4.070985889434814, ETA in seconds: 5524250.583\n",
      "epoch: 114500, train loss: 4.050478076934814, val loss: 4.069423389434815, ETA in seconds: 5528418.911\n",
      "epoch: 114600, train loss: 4.055360889434814, val loss: 4.065126514434814, ETA in seconds: 5532501.824\n",
      "epoch: 114700, train loss: 4.0481343269348145, val loss: 4.067860889434814, ETA in seconds: 5536585.489\n",
      "epoch: 114800, train loss: 4.049501514434814, val loss: 4.062196826934814, ETA in seconds: 5540708.622\n",
      "epoch: 114900, train loss: 4.054384326934814, val loss: 4.063954639434814, ETA in seconds: 5544975.170\n",
      "epoch: 115000, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 5549213.700\n",
      "epoch: 115100, train loss: 4.055556201934815, val loss: 4.068446826934815, ETA in seconds: 5553516.641\n",
      "epoch: 115200, train loss: 4.055751514434815, val loss: 4.068056201934814, ETA in seconds: 5557803.576\n",
      "epoch: 115300, train loss: 4.047743701934815, val loss: 4.069814014434814, ETA in seconds: 5561865.649\n",
      "epoch: 115400, train loss: 4.050673389434815, val loss: 4.063954639434814, ETA in seconds: 5566041.425\n",
      "epoch: 115500, train loss: 4.051259326934814, val loss: 4.066493701934815, ETA in seconds: 5570128.370\n",
      "epoch: 115600, train loss: 4.058681201934815, val loss: 4.069423389434815, ETA in seconds: 5574247.225\n",
      "epoch: 115700, train loss: 4.0481343269348145, val loss: 4.070204639434815, ETA in seconds: 5578434.037\n",
      "epoch: 115800, train loss: 4.0530171394348145, val loss: 4.062587451934815, ETA in seconds: 5582611.575\n",
      "epoch: 115900, train loss: 4.049306201934814, val loss: 4.069228076934815, ETA in seconds: 5586635.202\n",
      "epoch: 116000, train loss: 4.049306201934814, val loss: 4.062978076934814, ETA in seconds: 5590689.692\n",
      "epoch: 116100, train loss: 4.053407764434814, val loss: 4.066493701934815, ETA in seconds: 5594665.366\n",
      "epoch: 116200, train loss: 4.045985889434815, val loss: 4.062978076934814, ETA in seconds: 5598702.073\n",
      "epoch: 116300, train loss: 4.057118701934814, val loss: 4.065907764434814, ETA in seconds: 5602889.981\n",
      "epoch: 116400, train loss: 4.048524951934814, val loss: 4.070009326934814, ETA in seconds: 5607151.597\n",
      "epoch: 116500, train loss: 4.054384326934814, val loss: 4.067860889434814, ETA in seconds: 5611486.567\n",
      "epoch: 116600, train loss: 4.046962451934815, val loss: 4.067860889434814, ETA in seconds: 5615706.042\n",
      "epoch: 116700, train loss: 4.053798389434815, val loss: 4.064149951934814, ETA in seconds: 5619957.031\n",
      "epoch: 116800, train loss: 4.049892139434815, val loss: 4.071767139434814, ETA in seconds: 5623913.718\n",
      "epoch: 116900, train loss: 4.058290576934814, val loss: 4.071962451934814, ETA in seconds: 5627963.031\n",
      "epoch: 117000, train loss: 4.0520405769348145, val loss: 4.065907764434814, ETA in seconds: 5632173.325\n",
      "epoch: 117100, train loss: 4.051845264434815, val loss: 4.063368701934815, ETA in seconds: 5636380.951\n",
      "epoch: 117200, train loss: 4.051454639434814, val loss: 4.071962451934814, ETA in seconds: 5640723.128\n",
      "epoch: 117300, train loss: 4.052626514434815, val loss: 4.071767139434814, ETA in seconds: 5644701.362\n",
      "epoch: 117400, train loss: 4.062196826934814, val loss: 4.068446826934815, ETA in seconds: 5649450.666\n",
      "epoch: 117500, train loss: 4.054384326934814, val loss: 4.0637593269348145, ETA in seconds: 5653525.226\n",
      "epoch: 117600, train loss: 4.054579639434815, val loss: 4.070790576934814, ETA in seconds: 5657827.686\n",
      "epoch: 117700, train loss: 4.050478076934814, val loss: 4.065126514434814, ETA in seconds: 5662269.823\n",
      "epoch: 117800, train loss: 4.057704639434815, val loss: 4.0618062019348145, ETA in seconds: 5666745.419\n",
      "epoch: 117900, train loss: 4.045009326934815, val loss: 4.0618062019348145, ETA in seconds: 5670941.280\n",
      "epoch: 118000, train loss: 4.056728076934815, val loss: 4.0666890144348145, ETA in seconds: 5674936.935\n",
      "epoch: 118100, train loss: 4.0510640144348145, val loss: 4.065321826934815, ETA in seconds: 5679500.988\n",
      "epoch: 118200, train loss: 4.0491108894348145, val loss: 4.063954639434814, ETA in seconds: 5683776.895\n",
      "epoch: 118300, train loss: 4.044032764434815, val loss: 4.0725483894348145, ETA in seconds: 5687957.665\n",
      "epoch: 118400, train loss: 4.048524951934814, val loss: 4.0696187019348145, ETA in seconds: 5692107.453\n",
      "epoch: 118500, train loss: 4.057509326934815, val loss: 4.0676655769348145, ETA in seconds: 5696217.428\n",
      "epoch: 118600, train loss: 4.048524951934814, val loss: 4.064540576934815, ETA in seconds: 5700254.960\n",
      "epoch: 118700, train loss: 4.055165576934814, val loss: 4.066493701934815, ETA in seconds: 5704304.195\n",
      "epoch: 118800, train loss: 4.055360889434814, val loss: 4.063954639434814, ETA in seconds: 5708464.251\n",
      "epoch: 118900, train loss: 4.040126514434815, val loss: 4.063564014434815, ETA in seconds: 5712669.327\n",
      "epoch: 119000, train loss: 4.051845264434815, val loss: 4.072939014434814, ETA in seconds: 5716849.718\n",
      "epoch: 119100, train loss: 4.050282764434814, val loss: 4.060439014434815, ETA in seconds: 5720882.624\n",
      "epoch: 119200, train loss: 4.053407764434814, val loss: 4.070985889434814, ETA in seconds: 5725046.575\n",
      "epoch: 119300, train loss: 4.0491108894348145, val loss: 4.062196826934814, ETA in seconds: 5729268.146\n",
      "epoch: 119400, train loss: 4.0481343269348145, val loss: 4.071767139434814, ETA in seconds: 5733412.537\n",
      "epoch: 119500, train loss: 4.052821826934815, val loss: 4.0647358894348145, ETA in seconds: 5737569.033\n",
      "epoch: 119600, train loss: 4.053798389434815, val loss: 4.063564014434815, ETA in seconds: 5741738.773\n",
      "epoch: 119700, train loss: 4.052821826934815, val loss: 4.064149951934814, ETA in seconds: 5745747.752\n",
      "epoch: 119800, train loss: 4.044032764434815, val loss: 4.068446826934815, ETA in seconds: 5749528.669\n",
      "epoch: 119900, train loss: 4.055751514434815, val loss: 4.068056201934814, ETA in seconds: 5753389.724\n",
      "epoch: 120000, train loss: 4.046571826934814, val loss: 4.0676655769348145, ETA in seconds: 5757563.999\n",
      "epoch: 120100, train loss: 4.041493701934814, val loss: 4.072743701934814, ETA in seconds: 5761652.387\n",
      "epoch: 120200, train loss: 4.051845264434815, val loss: 4.067470264434815, ETA in seconds: 5765891.011\n",
      "epoch: 120300, train loss: 4.041884326934815, val loss: 4.063368701934815, ETA in seconds: 5770010.452\n",
      "epoch: 120400, train loss: 4.054579639434815, val loss: 4.066298389434815, ETA in seconds: 5774154.769\n",
      "epoch: 120500, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 5778271.236\n",
      "epoch: 120600, train loss: 4.044618701934814, val loss: 4.0637593269348145, ETA in seconds: 5782378.178\n",
      "epoch: 120700, train loss: 4.051649951934815, val loss: 4.068056201934814, ETA in seconds: 5786453.381\n",
      "epoch: 120800, train loss: 4.047939014434815, val loss: 4.065517139434815, ETA in seconds: 5790520.968\n",
      "epoch: 120900, train loss: 4.0500874519348145, val loss: 4.069228076934815, ETA in seconds: 5794548.505\n",
      "epoch: 121000, train loss: 4.054579639434815, val loss: 4.062392139434815, ETA in seconds: 5798606.517\n",
      "epoch: 121100, train loss: 4.0530171394348145, val loss: 4.067860889434814, ETA in seconds: 5802794.721\n",
      "epoch: 121200, train loss: 4.047939014434815, val loss: 4.068056201934814, ETA in seconds: 5806936.230\n",
      "epoch: 121300, train loss: 4.055751514434815, val loss: 4.068056201934814, ETA in seconds: 5810842.827\n",
      "epoch: 121400, train loss: 4.051845264434815, val loss: 4.064345264434815, ETA in seconds: 5814862.161\n",
      "epoch: 121500, train loss: 4.060048389434814, val loss: 4.073720264434814, ETA in seconds: 5818805.967\n",
      "epoch: 121600, train loss: 4.0500874519348145, val loss: 4.071376514434815, ETA in seconds: 5822775.398\n",
      "epoch: 121700, train loss: 4.051649951934815, val loss: 4.061220264434814, ETA in seconds: 5826941.890\n",
      "epoch: 121800, train loss: 4.0588765144348145, val loss: 4.069423389434815, ETA in seconds: 5831305.343\n",
      "epoch: 121900, train loss: 4.054579639434815, val loss: 4.064149951934814, ETA in seconds: 5835630.463\n",
      "epoch: 122000, train loss: 4.051259326934814, val loss: 4.070009326934814, ETA in seconds: 5839888.783\n",
      "epoch: 122100, train loss: 4.052431201934814, val loss: 4.065126514434814, ETA in seconds: 5844369.062\n",
      "epoch: 122200, train loss: 4.050282764434814, val loss: 4.071767139434814, ETA in seconds: 5848676.950\n",
      "epoch: 122300, train loss: 4.0520405769348145, val loss: 4.070009326934814, ETA in seconds: 5852876.380\n",
      "epoch: 122400, train loss: 4.052821826934815, val loss: 4.065321826934815, ETA in seconds: 5856832.974\n",
      "epoch: 122500, train loss: 4.052235889434814, val loss: 4.070399951934815, ETA in seconds: 5860787.832\n",
      "epoch: 122600, train loss: 4.053603076934815, val loss: 4.060439014434815, ETA in seconds: 5864827.133\n",
      "epoch: 122700, train loss: 4.0549702644348145, val loss: 4.0686421394348145, ETA in seconds: 5869004.998\n",
      "epoch: 122800, train loss: 4.052821826934815, val loss: 4.067860889434814, ETA in seconds: 5873213.885\n",
      "epoch: 122900, train loss: 4.0539937019348145, val loss: 4.063173389434814, ETA in seconds: 5877165.214\n",
      "epoch: 123000, train loss: 4.050868701934815, val loss: 4.065517139434815, ETA in seconds: 5881194.534\n",
      "epoch: 123100, train loss: 4.055165576934814, val loss: 4.062978076934814, ETA in seconds: 5885308.783\n",
      "epoch: 123200, train loss: 4.046962451934815, val loss: 4.0657124519348145, ETA in seconds: 5889192.913\n",
      "epoch: 123300, train loss: 4.0491108894348145, val loss: 4.070399951934815, ETA in seconds: 5893078.073\n",
      "epoch: 123400, train loss: 4.052235889434814, val loss: 4.0725483894348145, ETA in seconds: 5897158.942\n",
      "epoch: 123500, train loss: 4.0588765144348145, val loss: 4.0676655769348145, ETA in seconds: 5901151.497\n",
      "epoch: 123600, train loss: 4.046767139434815, val loss: 4.069228076934815, ETA in seconds: 5905077.433\n",
      "epoch: 123700, train loss: 4.053407764434814, val loss: 4.065321826934815, ETA in seconds: 5909244.417\n",
      "epoch: 123800, train loss: 4.0520405769348145, val loss: 4.0657124519348145, ETA in seconds: 5913412.448\n",
      "epoch: 123900, train loss: 4.053407764434814, val loss: 4.0696187019348145, ETA in seconds: 5917541.843\n",
      "epoch: 124000, train loss: 4.053603076934815, val loss: 4.070790576934814, ETA in seconds: 5921485.786\n",
      "epoch: 124100, train loss: 4.047939014434815, val loss: 4.064931201934814, ETA in seconds: 5925555.690\n",
      "epoch: 124200, train loss: 4.052235889434814, val loss: 4.065126514434814, ETA in seconds: 5929737.334\n",
      "epoch: 124300, train loss: 4.0530171394348145, val loss: 4.0657124519348145, ETA in seconds: 5933744.811\n",
      "epoch: 124400, train loss: 4.051259326934814, val loss: 4.069228076934815, ETA in seconds: 5937815.923\n",
      "epoch: 124500, train loss: 4.056337451934814, val loss: 4.0676655769348145, ETA in seconds: 5941975.958\n",
      "epoch: 124600, train loss: 4.057118701934814, val loss: 4.0637593269348145, ETA in seconds: 5946245.227\n",
      "epoch: 124700, train loss: 4.048720264434815, val loss: 4.068837451934814, ETA in seconds: 5950405.094\n",
      "epoch: 124800, train loss: 4.0530171394348145, val loss: 4.0696187019348145, ETA in seconds: 5954438.348\n",
      "epoch: 124900, train loss: 4.058095264434814, val loss: 4.068056201934814, ETA in seconds: 5958597.341\n",
      "epoch: 125000, train loss: 4.059071826934814, val loss: 4.063368701934815, ETA in seconds: 5962642.829\n",
      "epoch: 125100, train loss: 4.045399951934814, val loss: 4.068837451934814, ETA in seconds: 5966603.056\n",
      "epoch: 125200, train loss: 4.054384326934814, val loss: 4.072157764434815, ETA in seconds: 5970691.859\n",
      "epoch: 125300, train loss: 4.054189014434814, val loss: 4.063368701934815, ETA in seconds: 5974792.796\n",
      "epoch: 125400, train loss: 4.045790576934815, val loss: 4.069814014434814, ETA in seconds: 5978931.129\n",
      "epoch: 125500, train loss: 4.045595264434814, val loss: 4.061220264434814, ETA in seconds: 5982973.751\n",
      "epoch: 125600, train loss: 4.053407764434814, val loss: 4.064345264434815, ETA in seconds: 5986953.756\n",
      "epoch: 125700, train loss: 4.050673389434815, val loss: 4.072939014434814, ETA in seconds: 5990993.251\n",
      "epoch: 125800, train loss: 4.0481343269348145, val loss: 4.066493701934815, ETA in seconds: 5994891.802\n",
      "epoch: 125900, train loss: 4.053798389434815, val loss: 4.0725483894348145, ETA in seconds: 5998842.346\n",
      "epoch: 126000, train loss: 4.052431201934814, val loss: 4.066493701934815, ETA in seconds: 6002733.288\n",
      "epoch: 126100, train loss: 4.051259326934814, val loss: 4.065126514434814, ETA in seconds: 6006641.099\n",
      "epoch: 126200, train loss: 4.050282764434814, val loss: 4.070009326934814, ETA in seconds: 6010715.564\n",
      "epoch: 126300, train loss: 4.057509326934815, val loss: 4.075868701934814, ETA in seconds: 6014808.597\n",
      "epoch: 126400, train loss: 4.054189014434814, val loss: 4.0696187019348145, ETA in seconds: 6018970.666\n",
      "epoch: 126500, train loss: 4.053798389434815, val loss: 4.068446826934815, ETA in seconds: 6023033.783\n",
      "epoch: 126600, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 6027133.762\n",
      "epoch: 126700, train loss: 4.053603076934815, val loss: 4.063564014434815, ETA in seconds: 6031242.783\n",
      "epoch: 126800, train loss: 4.048720264434815, val loss: 4.0715718269348145, ETA in seconds: 6035457.498\n",
      "epoch: 126900, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 6039704.598\n",
      "epoch: 127000, train loss: 4.0539937019348145, val loss: 4.063564014434815, ETA in seconds: 6043741.291\n",
      "epoch: 127100, train loss: 4.047548389434814, val loss: 4.067079639434814, ETA in seconds: 6047907.622\n",
      "epoch: 127200, train loss: 4.0539937019348145, val loss: 4.0666890144348145, ETA in seconds: 6051923.973\n",
      "epoch: 127300, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 6055904.799\n",
      "epoch: 127400, train loss: 4.049306201934814, val loss: 4.069423389434815, ETA in seconds: 6059617.848\n",
      "epoch: 127500, train loss: 4.050673389434815, val loss: 4.065321826934815, ETA in seconds: 6063095.525\n",
      "epoch: 127600, train loss: 4.056337451934814, val loss: 4.0657124519348145, ETA in seconds: 6066707.265\n",
      "epoch: 127700, train loss: 4.0471577644348145, val loss: 4.063368701934815, ETA in seconds: 6070395.460\n",
      "epoch: 127800, train loss: 4.053212451934814, val loss: 4.068446826934815, ETA in seconds: 6074109.026\n",
      "epoch: 127900, train loss: 4.051259326934814, val loss: 4.073329639434815, ETA in seconds: 6078008.888\n",
      "epoch: 128000, train loss: 4.051454639434814, val loss: 4.072939014434814, ETA in seconds: 6081824.947\n",
      "epoch: 128100, train loss: 4.054774951934815, val loss: 4.063564014434815, ETA in seconds: 6085585.307\n",
      "epoch: 128200, train loss: 4.051845264434815, val loss: 4.0647358894348145, ETA in seconds: 6089665.872\n",
      "epoch: 128300, train loss: 4.052626514434815, val loss: 4.065907764434814, ETA in seconds: 6093646.992\n",
      "epoch: 128400, train loss: 4.0559468269348145, val loss: 4.0666890144348145, ETA in seconds: 6097550.355\n",
      "epoch: 128500, train loss: 4.0442280769348145, val loss: 4.0618062019348145, ETA in seconds: 6101372.808\n",
      "epoch: 128600, train loss: 4.055165576934814, val loss: 4.064149951934814, ETA in seconds: 6105248.539\n",
      "epoch: 128700, train loss: 4.054384326934814, val loss: 4.069228076934815, ETA in seconds: 6109158.594\n",
      "epoch: 128800, train loss: 4.046767139434815, val loss: 4.072157764434815, ETA in seconds: 6113162.284\n",
      "epoch: 128900, train loss: 4.0481343269348145, val loss: 4.061415576934815, ETA in seconds: 6117132.901\n",
      "epoch: 129000, train loss: 4.047743701934815, val loss: 4.070399951934815, ETA in seconds: 6121215.455\n",
      "epoch: 129100, train loss: 4.0520405769348145, val loss: 4.069228076934815, ETA in seconds: 6125365.198\n",
      "epoch: 129200, train loss: 4.046571826934814, val loss: 4.0657124519348145, ETA in seconds: 6129317.235\n",
      "epoch: 129300, train loss: 4.047548389434814, val loss: 4.061024951934814, ETA in seconds: 6133383.878\n",
      "epoch: 129400, train loss: 4.054774951934815, val loss: 4.064931201934814, ETA in seconds: 6137823.320\n",
      "epoch: 129500, train loss: 4.048720264434815, val loss: 4.067079639434814, ETA in seconds: 6142055.279\n",
      "epoch: 129600, train loss: 4.0442280769348145, val loss: 4.067860889434814, ETA in seconds: 6146208.003\n",
      "epoch: 129700, train loss: 4.0530171394348145, val loss: 4.0676655769348145, ETA in seconds: 6150269.902\n",
      "epoch: 129800, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 6154520.206\n",
      "epoch: 129900, train loss: 4.0510640144348145, val loss: 4.065907764434814, ETA in seconds: 6158542.572\n",
      "epoch: 130000, train loss: 4.055360889434814, val loss: 4.065517139434815, ETA in seconds: 6162418.959\n",
      "epoch: 130100, train loss: 4.050282764434814, val loss: 4.0686421394348145, ETA in seconds: 6166512.425\n",
      "epoch: 130200, train loss: 4.053212451934814, val loss: 4.063954639434814, ETA in seconds: 6170478.271\n",
      "epoch: 130300, train loss: 4.051454639434814, val loss: 4.069423389434815, ETA in seconds: 6174476.418\n",
      "epoch: 130400, train loss: 4.050673389434815, val loss: 4.068446826934815, ETA in seconds: 6177570.492\n",
      "epoch: 130500, train loss: 4.0559468269348145, val loss: 4.064149951934814, ETA in seconds: 6181640.445\n",
      "epoch: 130600, train loss: 4.047939014434815, val loss: 4.0676655769348145, ETA in seconds: 6185345.898\n",
      "epoch: 130700, train loss: 4.051845264434815, val loss: 4.071181201934815, ETA in seconds: 6189327.864\n",
      "epoch: 130800, train loss: 4.0491108894348145, val loss: 4.0696187019348145, ETA in seconds: 6193905.248\n",
      "epoch: 130900, train loss: 4.054189014434814, val loss: 4.074696826934814, ETA in seconds: 6198023.607\n",
      "epoch: 131000, train loss: 4.056142139434814, val loss: 4.073915576934814, ETA in seconds: 6202365.696\n",
      "epoch: 131100, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 6206432.999\n",
      "epoch: 131200, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 6210391.128\n",
      "epoch: 131300, train loss: 4.0491108894348145, val loss: 4.0657124519348145, ETA in seconds: 6214646.544\n",
      "epoch: 131400, train loss: 4.048329639434814, val loss: 4.070009326934814, ETA in seconds: 6218634.579\n",
      "epoch: 131500, train loss: 4.057118701934814, val loss: 4.069032764434814, ETA in seconds: 6222628.369\n",
      "epoch: 131600, train loss: 4.0569233894348145, val loss: 4.067079639434814, ETA in seconds: 6226670.238\n",
      "epoch: 131700, train loss: 4.052821826934815, val loss: 4.0657124519348145, ETA in seconds: 6230667.592\n",
      "epoch: 131800, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 6234822.874\n",
      "epoch: 131900, train loss: 4.051649951934815, val loss: 4.070009326934814, ETA in seconds: 6238910.716\n",
      "epoch: 132000, train loss: 4.053603076934815, val loss: 4.067860889434814, ETA in seconds: 6242887.554\n",
      "epoch: 132100, train loss: 4.048524951934814, val loss: 4.071181201934815, ETA in seconds: 6246855.947\n",
      "epoch: 132200, train loss: 4.047548389434814, val loss: 4.075282764434815, ETA in seconds: 6250761.057\n",
      "epoch: 132300, train loss: 4.0510640144348145, val loss: 4.0657124519348145, ETA in seconds: 6254313.000\n",
      "epoch: 132400, train loss: 4.0530171394348145, val loss: 4.070399951934815, ETA in seconds: 6258180.732\n",
      "epoch: 132500, train loss: 4.050478076934814, val loss: 4.069228076934815, ETA in seconds: 6262369.600\n",
      "epoch: 132600, train loss: 4.046571826934814, val loss: 4.070790576934814, ETA in seconds: 6266551.576\n",
      "epoch: 132700, train loss: 4.047939014434815, val loss: 4.0666890144348145, ETA in seconds: 6270843.236\n",
      "epoch: 132800, train loss: 4.045009326934815, val loss: 4.064149951934814, ETA in seconds: 6275017.503\n",
      "epoch: 132900, train loss: 4.052626514434815, val loss: 4.069423389434815, ETA in seconds: 6279260.624\n",
      "epoch: 133000, train loss: 4.0539937019348145, val loss: 4.0715718269348145, ETA in seconds: 6283369.677\n",
      "epoch: 133100, train loss: 4.056728076934815, val loss: 4.068251514434815, ETA in seconds: 6287168.742\n",
      "epoch: 133200, train loss: 4.050673389434815, val loss: 4.064931201934814, ETA in seconds: 6291030.121\n",
      "epoch: 133300, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 6294993.676\n",
      "epoch: 133400, train loss: 4.0500874519348145, val loss: 4.066103076934814, ETA in seconds: 6298951.013\n",
      "epoch: 133500, train loss: 4.050282764434814, val loss: 4.0725483894348145, ETA in seconds: 6302865.065\n",
      "epoch: 133600, train loss: 4.053603076934815, val loss: 4.066493701934815, ETA in seconds: 6306919.745\n",
      "epoch: 133700, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 6310802.772\n",
      "epoch: 133800, train loss: 4.061220264434814, val loss: 4.063954639434814, ETA in seconds: 6314823.247\n",
      "epoch: 133900, train loss: 4.053798389434815, val loss: 4.065126514434814, ETA in seconds: 6318440.274\n",
      "epoch: 134000, train loss: 4.0520405769348145, val loss: 4.0686421394348145, ETA in seconds: 6322382.024\n",
      "epoch: 134100, train loss: 4.050478076934814, val loss: 4.0627827644348145, ETA in seconds: 6326229.566\n",
      "epoch: 134200, train loss: 4.051259326934814, val loss: 4.0657124519348145, ETA in seconds: 6330150.878\n",
      "epoch: 134300, train loss: 4.049501514434814, val loss: 4.066298389434815, ETA in seconds: 6333767.834\n",
      "epoch: 134400, train loss: 4.054579639434815, val loss: 4.071962451934814, ETA in seconds: 6337711.311\n",
      "epoch: 134500, train loss: 4.055751514434815, val loss: 4.070009326934814, ETA in seconds: 6341809.338\n",
      "epoch: 134600, train loss: 4.049892139434815, val loss: 4.065907764434814, ETA in seconds: 6345841.175\n",
      "epoch: 134700, train loss: 4.0481343269348145, val loss: 4.061024951934814, ETA in seconds: 6349888.496\n",
      "epoch: 134800, train loss: 4.060439014434815, val loss: 4.066884326934814, ETA in seconds: 6353911.871\n",
      "epoch: 134900, train loss: 4.053603076934815, val loss: 4.068251514434815, ETA in seconds: 6357843.049\n",
      "epoch: 135000, train loss: 4.049892139434815, val loss: 4.068251514434815, ETA in seconds: 6361853.678\n",
      "epoch: 135100, train loss: 4.0539937019348145, val loss: 4.062001514434814, ETA in seconds: 6365708.574\n",
      "epoch: 135200, train loss: 4.048329639434814, val loss: 4.065907764434814, ETA in seconds: 6369851.353\n",
      "epoch: 135300, train loss: 4.052235889434814, val loss: 4.0705952644348145, ETA in seconds: 6373954.471\n",
      "epoch: 135400, train loss: 4.056532764434815, val loss: 4.067079639434814, ETA in seconds: 6378135.217\n",
      "epoch: 135500, train loss: 4.0491108894348145, val loss: 4.063173389434814, ETA in seconds: 6382055.054\n",
      "epoch: 135600, train loss: 4.060243701934814, val loss: 4.0676655769348145, ETA in seconds: 6386115.827\n",
      "epoch: 135700, train loss: 4.053407764434814, val loss: 4.069814014434814, ETA in seconds: 6390074.472\n",
      "epoch: 135800, train loss: 4.0471577644348145, val loss: 4.073329639434815, ETA in seconds: 6393923.461\n",
      "epoch: 135900, train loss: 4.047743701934815, val loss: 4.068251514434815, ETA in seconds: 6397979.826\n",
      "epoch: 136000, train loss: 4.055751514434815, val loss: 4.063368701934815, ETA in seconds: 6401894.345\n",
      "epoch: 136100, train loss: 4.050868701934815, val loss: 4.069228076934815, ETA in seconds: 6405868.223\n",
      "epoch: 136200, train loss: 4.0481343269348145, val loss: 4.0715718269348145, ETA in seconds: 6409758.041\n",
      "epoch: 136300, train loss: 4.053212451934814, val loss: 4.066493701934815, ETA in seconds: 6413785.384\n",
      "epoch: 136400, train loss: 4.052626514434815, val loss: 4.067860889434814, ETA in seconds: 6417834.568\n",
      "epoch: 136500, train loss: 4.054579639434815, val loss: 4.0676655769348145, ETA in seconds: 6421874.827\n",
      "epoch: 136600, train loss: 4.0481343269348145, val loss: 4.065321826934815, ETA in seconds: 6425747.005\n",
      "epoch: 136700, train loss: 4.055556201934815, val loss: 4.0676655769348145, ETA in seconds: 6429574.073\n",
      "epoch: 136800, train loss: 4.0520405769348145, val loss: 4.060048389434814, ETA in seconds: 6433540.875\n",
      "epoch: 136900, train loss: 4.054384326934814, val loss: 4.062978076934814, ETA in seconds: 6437625.720\n",
      "epoch: 137000, train loss: 4.058290576934814, val loss: 4.0725483894348145, ETA in seconds: 6441695.526\n",
      "epoch: 137100, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 6445499.938\n",
      "epoch: 137200, train loss: 4.051454639434814, val loss: 4.073720264434814, ETA in seconds: 6449256.136\n",
      "epoch: 137300, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 6453143.652\n",
      "epoch: 137400, train loss: 4.0510640144348145, val loss: 4.0637593269348145, ETA in seconds: 6457115.015\n",
      "epoch: 137500, train loss: 4.051845264434815, val loss: 4.065321826934815, ETA in seconds: 6461029.321\n",
      "epoch: 137600, train loss: 4.051259326934814, val loss: 4.061415576934815, ETA in seconds: 6464938.858\n",
      "epoch: 137700, train loss: 4.049306201934814, val loss: 4.068251514434815, ETA in seconds: 6468886.148\n",
      "epoch: 137800, train loss: 4.057314014434814, val loss: 4.070009326934814, ETA in seconds: 6472885.068\n",
      "epoch: 137900, train loss: 4.050868701934815, val loss: 4.0647358894348145, ETA in seconds: 6476761.758\n",
      "epoch: 138000, train loss: 4.050673389434815, val loss: 4.0647358894348145, ETA in seconds: 6480895.788\n",
      "epoch: 138100, train loss: 4.054189014434814, val loss: 4.065321826934815, ETA in seconds: 6484793.219\n",
      "epoch: 138200, train loss: 4.0500874519348145, val loss: 4.067274951934815, ETA in seconds: 6488879.697\n",
      "epoch: 138300, train loss: 4.0578999519348145, val loss: 4.070399951934815, ETA in seconds: 6492737.222\n",
      "epoch: 138400, train loss: 4.052821826934815, val loss: 4.070790576934814, ETA in seconds: 6496680.529\n",
      "epoch: 138500, train loss: 4.055360889434814, val loss: 4.063954639434814, ETA in seconds: 6500545.041\n",
      "epoch: 138600, train loss: 4.049306201934814, val loss: 4.069814014434814, ETA in seconds: 6504495.628\n",
      "epoch: 138700, train loss: 4.055360889434814, val loss: 4.0637593269348145, ETA in seconds: 6508507.216\n",
      "epoch: 138800, train loss: 4.053407764434814, val loss: 4.060439014434815, ETA in seconds: 6512528.879\n",
      "epoch: 138900, train loss: 4.050282764434814, val loss: 4.067274951934815, ETA in seconds: 6516391.592\n",
      "epoch: 139000, train loss: 4.050478076934814, val loss: 4.0657124519348145, ETA in seconds: 6520192.100\n",
      "epoch: 139100, train loss: 4.0539937019348145, val loss: 4.067274951934815, ETA in seconds: 6523937.960\n",
      "epoch: 139200, train loss: 4.050478076934814, val loss: 4.0676655769348145, ETA in seconds: 6527582.738\n",
      "epoch: 139300, train loss: 4.054384326934814, val loss: 4.063564014434815, ETA in seconds: 6531306.050\n",
      "epoch: 139400, train loss: 4.059071826934814, val loss: 4.0705952644348145, ETA in seconds: 6535081.574\n",
      "epoch: 139500, train loss: 4.045790576934815, val loss: 4.067274951934815, ETA in seconds: 6539020.951\n",
      "epoch: 139600, train loss: 4.054579639434815, val loss: 4.0705952644348145, ETA in seconds: 6542833.570\n",
      "epoch: 139700, train loss: 4.045985889434815, val loss: 4.0657124519348145, ETA in seconds: 6546851.950\n",
      "epoch: 139800, train loss: 4.055556201934815, val loss: 4.0676655769348145, ETA in seconds: 6550592.286\n",
      "epoch: 139900, train loss: 4.048720264434815, val loss: 4.0657124519348145, ETA in seconds: 6554450.411\n",
      "epoch: 140000, train loss: 4.056142139434814, val loss: 4.068251514434815, ETA in seconds: 6558299.660\n",
      "epoch: 140100, train loss: 4.046376514434814, val loss: 4.067079639434814, ETA in seconds: 6562150.006\n",
      "epoch: 140200, train loss: 4.057118701934814, val loss: 4.067470264434815, ETA in seconds: 6566013.809\n",
      "epoch: 140300, train loss: 4.047743701934815, val loss: 4.070009326934814, ETA in seconds: 6569881.545\n",
      "epoch: 140400, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 6573688.956\n",
      "epoch: 140500, train loss: 4.051454639434814, val loss: 4.066103076934814, ETA in seconds: 6577495.484\n",
      "epoch: 140600, train loss: 4.0510640144348145, val loss: 4.065517139434815, ETA in seconds: 6581329.629\n",
      "epoch: 140700, train loss: 4.051454639434814, val loss: 4.069814014434814, ETA in seconds: 6585173.452\n",
      "epoch: 140800, train loss: 4.054774951934815, val loss: 4.064149951934814, ETA in seconds: 6588942.467\n",
      "epoch: 140900, train loss: 4.055360889434814, val loss: 4.066103076934814, ETA in seconds: 6592803.994\n",
      "epoch: 141000, train loss: 4.049306201934814, val loss: 4.0637593269348145, ETA in seconds: 6596718.632\n",
      "epoch: 141100, train loss: 4.048915576934815, val loss: 4.0657124519348145, ETA in seconds: 6600434.236\n",
      "epoch: 141200, train loss: 4.051649951934815, val loss: 4.063954639434814, ETA in seconds: 6604276.672\n",
      "epoch: 141300, train loss: 4.046962451934815, val loss: 4.067274951934815, ETA in seconds: 6608037.376\n",
      "epoch: 141400, train loss: 4.0539937019348145, val loss: 4.068837451934814, ETA in seconds: 6611912.617\n",
      "epoch: 141500, train loss: 4.053798389434815, val loss: 4.0696187019348145, ETA in seconds: 6615311.292\n",
      "epoch: 141600, train loss: 4.0569233894348145, val loss: 4.065321826934815, ETA in seconds: 6619137.712\n",
      "epoch: 141700, train loss: 4.052626514434815, val loss: 4.075087451934815, ETA in seconds: 6623295.709\n",
      "epoch: 141800, train loss: 4.050868701934815, val loss: 4.0705952644348145, ETA in seconds: 6627374.158\n",
      "epoch: 141900, train loss: 4.057118701934814, val loss: 4.067274951934815, ETA in seconds: 6631288.268\n",
      "epoch: 142000, train loss: 4.047353076934814, val loss: 4.072743701934814, ETA in seconds: 6635233.409\n",
      "epoch: 142100, train loss: 4.050868701934815, val loss: 4.064540576934815, ETA in seconds: 6639195.060\n",
      "epoch: 142200, train loss: 4.048720264434815, val loss: 4.072157764434815, ETA in seconds: 6643218.142\n",
      "epoch: 142300, train loss: 4.047939014434815, val loss: 4.065321826934815, ETA in seconds: 6647178.157\n",
      "epoch: 142400, train loss: 4.055360889434814, val loss: 4.070790576934814, ETA in seconds: 6651037.321\n",
      "epoch: 142500, train loss: 4.052235889434814, val loss: 4.0686421394348145, ETA in seconds: 6654962.678\n",
      "epoch: 142600, train loss: 4.053798389434815, val loss: 4.063564014434815, ETA in seconds: 6658855.871\n",
      "epoch: 142700, train loss: 4.056337451934814, val loss: 4.059657764434815, ETA in seconds: 6662795.268\n",
      "epoch: 142800, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 6666332.151\n",
      "epoch: 142900, train loss: 4.0588765144348145, val loss: 4.070009326934814, ETA in seconds: 6669677.652\n",
      "epoch: 143000, train loss: 4.048329639434814, val loss: 4.065126514434814, ETA in seconds: 6673608.315\n",
      "epoch: 143100, train loss: 4.054774951934815, val loss: 4.0686421394348145, ETA in seconds: 6677871.065\n",
      "epoch: 143200, train loss: 4.050868701934815, val loss: 4.070790576934814, ETA in seconds: 6681663.538\n",
      "epoch: 143300, train loss: 4.0491108894348145, val loss: 4.068251514434815, ETA in seconds: 6685564.355\n",
      "epoch: 143400, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 6689336.003\n",
      "epoch: 143500, train loss: 4.049892139434815, val loss: 4.070985889434814, ETA in seconds: 6693137.076\n",
      "epoch: 143600, train loss: 4.061415576934815, val loss: 4.066298389434815, ETA in seconds: 6696838.154\n",
      "epoch: 143700, train loss: 4.056142139434814, val loss: 4.070399951934815, ETA in seconds: 6700754.581\n",
      "epoch: 143800, train loss: 4.053407764434814, val loss: 4.0725483894348145, ETA in seconds: 6704148.649\n",
      "epoch: 143900, train loss: 4.053407764434814, val loss: 4.068446826934815, ETA in seconds: 6708351.727\n",
      "epoch: 144000, train loss: 4.047743701934815, val loss: 4.065907764434814, ETA in seconds: 6711795.176\n",
      "epoch: 144100, train loss: 4.044423389434814, val loss: 4.0666890144348145, ETA in seconds: 6715501.358\n",
      "epoch: 144200, train loss: 4.050673389434815, val loss: 4.070985889434814, ETA in seconds: 6719237.305\n",
      "epoch: 144300, train loss: 4.053603076934815, val loss: 4.0686421394348145, ETA in seconds: 6723150.514\n",
      "epoch: 144400, train loss: 4.052431201934814, val loss: 4.0647358894348145, ETA in seconds: 6726955.825\n",
      "epoch: 144500, train loss: 4.054384326934814, val loss: 4.069814014434814, ETA in seconds: 6730837.363\n",
      "epoch: 144600, train loss: 4.0520405769348145, val loss: 4.0725483894348145, ETA in seconds: 6734634.945\n",
      "epoch: 144700, train loss: 4.055556201934815, val loss: 4.064540576934815, ETA in seconds: 6738382.175\n",
      "epoch: 144800, train loss: 4.052431201934814, val loss: 4.065126514434814, ETA in seconds: 6742359.398\n",
      "epoch: 144900, train loss: 4.049501514434814, val loss: 4.0647358894348145, ETA in seconds: 6746166.767\n",
      "epoch: 145000, train loss: 4.054384326934814, val loss: 4.073329639434815, ETA in seconds: 6750063.455\n",
      "epoch: 145100, train loss: 4.048329639434814, val loss: 4.0705952644348145, ETA in seconds: 6753902.002\n",
      "epoch: 145200, train loss: 4.050282764434814, val loss: 4.063368701934815, ETA in seconds: 6757993.174\n",
      "epoch: 145300, train loss: 4.0500874519348145, val loss: 4.068446826934815, ETA in seconds: 6761788.192\n",
      "epoch: 145400, train loss: 4.053798389434815, val loss: 4.068446826934815, ETA in seconds: 6765570.722\n",
      "epoch: 145500, train loss: 4.051259326934814, val loss: 4.0676655769348145, ETA in seconds: 6769483.582\n",
      "epoch: 145600, train loss: 4.055751514434815, val loss: 4.065321826934815, ETA in seconds: 6773400.451\n",
      "epoch: 145700, train loss: 4.052626514434815, val loss: 4.060243701934814, ETA in seconds: 6777381.620\n",
      "epoch: 145800, train loss: 4.048329639434814, val loss: 4.066884326934814, ETA in seconds: 6781375.900\n",
      "epoch: 145900, train loss: 4.042470264434814, val loss: 4.064931201934814, ETA in seconds: 6785383.125\n",
      "epoch: 146000, train loss: 4.047743701934815, val loss: 4.066103076934814, ETA in seconds: 6789520.114\n",
      "epoch: 146100, train loss: 4.053603076934815, val loss: 4.072353076934815, ETA in seconds: 6793486.013\n",
      "epoch: 146200, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 6797510.547\n",
      "epoch: 146300, train loss: 4.0452046394348145, val loss: 4.0657124519348145, ETA in seconds: 6801478.380\n",
      "epoch: 146400, train loss: 4.060243701934814, val loss: 4.0686421394348145, ETA in seconds: 6805334.743\n",
      "epoch: 146500, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 6809137.023\n",
      "epoch: 146600, train loss: 4.0491108894348145, val loss: 4.064540576934815, ETA in seconds: 6812966.183\n",
      "epoch: 146700, train loss: 4.052626514434815, val loss: 4.065907764434814, ETA in seconds: 6816840.533\n",
      "epoch: 146800, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 6820784.304\n",
      "epoch: 146900, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 6824728.334\n",
      "epoch: 147000, train loss: 4.054189014434814, val loss: 4.0696187019348145, ETA in seconds: 6828680.552\n",
      "epoch: 147100, train loss: 4.049501514434814, val loss: 4.066103076934814, ETA in seconds: 6832738.007\n",
      "epoch: 147200, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 6836611.199\n",
      "epoch: 147300, train loss: 4.052626514434815, val loss: 4.071962451934814, ETA in seconds: 6840423.035\n",
      "epoch: 147400, train loss: 4.050282764434814, val loss: 4.064540576934815, ETA in seconds: 6844257.367\n",
      "epoch: 147500, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 6848081.971\n",
      "epoch: 147600, train loss: 4.0491108894348145, val loss: 4.069032764434814, ETA in seconds: 6851858.188\n",
      "epoch: 147700, train loss: 4.054774951934815, val loss: 4.070985889434814, ETA in seconds: 6855536.234\n",
      "epoch: 147800, train loss: 4.056337451934814, val loss: 4.067274951934815, ETA in seconds: 6859333.952\n",
      "epoch: 147900, train loss: 4.046376514434814, val loss: 4.067274951934815, ETA in seconds: 6862871.392\n",
      "epoch: 148000, train loss: 4.053603076934815, val loss: 4.066103076934814, ETA in seconds: 6866761.294\n",
      "epoch: 148100, train loss: 4.055751514434815, val loss: 4.064345264434815, ETA in seconds: 6870279.888\n",
      "epoch: 148200, train loss: 4.058095264434814, val loss: 4.066103076934814, ETA in seconds: 6873268.436\n",
      "epoch: 148300, train loss: 4.051649951934815, val loss: 4.070790576934814, ETA in seconds: 6877042.253\n",
      "epoch: 148400, train loss: 4.0491108894348145, val loss: 4.068837451934814, ETA in seconds: 6880642.154\n",
      "epoch: 148500, train loss: 4.0539937019348145, val loss: 4.070399951934815, ETA in seconds: 6884463.222\n",
      "epoch: 148600, train loss: 4.046376514434814, val loss: 4.067274951934815, ETA in seconds: 6888126.753\n",
      "epoch: 148700, train loss: 4.0481343269348145, val loss: 4.071962451934814, ETA in seconds: 6892066.270\n",
      "epoch: 148800, train loss: 4.048915576934815, val loss: 4.063173389434814, ETA in seconds: 6895855.766\n",
      "epoch: 148900, train loss: 4.056142139434814, val loss: 4.061415576934815, ETA in seconds: 6899538.318\n",
      "epoch: 149000, train loss: 4.054579639434815, val loss: 4.068056201934814, ETA in seconds: 6903499.000\n",
      "epoch: 149100, train loss: 4.0491108894348145, val loss: 4.070790576934814, ETA in seconds: 6907315.204\n",
      "epoch: 149200, train loss: 4.052821826934815, val loss: 4.0657124519348145, ETA in seconds: 6911193.640\n",
      "epoch: 149300, train loss: 4.056337451934814, val loss: 4.060243701934814, ETA in seconds: 6915060.235\n",
      "epoch: 149400, train loss: 4.047548389434814, val loss: 4.0657124519348145, ETA in seconds: 6918854.341\n",
      "epoch: 149500, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 6922947.025\n",
      "epoch: 149600, train loss: 4.048524951934814, val loss: 4.068446826934815, ETA in seconds: 6926950.251\n",
      "epoch: 149700, train loss: 4.049696826934815, val loss: 4.0686421394348145, ETA in seconds: 6930947.768\n",
      "epoch: 149800, train loss: 4.049501514434814, val loss: 4.0764546394348145, ETA in seconds: 6934820.377\n",
      "epoch: 149900, train loss: 4.051845264434815, val loss: 4.069814014434814, ETA in seconds: 6938651.558\n",
      "epoch: 150000, train loss: 4.056337451934814, val loss: 4.067079639434814, ETA in seconds: 6942565.908\n",
      "epoch: 150100, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 6946688.425\n",
      "epoch: 150200, train loss: 4.046962451934815, val loss: 4.064931201934814, ETA in seconds: 6950651.745\n",
      "epoch: 150300, train loss: 4.054189014434814, val loss: 4.0637593269348145, ETA in seconds: 6954976.623\n",
      "epoch: 150400, train loss: 4.050478076934814, val loss: 4.066298389434815, ETA in seconds: 6958847.031\n",
      "epoch: 150500, train loss: 4.050282764434814, val loss: 4.067860889434814, ETA in seconds: 6962896.636\n",
      "epoch: 150600, train loss: 4.045985889434815, val loss: 4.066884326934814, ETA in seconds: 6966902.854\n",
      "epoch: 150700, train loss: 4.050478076934814, val loss: 4.067470264434815, ETA in seconds: 6970843.483\n",
      "epoch: 150800, train loss: 4.053798389434815, val loss: 4.069032764434814, ETA in seconds: 6974831.933\n",
      "epoch: 150900, train loss: 4.048524951934814, val loss: 4.061415576934815, ETA in seconds: 6978804.635\n",
      "epoch: 151000, train loss: 4.058095264434814, val loss: 4.068837451934814, ETA in seconds: 6982756.398\n",
      "epoch: 151100, train loss: 4.050282764434814, val loss: 4.0686421394348145, ETA in seconds: 6986625.999\n",
      "epoch: 151200, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 6990425.419\n",
      "epoch: 151300, train loss: 4.054384326934814, val loss: 4.061220264434814, ETA in seconds: 6994211.729\n",
      "epoch: 151400, train loss: 4.049501514434814, val loss: 4.0686421394348145, ETA in seconds: 6998420.414\n",
      "epoch: 151500, train loss: 4.047548389434814, val loss: 4.0696187019348145, ETA in seconds: 7002444.605\n",
      "epoch: 151600, train loss: 4.052821826934815, val loss: 4.067860889434814, ETA in seconds: 7006509.455\n",
      "epoch: 151700, train loss: 4.056728076934815, val loss: 4.068056201934814, ETA in seconds: 7010416.192\n",
      "epoch: 151800, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 7014140.221\n",
      "epoch: 151900, train loss: 4.046962451934815, val loss: 4.066103076934814, ETA in seconds: 7017798.952\n",
      "epoch: 152000, train loss: 4.058290576934814, val loss: 4.0725483894348145, ETA in seconds: 7021572.781\n",
      "epoch: 152100, train loss: 4.050282764434814, val loss: 4.061415576934815, ETA in seconds: 7025264.022\n",
      "epoch: 152200, train loss: 4.056532764434815, val loss: 4.065321826934815, ETA in seconds: 7028976.686\n",
      "epoch: 152300, train loss: 4.050478076934814, val loss: 4.072743701934814, ETA in seconds: 7032821.855\n",
      "epoch: 152400, train loss: 4.049306201934814, val loss: 4.066298389434815, ETA in seconds: 7036513.993\n",
      "epoch: 152500, train loss: 4.053407764434814, val loss: 4.061610889434815, ETA in seconds: 7040305.250\n",
      "epoch: 152600, train loss: 4.047743701934815, val loss: 4.070204639434815, ETA in seconds: 7044124.834\n",
      "epoch: 152700, train loss: 4.049892139434815, val loss: 4.071767139434814, ETA in seconds: 7047875.346\n",
      "epoch: 152800, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 7051709.583\n",
      "epoch: 152900, train loss: 4.053407764434814, val loss: 4.071376514434815, ETA in seconds: 7055536.370\n",
      "epoch: 153000, train loss: 4.0510640144348145, val loss: 4.064149951934814, ETA in seconds: 7059214.048\n",
      "epoch: 153100, train loss: 4.0500874519348145, val loss: 4.067860889434814, ETA in seconds: 7062997.092\n",
      "epoch: 153200, train loss: 4.056142139434814, val loss: 4.070985889434814, ETA in seconds: 7066952.885\n",
      "epoch: 153300, train loss: 4.052235889434814, val loss: 4.066884326934814, ETA in seconds: 7071205.676\n",
      "epoch: 153400, train loss: 4.047939014434815, val loss: 4.066493701934815, ETA in seconds: 7075225.440\n",
      "epoch: 153500, train loss: 4.045790576934815, val loss: 4.0676655769348145, ETA in seconds: 7079017.192\n",
      "epoch: 153600, train loss: 4.052235889434814, val loss: 4.072743701934814, ETA in seconds: 7082715.763\n",
      "epoch: 153700, train loss: 4.050673389434815, val loss: 4.077821826934814, ETA in seconds: 7086421.910\n",
      "epoch: 153800, train loss: 4.052431201934814, val loss: 4.0735249519348145, ETA in seconds: 7090143.337\n",
      "epoch: 153900, train loss: 4.054774951934815, val loss: 4.0715718269348145, ETA in seconds: 7093977.456\n",
      "epoch: 154000, train loss: 4.0608296394348145, val loss: 4.0686421394348145, ETA in seconds: 7097788.699\n",
      "epoch: 154100, train loss: 4.056142139434814, val loss: 4.0686421394348145, ETA in seconds: 7101740.466\n",
      "epoch: 154200, train loss: 4.0481343269348145, val loss: 4.065907764434814, ETA in seconds: 7105703.721\n",
      "epoch: 154300, train loss: 4.047939014434815, val loss: 4.064540576934815, ETA in seconds: 7109493.827\n",
      "epoch: 154400, train loss: 4.057314014434814, val loss: 4.065126514434814, ETA in seconds: 7113584.805\n",
      "epoch: 154500, train loss: 4.051259326934814, val loss: 4.065907764434814, ETA in seconds: 7117385.880\n",
      "epoch: 154600, train loss: 4.051259326934814, val loss: 4.0686421394348145, ETA in seconds: 7121389.946\n",
      "epoch: 154700, train loss: 4.055751514434815, val loss: 4.0657124519348145, ETA in seconds: 7124981.808\n",
      "epoch: 154800, train loss: 4.051454639434814, val loss: 4.063954639434814, ETA in seconds: 7128655.026\n",
      "epoch: 154900, train loss: 4.049501514434814, val loss: 4.0657124519348145, ETA in seconds: 7132663.085\n",
      "epoch: 155000, train loss: 4.054384326934814, val loss: 4.0676655769348145, ETA in seconds: 7136515.097\n",
      "epoch: 155100, train loss: 4.053798389434815, val loss: 4.0618062019348145, ETA in seconds: 7140328.354\n",
      "epoch: 155200, train loss: 4.053798389434815, val loss: 4.0647358894348145, ETA in seconds: 7144147.532\n",
      "epoch: 155300, train loss: 4.046767139434815, val loss: 4.072939014434814, ETA in seconds: 7147860.550\n",
      "epoch: 155400, train loss: 4.055360889434814, val loss: 4.071962451934814, ETA in seconds: 7151548.116\n",
      "epoch: 155500, train loss: 4.047548389434814, val loss: 4.066493701934815, ETA in seconds: 7155343.746\n",
      "epoch: 155600, train loss: 4.054774951934815, val loss: 4.0676655769348145, ETA in seconds: 7159093.819\n",
      "epoch: 155700, train loss: 4.059657764434815, val loss: 4.0686421394348145, ETA in seconds: 7162742.891\n",
      "epoch: 155800, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 7166473.410\n",
      "epoch: 155900, train loss: 4.054189014434814, val loss: 4.061415576934815, ETA in seconds: 7170165.250\n",
      "epoch: 156000, train loss: 4.054774951934815, val loss: 4.068837451934814, ETA in seconds: 7173733.393\n",
      "epoch: 156100, train loss: 4.050282764434814, val loss: 4.068251514434815, ETA in seconds: 7177267.869\n",
      "epoch: 156200, train loss: 4.055556201934815, val loss: 4.0705952644348145, ETA in seconds: 7180879.754\n",
      "epoch: 156300, train loss: 4.056532764434815, val loss: 4.062587451934815, ETA in seconds: 7184282.515\n",
      "epoch: 156400, train loss: 4.048915576934815, val loss: 4.0618062019348145, ETA in seconds: 7188080.891\n",
      "epoch: 156500, train loss: 4.052235889434814, val loss: 4.065321826934815, ETA in seconds: 7192028.098\n",
      "epoch: 156600, train loss: 4.044618701934814, val loss: 4.068251514434815, ETA in seconds: 7195930.221\n",
      "epoch: 156700, train loss: 4.055360889434814, val loss: 4.062978076934814, ETA in seconds: 7200079.122\n",
      "epoch: 156800, train loss: 4.053603076934815, val loss: 4.063368701934815, ETA in seconds: 7203927.789\n",
      "epoch: 156900, train loss: 4.0500874519348145, val loss: 4.066103076934814, ETA in seconds: 7207817.439\n",
      "epoch: 157000, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 7211613.276\n",
      "epoch: 157100, train loss: 4.054384326934814, val loss: 4.0647358894348145, ETA in seconds: 7215600.490\n",
      "epoch: 157200, train loss: 4.051845264434815, val loss: 4.063564014434815, ETA in seconds: 7219595.660\n",
      "epoch: 157300, train loss: 4.0530171394348145, val loss: 4.064931201934814, ETA in seconds: 7223180.301\n",
      "epoch: 157400, train loss: 4.049306201934814, val loss: 4.062001514434814, ETA in seconds: 7226609.835\n",
      "epoch: 157500, train loss: 4.049501514434814, val loss: 4.067470264434815, ETA in seconds: 7230272.587\n",
      "epoch: 157600, train loss: 4.052821826934815, val loss: 4.065907764434814, ETA in seconds: 7233978.371\n",
      "epoch: 157700, train loss: 4.051259326934814, val loss: 4.0618062019348145, ETA in seconds: 7237674.599\n",
      "epoch: 157800, train loss: 4.052235889434814, val loss: 4.0696187019348145, ETA in seconds: 7241343.791\n",
      "epoch: 157900, train loss: 4.056532764434815, val loss: 4.0666890144348145, ETA in seconds: 7245087.074\n",
      "epoch: 158000, train loss: 4.0530171394348145, val loss: 4.063173389434814, ETA in seconds: 7248915.297\n",
      "epoch: 158100, train loss: 4.047939014434815, val loss: 4.067860889434814, ETA in seconds: 7252678.646\n",
      "epoch: 158200, train loss: 4.052235889434814, val loss: 4.070204639434815, ETA in seconds: 7256396.212\n",
      "epoch: 158300, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 7260175.432\n",
      "epoch: 158400, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 7263928.815\n",
      "epoch: 158500, train loss: 4.0510640144348145, val loss: 4.065907764434814, ETA in seconds: 7267691.524\n",
      "epoch: 158600, train loss: 4.052235889434814, val loss: 4.0657124519348145, ETA in seconds: 7271479.615\n",
      "epoch: 158700, train loss: 4.046571826934814, val loss: 4.063564014434815, ETA in seconds: 7275219.860\n",
      "epoch: 158800, train loss: 4.045399951934814, val loss: 4.065907764434814, ETA in seconds: 7278955.474\n",
      "epoch: 158900, train loss: 4.055556201934815, val loss: 4.070790576934814, ETA in seconds: 7282553.047\n",
      "epoch: 159000, train loss: 4.050282764434814, val loss: 4.069228076934815, ETA in seconds: 7286099.529\n",
      "epoch: 159100, train loss: 4.052821826934815, val loss: 4.070399951934815, ETA in seconds: 7289622.921\n",
      "epoch: 159200, train loss: 4.053798389434815, val loss: 4.067274951934815, ETA in seconds: 7293015.628\n",
      "epoch: 159300, train loss: 4.046571826934814, val loss: 4.069228076934815, ETA in seconds: 7296502.740\n",
      "epoch: 159400, train loss: 4.048915576934815, val loss: 4.065321826934815, ETA in seconds: 7300059.978\n",
      "epoch: 159500, train loss: 4.057509326934815, val loss: 4.069032764434814, ETA in seconds: 7303639.272\n",
      "epoch: 159600, train loss: 4.054774951934815, val loss: 4.0715718269348145, ETA in seconds: 7307249.887\n",
      "epoch: 159700, train loss: 4.045985889434815, val loss: 4.065126514434814, ETA in seconds: 7310739.961\n",
      "epoch: 159800, train loss: 4.049696826934815, val loss: 4.069228076934815, ETA in seconds: 7314420.789\n",
      "epoch: 159900, train loss: 4.050478076934814, val loss: 4.068251514434815, ETA in seconds: 7318248.844\n",
      "epoch: 160000, train loss: 4.0471577644348145, val loss: 4.0705952644348145, ETA in seconds: 7322071.946\n",
      "epoch: 160100, train loss: 4.057704639434815, val loss: 4.070399951934815, ETA in seconds: 7325794.431\n",
      "epoch: 160200, train loss: 4.056532764434815, val loss: 4.068446826934815, ETA in seconds: 7329789.925\n",
      "epoch: 160300, train loss: 4.048720264434815, val loss: 4.067470264434815, ETA in seconds: 7333775.326\n",
      "epoch: 160400, train loss: 4.054189014434814, val loss: 4.0696187019348145, ETA in seconds: 7337533.033\n",
      "epoch: 160500, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 7341220.462\n",
      "epoch: 160600, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 7344948.003\n",
      "epoch: 160700, train loss: 4.0481343269348145, val loss: 4.066103076934814, ETA in seconds: 7348646.749\n",
      "epoch: 160800, train loss: 4.056532764434815, val loss: 4.064345264434815, ETA in seconds: 7352393.881\n",
      "epoch: 160900, train loss: 4.055165576934814, val loss: 4.066884326934814, ETA in seconds: 7356001.524\n",
      "epoch: 161000, train loss: 4.0549702644348145, val loss: 4.063564014434815, ETA in seconds: 7359760.101\n",
      "epoch: 161100, train loss: 4.051649951934815, val loss: 4.068251514434815, ETA in seconds: 7363463.691\n",
      "epoch: 161200, train loss: 4.054384326934814, val loss: 4.0735249519348145, ETA in seconds: 7367305.687\n",
      "epoch: 161300, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 7371105.569\n",
      "epoch: 161400, train loss: 4.052821826934815, val loss: 4.065517139434815, ETA in seconds: 7374960.752\n",
      "epoch: 161500, train loss: 4.0569233894348145, val loss: 4.062196826934814, ETA in seconds: 7378821.919\n",
      "epoch: 161600, train loss: 4.0588765144348145, val loss: 4.063564014434815, ETA in seconds: 7382486.624\n",
      "epoch: 161700, train loss: 4.0569233894348145, val loss: 4.066103076934814, ETA in seconds: 7386062.114\n",
      "epoch: 161800, train loss: 4.048915576934815, val loss: 4.067470264434815, ETA in seconds: 7389657.008\n",
      "epoch: 161900, train loss: 4.0510640144348145, val loss: 4.066493701934815, ETA in seconds: 7393367.269\n",
      "epoch: 162000, train loss: 4.052821826934815, val loss: 4.0578999519348145, ETA in seconds: 7396980.977\n",
      "epoch: 162100, train loss: 4.050673389434815, val loss: 4.063368701934815, ETA in seconds: 7400552.017\n",
      "epoch: 162200, train loss: 4.0491108894348145, val loss: 4.068446826934815, ETA in seconds: 7404261.182\n",
      "epoch: 162300, train loss: 4.054384326934814, val loss: 4.065517139434815, ETA in seconds: 7407861.974\n",
      "epoch: 162400, train loss: 4.0510640144348145, val loss: 4.066103076934814, ETA in seconds: 7411496.594\n",
      "epoch: 162500, train loss: 4.052821826934815, val loss: 4.065517139434815, ETA in seconds: 7415183.848\n",
      "epoch: 162600, train loss: 4.052626514434815, val loss: 4.064540576934815, ETA in seconds: 7419021.855\n",
      "epoch: 162700, train loss: 4.057118701934814, val loss: 4.063173389434814, ETA in seconds: 7422562.762\n",
      "epoch: 162800, train loss: 4.049501514434814, val loss: 4.069423389434815, ETA in seconds: 7425432.057\n",
      "epoch: 162900, train loss: 4.0481343269348145, val loss: 4.0676655769348145, ETA in seconds: 7429024.163\n",
      "epoch: 163000, train loss: 4.051649951934815, val loss: 4.061220264434814, ETA in seconds: 7432642.245\n",
      "epoch: 163100, train loss: 4.055556201934815, val loss: 4.067274951934815, ETA in seconds: 7436363.023\n",
      "epoch: 163200, train loss: 4.054579639434815, val loss: 4.062196826934814, ETA in seconds: 7440348.950\n",
      "epoch: 163300, train loss: 4.050868701934815, val loss: 4.067470264434815, ETA in seconds: 7444127.551\n",
      "epoch: 163400, train loss: 4.054189014434814, val loss: 4.067274951934815, ETA in seconds: 7447887.369\n",
      "epoch: 163500, train loss: 4.047939014434815, val loss: 4.0725483894348145, ETA in seconds: 7451518.308\n",
      "epoch: 163600, train loss: 4.0500874519348145, val loss: 4.070009326934814, ETA in seconds: 7455186.267\n",
      "epoch: 163700, train loss: 4.0539937019348145, val loss: 4.069423389434815, ETA in seconds: 7459035.654\n",
      "epoch: 163800, train loss: 4.055556201934815, val loss: 4.070985889434814, ETA in seconds: 7462632.125\n",
      "epoch: 163900, train loss: 4.041493701934814, val loss: 4.066298389434815, ETA in seconds: 7466257.916\n",
      "epoch: 164000, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 7469991.227\n",
      "epoch: 164100, train loss: 4.051845264434815, val loss: 4.067274951934815, ETA in seconds: 7473768.381\n",
      "epoch: 164200, train loss: 4.054384326934814, val loss: 4.0686421394348145, ETA in seconds: 7477370.047\n",
      "epoch: 164300, train loss: 4.052626514434815, val loss: 4.073134326934815, ETA in seconds: 7480413.633\n",
      "epoch: 164400, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 7484437.524\n",
      "epoch: 164500, train loss: 4.045595264434814, val loss: 4.069814014434814, ETA in seconds: 7488349.132\n",
      "epoch: 164600, train loss: 4.053798389434815, val loss: 4.064149951934814, ETA in seconds: 7492279.709\n",
      "epoch: 164700, train loss: 4.060048389434814, val loss: 4.0715718269348145, ETA in seconds: 7495995.365\n",
      "epoch: 164800, train loss: 4.058681201934815, val loss: 4.065907764434814, ETA in seconds: 7499877.936\n",
      "epoch: 164900, train loss: 4.0491108894348145, val loss: 4.070399951934815, ETA in seconds: 7503794.964\n",
      "epoch: 165000, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 7507767.085\n",
      "epoch: 165100, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 7511408.353\n",
      "epoch: 165200, train loss: 4.061220264434814, val loss: 4.069032764434814, ETA in seconds: 7515079.722\n",
      "epoch: 165300, train loss: 4.055360889434814, val loss: 4.071767139434814, ETA in seconds: 7518796.558\n",
      "epoch: 165400, train loss: 4.053212451934814, val loss: 4.063954639434814, ETA in seconds: 7521969.642\n",
      "epoch: 165500, train loss: 4.0452046394348145, val loss: 4.064149951934814, ETA in seconds: 7525868.171\n",
      "epoch: 165600, train loss: 4.051845264434815, val loss: 4.070790576934814, ETA in seconds: 7529800.772\n",
      "epoch: 165700, train loss: 4.056532764434815, val loss: 4.066884326934814, ETA in seconds: 7533623.700\n",
      "epoch: 165800, train loss: 4.045790576934815, val loss: 4.070204639434815, ETA in seconds: 7537438.555\n",
      "epoch: 165900, train loss: 4.0452046394348145, val loss: 4.068837451934814, ETA in seconds: 7541437.947\n",
      "epoch: 166000, train loss: 4.052431201934814, val loss: 4.069423389434815, ETA in seconds: 7545424.880\n",
      "epoch: 166100, train loss: 4.051259326934814, val loss: 4.057509326934815, ETA in seconds: 7549309.287\n",
      "epoch: 166200, train loss: 4.055751514434815, val loss: 4.068056201934814, ETA in seconds: 7552963.191\n",
      "epoch: 166300, train loss: 4.048329639434814, val loss: 4.0657124519348145, ETA in seconds: 7556800.845\n",
      "epoch: 166400, train loss: 4.055556201934815, val loss: 4.062587451934815, ETA in seconds: 7560729.677\n",
      "epoch: 166500, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 7564419.898\n",
      "epoch: 166600, train loss: 4.055556201934815, val loss: 4.0647358894348145, ETA in seconds: 7568276.115\n",
      "epoch: 166700, train loss: 4.051259326934814, val loss: 4.0705952644348145, ETA in seconds: 7572139.203\n",
      "epoch: 166800, train loss: 4.053212451934814, val loss: 4.064931201934814, ETA in seconds: 7575903.106\n",
      "epoch: 166900, train loss: 4.051454639434814, val loss: 4.067274951934815, ETA in seconds: 7579702.772\n",
      "epoch: 167000, train loss: 4.049306201934814, val loss: 4.069032764434814, ETA in seconds: 7583629.999\n",
      "epoch: 167100, train loss: 4.057118701934814, val loss: 4.066493701934815, ETA in seconds: 7587722.515\n",
      "epoch: 167200, train loss: 4.049892139434815, val loss: 4.066298389434815, ETA in seconds: 7591554.149\n",
      "epoch: 167300, train loss: 4.048915576934815, val loss: 4.066493701934815, ETA in seconds: 7595362.266\n",
      "epoch: 167400, train loss: 4.053407764434814, val loss: 4.0647358894348145, ETA in seconds: 7599031.389\n",
      "epoch: 167500, train loss: 4.043446826934814, val loss: 4.066493701934815, ETA in seconds: 7602664.493\n",
      "epoch: 167600, train loss: 4.048524951934814, val loss: 4.0618062019348145, ETA in seconds: 7606464.392\n",
      "epoch: 167700, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 7610160.074\n",
      "epoch: 167800, train loss: 4.052431201934814, val loss: 4.064540576934815, ETA in seconds: 7613792.942\n",
      "epoch: 167900, train loss: 4.049501514434814, val loss: 4.064931201934814, ETA in seconds: 7617129.382\n",
      "epoch: 168000, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 7620485.234\n",
      "epoch: 168100, train loss: 4.053798389434815, val loss: 4.0676655769348145, ETA in seconds: 7623984.037\n",
      "epoch: 168200, train loss: 4.054189014434814, val loss: 4.0598530769348145, ETA in seconds: 7627579.909\n",
      "epoch: 168300, train loss: 4.047939014434815, val loss: 4.063954639434814, ETA in seconds: 7631530.845\n",
      "epoch: 168400, train loss: 4.048720264434815, val loss: 4.072743701934814, ETA in seconds: 7635363.650\n",
      "epoch: 168500, train loss: 4.052821826934815, val loss: 4.066884326934814, ETA in seconds: 7638996.289\n",
      "epoch: 168600, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 7642788.339\n",
      "epoch: 168700, train loss: 4.051259326934814, val loss: 4.069423389434815, ETA in seconds: 7646621.044\n",
      "epoch: 168800, train loss: 4.045595264434814, val loss: 4.066103076934814, ETA in seconds: 7650175.010\n",
      "epoch: 168900, train loss: 4.0549702644348145, val loss: 4.067470264434815, ETA in seconds: 7653755.010\n",
      "epoch: 169000, train loss: 4.054189014434814, val loss: 4.067274951934815, ETA in seconds: 7657283.574\n",
      "epoch: 169100, train loss: 4.058681201934815, val loss: 4.0745015144348145, ETA in seconds: 7660759.727\n",
      "epoch: 169200, train loss: 4.060243701934814, val loss: 4.065517139434815, ETA in seconds: 7664288.296\n",
      "epoch: 169300, train loss: 4.051259326934814, val loss: 4.067274951934815, ETA in seconds: 7667754.700\n",
      "epoch: 169400, train loss: 4.045985889434815, val loss: 4.067470264434815, ETA in seconds: 7671169.237\n",
      "epoch: 169500, train loss: 4.048329639434814, val loss: 4.064540576934815, ETA in seconds: 7674943.672\n",
      "epoch: 169600, train loss: 4.048915576934815, val loss: 4.072743701934814, ETA in seconds: 7678456.895\n",
      "epoch: 169700, train loss: 4.053407764434814, val loss: 4.068251514434815, ETA in seconds: 7681940.328\n",
      "epoch: 169800, train loss: 4.047939014434815, val loss: 4.065907764434814, ETA in seconds: 7685427.748\n",
      "epoch: 169900, train loss: 4.051259326934814, val loss: 4.063564014434815, ETA in seconds: 7688839.872\n",
      "epoch: 170000, train loss: 4.046376514434814, val loss: 4.067079639434814, ETA in seconds: 7692191.475\n",
      "epoch: 170100, train loss: 4.049306201934814, val loss: 4.066493701934815, ETA in seconds: 7696218.928\n",
      "epoch: 170200, train loss: 4.048524951934814, val loss: 4.065517139434815, ETA in seconds: 7699993.109\n",
      "epoch: 170300, train loss: 4.059071826934814, val loss: 4.069423389434815, ETA in seconds: 7703799.322\n",
      "epoch: 170400, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 7707722.789\n",
      "epoch: 170500, train loss: 4.0530171394348145, val loss: 4.066298389434815, ETA in seconds: 7711290.605\n",
      "epoch: 170600, train loss: 4.055751514434815, val loss: 4.066884326934814, ETA in seconds: 7714929.743\n",
      "epoch: 170700, train loss: 4.043446826934814, val loss: 4.060439014434815, ETA in seconds: 7718571.354\n",
      "epoch: 170800, train loss: 4.050868701934815, val loss: 4.066103076934814, ETA in seconds: 7722376.808\n",
      "epoch: 170900, train loss: 4.053407764434814, val loss: 4.0705952644348145, ETA in seconds: 7726262.227\n",
      "epoch: 171000, train loss: 4.054579639434815, val loss: 4.062978076934814, ETA in seconds: 7730177.630\n",
      "epoch: 171100, train loss: 4.055360889434814, val loss: 4.0637593269348145, ETA in seconds: 7733990.939\n",
      "epoch: 171200, train loss: 4.046962451934815, val loss: 4.0696187019348145, ETA in seconds: 7737876.971\n",
      "epoch: 171300, train loss: 4.051845264434815, val loss: 4.064540576934815, ETA in seconds: 7741807.811\n",
      "epoch: 171400, train loss: 4.054189014434814, val loss: 4.068837451934814, ETA in seconds: 7745411.913\n",
      "epoch: 171500, train loss: 4.047939014434815, val loss: 4.067079639434814, ETA in seconds: 7748978.382\n",
      "epoch: 171600, train loss: 4.0530171394348145, val loss: 4.062587451934815, ETA in seconds: 7752578.525\n",
      "epoch: 171700, train loss: 4.044618701934814, val loss: 4.069228076934815, ETA in seconds: 7756152.798\n",
      "epoch: 171800, train loss: 4.050282764434814, val loss: 4.0676655769348145, ETA in seconds: 7759855.940\n",
      "epoch: 171900, train loss: 4.051259326934814, val loss: 4.064345264434815, ETA in seconds: 7763466.464\n",
      "epoch: 172000, train loss: 4.050282764434814, val loss: 4.071181201934815, ETA in seconds: 7767124.189\n",
      "epoch: 172100, train loss: 4.053212451934814, val loss: 4.0686421394348145, ETA in seconds: 7770718.262\n",
      "epoch: 172200, train loss: 4.052821826934815, val loss: 4.0696187019348145, ETA in seconds: 7774304.882\n",
      "epoch: 172300, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 7778016.853\n",
      "epoch: 172400, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 7781806.441\n",
      "epoch: 172500, train loss: 4.049696826934815, val loss: 4.067860889434814, ETA in seconds: 7785468.984\n",
      "epoch: 172600, train loss: 4.0539937019348145, val loss: 4.0686421394348145, ETA in seconds: 7789248.902\n",
      "epoch: 172700, train loss: 4.052821826934815, val loss: 4.072157764434815, ETA in seconds: 7792853.353\n",
      "epoch: 172800, train loss: 4.052431201934814, val loss: 4.064540576934815, ETA in seconds: 7796558.281\n",
      "epoch: 172900, train loss: 4.054189014434814, val loss: 4.069423389434815, ETA in seconds: 7800159.957\n",
      "epoch: 173000, train loss: 4.0549702644348145, val loss: 4.070204639434815, ETA in seconds: 7804001.292\n",
      "epoch: 173100, train loss: 4.057704639434815, val loss: 4.062587451934815, ETA in seconds: 7807704.795\n",
      "epoch: 173200, train loss: 4.050282764434814, val loss: 4.064149951934814, ETA in seconds: 7811268.209\n",
      "epoch: 173300, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 7814829.326\n",
      "epoch: 173400, train loss: 4.046376514434814, val loss: 4.070985889434814, ETA in seconds: 7818399.775\n",
      "epoch: 173500, train loss: 4.055751514434815, val loss: 4.067274951934815, ETA in seconds: 7821859.096\n",
      "epoch: 173600, train loss: 4.056337451934814, val loss: 4.062978076934814, ETA in seconds: 7825360.129\n",
      "epoch: 173700, train loss: 4.054189014434814, val loss: 4.065517139434815, ETA in seconds: 7828870.891\n",
      "epoch: 173800, train loss: 4.054189014434814, val loss: 4.069032764434814, ETA in seconds: 7832206.314\n",
      "epoch: 173900, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 7835535.584\n",
      "epoch: 174000, train loss: 4.047548389434814, val loss: 4.066884326934814, ETA in seconds: 7839112.181\n",
      "epoch: 174100, train loss: 4.051649951934815, val loss: 4.064931201934814, ETA in seconds: 7842579.693\n",
      "epoch: 174200, train loss: 4.056142139434814, val loss: 4.066493701934815, ETA in seconds: 7846080.345\n",
      "epoch: 174300, train loss: 4.045399951934814, val loss: 4.071962451934814, ETA in seconds: 7849650.901\n",
      "epoch: 174400, train loss: 4.050673389434815, val loss: 4.062001514434814, ETA in seconds: 7853099.413\n",
      "epoch: 174500, train loss: 4.048915576934815, val loss: 4.073720264434814, ETA in seconds: 7856692.450\n",
      "epoch: 174600, train loss: 4.054384326934814, val loss: 4.070985889434814, ETA in seconds: 7860157.106\n",
      "epoch: 174700, train loss: 4.051454639434814, val loss: 4.064931201934814, ETA in seconds: 7863701.291\n",
      "epoch: 174800, train loss: 4.054579639434815, val loss: 4.0686421394348145, ETA in seconds: 7867169.985\n",
      "epoch: 174900, train loss: 4.058681201934815, val loss: 4.0657124519348145, ETA in seconds: 7870706.066\n",
      "epoch: 175000, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 7874105.048\n",
      "epoch: 175100, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 7877573.023\n",
      "epoch: 175200, train loss: 4.051259326934814, val loss: 4.0745015144348145, ETA in seconds: 7881165.157\n",
      "epoch: 175300, train loss: 4.051845264434815, val loss: 4.064149951934814, ETA in seconds: 7884798.236\n",
      "epoch: 175400, train loss: 4.047353076934814, val loss: 4.063954639434814, ETA in seconds: 7888373.596\n",
      "epoch: 175500, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 7891998.929\n",
      "epoch: 175600, train loss: 4.053798389434815, val loss: 4.064345264434815, ETA in seconds: 7895498.972\n",
      "epoch: 175700, train loss: 4.047548389434814, val loss: 4.068251514434815, ETA in seconds: 7899305.142\n",
      "epoch: 175800, train loss: 4.046571826934814, val loss: 4.070399951934815, ETA in seconds: 7903022.662\n",
      "epoch: 175900, train loss: 4.050868701934815, val loss: 4.067860889434814, ETA in seconds: 7906725.730\n",
      "epoch: 176000, train loss: 4.046962451934815, val loss: 4.066103076934814, ETA in seconds: 7910189.347\n",
      "epoch: 176100, train loss: 4.053212451934814, val loss: 4.062392139434815, ETA in seconds: 7913782.638\n",
      "epoch: 176200, train loss: 4.0549702644348145, val loss: 4.067860889434814, ETA in seconds: 7917283.761\n",
      "epoch: 176300, train loss: 4.056337451934814, val loss: 4.065321826934815, ETA in seconds: 7920752.897\n",
      "epoch: 176400, train loss: 4.054384326934814, val loss: 4.068837451934814, ETA in seconds: 7924267.524\n",
      "epoch: 176500, train loss: 4.046962451934815, val loss: 4.065321826934815, ETA in seconds: 7927797.632\n",
      "epoch: 176600, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 7931261.873\n",
      "epoch: 176700, train loss: 4.052431201934814, val loss: 4.066103076934814, ETA in seconds: 7934810.882\n",
      "epoch: 176800, train loss: 4.052626514434815, val loss: 4.065907764434814, ETA in seconds: 7938563.510\n",
      "epoch: 176900, train loss: 4.053798389434815, val loss: 4.060634326934815, ETA in seconds: 7942086.730\n",
      "epoch: 177000, train loss: 4.044618701934814, val loss: 4.070009326934814, ETA in seconds: 7945667.544\n",
      "epoch: 177100, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 7949278.368\n",
      "epoch: 177200, train loss: 4.0520405769348145, val loss: 4.071376514434815, ETA in seconds: 7953007.265\n",
      "epoch: 177300, train loss: 4.048915576934815, val loss: 4.068056201934814, ETA in seconds: 7956640.515\n",
      "epoch: 177400, train loss: 4.052235889434814, val loss: 4.070985889434814, ETA in seconds: 7960145.628\n",
      "epoch: 177500, train loss: 4.057118701934814, val loss: 4.065907764434814, ETA in seconds: 7963817.626\n",
      "epoch: 177600, train loss: 4.0520405769348145, val loss: 4.064345264434815, ETA in seconds: 7967450.417\n",
      "epoch: 177700, train loss: 4.047939014434815, val loss: 4.068837451934814, ETA in seconds: 7970551.582\n",
      "epoch: 177800, train loss: 4.0530171394348145, val loss: 4.061220264434814, ETA in seconds: 7974073.231\n",
      "epoch: 177900, train loss: 4.045009326934815, val loss: 4.068446826934815, ETA in seconds: 7977578.543\n",
      "epoch: 178000, train loss: 4.0569233894348145, val loss: 4.066298389434815, ETA in seconds: 7981046.476\n",
      "epoch: 178100, train loss: 4.044423389434814, val loss: 4.070985889434814, ETA in seconds: 7984307.936\n",
      "epoch: 178200, train loss: 4.0539937019348145, val loss: 4.0647358894348145, ETA in seconds: 7987721.963\n",
      "epoch: 178300, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 7991387.437\n",
      "epoch: 178400, train loss: 4.0442280769348145, val loss: 4.071962451934814, ETA in seconds: 7994845.741\n",
      "epoch: 178500, train loss: 4.052626514434815, val loss: 4.063954639434814, ETA in seconds: 7998263.914\n",
      "epoch: 178600, train loss: 4.055556201934815, val loss: 4.076649951934814, ETA in seconds: 8001561.162\n",
      "epoch: 178700, train loss: 4.0500874519348145, val loss: 4.0637593269348145, ETA in seconds: 8004887.349\n",
      "epoch: 178800, train loss: 4.059462451934815, val loss: 4.073720264434814, ETA in seconds: 8008479.200\n",
      "epoch: 178900, train loss: 4.052626514434815, val loss: 4.063564014434815, ETA in seconds: 8012134.679\n",
      "epoch: 179000, train loss: 4.046962451934815, val loss: 4.0666890144348145, ETA in seconds: 8015725.090\n",
      "epoch: 179100, train loss: 4.0549702644348145, val loss: 4.071376514434815, ETA in seconds: 8019234.640\n",
      "epoch: 179200, train loss: 4.050868701934815, val loss: 4.0705952644348145, ETA in seconds: 8022457.221\n",
      "epoch: 179300, train loss: 4.050673389434815, val loss: 4.061024951934814, ETA in seconds: 8025959.687\n",
      "epoch: 179400, train loss: 4.049306201934814, val loss: 4.062001514434814, ETA in seconds: 8029527.874\n",
      "epoch: 179500, train loss: 4.0598530769348145, val loss: 4.066493701934815, ETA in seconds: 8033114.749\n",
      "epoch: 179600, train loss: 4.056142139434814, val loss: 4.069814014434814, ETA in seconds: 8036480.017\n",
      "epoch: 179700, train loss: 4.049306201934814, val loss: 4.062587451934815, ETA in seconds: 8039859.753\n",
      "epoch: 179800, train loss: 4.055751514434815, val loss: 4.064345264434815, ETA in seconds: 8043091.921\n",
      "epoch: 179900, train loss: 4.052821826934815, val loss: 4.063954639434814, ETA in seconds: 8046468.655\n",
      "epoch: 180000, train loss: 4.048915576934815, val loss: 4.065321826934815, ETA in seconds: 8049890.159\n",
      "epoch: 180100, train loss: 4.051649951934815, val loss: 4.066103076934814, ETA in seconds: 8053417.084\n",
      "epoch: 180200, train loss: 4.048524951934814, val loss: 4.067860889434814, ETA in seconds: 8056834.370\n",
      "epoch: 180300, train loss: 4.0539937019348145, val loss: 4.067079639434814, ETA in seconds: 8060140.907\n",
      "epoch: 180400, train loss: 4.0491108894348145, val loss: 4.068251514434815, ETA in seconds: 8063398.863\n",
      "epoch: 180500, train loss: 4.0520405769348145, val loss: 4.069228076934815, ETA in seconds: 8066758.995\n",
      "epoch: 180600, train loss: 4.057118701934814, val loss: 4.066103076934814, ETA in seconds: 8070316.120\n",
      "epoch: 180700, train loss: 4.052821826934815, val loss: 4.062001514434814, ETA in seconds: 8073803.742\n",
      "epoch: 180800, train loss: 4.0569233894348145, val loss: 4.070009326934814, ETA in seconds: 8077265.775\n",
      "epoch: 180900, train loss: 4.056142139434814, val loss: 4.068446826934815, ETA in seconds: 8080829.919\n",
      "epoch: 181000, train loss: 4.047743701934815, val loss: 4.065517139434815, ETA in seconds: 8084309.442\n",
      "epoch: 181100, train loss: 4.054774951934815, val loss: 4.065517139434815, ETA in seconds: 8087927.758\n",
      "epoch: 181200, train loss: 4.0559468269348145, val loss: 4.071181201934815, ETA in seconds: 8091685.341\n",
      "epoch: 181300, train loss: 4.056728076934815, val loss: 4.070399951934815, ETA in seconds: 8095555.982\n",
      "epoch: 181400, train loss: 4.052626514434815, val loss: 4.0696187019348145, ETA in seconds: 8099294.563\n",
      "epoch: 181500, train loss: 4.059071826934814, val loss: 4.064540576934815, ETA in seconds: 8102942.203\n",
      "epoch: 181600, train loss: 4.061024951934814, val loss: 4.067274951934815, ETA in seconds: 8106497.165\n",
      "epoch: 181700, train loss: 4.0510640144348145, val loss: 4.073134326934815, ETA in seconds: 8109976.929\n",
      "epoch: 181800, train loss: 4.048915576934815, val loss: 4.067274951934815, ETA in seconds: 8113488.785\n",
      "epoch: 181900, train loss: 4.048524951934814, val loss: 4.061610889434815, ETA in seconds: 8117063.462\n",
      "epoch: 182000, train loss: 4.055556201934815, val loss: 4.061024951934814, ETA in seconds: 8120652.432\n",
      "epoch: 182100, train loss: 4.045009326934815, val loss: 4.071376514434815, ETA in seconds: 8124346.033\n",
      "epoch: 182200, train loss: 4.057118701934814, val loss: 4.0696187019348145, ETA in seconds: 8127919.109\n",
      "epoch: 182300, train loss: 4.051649951934815, val loss: 4.0676655769348145, ETA in seconds: 8131528.437\n",
      "epoch: 182400, train loss: 4.047743701934815, val loss: 4.065126514434814, ETA in seconds: 8134890.018\n",
      "epoch: 182500, train loss: 4.055751514434815, val loss: 4.0627827644348145, ETA in seconds: 8138181.757\n",
      "epoch: 182600, train loss: 4.046376514434814, val loss: 4.071767139434814, ETA in seconds: 8141485.156\n",
      "epoch: 182700, train loss: 4.056728076934815, val loss: 4.0627827644348145, ETA in seconds: 8144921.498\n",
      "epoch: 182800, train loss: 4.049501514434814, val loss: 4.062196826934814, ETA in seconds: 8148436.144\n",
      "epoch: 182900, train loss: 4.046962451934815, val loss: 4.0696187019348145, ETA in seconds: 8151961.323\n",
      "epoch: 183000, train loss: 4.047743701934815, val loss: 4.068837451934814, ETA in seconds: 8155483.184\n",
      "epoch: 183100, train loss: 4.049696826934815, val loss: 4.067470264434815, ETA in seconds: 8159031.323\n",
      "epoch: 183200, train loss: 4.046571826934814, val loss: 4.065126514434814, ETA in seconds: 8162620.860\n",
      "epoch: 183300, train loss: 4.0530171394348145, val loss: 4.067470264434815, ETA in seconds: 8166160.901\n",
      "epoch: 183400, train loss: 4.055751514434815, val loss: 4.067079639434814, ETA in seconds: 8169799.925\n",
      "epoch: 183500, train loss: 4.045985889434815, val loss: 4.0647358894348145, ETA in seconds: 8173373.370\n",
      "epoch: 183600, train loss: 4.047548389434814, val loss: 4.069814014434814, ETA in seconds: 8177107.855\n",
      "epoch: 183700, train loss: 4.053212451934814, val loss: 4.064149951934814, ETA in seconds: 8180787.332\n",
      "epoch: 183800, train loss: 4.056142139434814, val loss: 4.075868701934814, ETA in seconds: 8184352.601\n",
      "epoch: 183900, train loss: 4.052821826934815, val loss: 4.072157764434815, ETA in seconds: 8188173.032\n",
      "epoch: 184000, train loss: 4.047939014434815, val loss: 4.0676655769348145, ETA in seconds: 8191807.318\n",
      "epoch: 184100, train loss: 4.051259326934814, val loss: 4.065126514434814, ETA in seconds: 8195215.003\n",
      "epoch: 184200, train loss: 4.049306201934814, val loss: 4.066493701934815, ETA in seconds: 8198768.299\n",
      "epoch: 184300, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 8202069.394\n",
      "epoch: 184400, train loss: 4.053407764434814, val loss: 4.070399951934815, ETA in seconds: 8205307.647\n",
      "epoch: 184500, train loss: 4.051454639434814, val loss: 4.065126514434814, ETA in seconds: 8208632.176\n",
      "epoch: 184600, train loss: 4.0510640144348145, val loss: 4.072939014434814, ETA in seconds: 8212130.107\n",
      "epoch: 184700, train loss: 4.0539937019348145, val loss: 4.0686421394348145, ETA in seconds: 8215496.387\n",
      "epoch: 184800, train loss: 4.053212451934814, val loss: 4.070009326934814, ETA in seconds: 8218959.768\n",
      "epoch: 184900, train loss: 4.051845264434815, val loss: 4.068837451934814, ETA in seconds: 8222390.365\n",
      "epoch: 185000, train loss: 4.052235889434814, val loss: 4.063564014434815, ETA in seconds: 8225793.463\n",
      "epoch: 185100, train loss: 4.057118701934814, val loss: 4.065907764434814, ETA in seconds: 8228791.671\n",
      "epoch: 185200, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 8232312.196\n",
      "epoch: 185300, train loss: 4.052431201934814, val loss: 4.0627827644348145, ETA in seconds: 8235840.364\n",
      "epoch: 185400, train loss: 4.048524951934814, val loss: 4.069423389434815, ETA in seconds: 8239507.618\n",
      "epoch: 185500, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 8243101.778\n",
      "epoch: 185600, train loss: 4.050282764434814, val loss: 4.067274951934815, ETA in seconds: 8246559.555\n",
      "epoch: 185700, train loss: 4.054774951934815, val loss: 4.065321826934815, ETA in seconds: 8250104.699\n",
      "epoch: 185800, train loss: 4.055556201934815, val loss: 4.073915576934814, ETA in seconds: 8253751.230\n",
      "epoch: 185900, train loss: 4.0422749519348145, val loss: 4.063368701934815, ETA in seconds: 8257292.810\n",
      "epoch: 186000, train loss: 4.049306201934814, val loss: 4.064345264434815, ETA in seconds: 8260746.832\n",
      "epoch: 186100, train loss: 4.054774951934815, val loss: 4.069423389434815, ETA in seconds: 8264080.787\n",
      "epoch: 186200, train loss: 4.052235889434814, val loss: 4.065321826934815, ETA in seconds: 8266947.460\n",
      "epoch: 186300, train loss: 4.053603076934815, val loss: 4.072157764434815, ETA in seconds: 8270395.270\n",
      "epoch: 186400, train loss: 4.0432515144348145, val loss: 4.069423389434815, ETA in seconds: 8273899.852\n",
      "epoch: 186500, train loss: 4.0530171394348145, val loss: 4.065126514434814, ETA in seconds: 8277402.167\n",
      "epoch: 186600, train loss: 4.049892139434815, val loss: 4.068446826934815, ETA in seconds: 8280817.804\n",
      "epoch: 186700, train loss: 4.051259326934814, val loss: 4.0696187019348145, ETA in seconds: 8284267.623\n",
      "epoch: 186800, train loss: 4.057118701934814, val loss: 4.067860889434814, ETA in seconds: 8287624.974\n",
      "epoch: 186900, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 8291050.781\n",
      "epoch: 187000, train loss: 4.049501514434814, val loss: 4.066298389434815, ETA in seconds: 8294496.419\n",
      "epoch: 187100, train loss: 4.053798389434815, val loss: 4.066298389434815, ETA in seconds: 8297901.206\n",
      "epoch: 187200, train loss: 4.049306201934814, val loss: 4.068251514434815, ETA in seconds: 8301537.024\n",
      "epoch: 187300, train loss: 4.054384326934814, val loss: 4.067274951934815, ETA in seconds: 8305054.001\n",
      "epoch: 187400, train loss: 4.047743701934815, val loss: 4.0647358894348145, ETA in seconds: 8308685.397\n",
      "epoch: 187500, train loss: 4.055751514434815, val loss: 4.069228076934815, ETA in seconds: 8312048.766\n",
      "epoch: 187600, train loss: 4.042470264434814, val loss: 4.064345264434815, ETA in seconds: 8315548.784\n",
      "epoch: 187700, train loss: 4.043446826934814, val loss: 4.0686421394348145, ETA in seconds: 8319026.494\n",
      "epoch: 187800, train loss: 4.051649951934815, val loss: 4.0676655769348145, ETA in seconds: 8322406.740\n",
      "epoch: 187900, train loss: 4.050282764434814, val loss: 4.0657124519348145, ETA in seconds: 8325735.905\n",
      "epoch: 188000, train loss: 4.054579639434815, val loss: 4.073329639434815, ETA in seconds: 8329145.856\n",
      "epoch: 188100, train loss: 4.043837451934815, val loss: 4.068837451934814, ETA in seconds: 8332494.034\n",
      "epoch: 188200, train loss: 4.0530171394348145, val loss: 4.065907764434814, ETA in seconds: 8335787.618\n",
      "epoch: 188300, train loss: 4.0539937019348145, val loss: 4.068056201934814, ETA in seconds: 8338968.600\n",
      "epoch: 188400, train loss: 4.053798389434815, val loss: 4.076649951934814, ETA in seconds: 8342121.094\n",
      "epoch: 188500, train loss: 4.047548389434814, val loss: 4.064540576934815, ETA in seconds: 8345516.150\n",
      "epoch: 188600, train loss: 4.050673389434815, val loss: 4.070204639434815, ETA in seconds: 8348874.209\n",
      "epoch: 188700, train loss: 4.054384326934814, val loss: 4.071962451934814, ETA in seconds: 8352174.966\n",
      "epoch: 188800, train loss: 4.0452046394348145, val loss: 4.064345264434815, ETA in seconds: 8355499.124\n",
      "epoch: 188900, train loss: 4.0471577644348145, val loss: 4.070399951934815, ETA in seconds: 8358823.507\n",
      "epoch: 189000, train loss: 4.050282764434814, val loss: 4.062392139434815, ETA in seconds: 8362086.461\n",
      "epoch: 189100, train loss: 4.044618701934814, val loss: 4.065321826934815, ETA in seconds: 8365516.719\n",
      "epoch: 189200, train loss: 4.045399951934814, val loss: 4.065517139434815, ETA in seconds: 8368731.124\n",
      "epoch: 189300, train loss: 4.058095264434814, val loss: 4.068837451934814, ETA in seconds: 8371943.268\n",
      "epoch: 189400, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 8375140.160\n",
      "epoch: 189500, train loss: 4.051845264434815, val loss: 4.061220264434814, ETA in seconds: 8378481.709\n",
      "epoch: 189600, train loss: 4.047939014434815, val loss: 4.068446826934815, ETA in seconds: 8381817.033\n",
      "epoch: 189700, train loss: 4.050673389434815, val loss: 4.070009326934814, ETA in seconds: 8385144.160\n",
      "epoch: 189800, train loss: 4.051259326934814, val loss: 4.0676655769348145, ETA in seconds: 8388464.752\n",
      "epoch: 189900, train loss: 4.049501514434814, val loss: 4.069423389434815, ETA in seconds: 8391745.144\n",
      "epoch: 190000, train loss: 4.0569233894348145, val loss: 4.064345264434815, ETA in seconds: 8395653.716\n",
      "epoch: 190100, train loss: 4.055165576934814, val loss: 4.067274951934815, ETA in seconds: 8399068.093\n",
      "epoch: 190200, train loss: 4.0549702644348145, val loss: 4.062001514434814, ETA in seconds: 8402363.402\n",
      "epoch: 190300, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 8405614.622\n",
      "epoch: 190400, train loss: 4.054189014434814, val loss: 4.061610889434815, ETA in seconds: 8408825.810\n",
      "epoch: 190500, train loss: 4.054189014434814, val loss: 4.072939014434814, ETA in seconds: 8412139.247\n",
      "epoch: 190600, train loss: 4.053212451934814, val loss: 4.061220264434814, ETA in seconds: 8415600.120\n",
      "epoch: 190700, train loss: 4.051259326934814, val loss: 4.062392139434815, ETA in seconds: 8419066.055\n",
      "epoch: 190800, train loss: 4.0442280769348145, val loss: 4.0686421394348145, ETA in seconds: 8422305.612\n",
      "epoch: 190900, train loss: 4.052431201934814, val loss: 4.070204639434815, ETA in seconds: 8425874.920\n",
      "epoch: 191000, train loss: 4.0452046394348145, val loss: 4.067274951934815, ETA in seconds: 8429523.089\n",
      "epoch: 191100, train loss: 4.045790576934815, val loss: 4.070204639434815, ETA in seconds: 8432922.965\n",
      "epoch: 191200, train loss: 4.0578999519348145, val loss: 4.066493701934815, ETA in seconds: 8436318.284\n",
      "epoch: 191300, train loss: 4.0559468269348145, val loss: 4.065126514434814, ETA in seconds: 8439746.860\n",
      "epoch: 191400, train loss: 4.050478076934814, val loss: 4.063564014434815, ETA in seconds: 8443062.137\n",
      "epoch: 191500, train loss: 4.060634326934815, val loss: 4.065907764434814, ETA in seconds: 8446415.775\n",
      "epoch: 191600, train loss: 4.057704639434815, val loss: 4.063368701934815, ETA in seconds: 8449804.074\n",
      "epoch: 191700, train loss: 4.053603076934815, val loss: 4.069423389434815, ETA in seconds: 8453211.245\n",
      "epoch: 191800, train loss: 4.045985889434815, val loss: 4.067470264434815, ETA in seconds: 8456503.235\n",
      "epoch: 191900, train loss: 4.049306201934814, val loss: 4.068056201934814, ETA in seconds: 8459875.654\n",
      "epoch: 192000, train loss: 4.050478076934814, val loss: 4.0637593269348145, ETA in seconds: 8463151.125\n",
      "epoch: 192100, train loss: 4.051259326934814, val loss: 4.067274951934815, ETA in seconds: 8466449.126\n",
      "epoch: 192200, train loss: 4.054579639434815, val loss: 4.0627827644348145, ETA in seconds: 8469667.651\n",
      "epoch: 192300, train loss: 4.054189014434814, val loss: 4.062978076934814, ETA in seconds: 8472831.386\n",
      "epoch: 192400, train loss: 4.048329639434814, val loss: 4.069032764434814, ETA in seconds: 8476138.814\n",
      "epoch: 192500, train loss: 4.054189014434814, val loss: 4.068056201934814, ETA in seconds: 8479394.760\n",
      "epoch: 192600, train loss: 4.049892139434815, val loss: 4.0686421394348145, ETA in seconds: 8482547.951\n",
      "epoch: 192700, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 8485975.737\n",
      "epoch: 192800, train loss: 4.048720264434815, val loss: 4.0676655769348145, ETA in seconds: 8489395.705\n",
      "epoch: 192900, train loss: 4.050868701934815, val loss: 4.067860889434814, ETA in seconds: 8492835.401\n",
      "epoch: 193000, train loss: 4.053798389434815, val loss: 4.0627827644348145, ETA in seconds: 8496224.781\n",
      "epoch: 193100, train loss: 4.0481343269348145, val loss: 4.071962451934814, ETA in seconds: 8499413.480\n",
      "epoch: 193200, train loss: 4.054384326934814, val loss: 4.070790576934814, ETA in seconds: 8502670.808\n",
      "epoch: 193300, train loss: 4.057314014434814, val loss: 4.0647358894348145, ETA in seconds: 8505987.144\n",
      "epoch: 193400, train loss: 4.049892139434815, val loss: 4.0735249519348145, ETA in seconds: 8509321.843\n",
      "epoch: 193500, train loss: 4.051454639434814, val loss: 4.065517139434815, ETA in seconds: 8512531.965\n",
      "epoch: 193600, train loss: 4.050673389434815, val loss: 4.066493701934815, ETA in seconds: 8515755.124\n",
      "epoch: 193700, train loss: 4.055556201934815, val loss: 4.064345264434815, ETA in seconds: 8519030.694\n",
      "epoch: 193800, train loss: 4.047353076934814, val loss: 4.071767139434814, ETA in seconds: 8522336.814\n",
      "epoch: 193900, train loss: 4.0471577644348145, val loss: 4.064931201934814, ETA in seconds: 8525661.398\n",
      "epoch: 194000, train loss: 4.049501514434814, val loss: 4.0715718269348145, ETA in seconds: 8528970.838\n",
      "epoch: 194100, train loss: 4.044423389434814, val loss: 4.066884326934814, ETA in seconds: 8532226.471\n",
      "epoch: 194200, train loss: 4.0549702644348145, val loss: 4.068056201934814, ETA in seconds: 8535535.329\n",
      "epoch: 194300, train loss: 4.051259326934814, val loss: 4.066103076934814, ETA in seconds: 8538992.441\n",
      "epoch: 194400, train loss: 4.050282764434814, val loss: 4.061610889434815, ETA in seconds: 8542564.484\n",
      "epoch: 194500, train loss: 4.051845264434815, val loss: 4.0647358894348145, ETA in seconds: 8546110.658\n",
      "epoch: 194600, train loss: 4.047353076934814, val loss: 4.068251514434815, ETA in seconds: 8549625.305\n",
      "epoch: 194700, train loss: 4.0588765144348145, val loss: 4.063564014434815, ETA in seconds: 8552921.634\n",
      "epoch: 194800, train loss: 4.054579639434815, val loss: 4.066493701934815, ETA in seconds: 8556214.465\n",
      "epoch: 194900, train loss: 4.055751514434815, val loss: 4.0627827644348145, ETA in seconds: 8559717.215\n",
      "epoch: 195000, train loss: 4.052626514434815, val loss: 4.067470264434815, ETA in seconds: 8563059.550\n",
      "epoch: 195100, train loss: 4.042079639434815, val loss: 4.069032764434814, ETA in seconds: 8566270.855\n",
      "epoch: 195200, train loss: 4.051845264434815, val loss: 4.070790576934814, ETA in seconds: 8569604.923\n",
      "epoch: 195300, train loss: 4.049306201934814, val loss: 4.067860889434814, ETA in seconds: 8572805.691\n",
      "epoch: 195400, train loss: 4.0442280769348145, val loss: 4.0696187019348145, ETA in seconds: 8576097.408\n",
      "epoch: 195500, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 8579349.250\n",
      "epoch: 195600, train loss: 4.052821826934815, val loss: 4.070985889434814, ETA in seconds: 8582606.226\n",
      "epoch: 195700, train loss: 4.050673389434815, val loss: 4.070790576934814, ETA in seconds: 8585744.421\n",
      "epoch: 195800, train loss: 4.046962451934815, val loss: 4.067274951934815, ETA in seconds: 8588958.192\n",
      "epoch: 195900, train loss: 4.0539937019348145, val loss: 4.063368701934815, ETA in seconds: 8592244.030\n",
      "epoch: 196000, train loss: 4.052821826934815, val loss: 4.065126514434814, ETA in seconds: 8595538.809\n",
      "epoch: 196100, train loss: 4.046376514434814, val loss: 4.0754780769348145, ETA in seconds: 8598759.360\n",
      "epoch: 196200, train loss: 4.048915576934815, val loss: 4.069228076934815, ETA in seconds: 8602049.379\n",
      "epoch: 196300, train loss: 4.058290576934814, val loss: 4.072157764434815, ETA in seconds: 8605407.236\n",
      "epoch: 196400, train loss: 4.058681201934815, val loss: 4.0608296394348145, ETA in seconds: 8608620.269\n",
      "epoch: 196500, train loss: 4.045399951934814, val loss: 4.069814014434814, ETA in seconds: 8612013.531\n",
      "epoch: 196600, train loss: 4.0461812019348145, val loss: 4.067079639434814, ETA in seconds: 8615651.566\n",
      "epoch: 196700, train loss: 4.050673389434815, val loss: 4.070009326934814, ETA in seconds: 8619275.674\n",
      "epoch: 196800, train loss: 4.053212451934814, val loss: 4.069228076934815, ETA in seconds: 8622625.225\n",
      "epoch: 196900, train loss: 4.0559468269348145, val loss: 4.0657124519348145, ETA in seconds: 8625861.904\n",
      "epoch: 197000, train loss: 4.050478076934814, val loss: 4.0696187019348145, ETA in seconds: 8629112.508\n",
      "epoch: 197100, train loss: 4.056728076934815, val loss: 4.0705952644348145, ETA in seconds: 8632330.237\n",
      "epoch: 197200, train loss: 4.051845264434815, val loss: 4.066493701934815, ETA in seconds: 8635656.277\n",
      "epoch: 197300, train loss: 4.0549702644348145, val loss: 4.068837451934814, ETA in seconds: 8638912.476\n",
      "epoch: 197400, train loss: 4.054384326934814, val loss: 4.0696187019348145, ETA in seconds: 8642189.987\n",
      "epoch: 197500, train loss: 4.0491108894348145, val loss: 4.068446826934815, ETA in seconds: 8645366.132\n",
      "epoch: 197600, train loss: 4.051259326934814, val loss: 4.064149951934814, ETA in seconds: 8648560.670\n",
      "epoch: 197700, train loss: 4.0559468269348145, val loss: 4.066493701934815, ETA in seconds: 8651793.811\n",
      "epoch: 197800, train loss: 4.058485889434815, val loss: 4.066493701934815, ETA in seconds: 8654669.059\n",
      "epoch: 197900, train loss: 4.052431201934814, val loss: 4.067860889434814, ETA in seconds: 8657301.537\n",
      "epoch: 198000, train loss: 4.050282764434814, val loss: 4.063173389434814, ETA in seconds: 8660351.150\n",
      "epoch: 198100, train loss: 4.054774951934815, val loss: 4.066493701934815, ETA in seconds: 8663547.100\n",
      "epoch: 198200, train loss: 4.046571826934814, val loss: 4.065517139434815, ETA in seconds: 8666824.136\n",
      "epoch: 198300, train loss: 4.053603076934815, val loss: 4.065907764434814, ETA in seconds: 8669831.109\n",
      "epoch: 198400, train loss: 4.052626514434815, val loss: 4.064345264434815, ETA in seconds: 8672754.898\n",
      "epoch: 198500, train loss: 4.047548389434814, val loss: 4.066298389434815, ETA in seconds: 8676034.185\n",
      "epoch: 198600, train loss: 4.053212451934814, val loss: 4.067470264434815, ETA in seconds: 8679244.270\n",
      "epoch: 198700, train loss: 4.049892139434815, val loss: 4.072157764434815, ETA in seconds: 8682415.553\n",
      "epoch: 198800, train loss: 4.052626514434815, val loss: 4.064931201934814, ETA in seconds: 8685775.755\n",
      "epoch: 198900, train loss: 4.057704639434815, val loss: 4.070399951934815, ETA in seconds: 8689069.825\n",
      "epoch: 199000, train loss: 4.053407764434814, val loss: 4.066103076934814, ETA in seconds: 8692382.837\n",
      "epoch: 199100, train loss: 4.050673389434815, val loss: 4.074110889434815, ETA in seconds: 8695943.184\n",
      "epoch: 199200, train loss: 4.0520405769348145, val loss: 4.073329639434815, ETA in seconds: 8699178.957\n",
      "epoch: 199300, train loss: 4.049696826934815, val loss: 4.067860889434814, ETA in seconds: 8702413.886\n",
      "epoch: 199400, train loss: 4.054384326934814, val loss: 4.0618062019348145, ETA in seconds: 8705828.047\n",
      "epoch: 199500, train loss: 4.0500874519348145, val loss: 4.067470264434815, ETA in seconds: 8709241.895\n",
      "epoch: 199600, train loss: 4.053407764434814, val loss: 4.072353076934815, ETA in seconds: 8712578.911\n",
      "epoch: 199700, train loss: 4.0510640144348145, val loss: 4.070399951934815, ETA in seconds: 8715868.840\n",
      "epoch: 199800, train loss: 4.051649951934815, val loss: 4.065321826934815, ETA in seconds: 8719283.150\n",
      "epoch: 199900, train loss: 4.049501514434814, val loss: 4.066884326934814, ETA in seconds: 8722796.390\n",
      "epoch: 200000, train loss: 4.050282764434814, val loss: 4.062978076934814, ETA in seconds: 8726051.592\n",
      "epoch: 200100, train loss: 4.048720264434815, val loss: 4.0637593269348145, ETA in seconds: 8729157.800\n",
      "epoch: 200200, train loss: 4.055360889434814, val loss: 4.065321826934815, ETA in seconds: 8732340.693\n",
      "epoch: 200300, train loss: 4.050868701934815, val loss: 4.0666890144348145, ETA in seconds: 8735456.347\n",
      "epoch: 200400, train loss: 4.051454639434814, val loss: 4.067860889434814, ETA in seconds: 8738623.158\n",
      "epoch: 200500, train loss: 4.045009326934815, val loss: 4.0676655769348145, ETA in seconds: 8741931.226\n",
      "epoch: 200600, train loss: 4.0481343269348145, val loss: 4.070985889434814, ETA in seconds: 8745522.908\n",
      "epoch: 200700, train loss: 4.051845264434815, val loss: 4.072743701934814, ETA in seconds: 8748814.611\n",
      "epoch: 200800, train loss: 4.045009326934815, val loss: 4.0657124519348145, ETA in seconds: 8752118.426\n",
      "epoch: 200900, train loss: 4.050868701934815, val loss: 4.067274951934815, ETA in seconds: 8755368.975\n",
      "epoch: 201000, train loss: 4.045595264434814, val loss: 4.071181201934815, ETA in seconds: 8758555.102\n",
      "epoch: 201100, train loss: 4.0559468269348145, val loss: 4.0657124519348145, ETA in seconds: 8761833.994\n",
      "epoch: 201200, train loss: 4.0539937019348145, val loss: 4.064345264434815, ETA in seconds: 8765073.826\n",
      "epoch: 201300, train loss: 4.0549702644348145, val loss: 4.063173389434814, ETA in seconds: 8768360.122\n",
      "epoch: 201400, train loss: 4.053603076934815, val loss: 4.0715718269348145, ETA in seconds: 8772245.746\n",
      "epoch: 201500, train loss: 4.052821826934815, val loss: 4.063173389434814, ETA in seconds: 8775491.566\n",
      "epoch: 201600, train loss: 4.055556201934815, val loss: 4.062196826934814, ETA in seconds: 8778675.445\n",
      "epoch: 201700, train loss: 4.051454639434814, val loss: 4.0754780769348145, ETA in seconds: 8781605.515\n",
      "epoch: 201800, train loss: 4.053603076934815, val loss: 4.063954639434814, ETA in seconds: 8784582.777\n",
      "epoch: 201900, train loss: 4.043837451934815, val loss: 4.068251514434815, ETA in seconds: 8787690.985\n",
      "epoch: 202000, train loss: 4.0481343269348145, val loss: 4.0666890144348145, ETA in seconds: 8790842.868\n",
      "epoch: 202100, train loss: 4.050868701934815, val loss: 4.063954639434814, ETA in seconds: 8794063.693\n",
      "epoch: 202200, train loss: 4.049696826934815, val loss: 4.070985889434814, ETA in seconds: 8797322.743\n",
      "epoch: 202300, train loss: 4.053798389434815, val loss: 4.061415576934815, ETA in seconds: 8800518.287\n",
      "epoch: 202400, train loss: 4.048329639434814, val loss: 4.065907764434814, ETA in seconds: 8803636.076\n",
      "epoch: 202500, train loss: 4.053407764434814, val loss: 4.062001514434814, ETA in seconds: 8806837.106\n",
      "epoch: 202600, train loss: 4.055751514434815, val loss: 4.071962451934814, ETA in seconds: 8810119.998\n",
      "epoch: 202700, train loss: 4.046571826934814, val loss: 4.065907764434814, ETA in seconds: 8813241.785\n",
      "epoch: 202800, train loss: 4.051454639434814, val loss: 4.063368701934815, ETA in seconds: 8816380.331\n",
      "epoch: 202900, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 8819610.420\n",
      "epoch: 203000, train loss: 4.045985889434815, val loss: 4.065126514434814, ETA in seconds: 8822672.729\n",
      "epoch: 203100, train loss: 4.044618701934814, val loss: 4.0666890144348145, ETA in seconds: 8825839.703\n",
      "epoch: 203200, train loss: 4.055556201934815, val loss: 4.065126514434814, ETA in seconds: 8828969.725\n",
      "epoch: 203300, train loss: 4.049306201934814, val loss: 4.064931201934814, ETA in seconds: 8832310.841\n",
      "epoch: 203400, train loss: 4.054774951934815, val loss: 4.0686421394348145, ETA in seconds: 8835406.971\n",
      "epoch: 203500, train loss: 4.053603076934815, val loss: 4.070204639434815, ETA in seconds: 8838544.327\n",
      "epoch: 203600, train loss: 4.049501514434814, val loss: 4.062001514434814, ETA in seconds: 8841515.168\n",
      "epoch: 203700, train loss: 4.050282764434814, val loss: 4.0608296394348145, ETA in seconds: 8844542.255\n",
      "epoch: 203800, train loss: 4.0520405769348145, val loss: 4.072939014434814, ETA in seconds: 8847661.851\n",
      "epoch: 203900, train loss: 4.050282764434814, val loss: 4.068251514434815, ETA in seconds: 8850871.484\n",
      "epoch: 204000, train loss: 4.049501514434814, val loss: 4.061220264434814, ETA in seconds: 8854197.321\n",
      "epoch: 204100, train loss: 4.054774951934815, val loss: 4.066884326934814, ETA in seconds: 8857467.362\n",
      "epoch: 204200, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 8860792.245\n",
      "epoch: 204300, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 8864005.399\n",
      "epoch: 204400, train loss: 4.051454639434814, val loss: 4.064931201934814, ETA in seconds: 8867155.278\n",
      "epoch: 204500, train loss: 4.053407764434814, val loss: 4.066493701934815, ETA in seconds: 8870413.367\n",
      "epoch: 204600, train loss: 4.058485889434815, val loss: 4.0696187019348145, ETA in seconds: 8873609.590\n",
      "epoch: 204700, train loss: 4.051259326934814, val loss: 4.071767139434814, ETA in seconds: 8876934.445\n",
      "epoch: 204800, train loss: 4.048720264434815, val loss: 4.067470264434815, ETA in seconds: 8880204.036\n",
      "epoch: 204900, train loss: 4.051454639434814, val loss: 4.062392139434815, ETA in seconds: 8883420.716\n",
      "epoch: 205000, train loss: 4.047743701934815, val loss: 4.068837451934814, ETA in seconds: 8886776.049\n",
      "epoch: 205100, train loss: 4.052626514434815, val loss: 4.072157764434815, ETA in seconds: 8889862.115\n",
      "epoch: 205200, train loss: 4.0578999519348145, val loss: 4.063954639434814, ETA in seconds: 8892988.341\n",
      "epoch: 205300, train loss: 4.0588765144348145, val loss: 4.064149951934814, ETA in seconds: 8896103.731\n",
      "epoch: 205400, train loss: 4.052626514434815, val loss: 4.062392139434815, ETA in seconds: 8899117.352\n",
      "epoch: 205500, train loss: 4.051259326934814, val loss: 4.070009326934814, ETA in seconds: 8902283.386\n",
      "epoch: 205600, train loss: 4.0491108894348145, val loss: 4.071962451934814, ETA in seconds: 8905547.552\n",
      "epoch: 205700, train loss: 4.053603076934815, val loss: 4.066493701934815, ETA in seconds: 8908723.627\n",
      "epoch: 205800, train loss: 4.0452046394348145, val loss: 4.069228076934815, ETA in seconds: 8911952.998\n",
      "epoch: 205900, train loss: 4.058485889434815, val loss: 4.066493701934815, ETA in seconds: 8915240.621\n",
      "epoch: 206000, train loss: 4.049501514434814, val loss: 4.065321826934815, ETA in seconds: 8918915.107\n",
      "epoch: 206100, train loss: 4.061024951934814, val loss: 4.061415576934815, ETA in seconds: 8922056.639\n",
      "epoch: 206200, train loss: 4.048524951934814, val loss: 4.0686421394348145, ETA in seconds: 8925249.271\n",
      "epoch: 206300, train loss: 4.0403218269348145, val loss: 4.0666890144348145, ETA in seconds: 8928440.630\n",
      "epoch: 206400, train loss: 4.048915576934815, val loss: 4.070009326934814, ETA in seconds: 8931685.413\n",
      "epoch: 206500, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 8934932.717\n",
      "epoch: 206600, train loss: 4.0588765144348145, val loss: 4.064149951934814, ETA in seconds: 8938175.480\n",
      "epoch: 206700, train loss: 4.0422749519348145, val loss: 4.064931201934814, ETA in seconds: 8941304.405\n",
      "epoch: 206800, train loss: 4.0559468269348145, val loss: 4.064345264434815, ETA in seconds: 8944518.751\n",
      "epoch: 206900, train loss: 4.0530171394348145, val loss: 4.074110889434815, ETA in seconds: 8947728.808\n",
      "epoch: 207000, train loss: 4.046376514434814, val loss: 4.066103076934814, ETA in seconds: 8950719.864\n",
      "epoch: 207100, train loss: 4.053407764434814, val loss: 4.068446826934815, ETA in seconds: 8954042.441\n",
      "epoch: 207200, train loss: 4.051454639434814, val loss: 4.064149951934814, ETA in seconds: 8957351.103\n",
      "epoch: 207300, train loss: 4.058485889434815, val loss: 4.071181201934815, ETA in seconds: 8960589.537\n",
      "epoch: 207400, train loss: 4.050673389434815, val loss: 4.065126514434814, ETA in seconds: 8963860.893\n",
      "epoch: 207500, train loss: 4.048915576934815, val loss: 4.065907764434814, ETA in seconds: 8967100.226\n",
      "epoch: 207600, train loss: 4.049696826934815, val loss: 4.068837451934814, ETA in seconds: 8970325.998\n",
      "epoch: 207700, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 8973545.757\n",
      "epoch: 207800, train loss: 4.055165576934814, val loss: 4.0676655769348145, ETA in seconds: 8976813.375\n",
      "epoch: 207900, train loss: 4.061610889434815, val loss: 4.072939014434814, ETA in seconds: 8979996.512\n",
      "epoch: 208000, train loss: 4.053407764434814, val loss: 4.069423389434815, ETA in seconds: 8983205.323\n",
      "epoch: 208100, train loss: 4.051649951934815, val loss: 4.0657124519348145, ETA in seconds: 8986434.012\n",
      "epoch: 208200, train loss: 4.057118701934814, val loss: 4.072353076934815, ETA in seconds: 8989788.793\n",
      "epoch: 208300, train loss: 4.048720264434815, val loss: 4.075673389434814, ETA in seconds: 8993397.310\n",
      "epoch: 208400, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 8996731.686\n",
      "epoch: 208500, train loss: 4.0569233894348145, val loss: 4.069228076934815, ETA in seconds: 8999956.238\n",
      "epoch: 208600, train loss: 4.054189014434814, val loss: 4.066493701934815, ETA in seconds: 9003129.232\n",
      "epoch: 208700, train loss: 4.047743701934815, val loss: 4.066884326934814, ETA in seconds: 9006253.717\n",
      "epoch: 208800, train loss: 4.048720264434815, val loss: 4.064540576934815, ETA in seconds: 9009482.548\n",
      "epoch: 208900, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 9012692.620\n",
      "epoch: 209000, train loss: 4.0520405769348145, val loss: 4.069423389434815, ETA in seconds: 9016295.666\n",
      "epoch: 209100, train loss: 4.054774951934815, val loss: 4.0666890144348145, ETA in seconds: 9019594.727\n",
      "epoch: 209200, train loss: 4.056142139434814, val loss: 4.070985889434814, ETA in seconds: 9022805.591\n",
      "epoch: 209300, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 9026107.309\n",
      "epoch: 209400, train loss: 4.052235889434814, val loss: 4.062001514434814, ETA in seconds: 9029414.340\n",
      "epoch: 209500, train loss: 4.059462451934815, val loss: 4.069814014434814, ETA in seconds: 9032492.392\n",
      "epoch: 209600, train loss: 4.0491108894348145, val loss: 4.0657124519348145, ETA in seconds: 9035827.660\n",
      "epoch: 209700, train loss: 4.047939014434815, val loss: 4.0676655769348145, ETA in seconds: 9039055.378\n",
      "epoch: 209800, train loss: 4.045399951934814, val loss: 4.068056201934814, ETA in seconds: 9042351.335\n",
      "epoch: 209900, train loss: 4.0530171394348145, val loss: 4.067470264434815, ETA in seconds: 9045612.056\n",
      "epoch: 210000, train loss: 4.048524951934814, val loss: 4.0705952644348145, ETA in seconds: 9048676.321\n",
      "epoch: 210100, train loss: 4.0539937019348145, val loss: 4.065517139434815, ETA in seconds: 9051671.725\n",
      "epoch: 210200, train loss: 4.0559468269348145, val loss: 4.0657124519348145, ETA in seconds: 9054632.303\n",
      "epoch: 210300, train loss: 4.054384326934814, val loss: 4.0657124519348145, ETA in seconds: 9057594.863\n",
      "epoch: 210400, train loss: 4.0500874519348145, val loss: 4.059462451934815, ETA in seconds: 9060587.401\n",
      "epoch: 210500, train loss: 4.059071826934814, val loss: 4.061610889434815, ETA in seconds: 9063698.757\n",
      "epoch: 210600, train loss: 4.049696826934815, val loss: 4.073134326934815, ETA in seconds: 9066682.968\n",
      "epoch: 210700, train loss: 4.045790576934815, val loss: 4.0666890144348145, ETA in seconds: 9069671.264\n",
      "epoch: 210800, train loss: 4.0539937019348145, val loss: 4.071767139434814, ETA in seconds: 9072663.596\n",
      "epoch: 210900, train loss: 4.0491108894348145, val loss: 4.070204639434815, ETA in seconds: 9075715.482\n",
      "epoch: 211000, train loss: 4.060439014434815, val loss: 4.065907764434814, ETA in seconds: 9079063.112\n",
      "epoch: 211100, train loss: 4.049306201934814, val loss: 4.063173389434814, ETA in seconds: 9082015.143\n",
      "epoch: 211200, train loss: 4.053212451934814, val loss: 4.067274951934815, ETA in seconds: 9084936.413\n",
      "epoch: 211300, train loss: 4.049501514434814, val loss: 4.070399951934815, ETA in seconds: 9087835.848\n",
      "epoch: 211400, train loss: 4.046376514434814, val loss: 4.066298389434815, ETA in seconds: 9090797.120\n",
      "epoch: 211500, train loss: 4.049501514434814, val loss: 4.0666890144348145, ETA in seconds: 9093638.638\n",
      "epoch: 211600, train loss: 4.058681201934815, val loss: 4.069814014434814, ETA in seconds: 9096631.060\n",
      "epoch: 211700, train loss: 4.046571826934814, val loss: 4.0637593269348145, ETA in seconds: 9099922.982\n",
      "epoch: 211800, train loss: 4.050478076934814, val loss: 4.065321826934815, ETA in seconds: 9103102.829\n",
      "epoch: 211900, train loss: 4.049306201934814, val loss: 4.068446826934815, ETA in seconds: 9106231.539\n",
      "epoch: 212000, train loss: 4.048915576934815, val loss: 4.0657124519348145, ETA in seconds: 9109347.110\n",
      "epoch: 212100, train loss: 4.046376514434814, val loss: 4.0686421394348145, ETA in seconds: 9112482.777\n",
      "epoch: 212200, train loss: 4.055751514434815, val loss: 4.0686421394348145, ETA in seconds: 9115653.482\n",
      "epoch: 212300, train loss: 4.042079639434815, val loss: 4.0735249519348145, ETA in seconds: 9118738.439\n",
      "epoch: 212400, train loss: 4.050478076934814, val loss: 4.074892139434814, ETA in seconds: 9121903.216\n",
      "epoch: 212500, train loss: 4.058095264434814, val loss: 4.069228076934815, ETA in seconds: 9124824.397\n",
      "epoch: 212600, train loss: 4.054384326934814, val loss: 4.068056201934814, ETA in seconds: 9127784.013\n",
      "epoch: 212700, train loss: 4.049306201934814, val loss: 4.069032764434814, ETA in seconds: 9130891.290\n",
      "epoch: 212800, train loss: 4.050478076934814, val loss: 4.065321826934815, ETA in seconds: 9134100.864\n",
      "epoch: 212900, train loss: 4.055165576934814, val loss: 4.067860889434814, ETA in seconds: 9137639.507\n",
      "epoch: 213000, train loss: 4.059071826934814, val loss: 4.064345264434815, ETA in seconds: 9140817.055\n",
      "epoch: 213100, train loss: 4.055360889434814, val loss: 4.0657124519348145, ETA in seconds: 9143949.640\n",
      "epoch: 213200, train loss: 4.049501514434814, val loss: 4.063368701934815, ETA in seconds: 9147076.060\n",
      "epoch: 213300, train loss: 4.053407764434814, val loss: 4.0686421394348145, ETA in seconds: 9150167.181\n",
      "epoch: 213400, train loss: 4.051259326934814, val loss: 4.070204639434815, ETA in seconds: 9153504.386\n",
      "epoch: 213500, train loss: 4.047743701934815, val loss: 4.064931201934814, ETA in seconds: 9157162.890\n",
      "epoch: 213600, train loss: 4.051454639434814, val loss: 4.063954639434814, ETA in seconds: 9160630.298\n",
      "epoch: 213700, train loss: 4.050282764434814, val loss: 4.069228076934815, ETA in seconds: 9163632.190\n",
      "epoch: 213800, train loss: 4.0442280769348145, val loss: 4.071376514434815, ETA in seconds: 9166704.771\n",
      "epoch: 213900, train loss: 4.053603076934815, val loss: 4.0676655769348145, ETA in seconds: 9169876.275\n",
      "epoch: 214000, train loss: 4.048720264434815, val loss: 4.066103076934814, ETA in seconds: 9173066.391\n",
      "epoch: 214100, train loss: 4.045399951934814, val loss: 4.071767139434814, ETA in seconds: 9176273.960\n",
      "epoch: 214200, train loss: 4.053603076934815, val loss: 4.067079639434814, ETA in seconds: 9179315.364\n",
      "epoch: 214300, train loss: 4.044618701934814, val loss: 4.068837451934814, ETA in seconds: 9182529.236\n",
      "epoch: 214400, train loss: 4.056728076934815, val loss: 4.063368701934815, ETA in seconds: 9185755.760\n",
      "epoch: 214500, train loss: 4.048524951934814, val loss: 4.0725483894348145, ETA in seconds: 9188970.843\n",
      "epoch: 214600, train loss: 4.051649951934815, val loss: 4.066103076934814, ETA in seconds: 9192258.477\n",
      "epoch: 214700, train loss: 4.045399951934814, val loss: 4.068251514434815, ETA in seconds: 9195367.883\n",
      "epoch: 214800, train loss: 4.048329639434814, val loss: 4.064540576934815, ETA in seconds: 9198478.756\n",
      "epoch: 214900, train loss: 4.050673389434815, val loss: 4.064540576934815, ETA in seconds: 9201674.155\n",
      "epoch: 215000, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 9204933.141\n",
      "epoch: 215100, train loss: 4.050282764434814, val loss: 4.066884326934814, ETA in seconds: 9208174.768\n",
      "epoch: 215200, train loss: 4.0520405769348145, val loss: 4.065517139434815, ETA in seconds: 9211338.587\n",
      "epoch: 215300, train loss: 4.046962451934815, val loss: 4.068837451934814, ETA in seconds: 9214392.696\n",
      "epoch: 215400, train loss: 4.053212451934814, val loss: 4.066298389434815, ETA in seconds: 9217427.644\n",
      "epoch: 215500, train loss: 4.052626514434815, val loss: 4.073915576934814, ETA in seconds: 9220463.614\n",
      "epoch: 215600, train loss: 4.051649951934815, val loss: 4.066493701934815, ETA in seconds: 9223501.259\n",
      "epoch: 215700, train loss: 4.057314014434814, val loss: 4.0666890144348145, ETA in seconds: 9226515.719\n",
      "epoch: 215800, train loss: 4.058095264434814, val loss: 4.067860889434814, ETA in seconds: 9229512.758\n",
      "epoch: 215900, train loss: 4.050282764434814, val loss: 4.072939014434814, ETA in seconds: 9232597.591\n",
      "epoch: 216000, train loss: 4.055360889434814, val loss: 4.067274951934815, ETA in seconds: 9235659.994\n",
      "epoch: 216100, train loss: 4.045399951934814, val loss: 4.0666890144348145, ETA in seconds: 9238663.021\n",
      "epoch: 216200, train loss: 4.0520405769348145, val loss: 4.067079639434814, ETA in seconds: 9241720.472\n",
      "epoch: 216300, train loss: 4.044814014434815, val loss: 4.0705952644348145, ETA in seconds: 9244770.623\n",
      "epoch: 216400, train loss: 4.060439014434815, val loss: 4.062587451934815, ETA in seconds: 9247920.440\n",
      "epoch: 216500, train loss: 4.0530171394348145, val loss: 4.070985889434814, ETA in seconds: 9251033.581\n",
      "epoch: 216600, train loss: 4.0559468269348145, val loss: 4.066493701934815, ETA in seconds: 9254158.465\n",
      "epoch: 216700, train loss: 4.051454639434814, val loss: 4.069423389434815, ETA in seconds: 9257119.901\n",
      "epoch: 216800, train loss: 4.0491108894348145, val loss: 4.069814014434814, ETA in seconds: 9260133.691\n",
      "epoch: 216900, train loss: 4.054384326934814, val loss: 4.062392139434815, ETA in seconds: 9263163.318\n",
      "epoch: 217000, train loss: 4.048720264434815, val loss: 4.065126514434814, ETA in seconds: 9266191.521\n",
      "epoch: 217100, train loss: 4.057314014434814, val loss: 4.069228076934815, ETA in seconds: 9269242.018\n",
      "epoch: 217200, train loss: 4.055751514434815, val loss: 4.064149951934814, ETA in seconds: 9272379.992\n",
      "epoch: 217300, train loss: 4.051454639434814, val loss: 4.072939014434814, ETA in seconds: 9275459.184\n",
      "epoch: 217400, train loss: 4.047353076934814, val loss: 4.069814014434814, ETA in seconds: 9278456.732\n",
      "epoch: 217500, train loss: 4.0491108894348145, val loss: 4.069423389434815, ETA in seconds: 9281424.050\n",
      "epoch: 217600, train loss: 4.059267139434814, val loss: 4.068251514434815, ETA in seconds: 9284322.812\n",
      "epoch: 217700, train loss: 4.058485889434815, val loss: 4.0657124519348145, ETA in seconds: 9287634.391\n",
      "epoch: 217800, train loss: 4.045595264434814, val loss: 4.066103076934814, ETA in seconds: 9290766.937\n",
      "epoch: 217900, train loss: 4.055556201934815, val loss: 4.065321826934815, ETA in seconds: 9294180.283\n",
      "epoch: 218000, train loss: 4.048524951934814, val loss: 4.0647358894348145, ETA in seconds: 9297591.831\n",
      "epoch: 218100, train loss: 4.0559468269348145, val loss: 4.061024951934814, ETA in seconds: 9300901.833\n",
      "epoch: 218200, train loss: 4.050282764434814, val loss: 4.069814014434814, ETA in seconds: 9303920.441\n",
      "epoch: 218300, train loss: 4.0539937019348145, val loss: 4.0676655769348145, ETA in seconds: 9306918.278\n",
      "epoch: 218400, train loss: 4.046962451934815, val loss: 4.066298389434815, ETA in seconds: 9309987.074\n",
      "epoch: 218500, train loss: 4.0520405769348145, val loss: 4.067860889434814, ETA in seconds: 9312905.049\n",
      "epoch: 218600, train loss: 4.055165576934814, val loss: 4.067079639434814, ETA in seconds: 9315848.293\n",
      "epoch: 218700, train loss: 4.053212451934814, val loss: 4.065517139434815, ETA in seconds: 9318907.318\n",
      "epoch: 218800, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 9321967.053\n",
      "epoch: 218900, train loss: 4.053407764434814, val loss: 4.068837451934814, ETA in seconds: 9324812.275\n",
      "epoch: 219000, train loss: 4.052626514434815, val loss: 4.065517139434815, ETA in seconds: 9327775.217\n",
      "epoch: 219100, train loss: 4.053407764434814, val loss: 4.062196826934814, ETA in seconds: 9330829.092\n",
      "epoch: 219200, train loss: 4.050673389434815, val loss: 4.071767139434814, ETA in seconds: 9333906.455\n",
      "epoch: 219300, train loss: 4.050282764434814, val loss: 4.068251514434815, ETA in seconds: 9336777.364\n",
      "epoch: 219400, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 9339808.066\n",
      "epoch: 219500, train loss: 4.055751514434815, val loss: 4.068056201934814, ETA in seconds: 9343058.674\n",
      "epoch: 219600, train loss: 4.050868701934815, val loss: 4.067470264434815, ETA in seconds: 9346185.285\n",
      "epoch: 219700, train loss: 4.058681201934815, val loss: 4.069228076934815, ETA in seconds: 9349332.153\n",
      "epoch: 219800, train loss: 4.052821826934815, val loss: 4.070009326934814, ETA in seconds: 9352310.818\n",
      "epoch: 219900, train loss: 4.0491108894348145, val loss: 4.070399951934815, ETA in seconds: 9355631.224\n",
      "epoch: 220000, train loss: 4.0559468269348145, val loss: 4.068837451934814, ETA in seconds: 9359138.968\n",
      "epoch: 220100, train loss: 4.045790576934815, val loss: 4.067860889434814, ETA in seconds: 9362301.557\n",
      "epoch: 220200, train loss: 4.057509326934815, val loss: 4.072743701934814, ETA in seconds: 9365509.836\n",
      "epoch: 220300, train loss: 4.0530171394348145, val loss: 4.0705952644348145, ETA in seconds: 9368517.320\n",
      "epoch: 220400, train loss: 4.046376514434814, val loss: 4.067274951934815, ETA in seconds: 9371513.206\n",
      "epoch: 220500, train loss: 4.053407764434814, val loss: 4.067860889434814, ETA in seconds: 9374577.147\n",
      "epoch: 220600, train loss: 4.0530171394348145, val loss: 4.067860889434814, ETA in seconds: 9377594.847\n",
      "epoch: 220700, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 9380687.888\n",
      "epoch: 220800, train loss: 4.053798389434815, val loss: 4.0666890144348145, ETA in seconds: 9383634.896\n",
      "epoch: 220900, train loss: 4.050282764434814, val loss: 4.062196826934814, ETA in seconds: 9386666.888\n",
      "epoch: 221000, train loss: 4.046962451934815, val loss: 4.070204639434815, ETA in seconds: 9389731.510\n",
      "epoch: 221100, train loss: 4.0510640144348145, val loss: 4.071376514434815, ETA in seconds: 9393003.985\n",
      "epoch: 221200, train loss: 4.051649951934815, val loss: 4.071962451934814, ETA in seconds: 9396192.657\n",
      "epoch: 221300, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 9399223.415\n",
      "epoch: 221400, train loss: 4.047353076934814, val loss: 4.0686421394348145, ETA in seconds: 9402089.175\n",
      "epoch: 221500, train loss: 4.058290576934814, val loss: 4.065517139434815, ETA in seconds: 9405002.343\n",
      "epoch: 221600, train loss: 4.0510640144348145, val loss: 4.067470264434815, ETA in seconds: 9407716.872\n",
      "epoch: 221700, train loss: 4.056532764434815, val loss: 4.069814014434814, ETA in seconds: 9410739.545\n",
      "epoch: 221800, train loss: 4.048329639434814, val loss: 4.068837451934814, ETA in seconds: 9413637.094\n",
      "epoch: 221900, train loss: 4.054579639434815, val loss: 4.0735249519348145, ETA in seconds: 9416590.644\n",
      "epoch: 222000, train loss: 4.051649951934815, val loss: 4.062978076934814, ETA in seconds: 9419601.456\n",
      "epoch: 222100, train loss: 4.054579639434815, val loss: 4.067079639434814, ETA in seconds: 9422517.872\n",
      "epoch: 222200, train loss: 4.0520405769348145, val loss: 4.063564014434815, ETA in seconds: 9425493.579\n",
      "epoch: 222300, train loss: 4.0530171394348145, val loss: 4.067079639434814, ETA in seconds: 9428451.629\n",
      "epoch: 222400, train loss: 4.056728076934815, val loss: 4.0686421394348145, ETA in seconds: 9431484.264\n",
      "epoch: 222500, train loss: 4.0530171394348145, val loss: 4.064345264434815, ETA in seconds: 9434429.417\n",
      "epoch: 222600, train loss: 4.049306201934814, val loss: 4.068056201934814, ETA in seconds: 9437398.615\n",
      "epoch: 222700, train loss: 4.051454639434814, val loss: 4.071962451934814, ETA in seconds: 9440414.427\n",
      "epoch: 222800, train loss: 4.0549702644348145, val loss: 4.073720264434814, ETA in seconds: 9443578.037\n",
      "epoch: 222900, train loss: 4.048915576934815, val loss: 4.068837451934814, ETA in seconds: 9446977.566\n",
      "epoch: 223000, train loss: 4.052626514434815, val loss: 4.071962451934814, ETA in seconds: 9450182.525\n",
      "epoch: 223100, train loss: 4.0510640144348145, val loss: 4.062978076934814, ETA in seconds: 9453062.447\n",
      "epoch: 223200, train loss: 4.0491108894348145, val loss: 4.0657124519348145, ETA in seconds: 9456125.976\n",
      "epoch: 223300, train loss: 4.052431201934814, val loss: 4.062978076934814, ETA in seconds: 9459124.230\n",
      "epoch: 223400, train loss: 4.052235889434814, val loss: 4.066493701934815, ETA in seconds: 9462043.348\n",
      "epoch: 223500, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 9464950.048\n",
      "epoch: 223600, train loss: 4.051454639434814, val loss: 4.071181201934815, ETA in seconds: 9467965.214\n",
      "epoch: 223700, train loss: 4.0549702644348145, val loss: 4.065517139434815, ETA in seconds: 9470856.259\n",
      "epoch: 223800, train loss: 4.051649951934815, val loss: 4.067860889434814, ETA in seconds: 9473911.291\n",
      "epoch: 223900, train loss: 4.045009326934815, val loss: 4.066493701934815, ETA in seconds: 9476812.182\n",
      "epoch: 224000, train loss: 4.049696826934815, val loss: 4.065517139434815, ETA in seconds: 9479851.175\n",
      "epoch: 224100, train loss: 4.050478076934814, val loss: 4.065321826934815, ETA in seconds: 9482873.950\n",
      "epoch: 224200, train loss: 4.050868701934815, val loss: 4.067274951934815, ETA in seconds: 9485883.052\n",
      "epoch: 224300, train loss: 4.0598530769348145, val loss: 4.063954639434814, ETA in seconds: 9488824.439\n",
      "epoch: 224400, train loss: 4.055556201934815, val loss: 4.069228076934815, ETA in seconds: 9491916.900\n",
      "epoch: 224500, train loss: 4.048915576934815, val loss: 4.067470264434815, ETA in seconds: 9494954.515\n",
      "epoch: 224600, train loss: 4.046962451934815, val loss: 4.069032764434814, ETA in seconds: 9498006.605\n",
      "epoch: 224700, train loss: 4.058681201934815, val loss: 4.0696187019348145, ETA in seconds: 9501217.972\n",
      "epoch: 224800, train loss: 4.053798389434815, val loss: 4.062196826934814, ETA in seconds: 9504438.771\n",
      "epoch: 224900, train loss: 4.0549702644348145, val loss: 4.0686421394348145, ETA in seconds: 9507616.937\n",
      "epoch: 225000, train loss: 4.041884326934815, val loss: 4.061610889434815, ETA in seconds: 9510670.884\n",
      "epoch: 225100, train loss: 4.056532764434815, val loss: 4.070399951934815, ETA in seconds: 9513637.449\n",
      "epoch: 225200, train loss: 4.049501514434814, val loss: 4.067470264434815, ETA in seconds: 9516635.555\n",
      "epoch: 225300, train loss: 4.048720264434815, val loss: 4.066298389434815, ETA in seconds: 9519569.611\n",
      "epoch: 225400, train loss: 4.052626514434815, val loss: 4.0686421394348145, ETA in seconds: 9522457.537\n",
      "epoch: 225500, train loss: 4.051845264434815, val loss: 4.070399951934815, ETA in seconds: 9525447.932\n",
      "epoch: 225600, train loss: 4.0510640144348145, val loss: 4.066884326934814, ETA in seconds: 9528441.896\n",
      "epoch: 225700, train loss: 4.051259326934814, val loss: 4.0637593269348145, ETA in seconds: 9531397.982\n",
      "epoch: 225800, train loss: 4.051454639434814, val loss: 4.071376514434815, ETA in seconds: 9534339.790\n",
      "epoch: 225900, train loss: 4.055360889434814, val loss: 4.062001514434814, ETA in seconds: 9537292.546\n",
      "epoch: 226000, train loss: 4.0549702644348145, val loss: 4.064931201934814, ETA in seconds: 9540246.740\n",
      "epoch: 226100, train loss: 4.050673389434815, val loss: 4.0618062019348145, ETA in seconds: 9543312.679\n",
      "epoch: 226200, train loss: 4.052235889434814, val loss: 4.070985889434814, ETA in seconds: 9546806.268\n",
      "epoch: 226300, train loss: 4.0510640144348145, val loss: 4.071181201934815, ETA in seconds: 9549787.710\n",
      "epoch: 226400, train loss: 4.046767139434815, val loss: 4.066298389434815, ETA in seconds: 9552813.722\n",
      "epoch: 226500, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 9555822.010\n",
      "epoch: 226600, train loss: 4.052821826934815, val loss: 4.0676655769348145, ETA in seconds: 9558891.492\n",
      "epoch: 226700, train loss: 4.046571826934814, val loss: 4.0647358894348145, ETA in seconds: 9561704.584\n",
      "epoch: 226800, train loss: 4.045595264434814, val loss: 4.067274951934815, ETA in seconds: 9564578.731\n",
      "epoch: 226900, train loss: 4.046376514434814, val loss: 4.066884326934814, ETA in seconds: 9567458.436\n",
      "epoch: 227000, train loss: 4.053603076934815, val loss: 4.060439014434815, ETA in seconds: 9570282.415\n",
      "epoch: 227100, train loss: 4.0510640144348145, val loss: 4.070399951934815, ETA in seconds: 9573184.644\n",
      "epoch: 227200, train loss: 4.052431201934814, val loss: 4.0637593269348145, ETA in seconds: 9576193.118\n",
      "epoch: 227300, train loss: 4.052431201934814, val loss: 4.0666890144348145, ETA in seconds: 9579181.161\n",
      "epoch: 227400, train loss: 4.048720264434815, val loss: 4.0686421394348145, ETA in seconds: 9582085.310\n",
      "epoch: 227500, train loss: 4.055751514434815, val loss: 4.062001514434814, ETA in seconds: 9585181.777\n",
      "epoch: 227600, train loss: 4.051259326934814, val loss: 4.0647358894348145, ETA in seconds: 9588176.772\n",
      "epoch: 227700, train loss: 4.0539937019348145, val loss: 4.069228076934815, ETA in seconds: 9591126.607\n",
      "epoch: 227800, train loss: 4.058290576934814, val loss: 4.0705952644348145, ETA in seconds: 9594164.224\n",
      "epoch: 227900, train loss: 4.045595264434814, val loss: 4.070204639434815, ETA in seconds: 9596946.725\n",
      "epoch: 228000, train loss: 4.050673389434815, val loss: 4.066103076934814, ETA in seconds: 9599949.364\n",
      "epoch: 228100, train loss: 4.060439014434815, val loss: 4.070399951934815, ETA in seconds: 9602815.126\n",
      "epoch: 228200, train loss: 4.051845264434815, val loss: 4.070204639434815, ETA in seconds: 9605486.369\n",
      "epoch: 228300, train loss: 4.044423389434814, val loss: 4.0657124519348145, ETA in seconds: 9608387.177\n",
      "epoch: 228400, train loss: 4.055165576934814, val loss: 4.063954639434814, ETA in seconds: 9611434.382\n",
      "epoch: 228500, train loss: 4.053603076934815, val loss: 4.068251514434815, ETA in seconds: 9614263.928\n",
      "epoch: 228600, train loss: 4.056728076934815, val loss: 4.067470264434815, ETA in seconds: 9616995.739\n",
      "epoch: 228700, train loss: 4.054774951934815, val loss: 4.065907764434814, ETA in seconds: 9619664.785\n",
      "epoch: 228800, train loss: 4.050282764434814, val loss: 4.062978076934814, ETA in seconds: 9622508.863\n",
      "epoch: 228900, train loss: 4.0510640144348145, val loss: 4.067470264434815, ETA in seconds: 9625398.421\n",
      "epoch: 229000, train loss: 4.059071826934814, val loss: 4.0666890144348145, ETA in seconds: 9628250.900\n",
      "epoch: 229100, train loss: 4.056532764434815, val loss: 4.061610889434815, ETA in seconds: 9631065.219\n",
      "epoch: 229200, train loss: 4.0510640144348145, val loss: 4.067079639434814, ETA in seconds: 9634042.463\n",
      "epoch: 229300, train loss: 4.0578999519348145, val loss: 4.064149951934814, ETA in seconds: 9636931.401\n",
      "epoch: 229400, train loss: 4.047548389434814, val loss: 4.0696187019348145, ETA in seconds: 9639784.420\n",
      "epoch: 229500, train loss: 4.053603076934815, val loss: 4.063954639434814, ETA in seconds: 9642650.408\n",
      "epoch: 229600, train loss: 4.056142139434814, val loss: 4.068251514434815, ETA in seconds: 9645546.375\n",
      "epoch: 229700, train loss: 4.054579639434815, val loss: 4.063368701934815, ETA in seconds: 9648355.140\n",
      "epoch: 229800, train loss: 4.051454639434814, val loss: 4.0696187019348145, ETA in seconds: 9651229.411\n",
      "epoch: 229900, train loss: 4.048720264434815, val loss: 4.062978076934814, ETA in seconds: 9654150.254\n",
      "epoch: 230000, train loss: 4.058290576934814, val loss: 4.0696187019348145, ETA in seconds: 9657114.302\n",
      "epoch: 230100, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 9659976.254\n",
      "epoch: 230200, train loss: 4.051454639434814, val loss: 4.073329639434815, ETA in seconds: 9662788.219\n",
      "epoch: 230300, train loss: 4.047548389434814, val loss: 4.066884326934814, ETA in seconds: 9665668.559\n",
      "epoch: 230400, train loss: 4.0510640144348145, val loss: 4.072157764434815, ETA in seconds: 9668427.359\n",
      "epoch: 230500, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 9671291.993\n",
      "epoch: 230600, train loss: 4.057118701934814, val loss: 4.066493701934815, ETA in seconds: 9674147.289\n",
      "epoch: 230700, train loss: 4.059267139434814, val loss: 4.063368701934815, ETA in seconds: 9676667.765\n",
      "epoch: 230800, train loss: 4.0569233894348145, val loss: 4.069228076934815, ETA in seconds: 9679399.428\n",
      "epoch: 230900, train loss: 4.057314014434814, val loss: 4.069423389434815, ETA in seconds: 9681628.725\n",
      "epoch: 231000, train loss: 4.056728076934815, val loss: 4.071767139434814, ETA in seconds: 9684469.474\n",
      "epoch: 231100, train loss: 4.046767139434815, val loss: 4.073329639434815, ETA in seconds: 9687157.278\n",
      "epoch: 231200, train loss: 4.0491108894348145, val loss: 4.068056201934814, ETA in seconds: 9689920.151\n",
      "epoch: 231300, train loss: 4.047548389434814, val loss: 4.067860889434814, ETA in seconds: 9692792.639\n",
      "epoch: 231400, train loss: 4.049501514434814, val loss: 4.0745015144348145, ETA in seconds: 9695588.632\n",
      "epoch: 231500, train loss: 4.052235889434814, val loss: 4.067860889434814, ETA in seconds: 9698395.100\n",
      "epoch: 231600, train loss: 4.053212451934814, val loss: 4.064345264434815, ETA in seconds: 9701210.187\n",
      "epoch: 231700, train loss: 4.047743701934815, val loss: 4.068251514434815, ETA in seconds: 9703981.634\n",
      "epoch: 231800, train loss: 4.0500874519348145, val loss: 4.070399951934815, ETA in seconds: 9706775.273\n",
      "epoch: 231900, train loss: 4.044814014434815, val loss: 4.0686421394348145, ETA in seconds: 9709628.876\n",
      "epoch: 232000, train loss: 4.053407764434814, val loss: 4.067274951934815, ETA in seconds: 9712413.668\n",
      "epoch: 232100, train loss: 4.050868701934815, val loss: 4.063368701934815, ETA in seconds: 9715301.159\n",
      "epoch: 232200, train loss: 4.049696826934815, val loss: 4.068446826934815, ETA in seconds: 9718158.841\n",
      "epoch: 232300, train loss: 4.049501514434814, val loss: 4.065321826934815, ETA in seconds: 9720899.029\n",
      "epoch: 232400, train loss: 4.0588765144348145, val loss: 4.069032764434814, ETA in seconds: 9723764.674\n",
      "epoch: 232500, train loss: 4.045790576934815, val loss: 4.068056201934814, ETA in seconds: 9726659.596\n",
      "epoch: 232600, train loss: 4.056728076934815, val loss: 4.063564014434815, ETA in seconds: 9729606.510\n",
      "epoch: 232700, train loss: 4.0549702644348145, val loss: 4.067079639434814, ETA in seconds: 9732639.993\n",
      "epoch: 232800, train loss: 4.045790576934815, val loss: 4.0696187019348145, ETA in seconds: 9735565.250\n",
      "epoch: 232900, train loss: 4.0481343269348145, val loss: 4.065907764434814, ETA in seconds: 9738412.064\n",
      "epoch: 233000, train loss: 4.0500874519348145, val loss: 4.060439014434815, ETA in seconds: 9741359.442\n",
      "epoch: 233100, train loss: 4.055165576934814, val loss: 4.0637593269348145, ETA in seconds: 9744380.347\n",
      "epoch: 233200, train loss: 4.0442280769348145, val loss: 4.060439014434815, ETA in seconds: 9747403.575\n",
      "epoch: 233300, train loss: 4.047353076934814, val loss: 4.066103076934814, ETA in seconds: 9750355.246\n",
      "epoch: 233400, train loss: 4.052431201934814, val loss: 4.070985889434814, ETA in seconds: 9753186.943\n",
      "epoch: 233500, train loss: 4.058095264434814, val loss: 4.072157764434815, ETA in seconds: 9756114.432\n",
      "epoch: 233600, train loss: 4.055360889434814, val loss: 4.069814014434814, ETA in seconds: 9758951.622\n",
      "epoch: 233700, train loss: 4.0549702644348145, val loss: 4.0696187019348145, ETA in seconds: 9761763.302\n",
      "epoch: 233800, train loss: 4.046962451934815, val loss: 4.062587451934815, ETA in seconds: 9764662.614\n",
      "epoch: 233900, train loss: 4.0520405769348145, val loss: 4.069423389434815, ETA in seconds: 9767515.648\n",
      "epoch: 234000, train loss: 4.049892139434815, val loss: 4.069228076934815, ETA in seconds: 9770372.536\n",
      "epoch: 234100, train loss: 4.041689014434814, val loss: 4.069228076934815, ETA in seconds: 9773301.597\n",
      "epoch: 234200, train loss: 4.0559468269348145, val loss: 4.0666890144348145, ETA in seconds: 9776153.121\n",
      "epoch: 234300, train loss: 4.0539937019348145, val loss: 4.0647358894348145, ETA in seconds: 9778999.632\n",
      "epoch: 234400, train loss: 4.0461812019348145, val loss: 4.068251514434815, ETA in seconds: 9781998.842\n",
      "epoch: 234500, train loss: 4.054579639434815, val loss: 4.073329639434815, ETA in seconds: 9785056.805\n",
      "epoch: 234600, train loss: 4.060634326934815, val loss: 4.069228076934815, ETA in seconds: 9787959.299\n",
      "epoch: 234700, train loss: 4.053798389434815, val loss: 4.070985889434814, ETA in seconds: 9790984.357\n",
      "epoch: 234800, train loss: 4.052431201934814, val loss: 4.0686421394348145, ETA in seconds: 9793928.820\n",
      "epoch: 234900, train loss: 4.049501514434814, val loss: 4.071181201934815, ETA in seconds: 9796942.889\n",
      "epoch: 235000, train loss: 4.048720264434815, val loss: 4.065126514434814, ETA in seconds: 9799875.101\n",
      "epoch: 235100, train loss: 4.0549702644348145, val loss: 4.067860889434814, ETA in seconds: 9802916.885\n",
      "epoch: 235200, train loss: 4.059071826934814, val loss: 4.070009326934814, ETA in seconds: 9805926.963\n",
      "epoch: 235300, train loss: 4.0569233894348145, val loss: 4.062587451934815, ETA in seconds: 9808823.482\n",
      "epoch: 235400, train loss: 4.052626514434815, val loss: 4.070009326934814, ETA in seconds: 9811580.481\n",
      "epoch: 235500, train loss: 4.0549702644348145, val loss: 4.062587451934815, ETA in seconds: 9814527.267\n",
      "epoch: 235600, train loss: 4.043837451934815, val loss: 4.070009326934814, ETA in seconds: 9817827.014\n",
      "epoch: 235700, train loss: 4.047743701934815, val loss: 4.068056201934814, ETA in seconds: 9820890.244\n",
      "epoch: 235800, train loss: 4.058290576934814, val loss: 4.070009326934814, ETA in seconds: 9823762.386\n",
      "epoch: 235900, train loss: 4.048524951934814, val loss: 4.067079639434814, ETA in seconds: 9826629.618\n",
      "epoch: 236000, train loss: 4.0559468269348145, val loss: 4.062978076934814, ETA in seconds: 9829810.971\n",
      "epoch: 236100, train loss: 4.0539937019348145, val loss: 4.067274951934815, ETA in seconds: 9832891.924\n",
      "epoch: 236200, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 9835794.963\n",
      "epoch: 236300, train loss: 4.052821826934815, val loss: 4.066884326934814, ETA in seconds: 9838668.621\n",
      "epoch: 236400, train loss: 4.0530171394348145, val loss: 4.064149951934814, ETA in seconds: 9841505.938\n",
      "epoch: 236500, train loss: 4.0461812019348145, val loss: 4.067274951934815, ETA in seconds: 9844347.491\n",
      "epoch: 236600, train loss: 4.053603076934815, val loss: 4.068837451934814, ETA in seconds: 9847088.487\n",
      "epoch: 236700, train loss: 4.0549702644348145, val loss: 4.070985889434814, ETA in seconds: 9849827.122\n",
      "epoch: 236800, train loss: 4.050673389434815, val loss: 4.061220264434814, ETA in seconds: 9852585.103\n",
      "epoch: 236900, train loss: 4.054189014434814, val loss: 4.068837451934814, ETA in seconds: 9855470.833\n",
      "epoch: 237000, train loss: 4.048720264434815, val loss: 4.065321826934815, ETA in seconds: 9858310.463\n",
      "epoch: 237100, train loss: 4.052626514434815, val loss: 4.069032764434814, ETA in seconds: 9861245.861\n",
      "epoch: 237200, train loss: 4.055556201934815, val loss: 4.066298389434815, ETA in seconds: 9864233.628\n",
      "epoch: 237300, train loss: 4.054189014434814, val loss: 4.068056201934814, ETA in seconds: 9867060.978\n",
      "epoch: 237400, train loss: 4.049501514434814, val loss: 4.063564014434815, ETA in seconds: 9870057.387\n",
      "epoch: 237500, train loss: 4.0520405769348145, val loss: 4.0657124519348145, ETA in seconds: 9872878.176\n",
      "epoch: 237600, train loss: 4.048329639434814, val loss: 4.066884326934814, ETA in seconds: 9875061.747\n",
      "epoch: 237700, train loss: 4.039149951934815, val loss: 4.0705952644348145, ETA in seconds: 9877253.853\n",
      "epoch: 237800, train loss: 4.051649951934815, val loss: 4.069228076934815, ETA in seconds: 9879488.444\n",
      "epoch: 237900, train loss: 4.048329639434814, val loss: 4.068446826934815, ETA in seconds: 9882361.876\n",
      "epoch: 238000, train loss: 4.052235889434814, val loss: 4.0686421394348145, ETA in seconds: 9885262.020\n",
      "epoch: 238100, train loss: 4.053798389434815, val loss: 4.067079639434814, ETA in seconds: 9888251.907\n",
      "epoch: 238200, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 9891300.547\n",
      "epoch: 238300, train loss: 4.055360889434814, val loss: 4.063368701934815, ETA in seconds: 9894077.007\n",
      "epoch: 238400, train loss: 4.057118701934814, val loss: 4.069423389434815, ETA in seconds: 9896989.485\n",
      "epoch: 238500, train loss: 4.055751514434815, val loss: 4.066884326934814, ETA in seconds: 9899571.891\n",
      "epoch: 238600, train loss: 4.050478076934814, val loss: 4.0705952644348145, ETA in seconds: 9902668.426\n",
      "epoch: 238700, train loss: 4.053603076934815, val loss: 4.065126514434814, ETA in seconds: 9905849.291\n",
      "epoch: 238800, train loss: 4.054384326934814, val loss: 4.071962451934814, ETA in seconds: 9909054.262\n",
      "epoch: 238900, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 9911978.919\n",
      "epoch: 239000, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 9914792.022\n",
      "epoch: 239100, train loss: 4.048524951934814, val loss: 4.0676655769348145, ETA in seconds: 9917614.677\n",
      "epoch: 239200, train loss: 4.047743701934815, val loss: 4.070985889434814, ETA in seconds: 9920475.910\n",
      "epoch: 239300, train loss: 4.047353076934814, val loss: 4.064345264434815, ETA in seconds: 9923317.638\n",
      "epoch: 239400, train loss: 4.051454639434814, val loss: 4.0657124519348145, ETA in seconds: 9926142.665\n",
      "epoch: 239500, train loss: 4.051259326934814, val loss: 4.0696187019348145, ETA in seconds: 9928637.379\n",
      "epoch: 239600, train loss: 4.051845264434815, val loss: 4.070790576934814, ETA in seconds: 9931307.615\n",
      "epoch: 239700, train loss: 4.047743701934815, val loss: 4.066298389434815, ETA in seconds: 9934040.888\n",
      "epoch: 239800, train loss: 4.052235889434814, val loss: 4.068056201934814, ETA in seconds: 9936854.264\n",
      "epoch: 239900, train loss: 4.058290576934814, val loss: 4.070204639434815, ETA in seconds: 9939765.614\n",
      "epoch: 240000, train loss: 4.047548389434814, val loss: 4.066884326934814, ETA in seconds: 9942678.751\n",
      "epoch: 240100, train loss: 4.051649951934815, val loss: 4.071767139434814, ETA in seconds: 9945599.254\n",
      "epoch: 240200, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 9948431.283\n",
      "epoch: 240300, train loss: 4.0559468269348145, val loss: 4.0627827644348145, ETA in seconds: 9951283.240\n",
      "epoch: 240400, train loss: 4.052431201934814, val loss: 4.0666890144348145, ETA in seconds: 9954389.335\n",
      "epoch: 240500, train loss: 4.044814014434815, val loss: 4.072157764434815, ETA in seconds: 9957242.867\n",
      "epoch: 240600, train loss: 4.0491108894348145, val loss: 4.062196826934814, ETA in seconds: 9960043.424\n",
      "epoch: 240700, train loss: 4.0549702644348145, val loss: 4.066493701934815, ETA in seconds: 9962964.607\n",
      "epoch: 240800, train loss: 4.055165576934814, val loss: 4.065321826934815, ETA in seconds: 9965763.857\n",
      "epoch: 240900, train loss: 4.055556201934815, val loss: 4.0696187019348145, ETA in seconds: 9968581.239\n",
      "epoch: 241000, train loss: 4.051454639434814, val loss: 4.061024951934814, ETA in seconds: 9971342.260\n",
      "epoch: 241100, train loss: 4.046767139434815, val loss: 4.064149951934814, ETA in seconds: 9974014.153\n",
      "epoch: 241200, train loss: 4.047353076934814, val loss: 4.072157764434815, ETA in seconds: 9976821.020\n",
      "epoch: 241300, train loss: 4.0530171394348145, val loss: 4.065907764434814, ETA in seconds: 9979567.372\n",
      "epoch: 241400, train loss: 4.049892139434815, val loss: 4.0637593269348145, ETA in seconds: 9982400.465\n",
      "epoch: 241500, train loss: 4.0412983894348145, val loss: 4.0657124519348145, ETA in seconds: 9985187.174\n",
      "epoch: 241600, train loss: 4.047548389434814, val loss: 4.069032764434814, ETA in seconds: 9988040.841\n",
      "epoch: 241700, train loss: 4.049306201934814, val loss: 4.067470264434815, ETA in seconds: 9990897.219\n",
      "epoch: 241800, train loss: 4.049501514434814, val loss: 4.066493701934815, ETA in seconds: 9993673.168\n",
      "epoch: 241900, train loss: 4.060243701934814, val loss: 4.063564014434815, ETA in seconds: 9996467.130\n",
      "epoch: 242000, train loss: 4.045399951934814, val loss: 4.066103076934814, ETA in seconds: 9999266.561\n",
      "epoch: 242100, train loss: 4.054384326934814, val loss: 4.065517139434815, ETA in seconds: 10002017.111\n",
      "epoch: 242200, train loss: 4.053212451934814, val loss: 4.065126514434814, ETA in seconds: 10004711.453\n",
      "epoch: 242300, train loss: 4.054579639434815, val loss: 4.067470264434815, ETA in seconds: 10007480.730\n",
      "epoch: 242400, train loss: 4.050282764434814, val loss: 4.058681201934815, ETA in seconds: 10010413.300\n",
      "epoch: 242500, train loss: 4.052821826934815, val loss: 4.063173389434814, ETA in seconds: 10013183.311\n",
      "epoch: 242600, train loss: 4.053603076934815, val loss: 4.064540576934815, ETA in seconds: 10016022.171\n",
      "epoch: 242700, train loss: 4.052235889434814, val loss: 4.065126514434814, ETA in seconds: 10018906.771\n",
      "epoch: 242800, train loss: 4.051454639434814, val loss: 4.0608296394348145, ETA in seconds: 10021680.026\n",
      "epoch: 242900, train loss: 4.049501514434814, val loss: 4.068251514434815, ETA in seconds: 10024505.756\n",
      "epoch: 243000, train loss: 4.047939014434815, val loss: 4.065517139434815, ETA in seconds: 10027361.079\n",
      "epoch: 243100, train loss: 4.046767139434815, val loss: 4.063954639434814, ETA in seconds: 10030289.084\n",
      "epoch: 243200, train loss: 4.0539937019348145, val loss: 4.068837451934814, ETA in seconds: 10033059.718\n",
      "epoch: 243300, train loss: 4.059657764434815, val loss: 4.0696187019348145, ETA in seconds: 10035689.199\n",
      "epoch: 243400, train loss: 4.0461812019348145, val loss: 4.067470264434815, ETA in seconds: 10038484.093\n",
      "epoch: 243500, train loss: 4.055360889434814, val loss: 4.0657124519348145, ETA in seconds: 10041286.826\n",
      "epoch: 243600, train loss: 4.054384326934814, val loss: 4.062587451934815, ETA in seconds: 10043591.802\n",
      "epoch: 243700, train loss: 4.047548389434814, val loss: 4.072939014434814, ETA in seconds: 10046191.574\n",
      "epoch: 243800, train loss: 4.050478076934814, val loss: 4.068251514434815, ETA in seconds: 10049087.508\n",
      "epoch: 243900, train loss: 4.0500874519348145, val loss: 4.070009326934814, ETA in seconds: 10051803.491\n",
      "epoch: 244000, train loss: 4.052821826934815, val loss: 4.070790576934814, ETA in seconds: 10054672.034\n",
      "epoch: 244100, train loss: 4.048915576934815, val loss: 4.067470264434815, ETA in seconds: 10057503.406\n",
      "epoch: 244200, train loss: 4.054579639434815, val loss: 4.069423389434815, ETA in seconds: 10060501.829\n",
      "epoch: 244300, train loss: 4.059462451934815, val loss: 4.069814014434814, ETA in seconds: 10063832.059\n",
      "epoch: 244400, train loss: 4.0491108894348145, val loss: 4.066884326934814, ETA in seconds: 10067207.797\n",
      "epoch: 244500, train loss: 4.049306201934814, val loss: 4.072939014434814, ETA in seconds: 10070260.436\n",
      "epoch: 244600, train loss: 4.047939014434815, val loss: 4.064345264434815, ETA in seconds: 10073337.515\n",
      "epoch: 244700, train loss: 4.053407764434814, val loss: 4.068056201934814, ETA in seconds: 10076387.233\n",
      "epoch: 244800, train loss: 4.053798389434815, val loss: 4.066103076934814, ETA in seconds: 10079480.830\n",
      "epoch: 244900, train loss: 4.052626514434815, val loss: 4.069814014434814, ETA in seconds: 10082345.235\n",
      "epoch: 245000, train loss: 4.0559468269348145, val loss: 4.0686421394348145, ETA in seconds: 10085171.266\n",
      "epoch: 245100, train loss: 4.0559468269348145, val loss: 4.070204639434815, ETA in seconds: 10088127.477\n",
      "epoch: 245200, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 10091134.235\n",
      "epoch: 245300, train loss: 4.051259326934814, val loss: 4.072157764434815, ETA in seconds: 10094000.192\n",
      "epoch: 245400, train loss: 4.0510640144348145, val loss: 4.072353076934815, ETA in seconds: 10096786.036\n",
      "epoch: 245500, train loss: 4.049501514434814, val loss: 4.063954639434814, ETA in seconds: 10099496.759\n",
      "epoch: 245600, train loss: 4.046962451934815, val loss: 4.073134326934815, ETA in seconds: 10102349.548\n",
      "epoch: 245700, train loss: 4.0510640144348145, val loss: 4.065907764434814, ETA in seconds: 10105083.518\n",
      "epoch: 245800, train loss: 4.050673389434815, val loss: 4.066298389434815, ETA in seconds: 10107870.632\n",
      "epoch: 245900, train loss: 4.061220264434814, val loss: 4.0637593269348145, ETA in seconds: 10110613.278\n",
      "epoch: 246000, train loss: 4.051649951934815, val loss: 4.0657124519348145, ETA in seconds: 10113446.185\n",
      "epoch: 246100, train loss: 4.051649951934815, val loss: 4.066493701934815, ETA in seconds: 10116493.046\n",
      "epoch: 246200, train loss: 4.056532764434815, val loss: 4.0666890144348145, ETA in seconds: 10119216.184\n",
      "epoch: 246300, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 10121910.609\n",
      "epoch: 246400, train loss: 4.048524951934814, val loss: 4.066298389434815, ETA in seconds: 10124654.996\n",
      "epoch: 246500, train loss: 4.053407764434814, val loss: 4.064345264434815, ETA in seconds: 10127479.073\n",
      "epoch: 246600, train loss: 4.054189014434814, val loss: 4.0618062019348145, ETA in seconds: 10130262.071\n",
      "epoch: 246700, train loss: 4.056532764434815, val loss: 4.067079639434814, ETA in seconds: 10133017.506\n",
      "epoch: 246800, train loss: 4.048329639434814, val loss: 4.0637593269348145, ETA in seconds: 10135683.672\n",
      "epoch: 246900, train loss: 4.051259326934814, val loss: 4.0705952644348145, ETA in seconds: 10138467.117\n",
      "epoch: 247000, train loss: 4.050282764434814, val loss: 4.0686421394348145, ETA in seconds: 10141274.959\n",
      "epoch: 247100, train loss: 4.050282764434814, val loss: 4.066493701934815, ETA in seconds: 10143949.525\n",
      "epoch: 247200, train loss: 4.047353076934814, val loss: 4.074306201934815, ETA in seconds: 10146478.768\n",
      "epoch: 247300, train loss: 4.055751514434815, val loss: 4.066493701934815, ETA in seconds: 10149128.880\n",
      "epoch: 247400, train loss: 4.0461812019348145, val loss: 4.066298389434815, ETA in seconds: 10151673.922\n",
      "epoch: 247500, train loss: 4.053407764434814, val loss: 4.065517139434815, ETA in seconds: 10154324.681\n",
      "epoch: 247600, train loss: 4.048915576934815, val loss: 4.069032764434814, ETA in seconds: 10157060.036\n",
      "epoch: 247700, train loss: 4.050673389434815, val loss: 4.0657124519348145, ETA in seconds: 10159532.163\n",
      "epoch: 247800, train loss: 4.0549702644348145, val loss: 4.0657124519348145, ETA in seconds: 10161469.465\n",
      "epoch: 247900, train loss: 4.0491108894348145, val loss: 4.067274951934815, ETA in seconds: 10163928.294\n",
      "epoch: 248000, train loss: 4.048329639434814, val loss: 4.068446826934815, ETA in seconds: 10166627.751\n",
      "epoch: 248100, train loss: 4.052235889434814, val loss: 4.067860889434814, ETA in seconds: 10169398.727\n",
      "epoch: 248200, train loss: 4.055360889434814, val loss: 4.064345264434815, ETA in seconds: 10172258.444\n",
      "epoch: 248300, train loss: 4.0500874519348145, val loss: 4.0588765144348145, ETA in seconds: 10174970.844\n",
      "epoch: 248400, train loss: 4.0588765144348145, val loss: 4.063954639434814, ETA in seconds: 10177739.864\n",
      "epoch: 248500, train loss: 4.060634326934815, val loss: 4.067860889434814, ETA in seconds: 10180588.785\n",
      "epoch: 248600, train loss: 4.051259326934814, val loss: 4.0745015144348145, ETA in seconds: 10183344.487\n",
      "epoch: 248700, train loss: 4.053407764434814, val loss: 4.072157764434815, ETA in seconds: 10186133.346\n",
      "epoch: 248800, train loss: 4.050478076934814, val loss: 4.0715718269348145, ETA in seconds: 10188785.637\n",
      "epoch: 248900, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 10191476.781\n",
      "epoch: 249000, train loss: 4.051454639434814, val loss: 4.062587451934815, ETA in seconds: 10194294.608\n",
      "epoch: 249100, train loss: 4.056728076934815, val loss: 4.068251514434815, ETA in seconds: 10196970.793\n",
      "epoch: 249200, train loss: 4.054384326934814, val loss: 4.0657124519348145, ETA in seconds: 10199773.838\n",
      "epoch: 249300, train loss: 4.055751514434815, val loss: 4.062978076934814, ETA in seconds: 10202579.030\n",
      "epoch: 249400, train loss: 4.0481343269348145, val loss: 4.067274951934815, ETA in seconds: 10205405.992\n",
      "epoch: 249500, train loss: 4.049501514434814, val loss: 4.063173389434814, ETA in seconds: 10208098.308\n",
      "epoch: 249600, train loss: 4.0510640144348145, val loss: 4.068446826934815, ETA in seconds: 10210754.495\n",
      "epoch: 249700, train loss: 4.052431201934814, val loss: 4.0637593269348145, ETA in seconds: 10213302.604\n",
      "epoch: 249800, train loss: 4.045985889434815, val loss: 4.066493701934815, ETA in seconds: 10215967.579\n",
      "epoch: 249900, train loss: 4.048524951934814, val loss: 4.0686421394348145, ETA in seconds: 10218631.248\n",
      "epoch: 250000, train loss: 4.052235889434814, val loss: 4.0705952644348145, ETA in seconds: 10221272.382\n",
      "epoch: 250100, train loss: 4.057314014434814, val loss: 4.064149951934814, ETA in seconds: 10224022.222\n",
      "epoch: 250200, train loss: 4.046767139434815, val loss: 4.066493701934815, ETA in seconds: 10226769.717\n",
      "epoch: 250300, train loss: 4.0539937019348145, val loss: 4.069814014434814, ETA in seconds: 10229334.544\n",
      "epoch: 250400, train loss: 4.0539937019348145, val loss: 4.062001514434814, ETA in seconds: 10232017.873\n",
      "epoch: 250500, train loss: 4.054774951934815, val loss: 4.063368701934815, ETA in seconds: 10234888.857\n",
      "epoch: 250600, train loss: 4.046767139434815, val loss: 4.0637593269348145, ETA in seconds: 10237604.869\n",
      "epoch: 250700, train loss: 4.051259326934814, val loss: 4.065126514434814, ETA in seconds: 10240331.498\n",
      "epoch: 250800, train loss: 4.055751514434815, val loss: 4.0705952644348145, ETA in seconds: 10243015.038\n",
      "epoch: 250900, train loss: 4.045790576934815, val loss: 4.064931201934814, ETA in seconds: 10245619.284\n",
      "epoch: 251000, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 10248194.489\n",
      "epoch: 251100, train loss: 4.047548389434814, val loss: 4.069228076934815, ETA in seconds: 10250946.008\n",
      "epoch: 251200, train loss: 4.054579639434815, val loss: 4.0676655769348145, ETA in seconds: 10253875.082\n",
      "epoch: 251300, train loss: 4.054189014434814, val loss: 4.068837451934814, ETA in seconds: 10256522.702\n",
      "epoch: 251400, train loss: 4.0549702644348145, val loss: 4.063368701934815, ETA in seconds: 10259088.632\n",
      "epoch: 251500, train loss: 4.052821826934815, val loss: 4.065907764434814, ETA in seconds: 10261906.531\n",
      "epoch: 251600, train loss: 4.056532764434815, val loss: 4.067079639434814, ETA in seconds: 10264641.259\n",
      "epoch: 251700, train loss: 4.054774951934815, val loss: 4.0686421394348145, ETA in seconds: 10267352.276\n",
      "epoch: 251800, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 10270019.213\n",
      "epoch: 251900, train loss: 4.052235889434814, val loss: 4.0715718269348145, ETA in seconds: 10272753.092\n",
      "epoch: 252000, train loss: 4.048720264434815, val loss: 4.067860889434814, ETA in seconds: 10275725.162\n",
      "epoch: 252100, train loss: 4.0569233894348145, val loss: 4.063368701934815, ETA in seconds: 10278319.639\n",
      "epoch: 252200, train loss: 4.058095264434814, val loss: 4.0666890144348145, ETA in seconds: 10280826.760\n",
      "epoch: 252300, train loss: 4.0549702644348145, val loss: 4.067079639434814, ETA in seconds: 10283455.373\n",
      "epoch: 252400, train loss: 4.054774951934815, val loss: 4.075087451934815, ETA in seconds: 10286175.832\n",
      "epoch: 252500, train loss: 4.053603076934815, val loss: 4.059657764434815, ETA in seconds: 10288806.072\n",
      "epoch: 252600, train loss: 4.050478076934814, val loss: 4.0657124519348145, ETA in seconds: 10291431.979\n",
      "epoch: 252700, train loss: 4.048524951934814, val loss: 4.067079639434814, ETA in seconds: 10294066.740\n",
      "epoch: 252800, train loss: 4.053212451934814, val loss: 4.067274951934815, ETA in seconds: 10296750.359\n",
      "epoch: 252900, train loss: 4.049306201934814, val loss: 4.0764546394348145, ETA in seconds: 10299485.315\n",
      "epoch: 253000, train loss: 4.055556201934815, val loss: 4.068056201934814, ETA in seconds: 10302204.435\n",
      "epoch: 253100, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 10304939.704\n",
      "epoch: 253200, train loss: 4.049501514434814, val loss: 4.070204639434815, ETA in seconds: 10307278.278\n",
      "epoch: 253300, train loss: 4.054579639434815, val loss: 4.069032764434814, ETA in seconds: 10309942.980\n",
      "epoch: 253400, train loss: 4.051845264434815, val loss: 4.069032764434814, ETA in seconds: 10312597.013\n",
      "epoch: 253500, train loss: 4.048915576934815, val loss: 4.0666890144348145, ETA in seconds: 10315277.673\n",
      "epoch: 253600, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 10318024.780\n",
      "epoch: 253700, train loss: 4.055165576934814, val loss: 4.064345264434815, ETA in seconds: 10320558.580\n",
      "epoch: 253800, train loss: 4.0539937019348145, val loss: 4.064345264434815, ETA in seconds: 10323302.623\n",
      "epoch: 253900, train loss: 4.058095264434814, val loss: 4.073329639434815, ETA in seconds: 10326004.863\n",
      "epoch: 254000, train loss: 4.053212451934814, val loss: 4.0598530769348145, ETA in seconds: 10328976.653\n",
      "epoch: 254100, train loss: 4.059071826934814, val loss: 4.063368701934815, ETA in seconds: 10331558.665\n",
      "epoch: 254200, train loss: 4.047939014434815, val loss: 4.063564014434815, ETA in seconds: 10334058.833\n",
      "epoch: 254300, train loss: 4.0481343269348145, val loss: 4.067274951934815, ETA in seconds: 10336569.670\n",
      "epoch: 254400, train loss: 4.056728076934815, val loss: 4.068056201934814, ETA in seconds: 10339118.307\n",
      "epoch: 254500, train loss: 4.054384326934814, val loss: 4.065126514434814, ETA in seconds: 10341657.395\n",
      "epoch: 254600, train loss: 4.048329639434814, val loss: 4.0696187019348145, ETA in seconds: 10344258.827\n",
      "epoch: 254700, train loss: 4.047548389434814, val loss: 4.067860889434814, ETA in seconds: 10346889.557\n",
      "epoch: 254800, train loss: 4.052821826934815, val loss: 4.068446826934815, ETA in seconds: 10349544.323\n",
      "epoch: 254900, train loss: 4.048524951934814, val loss: 4.070399951934815, ETA in seconds: 10352067.769\n",
      "epoch: 255000, train loss: 4.052431201934814, val loss: 4.060634326934815, ETA in seconds: 10354879.800\n",
      "epoch: 255100, train loss: 4.053603076934815, val loss: 4.065321826934815, ETA in seconds: 10357549.152\n",
      "epoch: 255200, train loss: 4.054579639434815, val loss: 4.069228076934815, ETA in seconds: 10360085.719\n",
      "epoch: 255300, train loss: 4.050673389434815, val loss: 4.070009326934814, ETA in seconds: 10362448.989\n",
      "epoch: 255400, train loss: 4.050868701934815, val loss: 4.069032764434814, ETA in seconds: 10365055.957\n",
      "epoch: 255500, train loss: 4.0559468269348145, val loss: 4.065517139434815, ETA in seconds: 10367633.320\n",
      "epoch: 255600, train loss: 4.049501514434814, val loss: 4.062001514434814, ETA in seconds: 10370348.426\n",
      "epoch: 255700, train loss: 4.0491108894348145, val loss: 4.066298389434815, ETA in seconds: 10373022.580\n",
      "epoch: 255800, train loss: 4.048329639434814, val loss: 4.070009326934814, ETA in seconds: 10375668.409\n",
      "epoch: 255900, train loss: 4.053407764434814, val loss: 4.059462451934815, ETA in seconds: 10378229.099\n",
      "epoch: 256000, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 10380961.389\n",
      "epoch: 256100, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 10383652.170\n",
      "epoch: 256200, train loss: 4.048524951934814, val loss: 4.068446826934815, ETA in seconds: 10386104.934\n",
      "epoch: 256300, train loss: 4.046962451934815, val loss: 4.067470264434815, ETA in seconds: 10388571.741\n",
      "epoch: 256400, train loss: 4.0530171394348145, val loss: 4.069814014434814, ETA in seconds: 10391203.076\n",
      "epoch: 256500, train loss: 4.049306201934814, val loss: 4.066103076934814, ETA in seconds: 10393870.466\n",
      "epoch: 256600, train loss: 4.052431201934814, val loss: 4.068446826934815, ETA in seconds: 10396457.750\n",
      "epoch: 256700, train loss: 4.060634326934815, val loss: 4.064540576934815, ETA in seconds: 10398958.651\n",
      "epoch: 256800, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 10401564.578\n",
      "epoch: 256900, train loss: 4.050673389434815, val loss: 4.065126514434814, ETA in seconds: 10404193.341\n",
      "epoch: 257000, train loss: 4.048524951934814, val loss: 4.069423389434815, ETA in seconds: 10406883.431\n",
      "epoch: 257100, train loss: 4.048524951934814, val loss: 4.065517139434815, ETA in seconds: 10409389.276\n",
      "epoch: 257200, train loss: 4.054774951934815, val loss: 4.0666890144348145, ETA in seconds: 10411925.280\n",
      "epoch: 257300, train loss: 4.049696826934815, val loss: 4.070204639434815, ETA in seconds: 10414395.670\n",
      "epoch: 257400, train loss: 4.047939014434815, val loss: 4.0647358894348145, ETA in seconds: 10416904.149\n",
      "epoch: 257500, train loss: 4.049892139434815, val loss: 4.067274951934815, ETA in seconds: 10419395.252\n",
      "epoch: 257600, train loss: 4.056728076934815, val loss: 4.066884326934814, ETA in seconds: 10422098.601\n",
      "epoch: 257700, train loss: 4.051845264434815, val loss: 4.070009326934814, ETA in seconds: 10424726.816\n",
      "epoch: 257800, train loss: 4.0539937019348145, val loss: 4.067470264434815, ETA in seconds: 10427272.963\n",
      "epoch: 257900, train loss: 4.049892139434815, val loss: 4.060048389434814, ETA in seconds: 10429794.565\n",
      "epoch: 258000, train loss: 4.045399951934814, val loss: 4.0627827644348145, ETA in seconds: 10432501.757\n",
      "epoch: 258100, train loss: 4.049892139434815, val loss: 4.062587451934815, ETA in seconds: 10435021.435\n",
      "epoch: 258200, train loss: 4.048329639434814, val loss: 4.062001514434814, ETA in seconds: 10437642.138\n",
      "epoch: 258300, train loss: 4.052431201934814, val loss: 4.068837451934814, ETA in seconds: 10440332.155\n",
      "epoch: 258400, train loss: 4.050282764434814, val loss: 4.072939014434814, ETA in seconds: 10442886.830\n",
      "epoch: 258500, train loss: 4.0569233894348145, val loss: 4.065126514434814, ETA in seconds: 10445385.386\n",
      "epoch: 258600, train loss: 4.056337451934814, val loss: 4.070204639434815, ETA in seconds: 10448106.596\n",
      "epoch: 258700, train loss: 4.051845264434815, val loss: 4.0705952644348145, ETA in seconds: 10450801.769\n",
      "epoch: 258800, train loss: 4.051649951934815, val loss: 4.064540576934815, ETA in seconds: 10453377.040\n",
      "epoch: 258900, train loss: 4.053798389434815, val loss: 4.073329639434815, ETA in seconds: 10455982.308\n",
      "epoch: 259000, train loss: 4.0520405769348145, val loss: 4.0715718269348145, ETA in seconds: 10458504.803\n",
      "epoch: 259100, train loss: 4.0569233894348145, val loss: 4.067274951934815, ETA in seconds: 10461090.109\n",
      "epoch: 259200, train loss: 4.057118701934814, val loss: 4.069814014434814, ETA in seconds: 10463696.612\n",
      "epoch: 259300, train loss: 4.0500874519348145, val loss: 4.0696187019348145, ETA in seconds: 10466395.912\n",
      "epoch: 259400, train loss: 4.057314014434814, val loss: 4.063954639434814, ETA in seconds: 10469373.187\n",
      "epoch: 259500, train loss: 4.049501514434814, val loss: 4.068251514434815, ETA in seconds: 10472143.802\n",
      "epoch: 259600, train loss: 4.0549702644348145, val loss: 4.067470264434815, ETA in seconds: 10474742.025\n",
      "epoch: 259700, train loss: 4.055556201934815, val loss: 4.067470264434815, ETA in seconds: 10477316.468\n",
      "epoch: 259800, train loss: 4.058290576934814, val loss: 4.067860889434814, ETA in seconds: 10479692.538\n",
      "epoch: 259900, train loss: 4.050282764434814, val loss: 4.072157764434815, ETA in seconds: 10481989.958\n",
      "epoch: 260000, train loss: 4.049501514434814, val loss: 4.069423389434815, ETA in seconds: 10484298.913\n",
      "epoch: 260100, train loss: 4.050478076934814, val loss: 4.063173389434814, ETA in seconds: 10486674.276\n",
      "epoch: 260200, train loss: 4.047743701934815, val loss: 4.067470264434815, ETA in seconds: 10489047.269\n",
      "epoch: 260300, train loss: 4.0539937019348145, val loss: 4.0686421394348145, ETA in seconds: 10491515.273\n",
      "epoch: 260400, train loss: 4.056532764434815, val loss: 4.0657124519348145, ETA in seconds: 10493953.960\n",
      "epoch: 260500, train loss: 4.052821826934815, val loss: 4.067470264434815, ETA in seconds: 10496349.399\n",
      "epoch: 260600, train loss: 4.053798389434815, val loss: 4.066298389434815, ETA in seconds: 10498847.441\n",
      "epoch: 260700, train loss: 4.045790576934815, val loss: 4.0666890144348145, ETA in seconds: 10501367.142\n",
      "epoch: 260800, train loss: 4.046962451934815, val loss: 4.063564014434815, ETA in seconds: 10503805.154\n",
      "epoch: 260900, train loss: 4.048329639434814, val loss: 4.0686421394348145, ETA in seconds: 10506277.907\n",
      "epoch: 261000, train loss: 4.057314014434814, val loss: 4.074306201934815, ETA in seconds: 10508713.853\n",
      "epoch: 261100, train loss: 4.055751514434815, val loss: 4.071181201934815, ETA in seconds: 10511219.547\n",
      "epoch: 261200, train loss: 4.052431201934814, val loss: 4.069814014434814, ETA in seconds: 10513708.811\n",
      "epoch: 261300, train loss: 4.049892139434815, val loss: 4.067470264434815, ETA in seconds: 10516161.685\n",
      "epoch: 261400, train loss: 4.046571826934814, val loss: 4.067860889434814, ETA in seconds: 10518827.149\n",
      "epoch: 261500, train loss: 4.056142139434814, val loss: 4.070204639434815, ETA in seconds: 10521817.641\n",
      "epoch: 261600, train loss: 4.058095264434814, val loss: 4.067079639434814, ETA in seconds: 10524424.829\n",
      "epoch: 261700, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 10526963.373\n",
      "epoch: 261800, train loss: 4.049501514434814, val loss: 4.0657124519348145, ETA in seconds: 10529565.316\n",
      "epoch: 261900, train loss: 4.050282764434814, val loss: 4.0618062019348145, ETA in seconds: 10532125.988\n",
      "epoch: 262000, train loss: 4.054384326934814, val loss: 4.068837451934814, ETA in seconds: 10534570.132\n",
      "epoch: 262100, train loss: 4.055556201934815, val loss: 4.068837451934814, ETA in seconds: 10537239.165\n",
      "epoch: 262200, train loss: 4.0569233894348145, val loss: 4.0676655769348145, ETA in seconds: 10539811.602\n",
      "epoch: 262300, train loss: 4.0510640144348145, val loss: 4.064345264434815, ETA in seconds: 10542257.944\n",
      "epoch: 262400, train loss: 4.046571826934814, val loss: 4.065907764434814, ETA in seconds: 10544792.405\n",
      "epoch: 262500, train loss: 4.052235889434814, val loss: 4.067079639434814, ETA in seconds: 10547282.181\n",
      "epoch: 262600, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 10549749.480\n",
      "epoch: 262700, train loss: 4.0452046394348145, val loss: 4.066884326934814, ETA in seconds: 10552199.917\n",
      "epoch: 262800, train loss: 4.0569233894348145, val loss: 4.067860889434814, ETA in seconds: 10554905.545\n",
      "epoch: 262900, train loss: 4.048915576934815, val loss: 4.0666890144348145, ETA in seconds: 10557582.195\n",
      "epoch: 263000, train loss: 4.050868701934815, val loss: 4.068837451934814, ETA in seconds: 10560078.724\n",
      "epoch: 263100, train loss: 4.0452046394348145, val loss: 4.066493701934815, ETA in seconds: 10562643.110\n",
      "epoch: 263200, train loss: 4.050282764434814, val loss: 4.0657124519348145, ETA in seconds: 10565203.982\n",
      "epoch: 263300, train loss: 4.054579639434815, val loss: 4.063954639434814, ETA in seconds: 10567910.878\n",
      "epoch: 263400, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 10570653.609\n",
      "epoch: 263500, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 10573223.633\n",
      "epoch: 263600, train loss: 4.049501514434814, val loss: 4.068837451934814, ETA in seconds: 10575892.313\n",
      "epoch: 263700, train loss: 4.0491108894348145, val loss: 4.069228076934815, ETA in seconds: 10578603.450\n",
      "epoch: 263800, train loss: 4.0510640144348145, val loss: 4.067470264434815, ETA in seconds: 10581160.793\n",
      "epoch: 263900, train loss: 4.0452046394348145, val loss: 4.070399951934815, ETA in seconds: 10583887.633\n",
      "epoch: 264000, train loss: 4.0530171394348145, val loss: 4.0715718269348145, ETA in seconds: 10586282.389\n",
      "epoch: 264100, train loss: 4.050478076934814, val loss: 4.070399951934815, ETA in seconds: 10588833.556\n",
      "epoch: 264200, train loss: 4.057704639434815, val loss: 4.070204639434815, ETA in seconds: 10591386.655\n",
      "epoch: 264300, train loss: 4.056142139434814, val loss: 4.066298389434815, ETA in seconds: 10593938.261\n",
      "epoch: 264400, train loss: 4.044618701934814, val loss: 4.067274951934815, ETA in seconds: 10596640.253\n",
      "epoch: 264500, train loss: 4.050282764434814, val loss: 4.070204639434815, ETA in seconds: 10599287.851\n",
      "epoch: 264600, train loss: 4.048329639434814, val loss: 4.068056201934814, ETA in seconds: 10601776.708\n",
      "epoch: 264700, train loss: 4.047743701934815, val loss: 4.068251514434815, ETA in seconds: 10604090.853\n",
      "epoch: 264800, train loss: 4.0559468269348145, val loss: 4.0686421394348145, ETA in seconds: 10605847.676\n",
      "epoch: 264900, train loss: 4.048329639434814, val loss: 4.066493701934815, ETA in seconds: 10608237.903\n",
      "epoch: 265000, train loss: 4.046571826934814, val loss: 4.073329639434815, ETA in seconds: 10610778.376\n",
      "epoch: 265100, train loss: 4.052235889434814, val loss: 4.070009326934814, ETA in seconds: 10613361.171\n",
      "epoch: 265200, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 10615826.209\n",
      "epoch: 265300, train loss: 4.0510640144348145, val loss: 4.067470264434815, ETA in seconds: 10618170.007\n",
      "epoch: 265400, train loss: 4.049892139434815, val loss: 4.064931201934814, ETA in seconds: 10620724.442\n",
      "epoch: 265500, train loss: 4.052626514434815, val loss: 4.0657124519348145, ETA in seconds: 10623219.273\n",
      "epoch: 265600, train loss: 4.052235889434814, val loss: 4.0715718269348145, ETA in seconds: 10625775.853\n",
      "epoch: 265700, train loss: 4.048915576934815, val loss: 4.0686421394348145, ETA in seconds: 10628177.968\n",
      "epoch: 265800, train loss: 4.053798389434815, val loss: 4.069814014434814, ETA in seconds: 10630507.935\n",
      "epoch: 265900, train loss: 4.052821826934815, val loss: 4.074110889434815, ETA in seconds: 10632929.599\n",
      "epoch: 266000, train loss: 4.052235889434814, val loss: 4.067079639434814, ETA in seconds: 10635151.685\n",
      "epoch: 266100, train loss: 4.052821826934815, val loss: 4.062587451934815, ETA in seconds: 10637476.778\n",
      "epoch: 266200, train loss: 4.059462451934815, val loss: 4.066884326934814, ETA in seconds: 10639847.487\n",
      "epoch: 266300, train loss: 4.054774951934815, val loss: 4.072743701934814, ETA in seconds: 10642524.051\n",
      "epoch: 266400, train loss: 4.051845264434815, val loss: 4.0647358894348145, ETA in seconds: 10645024.751\n",
      "epoch: 266500, train loss: 4.051259326934814, val loss: 4.0666890144348145, ETA in seconds: 10647464.033\n",
      "epoch: 266600, train loss: 4.055165576934814, val loss: 4.064931201934814, ETA in seconds: 10649975.039\n",
      "epoch: 266700, train loss: 4.052431201934814, val loss: 4.062392139434815, ETA in seconds: 10652505.649\n",
      "epoch: 266800, train loss: 4.051454639434814, val loss: 4.061610889434815, ETA in seconds: 10654961.181\n",
      "epoch: 266900, train loss: 4.046571826934814, val loss: 4.066884326934814, ETA in seconds: 10657502.608\n",
      "epoch: 267000, train loss: 4.055556201934815, val loss: 4.065517139434815, ETA in seconds: 10660141.732\n",
      "epoch: 267100, train loss: 4.052431201934814, val loss: 4.066884326934814, ETA in seconds: 10662652.427\n",
      "epoch: 267200, train loss: 4.052821826934815, val loss: 4.071962451934814, ETA in seconds: 10665183.111\n",
      "epoch: 267300, train loss: 4.047353076934814, val loss: 4.069814014434814, ETA in seconds: 10667644.163\n",
      "epoch: 267400, train loss: 4.053603076934815, val loss: 4.063564014434815, ETA in seconds: 10670096.803\n",
      "epoch: 267500, train loss: 4.043446826934814, val loss: 4.067470264434815, ETA in seconds: 10672627.434\n",
      "epoch: 267600, train loss: 4.054189014434814, val loss: 4.066493701934815, ETA in seconds: 10675090.682\n",
      "epoch: 267700, train loss: 4.058290576934814, val loss: 4.071767139434814, ETA in seconds: 10677510.766\n",
      "epoch: 267800, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 10679939.616\n",
      "epoch: 267900, train loss: 4.050673389434815, val loss: 4.070985889434814, ETA in seconds: 10682339.942\n",
      "epoch: 268000, train loss: 4.052235889434814, val loss: 4.073134326934815, ETA in seconds: 10684820.581\n",
      "epoch: 268100, train loss: 4.058095264434814, val loss: 4.070790576934814, ETA in seconds: 10687313.555\n",
      "epoch: 268200, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 10689798.075\n",
      "epoch: 268300, train loss: 4.054384326934814, val loss: 4.0627827644348145, ETA in seconds: 10692339.308\n",
      "epoch: 268400, train loss: 4.056532764434815, val loss: 4.070985889434814, ETA in seconds: 10694847.509\n",
      "epoch: 268500, train loss: 4.057704639434815, val loss: 4.065907764434814, ETA in seconds: 10697328.985\n",
      "epoch: 268600, train loss: 4.0491108894348145, val loss: 4.066298389434815, ETA in seconds: 10699806.946\n",
      "epoch: 268700, train loss: 4.0481343269348145, val loss: 4.063368701934815, ETA in seconds: 10702332.039\n",
      "epoch: 268800, train loss: 4.049696826934815, val loss: 4.067860889434814, ETA in seconds: 10704825.536\n",
      "epoch: 268900, train loss: 4.051454639434814, val loss: 4.070204639434815, ETA in seconds: 10707234.859\n",
      "epoch: 269000, train loss: 4.0510640144348145, val loss: 4.0666890144348145, ETA in seconds: 10709700.441\n",
      "epoch: 269100, train loss: 4.056337451934814, val loss: 4.0715718269348145, ETA in seconds: 10712235.696\n",
      "epoch: 269200, train loss: 4.050282764434814, val loss: 4.063173389434814, ETA in seconds: 10714633.411\n",
      "epoch: 269300, train loss: 4.051259326934814, val loss: 4.064540576934815, ETA in seconds: 10717123.311\n",
      "epoch: 269400, train loss: 4.052821826934815, val loss: 4.062587451934815, ETA in seconds: 10719747.722\n",
      "epoch: 269500, train loss: 4.049501514434814, val loss: 4.068056201934814, ETA in seconds: 10722284.063\n",
      "epoch: 269600, train loss: 4.049306201934814, val loss: 4.0705952644348145, ETA in seconds: 10724077.338\n",
      "epoch: 269700, train loss: 4.052626514434815, val loss: 4.063368701934815, ETA in seconds: 10726254.586\n",
      "epoch: 269800, train loss: 4.048329639434814, val loss: 4.0715718269348145, ETA in seconds: 10728620.529\n",
      "epoch: 269900, train loss: 4.046767139434815, val loss: 4.069814014434814, ETA in seconds: 10731086.037\n",
      "epoch: 270000, train loss: 4.052431201934814, val loss: 4.065126514434814, ETA in seconds: 10733596.408\n",
      "epoch: 270100, train loss: 4.057118701934814, val loss: 4.0705952644348145, ETA in seconds: 10736071.633\n",
      "epoch: 270200, train loss: 4.057509326934815, val loss: 4.063954639434814, ETA in seconds: 10738556.053\n",
      "epoch: 270300, train loss: 4.055751514434815, val loss: 4.068251514434815, ETA in seconds: 10741022.390\n",
      "epoch: 270400, train loss: 4.048720264434815, val loss: 4.068446826934815, ETA in seconds: 10743541.865\n",
      "epoch: 270500, train loss: 4.051259326934814, val loss: 4.069423389434815, ETA in seconds: 10745922.294\n",
      "epoch: 270600, train loss: 4.055556201934815, val loss: 4.064540576934815, ETA in seconds: 10748315.742\n",
      "epoch: 270700, train loss: 4.044618701934814, val loss: 4.070204639434815, ETA in seconds: 10750715.347\n",
      "epoch: 270800, train loss: 4.052431201934814, val loss: 4.063954639434814, ETA in seconds: 10753075.769\n",
      "epoch: 270900, train loss: 4.052626514434815, val loss: 4.065321826934815, ETA in seconds: 10755572.845\n",
      "epoch: 271000, train loss: 4.053798389434815, val loss: 4.068837451934814, ETA in seconds: 10758061.758\n",
      "epoch: 271100, train loss: 4.0559468269348145, val loss: 4.067079639434814, ETA in seconds: 10760712.992\n",
      "epoch: 271200, train loss: 4.053407764434814, val loss: 4.062392139434815, ETA in seconds: 10763142.078\n",
      "epoch: 271300, train loss: 4.0530171394348145, val loss: 4.0647358894348145, ETA in seconds: 10765817.421\n",
      "epoch: 271400, train loss: 4.054189014434814, val loss: 4.070009326934814, ETA in seconds: 10768439.110\n",
      "epoch: 271500, train loss: 4.050673389434815, val loss: 4.0696187019348145, ETA in seconds: 10771025.849\n",
      "epoch: 271600, train loss: 4.049306201934814, val loss: 4.065907764434814, ETA in seconds: 10773488.305\n",
      "epoch: 271700, train loss: 4.059071826934814, val loss: 4.073329639434815, ETA in seconds: 10776027.433\n",
      "epoch: 271800, train loss: 4.059071826934814, val loss: 4.070009326934814, ETA in seconds: 10778540.883\n",
      "epoch: 271900, train loss: 4.055165576934814, val loss: 4.064931201934814, ETA in seconds: 10780958.754\n",
      "epoch: 272000, train loss: 4.053798389434815, val loss: 4.067079639434814, ETA in seconds: 10783331.472\n",
      "epoch: 272100, train loss: 4.060243701934814, val loss: 4.068837451934814, ETA in seconds: 10785745.599\n",
      "epoch: 272200, train loss: 4.055751514434815, val loss: 4.069814014434814, ETA in seconds: 10788142.174\n",
      "epoch: 272300, train loss: 4.0471577644348145, val loss: 4.063564014434815, ETA in seconds: 10790522.539\n",
      "epoch: 272400, train loss: 4.0500874519348145, val loss: 4.065321826934815, ETA in seconds: 10792964.470\n",
      "epoch: 272500, train loss: 4.045985889434815, val loss: 4.069423389434815, ETA in seconds: 10795356.906\n",
      "epoch: 272600, train loss: 4.058290576934814, val loss: 4.066298389434815, ETA in seconds: 10797732.326\n",
      "epoch: 272700, train loss: 4.055751514434815, val loss: 4.065517139434815, ETA in seconds: 10800159.130\n",
      "epoch: 272800, train loss: 4.048524951934814, val loss: 4.067079639434814, ETA in seconds: 10802627.876\n",
      "epoch: 272900, train loss: 4.047743701934815, val loss: 4.067470264434815, ETA in seconds: 10805045.048\n",
      "epoch: 273000, train loss: 4.055165576934814, val loss: 4.067470264434815, ETA in seconds: 10807542.922\n",
      "epoch: 273100, train loss: 4.061415576934815, val loss: 4.060243701934814, ETA in seconds: 10810118.113\n",
      "epoch: 273200, train loss: 4.046767139434815, val loss: 4.069423389434815, ETA in seconds: 10812661.404\n",
      "epoch: 273300, train loss: 4.049501514434814, val loss: 4.0705952644348145, ETA in seconds: 10815191.902\n",
      "epoch: 273400, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 10817704.770\n",
      "epoch: 273500, train loss: 4.0520405769348145, val loss: 4.073915576934814, ETA in seconds: 10820140.719\n",
      "epoch: 273600, train loss: 4.050478076934814, val loss: 4.0666890144348145, ETA in seconds: 10822468.448\n",
      "epoch: 273700, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 10824773.090\n",
      "epoch: 273800, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 10827162.583\n",
      "epoch: 273900, train loss: 4.053798389434815, val loss: 4.063564014434815, ETA in seconds: 10829524.137\n",
      "epoch: 274000, train loss: 4.049696826934815, val loss: 4.068837451934814, ETA in seconds: 10831930.550\n",
      "epoch: 274100, train loss: 4.0559468269348145, val loss: 4.067470264434815, ETA in seconds: 10834688.954\n",
      "epoch: 274200, train loss: 4.049501514434814, val loss: 4.064540576934815, ETA in seconds: 10837487.211\n",
      "epoch: 274300, train loss: 4.053212451934814, val loss: 4.0676655769348145, ETA in seconds: 10840105.440\n",
      "epoch: 274400, train loss: 4.055165576934814, val loss: 4.062392139434815, ETA in seconds: 10842433.510\n",
      "epoch: 274500, train loss: 4.046962451934815, val loss: 4.067860889434814, ETA in seconds: 10844730.215\n",
      "epoch: 274600, train loss: 4.058290576934814, val loss: 4.065126514434814, ETA in seconds: 10847030.941\n",
      "epoch: 274700, train loss: 4.0530171394348145, val loss: 4.069032764434814, ETA in seconds: 10849394.113\n",
      "epoch: 274800, train loss: 4.049892139434815, val loss: 4.065517139434815, ETA in seconds: 10851821.493\n",
      "epoch: 274900, train loss: 4.055360889434814, val loss: 4.066493701934815, ETA in seconds: 10854313.232\n",
      "epoch: 275000, train loss: 4.050868701934815, val loss: 4.067860889434814, ETA in seconds: 10856815.856\n",
      "epoch: 275100, train loss: 4.053603076934815, val loss: 4.068446826934815, ETA in seconds: 10859308.557\n",
      "epoch: 275200, train loss: 4.049501514434814, val loss: 4.065517139434815, ETA in seconds: 10861808.841\n",
      "epoch: 275300, train loss: 4.049501514434814, val loss: 4.0696187019348145, ETA in seconds: 10864320.333\n",
      "epoch: 275400, train loss: 4.046571826934814, val loss: 4.065517139434815, ETA in seconds: 10866770.377\n",
      "epoch: 275500, train loss: 4.051454639434814, val loss: 4.0618062019348145, ETA in seconds: 10869162.325\n",
      "epoch: 275600, train loss: 4.0520405769348145, val loss: 4.0657124519348145, ETA in seconds: 10871598.510\n",
      "epoch: 275700, train loss: 4.054384326934814, val loss: 4.069814014434814, ETA in seconds: 10873982.683\n",
      "epoch: 275800, train loss: 4.044423389434814, val loss: 4.0647358894348145, ETA in seconds: 10876474.781\n",
      "epoch: 275900, train loss: 4.0481343269348145, val loss: 4.064149951934814, ETA in seconds: 10878947.113\n",
      "epoch: 276000, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 10881499.993\n",
      "epoch: 276100, train loss: 4.0569233894348145, val loss: 4.068251514434815, ETA in seconds: 10884082.256\n",
      "epoch: 276200, train loss: 4.052235889434814, val loss: 4.068056201934814, ETA in seconds: 10886472.061\n",
      "epoch: 276300, train loss: 4.051845264434815, val loss: 4.072939014434814, ETA in seconds: 10888942.524\n",
      "epoch: 276400, train loss: 4.047548389434814, val loss: 4.069228076934815, ETA in seconds: 10891248.437\n",
      "epoch: 276500, train loss: 4.046962451934815, val loss: 4.066884326934814, ETA in seconds: 10893536.657\n",
      "epoch: 276600, train loss: 4.050282764434814, val loss: 4.065907764434814, ETA in seconds: 10895889.612\n",
      "epoch: 276700, train loss: 4.054189014434814, val loss: 4.064540576934815, ETA in seconds: 10898183.443\n",
      "epoch: 276800, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 10900552.257\n",
      "epoch: 276900, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 10903000.249\n",
      "epoch: 277000, train loss: 4.046376514434814, val loss: 4.072157764434815, ETA in seconds: 10905542.607\n",
      "epoch: 277100, train loss: 4.0500874519348145, val loss: 4.059071826934814, ETA in seconds: 10908082.016\n",
      "epoch: 277200, train loss: 4.050478076934814, val loss: 4.0637593269348145, ETA in seconds: 10910429.754\n",
      "epoch: 277300, train loss: 4.051454639434814, val loss: 4.076064014434815, ETA in seconds: 10912824.976\n",
      "epoch: 277400, train loss: 4.057509326934815, val loss: 4.067860889434814, ETA in seconds: 10915118.537\n",
      "epoch: 277500, train loss: 4.053212451934814, val loss: 4.064345264434815, ETA in seconds: 10917445.781\n",
      "epoch: 277600, train loss: 4.052235889434814, val loss: 4.0705952644348145, ETA in seconds: 10919965.717\n",
      "epoch: 277700, train loss: 4.0530171394348145, val loss: 4.066493701934815, ETA in seconds: 10922360.297\n",
      "epoch: 277800, train loss: 4.0471577644348145, val loss: 4.062587451934815, ETA in seconds: 10924849.976\n",
      "epoch: 277900, train loss: 4.0549702644348145, val loss: 4.066884326934814, ETA in seconds: 10927315.587\n",
      "epoch: 278000, train loss: 4.045985889434815, val loss: 4.066493701934815, ETA in seconds: 10929688.815\n",
      "epoch: 278100, train loss: 4.0539937019348145, val loss: 4.064149951934814, ETA in seconds: 10932009.275\n",
      "epoch: 278200, train loss: 4.046571826934814, val loss: 4.063954639434814, ETA in seconds: 10934244.005\n",
      "epoch: 278300, train loss: 4.0569233894348145, val loss: 4.066884326934814, ETA in seconds: 10936700.290\n",
      "epoch: 278400, train loss: 4.057314014434814, val loss: 4.068837451934814, ETA in seconds: 10939059.689\n",
      "epoch: 278500, train loss: 4.059267139434814, val loss: 4.064345264434815, ETA in seconds: 10941416.268\n",
      "epoch: 278600, train loss: 4.049306201934814, val loss: 4.069423389434815, ETA in seconds: 10943787.023\n",
      "epoch: 278700, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 10946209.484\n",
      "epoch: 278800, train loss: 4.055165576934814, val loss: 4.065321826934815, ETA in seconds: 10948572.063\n",
      "epoch: 278900, train loss: 4.0530171394348145, val loss: 4.075282764434815, ETA in seconds: 10951025.145\n",
      "epoch: 279000, train loss: 4.055556201934815, val loss: 4.0666890144348145, ETA in seconds: 10953415.849\n",
      "epoch: 279100, train loss: 4.053212451934814, val loss: 4.064149951934814, ETA in seconds: 10955802.927\n",
      "epoch: 279200, train loss: 4.047353076934814, val loss: 4.064931201934814, ETA in seconds: 10958174.480\n",
      "epoch: 279300, train loss: 4.048524951934814, val loss: 4.0696187019348145, ETA in seconds: 10960594.329\n",
      "epoch: 279400, train loss: 4.052235889434814, val loss: 4.065517139434815, ETA in seconds: 10962980.502\n",
      "epoch: 279500, train loss: 4.058290576934814, val loss: 4.068446826934815, ETA in seconds: 10965442.985\n",
      "epoch: 279600, train loss: 4.054579639434815, val loss: 4.063954639434814, ETA in seconds: 10967692.184\n",
      "epoch: 279700, train loss: 4.062001514434814, val loss: 4.064540576934815, ETA in seconds: 10969777.379\n",
      "epoch: 279800, train loss: 4.049306201934814, val loss: 4.067470264434815, ETA in seconds: 10972165.581\n",
      "epoch: 279900, train loss: 4.061610889434815, val loss: 4.071376514434815, ETA in seconds: 10974473.313\n",
      "epoch: 280000, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 10976899.051\n",
      "epoch: 280100, train loss: 4.0491108894348145, val loss: 4.067079639434814, ETA in seconds: 10979384.126\n",
      "epoch: 280200, train loss: 4.052626514434815, val loss: 4.065517139434815, ETA in seconds: 10981844.932\n",
      "epoch: 280300, train loss: 4.044423389434814, val loss: 4.061610889434815, ETA in seconds: 10984043.466\n",
      "epoch: 280400, train loss: 4.051845264434815, val loss: 4.061610889434815, ETA in seconds: 10986358.883\n",
      "epoch: 280500, train loss: 4.0461812019348145, val loss: 4.071767139434814, ETA in seconds: 10988659.187\n",
      "epoch: 280600, train loss: 4.052431201934814, val loss: 4.068837451934814, ETA in seconds: 10990985.027\n",
      "epoch: 280700, train loss: 4.052821826934815, val loss: 4.0657124519348145, ETA in seconds: 10993299.056\n",
      "epoch: 280800, train loss: 4.048915576934815, val loss: 4.064931201934814, ETA in seconds: 10995686.447\n",
      "epoch: 280900, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 10998176.365\n",
      "epoch: 281000, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 11000456.975\n",
      "epoch: 281100, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 11002685.888\n",
      "epoch: 281200, train loss: 4.046376514434814, val loss: 4.0705952644348145, ETA in seconds: 11004737.169\n",
      "epoch: 281300, train loss: 4.054774951934815, val loss: 4.063954639434814, ETA in seconds: 11006936.981\n",
      "epoch: 281400, train loss: 4.043837451934815, val loss: 4.071181201934815, ETA in seconds: 11009208.625\n",
      "epoch: 281500, train loss: 4.048329639434814, val loss: 4.071376514434815, ETA in seconds: 11011655.218\n",
      "epoch: 281600, train loss: 4.054384326934814, val loss: 4.065907764434814, ETA in seconds: 11013869.527\n",
      "epoch: 281700, train loss: 4.043642139434814, val loss: 4.068056201934814, ETA in seconds: 11016235.903\n",
      "epoch: 281800, train loss: 4.049892139434815, val loss: 4.068056201934814, ETA in seconds: 11018785.871\n",
      "epoch: 281900, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 11021422.575\n",
      "epoch: 282000, train loss: 4.053407764434814, val loss: 4.071181201934815, ETA in seconds: 11023716.403\n",
      "epoch: 282100, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 11025971.020\n",
      "epoch: 282200, train loss: 4.062587451934815, val loss: 4.067470264434815, ETA in seconds: 11028307.026\n",
      "epoch: 282300, train loss: 4.0461812019348145, val loss: 4.0686421394348145, ETA in seconds: 11030694.368\n",
      "epoch: 282400, train loss: 4.056142139434814, val loss: 4.070009326934814, ETA in seconds: 11032971.310\n",
      "epoch: 282500, train loss: 4.045399951934814, val loss: 4.071376514434815, ETA in seconds: 11035285.070\n",
      "epoch: 282600, train loss: 4.055360889434814, val loss: 4.073134326934815, ETA in seconds: 11037521.656\n",
      "epoch: 282700, train loss: 4.050282764434814, val loss: 4.069423389434815, ETA in seconds: 11039746.413\n",
      "epoch: 282800, train loss: 4.059657764434815, val loss: 4.070009326934814, ETA in seconds: 11041727.594\n",
      "epoch: 282900, train loss: 4.050478076934814, val loss: 4.068056201934814, ETA in seconds: 11044008.554\n",
      "epoch: 283000, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 11046261.909\n",
      "epoch: 283100, train loss: 4.0578999519348145, val loss: 4.064540576934815, ETA in seconds: 11048610.493\n",
      "epoch: 283200, train loss: 4.054774951934815, val loss: 4.067274951934815, ETA in seconds: 11050911.064\n",
      "epoch: 283300, train loss: 4.049696826934815, val loss: 4.072353076934815, ETA in seconds: 11053174.790\n",
      "epoch: 283400, train loss: 4.053407764434814, val loss: 4.0647358894348145, ETA in seconds: 11055317.570\n",
      "epoch: 283500, train loss: 4.0461812019348145, val loss: 4.066298389434815, ETA in seconds: 11057499.223\n",
      "epoch: 283600, train loss: 4.0500874519348145, val loss: 4.069423389434815, ETA in seconds: 11059997.125\n",
      "epoch: 283700, train loss: 4.051845264434815, val loss: 4.070985889434814, ETA in seconds: 11062436.103\n",
      "epoch: 283800, train loss: 4.052235889434814, val loss: 4.073915576934814, ETA in seconds: 11064843.715\n",
      "epoch: 283900, train loss: 4.052626514434815, val loss: 4.068056201934814, ETA in seconds: 11067125.396\n",
      "epoch: 284000, train loss: 4.046767139434815, val loss: 4.068251514434815, ETA in seconds: 11069558.719\n",
      "epoch: 284100, train loss: 4.051649951934815, val loss: 4.0686421394348145, ETA in seconds: 11072283.176\n",
      "epoch: 284200, train loss: 4.053603076934815, val loss: 4.064540576934815, ETA in seconds: 11074644.181\n",
      "epoch: 284300, train loss: 4.0549702644348145, val loss: 4.063954639434814, ETA in seconds: 11077000.056\n",
      "epoch: 284400, train loss: 4.044423389434814, val loss: 4.070985889434814, ETA in seconds: 11079184.209\n",
      "epoch: 284500, train loss: 4.0549702644348145, val loss: 4.065321826934815, ETA in seconds: 11081597.318\n",
      "epoch: 284600, train loss: 4.051454639434814, val loss: 4.067860889434814, ETA in seconds: 11084114.281\n",
      "epoch: 284700, train loss: 4.054384326934814, val loss: 4.068056201934814, ETA in seconds: 11086533.166\n",
      "epoch: 284800, train loss: 4.046767139434815, val loss: 4.0676655769348145, ETA in seconds: 11088834.020\n",
      "epoch: 284900, train loss: 4.046571826934814, val loss: 4.0745015144348145, ETA in seconds: 11090939.562\n",
      "epoch: 285000, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 11093260.650\n",
      "epoch: 285100, train loss: 4.047548389434814, val loss: 4.067860889434814, ETA in seconds: 11095521.129\n",
      "epoch: 285200, train loss: 4.048915576934815, val loss: 4.065126514434814, ETA in seconds: 11097789.355\n",
      "epoch: 285300, train loss: 4.0559468269348145, val loss: 4.064540576934815, ETA in seconds: 11100093.452\n",
      "epoch: 285400, train loss: 4.045985889434815, val loss: 4.0637593269348145, ETA in seconds: 11102319.218\n",
      "epoch: 285500, train loss: 4.0530171394348145, val loss: 4.061610889434815, ETA in seconds: 11104947.702\n",
      "epoch: 285600, train loss: 4.051259326934814, val loss: 4.0715718269348145, ETA in seconds: 11107427.866\n",
      "epoch: 285700, train loss: 4.052235889434814, val loss: 4.068056201934814, ETA in seconds: 11109854.697\n",
      "epoch: 285800, train loss: 4.047939014434815, val loss: 4.067274951934815, ETA in seconds: 11112399.816\n",
      "epoch: 285900, train loss: 4.054189014434814, val loss: 4.071962451934814, ETA in seconds: 11114688.721\n",
      "epoch: 286000, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 11117177.519\n",
      "epoch: 286100, train loss: 4.054384326934814, val loss: 4.065321826934815, ETA in seconds: 11119435.380\n",
      "epoch: 286200, train loss: 4.049892139434815, val loss: 4.067860889434814, ETA in seconds: 11121847.084\n",
      "epoch: 286300, train loss: 4.055165576934814, val loss: 4.070790576934814, ETA in seconds: 11124253.445\n",
      "epoch: 286400, train loss: 4.0539937019348145, val loss: 4.0705952644348145, ETA in seconds: 11126546.947\n",
      "epoch: 286500, train loss: 4.046767139434815, val loss: 4.0637593269348145, ETA in seconds: 11128917.346\n",
      "epoch: 286600, train loss: 4.047353076934814, val loss: 4.062978076934814, ETA in seconds: 11131356.628\n",
      "epoch: 286700, train loss: 4.047743701934815, val loss: 4.070204639434815, ETA in seconds: 11133763.177\n",
      "epoch: 286800, train loss: 4.0500874519348145, val loss: 4.064149951934814, ETA in seconds: 11135985.918\n",
      "epoch: 286900, train loss: 4.048329639434814, val loss: 4.070985889434814, ETA in seconds: 11138379.664\n",
      "epoch: 287000, train loss: 4.053212451934814, val loss: 4.0647358894348145, ETA in seconds: 11140629.337\n",
      "epoch: 287100, train loss: 4.050868701934815, val loss: 4.068251514434815, ETA in seconds: 11143011.483\n",
      "epoch: 287200, train loss: 4.059267139434814, val loss: 4.0686421394348145, ETA in seconds: 11145388.564\n",
      "epoch: 287300, train loss: 4.0491108894348145, val loss: 4.064540576934815, ETA in seconds: 11147749.859\n",
      "epoch: 287400, train loss: 4.050478076934814, val loss: 4.069814014434814, ETA in seconds: 11149979.337\n",
      "epoch: 287500, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 11152292.768\n",
      "epoch: 287600, train loss: 4.048720264434815, val loss: 4.063173389434814, ETA in seconds: 11154591.050\n",
      "epoch: 287700, train loss: 4.057314014434814, val loss: 4.068446826934815, ETA in seconds: 11156881.907\n",
      "epoch: 287800, train loss: 4.047548389434814, val loss: 4.0686421394348145, ETA in seconds: 11159260.446\n",
      "epoch: 287900, train loss: 4.057509326934815, val loss: 4.0666890144348145, ETA in seconds: 11161697.309\n",
      "epoch: 288000, train loss: 4.057314014434814, val loss: 4.066298389434815, ETA in seconds: 11164126.934\n",
      "epoch: 288100, train loss: 4.049696826934815, val loss: 4.071181201934815, ETA in seconds: 11166692.646\n",
      "epoch: 288200, train loss: 4.054384326934814, val loss: 4.066298389434815, ETA in seconds: 11169492.856\n",
      "epoch: 288300, train loss: 4.050673389434815, val loss: 4.066884326934814, ETA in seconds: 11171865.601\n",
      "epoch: 288400, train loss: 4.050673389434815, val loss: 4.064149951934814, ETA in seconds: 11174190.489\n",
      "epoch: 288500, train loss: 4.054774951934815, val loss: 4.069032764434814, ETA in seconds: 11176527.994\n",
      "epoch: 288600, train loss: 4.054189014434814, val loss: 4.071181201934815, ETA in seconds: 11178879.212\n",
      "epoch: 288700, train loss: 4.049306201934814, val loss: 4.067860889434814, ETA in seconds: 11181275.604\n",
      "epoch: 288800, train loss: 4.046376514434814, val loss: 4.066493701934815, ETA in seconds: 11183516.833\n",
      "epoch: 288900, train loss: 4.052235889434814, val loss: 4.066103076934814, ETA in seconds: 11185904.697\n",
      "epoch: 289000, train loss: 4.051259326934814, val loss: 4.071376514434815, ETA in seconds: 11188176.918\n",
      "epoch: 289100, train loss: 4.051845264434815, val loss: 4.065517139434815, ETA in seconds: 11190353.005\n",
      "epoch: 289200, train loss: 4.050478076934814, val loss: 4.064931201934814, ETA in seconds: 11192636.662\n",
      "epoch: 289300, train loss: 4.054189014434814, val loss: 4.0666890144348145, ETA in seconds: 11195098.544\n",
      "epoch: 289400, train loss: 4.050868701934815, val loss: 4.063954639434814, ETA in seconds: 11197317.282\n",
      "epoch: 289500, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 11199576.045\n",
      "epoch: 289600, train loss: 4.0471577644348145, val loss: 4.0618062019348145, ETA in seconds: 11201714.707\n",
      "epoch: 289700, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 11203853.330\n",
      "epoch: 289800, train loss: 4.057704639434815, val loss: 4.064931201934814, ETA in seconds: 11206090.899\n",
      "epoch: 289900, train loss: 4.055751514434815, val loss: 4.064931201934814, ETA in seconds: 11208414.297\n",
      "epoch: 290000, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 11210625.163\n",
      "epoch: 290100, train loss: 4.050868701934815, val loss: 4.065517139434815, ETA in seconds: 11212808.244\n",
      "epoch: 290200, train loss: 4.0559468269348145, val loss: 4.0715718269348145, ETA in seconds: 11214569.282\n",
      "epoch: 290300, train loss: 4.054579639434815, val loss: 4.0598530769348145, ETA in seconds: 11216700.736\n",
      "epoch: 290400, train loss: 4.046571826934814, val loss: 4.074306201934815, ETA in seconds: 11218777.553\n",
      "epoch: 290500, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 11220945.165\n",
      "epoch: 290600, train loss: 4.045790576934815, val loss: 4.068446826934815, ETA in seconds: 11223143.329\n",
      "epoch: 290700, train loss: 4.055556201934815, val loss: 4.0676655769348145, ETA in seconds: 11225273.395\n",
      "epoch: 290800, train loss: 4.052821826934815, val loss: 4.0696187019348145, ETA in seconds: 11226907.798\n",
      "epoch: 290900, train loss: 4.053407764434814, val loss: 4.067274951934815, ETA in seconds: 11229165.649\n",
      "epoch: 291000, train loss: 4.050868701934815, val loss: 4.0686421394348145, ETA in seconds: 11231341.578\n",
      "epoch: 291100, train loss: 4.056337451934814, val loss: 4.067470264434815, ETA in seconds: 11233577.007\n",
      "epoch: 291200, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 11235788.525\n",
      "epoch: 291300, train loss: 4.048915576934815, val loss: 4.066493701934815, ETA in seconds: 11238060.418\n",
      "epoch: 291400, train loss: 4.0491108894348145, val loss: 4.0647358894348145, ETA in seconds: 11240281.936\n",
      "epoch: 291500, train loss: 4.054774951934815, val loss: 4.070204639434815, ETA in seconds: 11242472.446\n",
      "epoch: 291600, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 11244627.788\n",
      "epoch: 291700, train loss: 4.053407764434814, val loss: 4.068251514434815, ETA in seconds: 11247045.341\n",
      "epoch: 291800, train loss: 4.0500874519348145, val loss: 4.070790576934814, ETA in seconds: 11249289.669\n",
      "epoch: 291900, train loss: 4.050478076934814, val loss: 4.0666890144348145, ETA in seconds: 11251446.858\n",
      "epoch: 292000, train loss: 4.050282764434814, val loss: 4.065126514434814, ETA in seconds: 11254145.404\n",
      "epoch: 292100, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 11256484.350\n",
      "epoch: 292200, train loss: 4.056142139434814, val loss: 4.067079639434814, ETA in seconds: 11258762.552\n",
      "epoch: 292300, train loss: 4.0549702644348145, val loss: 4.0647358894348145, ETA in seconds: 11260959.993\n",
      "epoch: 292400, train loss: 4.055751514434815, val loss: 4.067470264434815, ETA in seconds: 11263135.489\n",
      "epoch: 292500, train loss: 4.046962451934815, val loss: 4.067274951934815, ETA in seconds: 11265326.000\n",
      "epoch: 292600, train loss: 4.046376514434814, val loss: 4.067274951934815, ETA in seconds: 11267431.326\n",
      "epoch: 292700, train loss: 4.0530171394348145, val loss: 4.061220264434814, ETA in seconds: 11269702.130\n",
      "epoch: 292800, train loss: 4.0549702644348145, val loss: 4.067274951934815, ETA in seconds: 11271921.479\n",
      "epoch: 292900, train loss: 4.0530171394348145, val loss: 4.066103076934814, ETA in seconds: 11274266.883\n",
      "epoch: 293000, train loss: 4.045790576934815, val loss: 4.0637593269348145, ETA in seconds: 11276443.707\n",
      "epoch: 293100, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 11278701.123\n",
      "epoch: 293200, train loss: 4.046767139434815, val loss: 4.071767139434814, ETA in seconds: 11281035.307\n",
      "epoch: 293300, train loss: 4.054384326934814, val loss: 4.067274951934815, ETA in seconds: 11283138.373\n",
      "epoch: 293400, train loss: 4.056728076934815, val loss: 4.0705952644348145, ETA in seconds: 11285401.873\n",
      "epoch: 293500, train loss: 4.0559468269348145, val loss: 4.060048389434814, ETA in seconds: 11287571.980\n",
      "epoch: 293600, train loss: 4.050478076934814, val loss: 4.0647358894348145, ETA in seconds: 11289929.404\n",
      "epoch: 293700, train loss: 4.048524951934814, val loss: 4.068251514434815, ETA in seconds: 11292381.268\n",
      "epoch: 293800, train loss: 4.050282764434814, val loss: 4.067274951934815, ETA in seconds: 11294508.050\n",
      "epoch: 293900, train loss: 4.050478076934814, val loss: 4.062587451934815, ETA in seconds: 11296758.782\n",
      "epoch: 294000, train loss: 4.047353076934814, val loss: 4.063954639434814, ETA in seconds: 11298969.937\n",
      "epoch: 294100, train loss: 4.053798389434815, val loss: 4.069423389434815, ETA in seconds: 11301087.659\n",
      "epoch: 294200, train loss: 4.057509326934815, val loss: 4.0686421394348145, ETA in seconds: 11303381.901\n",
      "epoch: 294300, train loss: 4.050868701934815, val loss: 4.068446826934815, ETA in seconds: 11305615.021\n",
      "epoch: 294400, train loss: 4.053407764434814, val loss: 4.065126514434814, ETA in seconds: 11307678.431\n",
      "epoch: 294500, train loss: 4.057118701934814, val loss: 4.066298389434815, ETA in seconds: 11309863.367\n",
      "epoch: 294600, train loss: 4.0432515144348145, val loss: 4.066298389434815, ETA in seconds: 11311993.987\n",
      "epoch: 294700, train loss: 4.054774951934815, val loss: 4.072157764434815, ETA in seconds: 11314145.926\n",
      "epoch: 294800, train loss: 4.053603076934815, val loss: 4.069032764434814, ETA in seconds: 11316238.280\n",
      "epoch: 294900, train loss: 4.0559468269348145, val loss: 4.071767139434814, ETA in seconds: 11318403.403\n",
      "epoch: 295000, train loss: 4.052626514434815, val loss: 4.0637593269348145, ETA in seconds: 11320549.590\n",
      "epoch: 295100, train loss: 4.050282764434814, val loss: 4.066103076934814, ETA in seconds: 11322782.608\n",
      "epoch: 295200, train loss: 4.053603076934815, val loss: 4.062587451934815, ETA in seconds: 11325056.363\n",
      "epoch: 295300, train loss: 4.049501514434814, val loss: 4.0676655769348145, ETA in seconds: 11327328.513\n",
      "epoch: 295400, train loss: 4.051649951934815, val loss: 4.072157764434815, ETA in seconds: 11329625.455\n",
      "epoch: 295500, train loss: 4.051649951934815, val loss: 4.065517139434815, ETA in seconds: 11331973.605\n",
      "epoch: 295600, train loss: 4.045399951934814, val loss: 4.067860889434814, ETA in seconds: 11334272.356\n",
      "epoch: 295700, train loss: 4.0471577644348145, val loss: 4.067470264434815, ETA in seconds: 11336600.440\n",
      "epoch: 295800, train loss: 4.0520405769348145, val loss: 4.0745015144348145, ETA in seconds: 11338925.024\n",
      "epoch: 295900, train loss: 4.058290576934814, val loss: 4.072353076934815, ETA in seconds: 11341401.363\n",
      "epoch: 296000, train loss: 4.047353076934814, val loss: 4.069032764434814, ETA in seconds: 11343646.521\n",
      "epoch: 296100, train loss: 4.055360889434814, val loss: 4.070399951934815, ETA in seconds: 11345782.398\n",
      "epoch: 296200, train loss: 4.050282764434814, val loss: 4.071767139434814, ETA in seconds: 11347939.812\n",
      "epoch: 296300, train loss: 4.053212451934814, val loss: 4.068251514434815, ETA in seconds: 11350177.290\n",
      "epoch: 296400, train loss: 4.052431201934814, val loss: 4.061415576934815, ETA in seconds: 11352431.476\n",
      "epoch: 296500, train loss: 4.056337451934814, val loss: 4.063173389434814, ETA in seconds: 11354572.554\n",
      "epoch: 296600, train loss: 4.0471577644348145, val loss: 4.072939014434814, ETA in seconds: 11356905.170\n",
      "epoch: 296700, train loss: 4.048915576934815, val loss: 4.066884326934814, ETA in seconds: 11358998.187\n",
      "epoch: 296800, train loss: 4.0598530769348145, val loss: 4.067860889434814, ETA in seconds: 11361233.505\n",
      "epoch: 296900, train loss: 4.0471577644348145, val loss: 4.072743701934814, ETA in seconds: 11363512.288\n",
      "epoch: 297000, train loss: 4.0481343269348145, val loss: 4.062587451934815, ETA in seconds: 11365706.775\n",
      "epoch: 297100, train loss: 4.054579639434815, val loss: 4.070399951934815, ETA in seconds: 11368055.042\n",
      "epoch: 297200, train loss: 4.0539937019348145, val loss: 4.071962451934814, ETA in seconds: 11370295.994\n",
      "epoch: 297300, train loss: 4.048720264434815, val loss: 4.059071826934814, ETA in seconds: 11372532.059\n",
      "epoch: 297400, train loss: 4.0539937019348145, val loss: 4.066298389434815, ETA in seconds: 11374592.785\n",
      "epoch: 297500, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 11376775.993\n",
      "epoch: 297600, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 11379356.601\n",
      "epoch: 297700, train loss: 4.049696826934815, val loss: 4.071181201934815, ETA in seconds: 11381713.587\n",
      "epoch: 297800, train loss: 4.046767139434815, val loss: 4.065321826934815, ETA in seconds: 11384033.565\n",
      "epoch: 297900, train loss: 4.052235889434814, val loss: 4.062001514434814, ETA in seconds: 11386212.127\n",
      "epoch: 298000, train loss: 4.0520405769348145, val loss: 4.0666890144348145, ETA in seconds: 11388518.144\n",
      "epoch: 298100, train loss: 4.051845264434815, val loss: 4.069228076934815, ETA in seconds: 11390853.851\n",
      "epoch: 298200, train loss: 4.0500874519348145, val loss: 4.070204639434815, ETA in seconds: 11392982.059\n",
      "epoch: 298300, train loss: 4.054189014434814, val loss: 4.068837451934814, ETA in seconds: 11395114.007\n",
      "epoch: 298400, train loss: 4.050868701934815, val loss: 4.068837451934814, ETA in seconds: 11397293.189\n",
      "epoch: 298500, train loss: 4.049501514434814, val loss: 4.068446826934815, ETA in seconds: 11399387.642\n",
      "epoch: 298600, train loss: 4.0520405769348145, val loss: 4.070009326934814, ETA in seconds: 11401741.018\n",
      "epoch: 298700, train loss: 4.052235889434814, val loss: 4.065907764434814, ETA in seconds: 11404060.465\n",
      "epoch: 298800, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 11406368.795\n",
      "epoch: 298900, train loss: 4.052821826934815, val loss: 4.0696187019348145, ETA in seconds: 11408445.092\n",
      "epoch: 299000, train loss: 4.055751514434815, val loss: 4.070204639434815, ETA in seconds: 11410603.279\n",
      "epoch: 299100, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 11412702.666\n",
      "epoch: 299200, train loss: 4.0559468269348145, val loss: 4.061220264434814, ETA in seconds: 11414948.666\n",
      "epoch: 299300, train loss: 4.051259326934814, val loss: 4.069228076934815, ETA in seconds: 11417008.426\n",
      "epoch: 299400, train loss: 4.047939014434815, val loss: 4.0745015144348145, ETA in seconds: 11419154.214\n",
      "epoch: 299500, train loss: 4.047353076934814, val loss: 4.066298389434815, ETA in seconds: 11421319.402\n",
      "epoch: 299600, train loss: 4.048915576934815, val loss: 4.067079639434814, ETA in seconds: 11423410.546\n",
      "epoch: 299700, train loss: 4.051649951934815, val loss: 4.070790576934814, ETA in seconds: 11425591.396\n",
      "epoch: 299800, train loss: 4.058095264434814, val loss: 4.064540576934815, ETA in seconds: 11427719.711\n",
      "epoch: 299900, train loss: 4.055360889434814, val loss: 4.071181201934815, ETA in seconds: 11429909.269\n",
      "epoch: 300000, train loss: 4.0471577644348145, val loss: 4.063954639434814, ETA in seconds: 11432033.046\n",
      "epoch: 300100, train loss: 4.051845264434815, val loss: 4.067274951934815, ETA in seconds: 11434083.816\n",
      "epoch: 300200, train loss: 4.053212451934814, val loss: 4.062978076934814, ETA in seconds: 11436293.729\n",
      "epoch: 300300, train loss: 4.0481343269348145, val loss: 4.0627827644348145, ETA in seconds: 11438367.040\n",
      "epoch: 300400, train loss: 4.054189014434814, val loss: 4.066103076934814, ETA in seconds: 11440382.319\n",
      "epoch: 300500, train loss: 4.049501514434814, val loss: 4.062001514434814, ETA in seconds: 11442401.302\n",
      "epoch: 300600, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 11444518.529\n",
      "epoch: 300700, train loss: 4.053407764434814, val loss: 4.072743701934814, ETA in seconds: 11446631.637\n",
      "epoch: 300800, train loss: 4.0530171394348145, val loss: 4.070009326934814, ETA in seconds: 11448802.691\n",
      "epoch: 300900, train loss: 4.052626514434815, val loss: 4.065517139434815, ETA in seconds: 11450949.745\n",
      "epoch: 301000, train loss: 4.054384326934814, val loss: 4.066298389434815, ETA in seconds: 11452897.457\n",
      "epoch: 301100, train loss: 4.053798389434815, val loss: 4.0686421394348145, ETA in seconds: 11454896.703\n",
      "epoch: 301200, train loss: 4.043446826934814, val loss: 4.069032764434814, ETA in seconds: 11457044.243\n",
      "epoch: 301300, train loss: 4.050282764434814, val loss: 4.063954639434814, ETA in seconds: 11459273.367\n",
      "epoch: 301400, train loss: 4.051845264434815, val loss: 4.064149951934814, ETA in seconds: 11461736.612\n",
      "epoch: 301500, train loss: 4.049306201934814, val loss: 4.064540576934815, ETA in seconds: 11464047.964\n",
      "epoch: 301600, train loss: 4.053407764434814, val loss: 4.073329639434815, ETA in seconds: 11466300.167\n",
      "epoch: 301700, train loss: 4.055751514434815, val loss: 4.065126514434814, ETA in seconds: 11468553.844\n",
      "epoch: 301800, train loss: 4.055556201934815, val loss: 4.067274951934815, ETA in seconds: 11470857.311\n",
      "epoch: 301900, train loss: 4.052821826934815, val loss: 4.068056201934814, ETA in seconds: 11473044.643\n",
      "epoch: 302000, train loss: 4.0530171394348145, val loss: 4.0657124519348145, ETA in seconds: 11475170.895\n",
      "epoch: 302100, train loss: 4.047939014434815, val loss: 4.069032764434814, ETA in seconds: 11477313.724\n",
      "epoch: 302200, train loss: 4.054579639434815, val loss: 4.075673389434814, ETA in seconds: 11479480.540\n",
      "epoch: 302300, train loss: 4.054774951934815, val loss: 4.064345264434815, ETA in seconds: 11481556.346\n",
      "epoch: 302400, train loss: 4.048329639434814, val loss: 4.067860889434814, ETA in seconds: 11483877.846\n",
      "epoch: 302500, train loss: 4.053407764434814, val loss: 4.0666890144348145, ETA in seconds: 11486107.412\n",
      "epoch: 302600, train loss: 4.050673389434815, val loss: 4.0647358894348145, ETA in seconds: 11488390.583\n",
      "epoch: 302700, train loss: 4.059267139434814, val loss: 4.064540576934815, ETA in seconds: 11490766.113\n",
      "epoch: 302800, train loss: 4.058095264434814, val loss: 4.066103076934814, ETA in seconds: 11493016.045\n",
      "epoch: 302900, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 11495133.974\n",
      "epoch: 303000, train loss: 4.047743701934815, val loss: 4.067860889434814, ETA in seconds: 11497235.270\n",
      "epoch: 303100, train loss: 4.055360889434814, val loss: 4.068446826934815, ETA in seconds: 11499441.845\n",
      "epoch: 303200, train loss: 4.0481343269348145, val loss: 4.065907764434814, ETA in seconds: 11501549.041\n",
      "epoch: 303300, train loss: 4.044814014434815, val loss: 4.0666890144348145, ETA in seconds: 11503581.889\n",
      "epoch: 303400, train loss: 4.050282764434814, val loss: 4.070009326934814, ETA in seconds: 11505821.383\n",
      "epoch: 303500, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 11507873.541\n",
      "epoch: 303600, train loss: 4.049306201934814, val loss: 4.064149951934814, ETA in seconds: 11509826.056\n",
      "epoch: 303700, train loss: 4.049892139434815, val loss: 4.072743701934814, ETA in seconds: 11511891.303\n",
      "epoch: 303800, train loss: 4.058485889434815, val loss: 4.0696187019348145, ETA in seconds: 11513984.665\n",
      "epoch: 303900, train loss: 4.049892139434815, val loss: 4.063173389434814, ETA in seconds: 11516126.981\n",
      "epoch: 304000, train loss: 4.051845264434815, val loss: 4.066103076934814, ETA in seconds: 11518221.401\n",
      "epoch: 304100, train loss: 4.055360889434814, val loss: 4.073329639434815, ETA in seconds: 11520267.740\n",
      "epoch: 304200, train loss: 4.045985889434815, val loss: 4.066493701934815, ETA in seconds: 11522339.540\n",
      "epoch: 304300, train loss: 4.054384326934814, val loss: 4.064540576934815, ETA in seconds: 11524421.033\n",
      "epoch: 304400, train loss: 4.0510640144348145, val loss: 4.0666890144348145, ETA in seconds: 11526457.195\n",
      "epoch: 304500, train loss: 4.053798389434815, val loss: 4.070009326934814, ETA in seconds: 11528497.538\n",
      "epoch: 304600, train loss: 4.0510640144348145, val loss: 4.062587451934815, ETA in seconds: 11530506.721\n",
      "epoch: 304700, train loss: 4.051259326934814, val loss: 4.067860889434814, ETA in seconds: 11532542.119\n",
      "epoch: 304800, train loss: 4.056532764434815, val loss: 4.0715718269348145, ETA in seconds: 11534769.015\n",
      "epoch: 304900, train loss: 4.0530171394348145, val loss: 4.066103076934814, ETA in seconds: 11536851.600\n",
      "epoch: 305000, train loss: 4.054579639434815, val loss: 4.064149951934814, ETA in seconds: 11539007.380\n",
      "epoch: 305100, train loss: 4.046376514434814, val loss: 4.066884326934814, ETA in seconds: 11541151.919\n",
      "epoch: 305200, train loss: 4.056532764434815, val loss: 4.069814014434814, ETA in seconds: 11543118.070\n",
      "epoch: 305300, train loss: 4.053407764434814, val loss: 4.060243701934814, ETA in seconds: 11545137.529\n",
      "epoch: 305400, train loss: 4.055165576934814, val loss: 4.0754780769348145, ETA in seconds: 11547221.079\n",
      "epoch: 305500, train loss: 4.0500874519348145, val loss: 4.064345264434815, ETA in seconds: 11549165.395\n",
      "epoch: 305600, train loss: 4.052626514434815, val loss: 4.064345264434815, ETA in seconds: 11551277.206\n",
      "epoch: 305700, train loss: 4.054579639434815, val loss: 4.066493701934815, ETA in seconds: 11553529.674\n",
      "epoch: 305800, train loss: 4.051845264434815, val loss: 4.068056201934814, ETA in seconds: 11555688.969\n",
      "epoch: 305900, train loss: 4.055751514434815, val loss: 4.069032764434814, ETA in seconds: 11557773.065\n",
      "epoch: 306000, train loss: 4.055556201934815, val loss: 4.062587451934815, ETA in seconds: 11559817.089\n",
      "epoch: 306100, train loss: 4.056337451934814, val loss: 4.065517139434815, ETA in seconds: 11561832.678\n",
      "epoch: 306200, train loss: 4.0539937019348145, val loss: 4.0608296394348145, ETA in seconds: 11563919.111\n",
      "epoch: 306300, train loss: 4.049892139434815, val loss: 4.062392139434815, ETA in seconds: 11565998.702\n",
      "epoch: 306400, train loss: 4.056142139434814, val loss: 4.071767139434814, ETA in seconds: 11567981.233\n",
      "epoch: 306500, train loss: 4.056532764434815, val loss: 4.064149951934814, ETA in seconds: 11570026.117\n",
      "epoch: 306600, train loss: 4.0520405769348145, val loss: 4.0666890144348145, ETA in seconds: 11572038.443\n",
      "epoch: 306700, train loss: 4.051649951934815, val loss: 4.070790576934814, ETA in seconds: 11574342.437\n",
      "epoch: 306800, train loss: 4.0422749519348145, val loss: 4.063564014434815, ETA in seconds: 11576436.415\n",
      "epoch: 306900, train loss: 4.0481343269348145, val loss: 4.068837451934814, ETA in seconds: 11578564.522\n",
      "epoch: 307000, train loss: 4.052821826934815, val loss: 4.066884326934814, ETA in seconds: 11580623.526\n",
      "epoch: 307100, train loss: 4.052821826934815, val loss: 4.067470264434815, ETA in seconds: 11582882.265\n",
      "epoch: 307200, train loss: 4.056337451934814, val loss: 4.070790576934814, ETA in seconds: 11585155.305\n",
      "epoch: 307300, train loss: 4.049501514434814, val loss: 4.0657124519348145, ETA in seconds: 11587259.214\n",
      "epoch: 307400, train loss: 4.057118701934814, val loss: 4.0705952644348145, ETA in seconds: 11589435.392\n",
      "epoch: 307500, train loss: 4.055751514434815, val loss: 4.069032764434814, ETA in seconds: 11591600.425\n",
      "epoch: 307600, train loss: 4.0559468269348145, val loss: 4.072939014434814, ETA in seconds: 11593828.441\n",
      "epoch: 307700, train loss: 4.047548389434814, val loss: 4.071376514434815, ETA in seconds: 11595982.050\n",
      "epoch: 307800, train loss: 4.0549702644348145, val loss: 4.070399951934815, ETA in seconds: 11598160.273\n",
      "epoch: 307900, train loss: 4.052235889434814, val loss: 4.0608296394348145, ETA in seconds: 11600364.745\n",
      "epoch: 308000, train loss: 4.0530171394348145, val loss: 4.0666890144348145, ETA in seconds: 11602490.923\n",
      "epoch: 308100, train loss: 4.056337451934814, val loss: 4.068251514434815, ETA in seconds: 11604753.786\n",
      "epoch: 308200, train loss: 4.055360889434814, val loss: 4.0696187019348145, ETA in seconds: 11606853.123\n",
      "epoch: 308300, train loss: 4.049892139434815, val loss: 4.062392139434815, ETA in seconds: 11609004.472\n",
      "epoch: 308400, train loss: 4.057118701934814, val loss: 4.074110889434815, ETA in seconds: 11611138.897\n",
      "epoch: 308500, train loss: 4.051649951934815, val loss: 4.062978076934814, ETA in seconds: 11613145.622\n",
      "epoch: 308600, train loss: 4.047548389434814, val loss: 4.0666890144348145, ETA in seconds: 11615291.050\n",
      "epoch: 308700, train loss: 4.0510640144348145, val loss: 4.067860889434814, ETA in seconds: 11617247.966\n",
      "epoch: 308800, train loss: 4.052235889434814, val loss: 4.0657124519348145, ETA in seconds: 11619250.115\n",
      "epoch: 308900, train loss: 4.053798389434815, val loss: 4.068837451934814, ETA in seconds: 11621434.159\n",
      "epoch: 309000, train loss: 4.050478076934814, val loss: 4.0676655769348145, ETA in seconds: 11623493.678\n",
      "epoch: 309100, train loss: 4.053798389434815, val loss: 4.0666890144348145, ETA in seconds: 11625519.348\n",
      "epoch: 309200, train loss: 4.043446826934814, val loss: 4.066103076934814, ETA in seconds: 11627582.047\n",
      "epoch: 309300, train loss: 4.0481343269348145, val loss: 4.068446826934815, ETA in seconds: 11629585.769\n",
      "epoch: 309400, train loss: 4.051454639434814, val loss: 4.069423389434815, ETA in seconds: 11631701.249\n",
      "epoch: 309500, train loss: 4.0539937019348145, val loss: 4.069032764434814, ETA in seconds: 11633775.251\n",
      "epoch: 309600, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 11635830.928\n",
      "epoch: 309700, train loss: 4.0578999519348145, val loss: 4.071181201934815, ETA in seconds: 11637772.786\n",
      "epoch: 309800, train loss: 4.0569233894348145, val loss: 4.0666890144348145, ETA in seconds: 11639892.329\n",
      "epoch: 309900, train loss: 4.049306201934814, val loss: 4.067470264434815, ETA in seconds: 11641951.277\n",
      "epoch: 310000, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 11643983.978\n",
      "epoch: 310100, train loss: 4.0539937019348145, val loss: 4.068251514434815, ETA in seconds: 11645953.668\n",
      "epoch: 310200, train loss: 4.044618701934814, val loss: 4.0608296394348145, ETA in seconds: 11647951.001\n",
      "epoch: 310300, train loss: 4.053798389434815, val loss: 4.070204639434815, ETA in seconds: 11650114.863\n",
      "epoch: 310400, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 11652228.456\n",
      "epoch: 310500, train loss: 4.050282764434814, val loss: 4.070399951934815, ETA in seconds: 11654391.458\n",
      "epoch: 310600, train loss: 4.053407764434814, val loss: 4.0657124519348145, ETA in seconds: 11656567.606\n",
      "epoch: 310700, train loss: 4.056142139434814, val loss: 4.068056201934814, ETA in seconds: 11658592.788\n",
      "epoch: 310800, train loss: 4.050673389434815, val loss: 4.0657124519348145, ETA in seconds: 11660689.603\n",
      "epoch: 310900, train loss: 4.048329639434814, val loss: 4.066493701934815, ETA in seconds: 11662732.169\n",
      "epoch: 311000, train loss: 4.049501514434814, val loss: 4.065321826934815, ETA in seconds: 11664831.445\n",
      "epoch: 311100, train loss: 4.0549702644348145, val loss: 4.068837451934814, ETA in seconds: 11666966.078\n",
      "epoch: 311200, train loss: 4.0539937019348145, val loss: 4.0657124519348145, ETA in seconds: 11669230.187\n",
      "epoch: 311300, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 11671496.911\n",
      "epoch: 311400, train loss: 4.054189014434814, val loss: 4.066298389434815, ETA in seconds: 11673721.186\n",
      "epoch: 311500, train loss: 4.057509326934815, val loss: 4.068837451934814, ETA in seconds: 11675902.421\n",
      "epoch: 311600, train loss: 4.047939014434815, val loss: 4.0666890144348145, ETA in seconds: 11678050.080\n",
      "epoch: 311700, train loss: 4.044423389434814, val loss: 4.062392139434815, ETA in seconds: 11680107.469\n",
      "epoch: 311800, train loss: 4.045790576934815, val loss: 4.065321826934815, ETA in seconds: 11682208.725\n",
      "epoch: 311900, train loss: 4.057704639434815, val loss: 4.063368701934815, ETA in seconds: 11684252.374\n",
      "epoch: 312000, train loss: 4.052821826934815, val loss: 4.0657124519348145, ETA in seconds: 11686353.589\n",
      "epoch: 312100, train loss: 4.050868701934815, val loss: 4.065321826934815, ETA in seconds: 11688479.162\n",
      "epoch: 312200, train loss: 4.051649951934815, val loss: 4.066884326934814, ETA in seconds: 11690487.804\n",
      "epoch: 312300, train loss: 4.056142139434814, val loss: 4.062978076934814, ETA in seconds: 11692613.141\n",
      "epoch: 312400, train loss: 4.045009326934815, val loss: 4.070009326934814, ETA in seconds: 11694713.949\n",
      "epoch: 312500, train loss: 4.0500874519348145, val loss: 4.065126514434814, ETA in seconds: 11696552.857\n",
      "epoch: 312600, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 11698575.265\n",
      "epoch: 312700, train loss: 4.0471577644348145, val loss: 4.0686421394348145, ETA in seconds: 11700606.570\n",
      "epoch: 312800, train loss: 4.0510640144348145, val loss: 4.066103076934814, ETA in seconds: 11702614.089\n",
      "epoch: 312900, train loss: 4.054384326934814, val loss: 4.070204639434815, ETA in seconds: 11704732.991\n",
      "epoch: 313000, train loss: 4.045985889434815, val loss: 4.065907764434814, ETA in seconds: 11706696.535\n",
      "epoch: 313100, train loss: 4.053407764434814, val loss: 4.0696187019348145, ETA in seconds: 11708464.897\n",
      "epoch: 313200, train loss: 4.0569233894348145, val loss: 4.070790576934814, ETA in seconds: 11710225.887\n",
      "epoch: 313300, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 11712273.050\n",
      "epoch: 313400, train loss: 4.055751514434815, val loss: 4.064931201934814, ETA in seconds: 11714271.226\n",
      "epoch: 313500, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 11716381.833\n",
      "epoch: 313600, train loss: 4.044618701934814, val loss: 4.068446826934815, ETA in seconds: 11718369.421\n",
      "epoch: 313700, train loss: 4.040126514434815, val loss: 4.062196826934814, ETA in seconds: 11720432.775\n",
      "epoch: 313800, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 11722656.700\n",
      "epoch: 313900, train loss: 4.057314014434814, val loss: 4.0657124519348145, ETA in seconds: 11724660.279\n",
      "epoch: 314000, train loss: 4.049306201934814, val loss: 4.069423389434815, ETA in seconds: 11726618.731\n",
      "epoch: 314100, train loss: 4.048524951934814, val loss: 4.068446826934815, ETA in seconds: 11728607.925\n",
      "epoch: 314200, train loss: 4.054189014434814, val loss: 4.0647358894348145, ETA in seconds: 11730638.799\n",
      "epoch: 314300, train loss: 4.057118701934814, val loss: 4.070399951934815, ETA in seconds: 11732611.731\n",
      "epoch: 314400, train loss: 4.052431201934814, val loss: 4.065126514434814, ETA in seconds: 11734470.579\n",
      "epoch: 314500, train loss: 4.0510640144348145, val loss: 4.069814014434814, ETA in seconds: 11736405.359\n",
      "epoch: 314600, train loss: 4.051259326934814, val loss: 4.0647358894348145, ETA in seconds: 11738512.824\n",
      "epoch: 314700, train loss: 4.048915576934815, val loss: 4.067860889434814, ETA in seconds: 11740656.063\n",
      "epoch: 314800, train loss: 4.047939014434815, val loss: 4.069032764434814, ETA in seconds: 11742666.068\n",
      "epoch: 314900, train loss: 4.053603076934815, val loss: 4.058290576934814, ETA in seconds: 11744639.758\n",
      "epoch: 315000, train loss: 4.049501514434814, val loss: 4.069228076934815, ETA in seconds: 11746525.051\n",
      "epoch: 315100, train loss: 4.055751514434815, val loss: 4.0705952644348145, ETA in seconds: 11748515.430\n",
      "epoch: 315200, train loss: 4.052235889434814, val loss: 4.073720264434814, ETA in seconds: 11750485.359\n",
      "epoch: 315300, train loss: 4.049696826934815, val loss: 4.066298389434815, ETA in seconds: 11752512.115\n",
      "epoch: 315400, train loss: 4.048524951934814, val loss: 4.072743701934814, ETA in seconds: 11754477.070\n",
      "epoch: 315500, train loss: 4.0491108894348145, val loss: 4.069814014434814, ETA in seconds: 11756599.486\n",
      "epoch: 315600, train loss: 4.053212451934814, val loss: 4.0666890144348145, ETA in seconds: 11758554.045\n",
      "epoch: 315700, train loss: 4.047548389434814, val loss: 4.062196826934814, ETA in seconds: 11760700.097\n",
      "epoch: 315800, train loss: 4.049306201934814, val loss: 4.068251514434815, ETA in seconds: 11762648.279\n",
      "epoch: 315900, train loss: 4.051845264434815, val loss: 4.064149951934814, ETA in seconds: 11764672.346\n",
      "epoch: 316000, train loss: 4.0491108894348145, val loss: 4.069423389434815, ETA in seconds: 11766743.024\n",
      "epoch: 316100, train loss: 4.051454639434814, val loss: 4.068837451934814, ETA in seconds: 11769121.731\n",
      "epoch: 316200, train loss: 4.057118701934814, val loss: 4.069423389434815, ETA in seconds: 11771344.811\n",
      "epoch: 316300, train loss: 4.053798389434815, val loss: 4.060439014434815, ETA in seconds: 11773443.409\n",
      "epoch: 316400, train loss: 4.059657764434815, val loss: 4.067860889434814, ETA in seconds: 11775359.207\n",
      "epoch: 316500, train loss: 4.045790576934815, val loss: 4.065517139434815, ETA in seconds: 11777306.122\n",
      "epoch: 316600, train loss: 4.048524951934814, val loss: 4.063368701934815, ETA in seconds: 11779300.555\n",
      "epoch: 316700, train loss: 4.054189014434814, val loss: 4.074110889434815, ETA in seconds: 11781264.962\n",
      "epoch: 316800, train loss: 4.050282764434814, val loss: 4.065517139434815, ETA in seconds: 11783299.605\n",
      "epoch: 316900, train loss: 4.046962451934815, val loss: 4.070399951934815, ETA in seconds: 11785407.854\n",
      "epoch: 317000, train loss: 4.052431201934814, val loss: 4.068251514434815, ETA in seconds: 11787379.044\n",
      "epoch: 317100, train loss: 4.048720264434815, val loss: 4.070009326934814, ETA in seconds: 11789453.106\n",
      "epoch: 317200, train loss: 4.054189014434814, val loss: 4.066884326934814, ETA in seconds: 11791630.492\n",
      "epoch: 317300, train loss: 4.055751514434815, val loss: 4.0657124519348145, ETA in seconds: 11793599.200\n",
      "epoch: 317400, train loss: 4.050868701934815, val loss: 4.070985889434814, ETA in seconds: 11795558.856\n",
      "epoch: 317500, train loss: 4.044423389434814, val loss: 4.062196826934814, ETA in seconds: 11797572.578\n",
      "epoch: 317600, train loss: 4.045009326934815, val loss: 4.071376514434815, ETA in seconds: 11799627.026\n",
      "epoch: 317700, train loss: 4.048915576934815, val loss: 4.061610889434815, ETA in seconds: 11801621.841\n",
      "epoch: 317800, train loss: 4.0452046394348145, val loss: 4.071767139434814, ETA in seconds: 11803584.042\n",
      "epoch: 317900, train loss: 4.049306201934814, val loss: 4.0696187019348145, ETA in seconds: 11805704.745\n",
      "epoch: 318000, train loss: 4.0481343269348145, val loss: 4.0686421394348145, ETA in seconds: 11807802.110\n",
      "epoch: 318100, train loss: 4.0491108894348145, val loss: 4.069228076934815, ETA in seconds: 11809850.604\n",
      "epoch: 318200, train loss: 4.0569233894348145, val loss: 4.0676655769348145, ETA in seconds: 11811700.975\n",
      "epoch: 318300, train loss: 4.0510640144348145, val loss: 4.063564014434815, ETA in seconds: 11813501.064\n",
      "epoch: 318400, train loss: 4.053798389434815, val loss: 4.067274951934815, ETA in seconds: 11815576.479\n",
      "epoch: 318500, train loss: 4.054579639434815, val loss: 4.067860889434814, ETA in seconds: 11817680.971\n",
      "epoch: 318600, train loss: 4.057314014434814, val loss: 4.061415576934815, ETA in seconds: 11819836.993\n",
      "epoch: 318700, train loss: 4.0539937019348145, val loss: 4.068251514434815, ETA in seconds: 11821692.654\n",
      "epoch: 318800, train loss: 4.0588765144348145, val loss: 4.068446826934815, ETA in seconds: 11823463.992\n",
      "epoch: 318900, train loss: 4.0588765144348145, val loss: 4.063954639434814, ETA in seconds: 11825284.155\n",
      "epoch: 319000, train loss: 4.050478076934814, val loss: 4.0618062019348145, ETA in seconds: 11827358.777\n",
      "epoch: 319100, train loss: 4.051845264434815, val loss: 4.065517139434815, ETA in seconds: 11829297.659\n",
      "epoch: 319200, train loss: 4.053798389434815, val loss: 4.068251514434815, ETA in seconds: 11831350.798\n",
      "epoch: 319300, train loss: 4.051259326934814, val loss: 4.064540576934815, ETA in seconds: 11833457.798\n",
      "epoch: 319400, train loss: 4.053212451934814, val loss: 4.068251514434815, ETA in seconds: 11835480.079\n",
      "epoch: 319500, train loss: 4.0608296394348145, val loss: 4.061415576934815, ETA in seconds: 11837435.605\n",
      "epoch: 319600, train loss: 4.048329639434814, val loss: 4.068837451934814, ETA in seconds: 11839201.713\n",
      "epoch: 319700, train loss: 4.053798389434815, val loss: 4.071181201934815, ETA in seconds: 11840454.764\n",
      "epoch: 319800, train loss: 4.045399951934814, val loss: 4.068446826934815, ETA in seconds: 11842067.955\n",
      "epoch: 319900, train loss: 4.049306201934814, val loss: 4.071376514434815, ETA in seconds: 11843922.911\n",
      "epoch: 320000, train loss: 4.052626514434815, val loss: 4.066298389434815, ETA in seconds: 11845924.381\n",
      "epoch: 320100, train loss: 4.0520405769348145, val loss: 4.074110889434815, ETA in seconds: 11847761.486\n",
      "epoch: 320200, train loss: 4.047353076934814, val loss: 4.065126514434814, ETA in seconds: 11849716.884\n",
      "epoch: 320300, train loss: 4.053212451934814, val loss: 4.065907764434814, ETA in seconds: 11851748.895\n",
      "epoch: 320400, train loss: 4.049696826934815, val loss: 4.068837451934814, ETA in seconds: 11853619.140\n",
      "epoch: 320500, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 11855683.614\n",
      "epoch: 320600, train loss: 4.053603076934815, val loss: 4.069814014434814, ETA in seconds: 11857692.553\n",
      "epoch: 320700, train loss: 4.048720264434815, val loss: 4.068056201934814, ETA in seconds: 11859566.271\n",
      "epoch: 320800, train loss: 4.057314014434814, val loss: 4.069228076934815, ETA in seconds: 11861560.194\n",
      "epoch: 320900, train loss: 4.046767139434815, val loss: 4.068056201934814, ETA in seconds: 11863461.009\n",
      "epoch: 321000, train loss: 4.051649951934815, val loss: 4.0618062019348145, ETA in seconds: 11865334.910\n",
      "epoch: 321100, train loss: 4.048915576934815, val loss: 4.069423389434815, ETA in seconds: 11867237.535\n",
      "epoch: 321200, train loss: 4.054774951934815, val loss: 4.067860889434814, ETA in seconds: 11869036.304\n",
      "epoch: 321300, train loss: 4.054189014434814, val loss: 4.071962451934814, ETA in seconds: 11870904.706\n",
      "epoch: 321400, train loss: 4.052431201934814, val loss: 4.061610889434815, ETA in seconds: 11872265.414\n",
      "epoch: 321500, train loss: 4.046962451934815, val loss: 4.0666890144348145, ETA in seconds: 11874097.055\n",
      "epoch: 321600, train loss: 4.047548389434814, val loss: 4.071181201934815, ETA in seconds: 11876048.842\n",
      "epoch: 321700, train loss: 4.048329639434814, val loss: 4.070009326934814, ETA in seconds: 11878069.559\n",
      "epoch: 321800, train loss: 4.050673389434815, val loss: 4.067079639434814, ETA in seconds: 11879898.825\n",
      "epoch: 321900, train loss: 4.0539937019348145, val loss: 4.067079639434814, ETA in seconds: 11881799.735\n",
      "epoch: 322000, train loss: 4.054384326934814, val loss: 4.063173389434814, ETA in seconds: 11883713.232\n",
      "epoch: 322100, train loss: 4.047548389434814, val loss: 4.064931201934814, ETA in seconds: 11885807.193\n",
      "epoch: 322200, train loss: 4.049696826934815, val loss: 4.065907764434814, ETA in seconds: 11887784.754\n",
      "epoch: 322300, train loss: 4.057118701934814, val loss: 4.0686421394348145, ETA in seconds: 11889599.329\n",
      "epoch: 322400, train loss: 4.0510640144348145, val loss: 4.063954639434814, ETA in seconds: 11891611.647\n",
      "epoch: 322500, train loss: 4.058290576934814, val loss: 4.067274951934815, ETA in seconds: 11893582.500\n",
      "epoch: 322600, train loss: 4.054579639434815, val loss: 4.0705952644348145, ETA in seconds: 11895539.554\n",
      "epoch: 322700, train loss: 4.0530171394348145, val loss: 4.068446826934815, ETA in seconds: 11897492.849\n",
      "epoch: 322800, train loss: 4.052235889434814, val loss: 4.0618062019348145, ETA in seconds: 11899523.419\n",
      "epoch: 322900, train loss: 4.049501514434814, val loss: 4.067860889434814, ETA in seconds: 11901449.985\n",
      "epoch: 323000, train loss: 4.050868701934815, val loss: 4.0637593269348145, ETA in seconds: 11903357.200\n",
      "epoch: 323100, train loss: 4.054774951934815, val loss: 4.066493701934815, ETA in seconds: 11905265.597\n",
      "epoch: 323200, train loss: 4.054384326934814, val loss: 4.068446826934815, ETA in seconds: 11907315.886\n",
      "epoch: 323300, train loss: 4.0539937019348145, val loss: 4.066493701934815, ETA in seconds: 11909308.905\n",
      "epoch: 323400, train loss: 4.044032764434815, val loss: 4.070204639434815, ETA in seconds: 11911158.584\n",
      "epoch: 323500, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 11912971.776\n",
      "epoch: 323600, train loss: 4.043446826934814, val loss: 4.0696187019348145, ETA in seconds: 11914676.366\n",
      "epoch: 323700, train loss: 4.051454639434814, val loss: 4.0637593269348145, ETA in seconds: 11916402.003\n",
      "epoch: 323800, train loss: 4.050478076934814, val loss: 4.068251514434815, ETA in seconds: 11918131.498\n",
      "epoch: 323900, train loss: 4.057509326934815, val loss: 4.072157764434815, ETA in seconds: 11919976.780\n",
      "epoch: 324000, train loss: 4.048329639434814, val loss: 4.066884326934814, ETA in seconds: 11921793.724\n",
      "epoch: 324100, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 11923770.768\n",
      "epoch: 324200, train loss: 4.053407764434814, val loss: 4.068446826934815, ETA in seconds: 11925634.476\n",
      "epoch: 324300, train loss: 4.048915576934815, val loss: 4.075087451934815, ETA in seconds: 11927708.421\n",
      "epoch: 324400, train loss: 4.055556201934815, val loss: 4.0696187019348145, ETA in seconds: 11929516.815\n",
      "epoch: 324500, train loss: 4.049501514434814, val loss: 4.0627827644348145, ETA in seconds: 11931211.229\n",
      "epoch: 324600, train loss: 4.044032764434815, val loss: 4.072939014434814, ETA in seconds: 11933045.896\n",
      "epoch: 324700, train loss: 4.049696826934815, val loss: 4.075673389434814, ETA in seconds: 11934973.409\n",
      "epoch: 324800, train loss: 4.054384326934814, val loss: 4.0696187019348145, ETA in seconds: 11936664.392\n",
      "epoch: 324900, train loss: 4.054774951934815, val loss: 4.0705952644348145, ETA in seconds: 11938530.625\n",
      "epoch: 325000, train loss: 4.052431201934814, val loss: 4.067470264434815, ETA in seconds: 11940412.153\n",
      "epoch: 325100, train loss: 4.049501514434814, val loss: 4.0647358894348145, ETA in seconds: 11942326.062\n",
      "epoch: 325200, train loss: 4.054384326934814, val loss: 4.057509326934815, ETA in seconds: 11944158.120\n",
      "epoch: 325300, train loss: 4.052235889434814, val loss: 4.0627827644348145, ETA in seconds: 11945995.396\n",
      "epoch: 325400, train loss: 4.056337451934814, val loss: 4.069814014434814, ETA in seconds: 11947848.825\n",
      "epoch: 325500, train loss: 4.056728076934815, val loss: 4.066103076934814, ETA in seconds: 11949715.802\n",
      "epoch: 325600, train loss: 4.051259326934814, val loss: 4.064931201934814, ETA in seconds: 11951717.969\n",
      "epoch: 325700, train loss: 4.0539937019348145, val loss: 4.0686421394348145, ETA in seconds: 11953618.452\n",
      "epoch: 325800, train loss: 4.056532764434815, val loss: 4.062978076934814, ETA in seconds: 11955545.564\n",
      "epoch: 325900, train loss: 4.0559468269348145, val loss: 4.071767139434814, ETA in seconds: 11957420.173\n",
      "epoch: 326000, train loss: 4.051259326934814, val loss: 4.067860889434814, ETA in seconds: 11959182.670\n",
      "epoch: 326100, train loss: 4.055556201934815, val loss: 4.070009326934814, ETA in seconds: 11961058.265\n",
      "epoch: 326200, train loss: 4.055360889434814, val loss: 4.070204639434815, ETA in seconds: 11962832.669\n",
      "epoch: 326300, train loss: 4.046376514434814, val loss: 4.067274951934815, ETA in seconds: 11964613.541\n",
      "epoch: 326400, train loss: 4.050673389434815, val loss: 4.068251514434815, ETA in seconds: 11966481.547\n",
      "epoch: 326500, train loss: 4.052821826934815, val loss: 4.067079639434814, ETA in seconds: 11968306.508\n",
      "epoch: 326600, train loss: 4.054774951934815, val loss: 4.064931201934814, ETA in seconds: 11970213.743\n",
      "epoch: 326700, train loss: 4.0549702644348145, val loss: 4.063564014434815, ETA in seconds: 11972012.425\n",
      "epoch: 326800, train loss: 4.0530171394348145, val loss: 4.066493701934815, ETA in seconds: 11973918.425\n",
      "epoch: 326900, train loss: 4.060243701934814, val loss: 4.067079639434814, ETA in seconds: 11975780.285\n",
      "epoch: 327000, train loss: 4.048915576934815, val loss: 4.062196826934814, ETA in seconds: 11977744.386\n",
      "epoch: 327100, train loss: 4.0588765144348145, val loss: 4.0696187019348145, ETA in seconds: 11979637.291\n",
      "epoch: 327200, train loss: 4.052626514434815, val loss: 4.0696187019348145, ETA in seconds: 11981624.186\n",
      "epoch: 327300, train loss: 4.051259326934814, val loss: 4.073134326934815, ETA in seconds: 11983635.734\n",
      "epoch: 327400, train loss: 4.047353076934814, val loss: 4.061415576934815, ETA in seconds: 11985404.087\n",
      "epoch: 327500, train loss: 4.053603076934815, val loss: 4.069032764434814, ETA in seconds: 11987305.384\n",
      "epoch: 327600, train loss: 4.050868701934815, val loss: 4.073915576934814, ETA in seconds: 11989227.089\n",
      "epoch: 327700, train loss: 4.045790576934815, val loss: 4.064149951934814, ETA in seconds: 11991346.148\n",
      "epoch: 327800, train loss: 4.050868701934815, val loss: 4.067274951934815, ETA in seconds: 11993434.712\n",
      "epoch: 327900, train loss: 4.053407764434814, val loss: 4.066493701934815, ETA in seconds: 11995225.463\n",
      "epoch: 328000, train loss: 4.048329639434814, val loss: 4.070790576934814, ETA in seconds: 11997121.108\n",
      "epoch: 328100, train loss: 4.049501514434814, val loss: 4.071767139434814, ETA in seconds: 11998889.657\n",
      "epoch: 328200, train loss: 4.051259326934814, val loss: 4.0637593269348145, ETA in seconds: 12000651.293\n",
      "epoch: 328300, train loss: 4.0530171394348145, val loss: 4.069423389434815, ETA in seconds: 12002659.856\n",
      "epoch: 328400, train loss: 4.047353076934814, val loss: 4.069228076934815, ETA in seconds: 12004561.420\n",
      "epoch: 328500, train loss: 4.0422749519348145, val loss: 4.068056201934814, ETA in seconds: 12006404.221\n",
      "epoch: 328600, train loss: 4.049696826934815, val loss: 4.070009326934814, ETA in seconds: 12008350.156\n",
      "epoch: 328700, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 12010235.607\n",
      "epoch: 328800, train loss: 4.0530171394348145, val loss: 4.069814014434814, ETA in seconds: 12012188.194\n",
      "epoch: 328900, train loss: 4.046962451934815, val loss: 4.066493701934815, ETA in seconds: 12014065.969\n",
      "epoch: 329000, train loss: 4.0588765144348145, val loss: 4.061220264434814, ETA in seconds: 12015876.733\n",
      "epoch: 329100, train loss: 4.045009326934815, val loss: 4.066884326934814, ETA in seconds: 12017678.882\n",
      "epoch: 329200, train loss: 4.048720264434815, val loss: 4.0696187019348145, ETA in seconds: 12019480.721\n",
      "epoch: 329300, train loss: 4.055165576934814, val loss: 4.0725483894348145, ETA in seconds: 12021329.824\n",
      "epoch: 329400, train loss: 4.0510640144348145, val loss: 4.062587451934815, ETA in seconds: 12023452.238\n",
      "epoch: 329500, train loss: 4.047939014434815, val loss: 4.065907764434814, ETA in seconds: 12025446.054\n",
      "epoch: 329600, train loss: 4.049501514434814, val loss: 4.067860889434814, ETA in seconds: 12027325.185\n",
      "epoch: 329700, train loss: 4.059267139434814, val loss: 4.062392139434815, ETA in seconds: 12029100.809\n",
      "epoch: 329800, train loss: 4.054774951934815, val loss: 4.0705952644348145, ETA in seconds: 12030852.504\n",
      "epoch: 329900, train loss: 4.0588765144348145, val loss: 4.063564014434815, ETA in seconds: 12032634.636\n",
      "epoch: 330000, train loss: 4.0500874519348145, val loss: 4.066298389434815, ETA in seconds: 12034317.707\n",
      "epoch: 330100, train loss: 4.0481343269348145, val loss: 4.0657124519348145, ETA in seconds: 12036130.482\n",
      "epoch: 330200, train loss: 4.057314014434814, val loss: 4.059657764434815, ETA in seconds: 12037896.208\n",
      "epoch: 330300, train loss: 4.056532764434815, val loss: 4.062587451934815, ETA in seconds: 12039575.220\n",
      "epoch: 330400, train loss: 4.051454639434814, val loss: 4.068446826934815, ETA in seconds: 12041352.003\n",
      "epoch: 330500, train loss: 4.059267139434814, val loss: 4.0666890144348145, ETA in seconds: 12043208.889\n",
      "epoch: 330600, train loss: 4.048329639434814, val loss: 4.069032764434814, ETA in seconds: 12045159.841\n",
      "epoch: 330700, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 12047105.775\n",
      "epoch: 330800, train loss: 4.052235889434814, val loss: 4.074892139434814, ETA in seconds: 12048950.321\n",
      "epoch: 330900, train loss: 4.049892139434815, val loss: 4.070399951934815, ETA in seconds: 12050794.866\n",
      "epoch: 331000, train loss: 4.050478076934814, val loss: 4.067079639434814, ETA in seconds: 12052600.548\n",
      "epoch: 331100, train loss: 4.052431201934814, val loss: 4.068446826934815, ETA in seconds: 12054424.670\n",
      "epoch: 331200, train loss: 4.054189014434814, val loss: 4.066103076934814, ETA in seconds: 12056252.772\n",
      "epoch: 331300, train loss: 4.049501514434814, val loss: 4.069814014434814, ETA in seconds: 12058193.687\n",
      "epoch: 331400, train loss: 4.0491108894348145, val loss: 4.068446826934815, ETA in seconds: 12060081.908\n",
      "epoch: 331500, train loss: 4.0432515144348145, val loss: 4.067860889434814, ETA in seconds: 12061961.906\n",
      "epoch: 331600, train loss: 4.054189014434814, val loss: 4.063954639434814, ETA in seconds: 12063835.122\n",
      "epoch: 331700, train loss: 4.046571826934814, val loss: 4.0647358894348145, ETA in seconds: 12065634.074\n",
      "epoch: 331800, train loss: 4.046767139434815, val loss: 4.061415576934815, ETA in seconds: 12067359.715\n",
      "epoch: 331900, train loss: 4.056532764434815, val loss: 4.068056201934814, ETA in seconds: 12069263.842\n",
      "epoch: 332000, train loss: 4.055360889434814, val loss: 4.062587451934815, ETA in seconds: 12071029.898\n",
      "epoch: 332100, train loss: 4.056532764434815, val loss: 4.066493701934815, ETA in seconds: 12072868.219\n",
      "epoch: 332200, train loss: 4.050868701934815, val loss: 4.070009326934814, ETA in seconds: 12074616.633\n",
      "epoch: 332300, train loss: 4.045009326934815, val loss: 4.063954639434814, ETA in seconds: 12076439.090\n",
      "epoch: 332400, train loss: 4.0530171394348145, val loss: 4.064345264434815, ETA in seconds: 12078310.533\n",
      "epoch: 332500, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 12080272.752\n",
      "epoch: 332600, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 12082272.886\n",
      "epoch: 332700, train loss: 4.0510640144348145, val loss: 4.068446826934815, ETA in seconds: 12084168.893\n",
      "epoch: 332800, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 12086070.342\n",
      "epoch: 332900, train loss: 4.053407764434814, val loss: 4.067470264434815, ETA in seconds: 12087811.404\n",
      "epoch: 333000, train loss: 4.055165576934814, val loss: 4.0647358894348145, ETA in seconds: 12089578.097\n",
      "epoch: 333100, train loss: 4.062001514434814, val loss: 4.066884326934814, ETA in seconds: 12091356.293\n",
      "epoch: 333200, train loss: 4.0452046394348145, val loss: 4.072939014434814, ETA in seconds: 12093112.439\n",
      "epoch: 333300, train loss: 4.056728076934815, val loss: 4.069228076934815, ETA in seconds: 12094810.578\n",
      "epoch: 333400, train loss: 4.051259326934814, val loss: 4.068056201934814, ETA in seconds: 12096641.447\n",
      "epoch: 333500, train loss: 4.0442280769348145, val loss: 4.067860889434814, ETA in seconds: 12098447.807\n",
      "epoch: 333600, train loss: 4.055165576934814, val loss: 4.067079639434814, ETA in seconds: 12100265.203\n",
      "epoch: 333700, train loss: 4.054579639434815, val loss: 4.069814014434814, ETA in seconds: 12102117.665\n",
      "epoch: 333800, train loss: 4.046767139434815, val loss: 4.066493701934815, ETA in seconds: 12103869.925\n",
      "epoch: 333900, train loss: 4.0520405769348145, val loss: 4.067079639434814, ETA in seconds: 12105633.596\n",
      "epoch: 334000, train loss: 4.051845264434815, val loss: 4.068056201934814, ETA in seconds: 12107430.013\n",
      "epoch: 334100, train loss: 4.0510640144348145, val loss: 4.062001514434814, ETA in seconds: 12109194.590\n",
      "epoch: 334200, train loss: 4.045985889434815, val loss: 4.0676655769348145, ETA in seconds: 12111099.062\n",
      "epoch: 334300, train loss: 4.053212451934814, val loss: 4.0618062019348145, ETA in seconds: 12113068.418\n",
      "epoch: 334400, train loss: 4.055360889434814, val loss: 4.065126514434814, ETA in seconds: 12114952.749\n",
      "epoch: 334500, train loss: 4.051454639434814, val loss: 4.068837451934814, ETA in seconds: 12116708.902\n",
      "epoch: 334600, train loss: 4.047548389434814, val loss: 4.069032764434814, ETA in seconds: 12118458.027\n",
      "epoch: 334700, train loss: 4.049306201934814, val loss: 4.062978076934814, ETA in seconds: 12120319.111\n",
      "epoch: 334800, train loss: 4.050478076934814, val loss: 4.070399951934815, ETA in seconds: 12121982.787\n",
      "epoch: 334900, train loss: 4.048524951934814, val loss: 4.066493701934815, ETA in seconds: 12123720.666\n",
      "epoch: 335000, train loss: 4.045985889434815, val loss: 4.0686421394348145, ETA in seconds: 12125776.308\n",
      "epoch: 335100, train loss: 4.0539937019348145, val loss: 4.0705952644348145, ETA in seconds: 12127701.960\n",
      "epoch: 335200, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 12129578.292\n",
      "epoch: 335300, train loss: 4.052235889434814, val loss: 4.067079639434814, ETA in seconds: 12131262.912\n",
      "epoch: 335400, train loss: 4.055165576934814, val loss: 4.064931201934814, ETA in seconds: 12132841.208\n",
      "epoch: 335500, train loss: 4.051454639434814, val loss: 4.061220264434814, ETA in seconds: 12134467.999\n",
      "epoch: 335600, train loss: 4.056532764434815, val loss: 4.0705952644348145, ETA in seconds: 12136163.844\n",
      "epoch: 335700, train loss: 4.052235889434814, val loss: 4.0696187019348145, ETA in seconds: 12137801.856\n",
      "epoch: 335800, train loss: 4.051845264434815, val loss: 4.069423389434815, ETA in seconds: 12139318.493\n",
      "epoch: 335900, train loss: 4.047548389434814, val loss: 4.064149951934814, ETA in seconds: 12140947.747\n",
      "epoch: 336000, train loss: 4.050868701934815, val loss: 4.062978076934814, ETA in seconds: 12142658.775\n",
      "epoch: 336100, train loss: 4.052431201934814, val loss: 4.073915576934814, ETA in seconds: 12144419.354\n",
      "epoch: 336200, train loss: 4.051649951934815, val loss: 4.0715718269348145, ETA in seconds: 12146153.504\n",
      "epoch: 336300, train loss: 4.051454639434814, val loss: 4.066298389434815, ETA in seconds: 12147855.184\n",
      "epoch: 336400, train loss: 4.054579639434815, val loss: 4.064540576934815, ETA in seconds: 12149693.243\n",
      "epoch: 336500, train loss: 4.0510640144348145, val loss: 4.067274951934815, ETA in seconds: 12151489.101\n",
      "epoch: 336600, train loss: 4.058290576934814, val loss: 4.067274951934815, ETA in seconds: 12153335.564\n",
      "epoch: 336700, train loss: 4.055165576934814, val loss: 4.065517139434815, ETA in seconds: 12155037.598\n",
      "epoch: 336800, train loss: 4.0500874519348145, val loss: 4.0676655769348145, ETA in seconds: 12156750.973\n",
      "epoch: 336900, train loss: 4.048329639434814, val loss: 4.0618062019348145, ETA in seconds: 12158382.897\n",
      "epoch: 337000, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 12160274.239\n",
      "epoch: 337100, train loss: 4.053603076934815, val loss: 4.067274951934815, ETA in seconds: 12162023.190\n",
      "epoch: 337200, train loss: 4.062392139434815, val loss: 4.069423389434815, ETA in seconds: 12163636.855\n",
      "epoch: 337300, train loss: 4.054774951934815, val loss: 4.065126514434814, ETA in seconds: 12165295.587\n",
      "epoch: 337400, train loss: 4.051454639434814, val loss: 4.068251514434815, ETA in seconds: 12166964.464\n",
      "epoch: 337500, train loss: 4.056532764434815, val loss: 4.068446826934815, ETA in seconds: 12168729.150\n",
      "epoch: 337600, train loss: 4.0559468269348145, val loss: 4.0676655769348145, ETA in seconds: 12170474.453\n",
      "epoch: 337700, train loss: 4.045009326934815, val loss: 4.0657124519348145, ETA in seconds: 12172183.465\n",
      "epoch: 337800, train loss: 4.056337451934814, val loss: 4.0686421394348145, ETA in seconds: 12173783.940\n",
      "epoch: 337900, train loss: 4.051259326934814, val loss: 4.0647358894348145, ETA in seconds: 12175514.131\n",
      "epoch: 338000, train loss: 4.049501514434814, val loss: 4.068251514434815, ETA in seconds: 12177164.228\n",
      "epoch: 338100, train loss: 4.060048389434814, val loss: 4.071181201934815, ETA in seconds: 12178863.372\n",
      "epoch: 338200, train loss: 4.053407764434814, val loss: 4.067470264434815, ETA in seconds: 12180810.959\n",
      "epoch: 338300, train loss: 4.051259326934814, val loss: 4.070204639434815, ETA in seconds: 12182641.953\n",
      "epoch: 338400, train loss: 4.058681201934815, val loss: 4.070790576934814, ETA in seconds: 12184422.783\n",
      "epoch: 338500, train loss: 4.048524951934814, val loss: 4.0647358894348145, ETA in seconds: 12186353.065\n",
      "epoch: 338600, train loss: 4.048720264434815, val loss: 4.068251514434815, ETA in seconds: 12188149.684\n",
      "epoch: 338700, train loss: 4.053603076934815, val loss: 4.070204639434815, ETA in seconds: 12189869.041\n",
      "epoch: 338800, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 12191671.918\n",
      "epoch: 338900, train loss: 4.042860889434815, val loss: 4.068056201934814, ETA in seconds: 12193401.015\n",
      "epoch: 339000, train loss: 4.053407764434814, val loss: 4.062978076934814, ETA in seconds: 12195156.705\n",
      "epoch: 339100, train loss: 4.0549702644348145, val loss: 4.072743701934814, ETA in seconds: 12196964.439\n",
      "epoch: 339200, train loss: 4.0520405769348145, val loss: 4.072157764434815, ETA in seconds: 12198613.638\n",
      "epoch: 339300, train loss: 4.050868701934815, val loss: 4.067860889434814, ETA in seconds: 12200272.784\n",
      "epoch: 339400, train loss: 4.053798389434815, val loss: 4.070399951934815, ETA in seconds: 12202128.834\n",
      "epoch: 339500, train loss: 4.052235889434814, val loss: 4.070204639434815, ETA in seconds: 12203938.989\n",
      "epoch: 339600, train loss: 4.0569233894348145, val loss: 4.066493701934815, ETA in seconds: 12205742.081\n",
      "epoch: 339700, train loss: 4.054384326934814, val loss: 4.0647358894348145, ETA in seconds: 12207532.588\n",
      "epoch: 339800, train loss: 4.048524951934814, val loss: 4.0696187019348145, ETA in seconds: 12209382.704\n",
      "epoch: 339900, train loss: 4.048915576934815, val loss: 4.064540576934815, ETA in seconds: 12211106.642\n",
      "epoch: 340000, train loss: 4.048524951934814, val loss: 4.060634326934815, ETA in seconds: 12212794.244\n",
      "epoch: 340100, train loss: 4.051454639434814, val loss: 4.069814014434814, ETA in seconds: 12214394.231\n",
      "epoch: 340200, train loss: 4.054579639434815, val loss: 4.069423389434815, ETA in seconds: 12216136.328\n",
      "epoch: 340300, train loss: 4.049696826934815, val loss: 4.0715718269348145, ETA in seconds: 12217762.511\n",
      "epoch: 340400, train loss: 4.0539937019348145, val loss: 4.065907764434814, ETA in seconds: 12219480.131\n",
      "epoch: 340500, train loss: 4.057118701934814, val loss: 4.060634326934815, ETA in seconds: 12221206.671\n",
      "epoch: 340600, train loss: 4.049696826934815, val loss: 4.0696187019348145, ETA in seconds: 12222834.974\n",
      "epoch: 340700, train loss: 4.0520405769348145, val loss: 4.0647358894348145, ETA in seconds: 12224498.145\n",
      "epoch: 340800, train loss: 4.053212451934814, val loss: 4.072939014434814, ETA in seconds: 12226212.722\n",
      "epoch: 340900, train loss: 4.049501514434814, val loss: 4.069814014434814, ETA in seconds: 12227953.622\n",
      "epoch: 341000, train loss: 4.050673389434815, val loss: 4.063368701934815, ETA in seconds: 12229623.917\n",
      "epoch: 341100, train loss: 4.055360889434814, val loss: 4.065907764434814, ETA in seconds: 12231348.813\n",
      "epoch: 341200, train loss: 4.055165576934814, val loss: 4.066884326934814, ETA in seconds: 12233199.903\n",
      "epoch: 341300, train loss: 4.045790576934815, val loss: 4.065517139434815, ETA in seconds: 12235180.440\n",
      "epoch: 341400, train loss: 4.0588765144348145, val loss: 4.065517139434815, ETA in seconds: 12237079.393\n",
      "epoch: 341500, train loss: 4.053798389434815, val loss: 4.0657124519348145, ETA in seconds: 12238754.166\n",
      "epoch: 341600, train loss: 4.053798389434815, val loss: 4.066884326934814, ETA in seconds: 12240427.316\n",
      "epoch: 341700, train loss: 4.052821826934815, val loss: 4.070790576934814, ETA in seconds: 12242501.394\n",
      "epoch: 341800, train loss: 4.055556201934815, val loss: 4.065126514434814, ETA in seconds: 12244710.484\n",
      "epoch: 341900, train loss: 4.051454639434814, val loss: 4.066298389434815, ETA in seconds: 12246462.255\n",
      "epoch: 342000, train loss: 4.046376514434814, val loss: 4.074306201934815, ETA in seconds: 12248137.873\n",
      "epoch: 342100, train loss: 4.0452046394348145, val loss: 4.0715718269348145, ETA in seconds: 12249707.707\n",
      "epoch: 342200, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 12251335.571\n",
      "epoch: 342300, train loss: 4.0500874519348145, val loss: 4.069814014434814, ETA in seconds: 12252988.670\n",
      "epoch: 342400, train loss: 4.060243701934814, val loss: 4.067860889434814, ETA in seconds: 12254760.326\n",
      "epoch: 342500, train loss: 4.0481343269348145, val loss: 4.0657124519348145, ETA in seconds: 12256368.456\n",
      "epoch: 342600, train loss: 4.0530171394348145, val loss: 4.067470264434815, ETA in seconds: 12257978.314\n",
      "epoch: 342700, train loss: 4.060634326934815, val loss: 4.069032764434814, ETA in seconds: 12259596.930\n",
      "epoch: 342800, train loss: 4.046376514434814, val loss: 4.0686421394348145, ETA in seconds: 12261199.994\n",
      "epoch: 342900, train loss: 4.053603076934815, val loss: 4.071962451934814, ETA in seconds: 12262999.597\n",
      "epoch: 343000, train loss: 4.0461812019348145, val loss: 4.063564014434815, ETA in seconds: 12264617.982\n",
      "epoch: 343100, train loss: 4.0530171394348145, val loss: 4.0666890144348145, ETA in seconds: 12266259.037\n",
      "epoch: 343200, train loss: 4.052431201934814, val loss: 4.0705952644348145, ETA in seconds: 12267943.116\n",
      "epoch: 343300, train loss: 4.058095264434814, val loss: 4.065126514434814, ETA in seconds: 12269555.548\n",
      "epoch: 343400, train loss: 4.052821826934815, val loss: 4.064149951934814, ETA in seconds: 12271214.608\n",
      "epoch: 343500, train loss: 4.050282764434814, val loss: 4.066103076934814, ETA in seconds: 12272759.305\n",
      "epoch: 343600, train loss: 4.049696826934815, val loss: 4.0686421394348145, ETA in seconds: 12274314.599\n",
      "epoch: 343700, train loss: 4.0491108894348145, val loss: 4.068251514434815, ETA in seconds: 12275901.155\n",
      "epoch: 343800, train loss: 4.060048389434814, val loss: 4.070790576934814, ETA in seconds: 12277632.239\n",
      "epoch: 343900, train loss: 4.050673389434815, val loss: 4.0637593269348145, ETA in seconds: 12279282.140\n",
      "epoch: 344000, train loss: 4.0510640144348145, val loss: 4.0735249519348145, ETA in seconds: 12280999.926\n",
      "epoch: 344100, train loss: 4.056142139434814, val loss: 4.068056201934814, ETA in seconds: 12282628.981\n",
      "epoch: 344200, train loss: 4.0500874519348145, val loss: 4.067860889434814, ETA in seconds: 12284154.618\n",
      "epoch: 344300, train loss: 4.053407764434814, val loss: 4.069032764434814, ETA in seconds: 12285768.290\n",
      "epoch: 344400, train loss: 4.063173389434814, val loss: 4.070009326934814, ETA in seconds: 12287465.599\n",
      "epoch: 344500, train loss: 4.056142139434814, val loss: 4.065517139434815, ETA in seconds: 12289154.517\n",
      "epoch: 344600, train loss: 4.060243701934814, val loss: 4.068056201934814, ETA in seconds: 12290836.280\n",
      "epoch: 344700, train loss: 4.054579639434815, val loss: 4.062587451934815, ETA in seconds: 12292483.115\n",
      "epoch: 344800, train loss: 4.0520405769348145, val loss: 4.062978076934814, ETA in seconds: 12294143.775\n",
      "epoch: 344900, train loss: 4.054384326934814, val loss: 4.068837451934814, ETA in seconds: 12295861.475\n",
      "epoch: 345000, train loss: 4.0510640144348145, val loss: 4.071376514434815, ETA in seconds: 12297515.629\n",
      "epoch: 345100, train loss: 4.049892139434815, val loss: 4.0676655769348145, ETA in seconds: 12299415.673\n",
      "epoch: 345200, train loss: 4.049696826934815, val loss: 4.067274951934815, ETA in seconds: 12301281.187\n",
      "epoch: 345300, train loss: 4.049501514434814, val loss: 4.064345264434815, ETA in seconds: 12303146.927\n",
      "epoch: 345400, train loss: 4.047743701934815, val loss: 4.068837451934814, ETA in seconds: 12304956.764\n",
      "epoch: 345500, train loss: 4.0559468269348145, val loss: 4.0647358894348145, ETA in seconds: 12306705.646\n",
      "epoch: 345600, train loss: 4.046962451934815, val loss: 4.063368701934815, ETA in seconds: 12308156.035\n",
      "epoch: 345700, train loss: 4.0491108894348145, val loss: 4.063954639434814, ETA in seconds: 12309728.520\n",
      "epoch: 345800, train loss: 4.0510640144348145, val loss: 4.067274951934815, ETA in seconds: 12311442.827\n",
      "epoch: 345900, train loss: 4.053798389434815, val loss: 4.069032764434814, ETA in seconds: 12313027.273\n",
      "epoch: 346000, train loss: 4.054579639434815, val loss: 4.0627827644348145, ETA in seconds: 12314590.304\n",
      "epoch: 346100, train loss: 4.0520405769348145, val loss: 4.0715718269348145, ETA in seconds: 12316173.998\n",
      "epoch: 346200, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 12317817.716\n",
      "epoch: 346300, train loss: 4.054579639434815, val loss: 4.067274951934815, ETA in seconds: 12319784.712\n",
      "epoch: 346400, train loss: 4.047548389434814, val loss: 4.069814014434814, ETA in seconds: 12321458.060\n",
      "epoch: 346500, train loss: 4.044032764434815, val loss: 4.071181201934815, ETA in seconds: 12323075.846\n",
      "epoch: 346600, train loss: 4.051845264434815, val loss: 4.065517139434815, ETA in seconds: 12324905.501\n",
      "epoch: 346700, train loss: 4.054189014434814, val loss: 4.0657124519348145, ETA in seconds: 12326535.750\n",
      "epoch: 346800, train loss: 4.048329639434814, val loss: 4.071962451934814, ETA in seconds: 12328182.615\n",
      "epoch: 346900, train loss: 4.050868701934815, val loss: 4.0647358894348145, ETA in seconds: 12329834.279\n",
      "epoch: 347000, train loss: 4.054189014434814, val loss: 4.063954639434814, ETA in seconds: 12331494.664\n",
      "epoch: 347100, train loss: 4.0471577644348145, val loss: 4.0715718269348145, ETA in seconds: 12333188.042\n",
      "epoch: 347200, train loss: 4.058485889434815, val loss: 4.067470264434815, ETA in seconds: 12334841.300\n",
      "epoch: 347300, train loss: 4.049892139434815, val loss: 4.070009326934814, ETA in seconds: 12336541.326\n",
      "epoch: 347400, train loss: 4.0422749519348145, val loss: 4.067470264434815, ETA in seconds: 12338249.968\n",
      "epoch: 347500, train loss: 4.061024951934814, val loss: 4.069814014434814, ETA in seconds: 12339766.014\n",
      "epoch: 347600, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 12341309.661\n",
      "epoch: 347700, train loss: 4.052626514434815, val loss: 4.068251514434815, ETA in seconds: 12342948.466\n",
      "epoch: 347800, train loss: 4.0491108894348145, val loss: 4.069032764434814, ETA in seconds: 12344545.281\n",
      "epoch: 347900, train loss: 4.049306201934814, val loss: 4.066103076934814, ETA in seconds: 12346142.919\n",
      "epoch: 348000, train loss: 4.0481343269348145, val loss: 4.0676655769348145, ETA in seconds: 12347754.917\n",
      "epoch: 348100, train loss: 4.050282764434814, val loss: 4.069032764434814, ETA in seconds: 12349382.120\n",
      "epoch: 348200, train loss: 4.051649951934815, val loss: 4.0657124519348145, ETA in seconds: 12351045.553\n",
      "epoch: 348300, train loss: 4.045985889434815, val loss: 4.070009326934814, ETA in seconds: 12352731.077\n",
      "epoch: 348400, train loss: 4.052821826934815, val loss: 4.0676655769348145, ETA in seconds: 12354317.974\n",
      "epoch: 348500, train loss: 4.059267139434814, val loss: 4.0696187019348145, ETA in seconds: 12355931.183\n",
      "epoch: 348600, train loss: 4.051454639434814, val loss: 4.067470264434815, ETA in seconds: 12357521.383\n",
      "epoch: 348700, train loss: 4.055360889434814, val loss: 4.068446826934815, ETA in seconds: 12359162.469\n",
      "epoch: 348800, train loss: 4.051454639434814, val loss: 4.071376514434815, ETA in seconds: 12360902.701\n",
      "epoch: 348900, train loss: 4.049892139434815, val loss: 4.069228076934815, ETA in seconds: 12362470.054\n",
      "epoch: 349000, train loss: 4.044618701934814, val loss: 4.068056201934814, ETA in seconds: 12364118.416\n",
      "epoch: 349100, train loss: 4.044423389434814, val loss: 4.0676655769348145, ETA in seconds: 12365684.656\n",
      "epoch: 349200, train loss: 4.0500874519348145, val loss: 4.073329639434815, ETA in seconds: 12367289.795\n",
      "epoch: 349300, train loss: 4.0569233894348145, val loss: 4.062978076934814, ETA in seconds: 12368793.759\n",
      "epoch: 349400, train loss: 4.050282764434814, val loss: 4.0696187019348145, ETA in seconds: 12370384.377\n",
      "epoch: 349500, train loss: 4.0539937019348145, val loss: 4.061220264434814, ETA in seconds: 12371876.781\n",
      "epoch: 349600, train loss: 4.049306201934814, val loss: 4.071962451934814, ETA in seconds: 12373661.216\n",
      "epoch: 349700, train loss: 4.054384326934814, val loss: 4.0705952644348145, ETA in seconds: 12375293.810\n",
      "epoch: 349800, train loss: 4.0520405769348145, val loss: 4.065907764434814, ETA in seconds: 12376887.635\n",
      "epoch: 349900, train loss: 4.0578999519348145, val loss: 4.061024951934814, ETA in seconds: 12378465.834\n",
      "epoch: 350000, train loss: 4.0510640144348145, val loss: 4.060634326934815, ETA in seconds: 12380209.691\n",
      "epoch: 350100, train loss: 4.0452046394348145, val loss: 4.060048389434814, ETA in seconds: 12381975.827\n",
      "epoch: 350200, train loss: 4.055360889434814, val loss: 4.066493701934815, ETA in seconds: 12383608.378\n",
      "epoch: 350300, train loss: 4.0510640144348145, val loss: 4.068837451934814, ETA in seconds: 12385202.582\n",
      "epoch: 350400, train loss: 4.0500874519348145, val loss: 4.0657124519348145, ETA in seconds: 12386822.133\n",
      "epoch: 350500, train loss: 4.053603076934815, val loss: 4.065126514434814, ETA in seconds: 12388560.665\n",
      "epoch: 350600, train loss: 4.053212451934814, val loss: 4.066298389434815, ETA in seconds: 12390193.854\n",
      "epoch: 350700, train loss: 4.0510640144348145, val loss: 4.0647358894348145, ETA in seconds: 12391763.521\n",
      "epoch: 350800, train loss: 4.0491108894348145, val loss: 4.070790576934814, ETA in seconds: 12393462.235\n",
      "epoch: 350900, train loss: 4.051454639434814, val loss: 4.069814014434814, ETA in seconds: 12395016.378\n",
      "epoch: 351000, train loss: 4.050673389434815, val loss: 4.0647358894348145, ETA in seconds: 12396593.615\n",
      "epoch: 351100, train loss: 4.055165576934814, val loss: 4.064345264434815, ETA in seconds: 12398268.563\n",
      "epoch: 351200, train loss: 4.054189014434814, val loss: 4.065517139434815, ETA in seconds: 12399857.470\n",
      "epoch: 351300, train loss: 4.0539937019348145, val loss: 4.067860889434814, ETA in seconds: 12401385.035\n",
      "epoch: 351400, train loss: 4.057314014434814, val loss: 4.063173389434814, ETA in seconds: 12403053.449\n",
      "epoch: 351500, train loss: 4.047939014434815, val loss: 4.068251514434815, ETA in seconds: 12404730.439\n",
      "epoch: 351600, train loss: 4.061024951934814, val loss: 4.069032764434814, ETA in seconds: 12406474.939\n",
      "epoch: 351700, train loss: 4.045985889434815, val loss: 4.067470264434815, ETA in seconds: 12408179.164\n",
      "epoch: 351800, train loss: 4.0491108894348145, val loss: 4.0647358894348145, ETA in seconds: 12409851.406\n",
      "epoch: 351900, train loss: 4.050478076934814, val loss: 4.071767139434814, ETA in seconds: 12411481.292\n",
      "epoch: 352000, train loss: 4.051845264434815, val loss: 4.074892139434814, ETA in seconds: 12413141.750\n",
      "epoch: 352100, train loss: 4.051649951934815, val loss: 4.065321826934815, ETA in seconds: 12414693.388\n",
      "epoch: 352200, train loss: 4.051454639434814, val loss: 4.074306201934815, ETA in seconds: 12416120.372\n",
      "epoch: 352300, train loss: 4.0510640144348145, val loss: 4.069814014434814, ETA in seconds: 12417556.152\n",
      "epoch: 352400, train loss: 4.047939014434815, val loss: 4.070204639434815, ETA in seconds: 12419100.135\n",
      "epoch: 352500, train loss: 4.052821826934815, val loss: 4.063564014434815, ETA in seconds: 12420639.934\n",
      "epoch: 352600, train loss: 4.048915576934815, val loss: 4.067470264434815, ETA in seconds: 12422139.963\n",
      "epoch: 352700, train loss: 4.048720264434815, val loss: 4.068251514434815, ETA in seconds: 12423736.142\n",
      "epoch: 352800, train loss: 4.059657764434815, val loss: 4.0686421394348145, ETA in seconds: 12425346.614\n",
      "epoch: 352900, train loss: 4.055751514434815, val loss: 4.067470264434815, ETA in seconds: 12426918.239\n",
      "epoch: 353000, train loss: 4.054579639434815, val loss: 4.066884326934814, ETA in seconds: 12428449.152\n",
      "epoch: 353100, train loss: 4.047353076934814, val loss: 4.070204639434815, ETA in seconds: 12430010.076\n",
      "epoch: 353200, train loss: 4.048524951934814, val loss: 4.067079639434814, ETA in seconds: 12431609.384\n",
      "epoch: 353300, train loss: 4.047939014434815, val loss: 4.065517139434815, ETA in seconds: 12433213.541\n",
      "epoch: 353400, train loss: 4.055165576934814, val loss: 4.069032764434814, ETA in seconds: 12434804.604\n",
      "epoch: 353500, train loss: 4.052431201934814, val loss: 4.065126514434814, ETA in seconds: 12436359.023\n",
      "epoch: 353600, train loss: 4.0491108894348145, val loss: 4.066103076934814, ETA in seconds: 12437897.882\n",
      "epoch: 353700, train loss: 4.046767139434815, val loss: 4.0705952644348145, ETA in seconds: 12439445.743\n",
      "epoch: 353800, train loss: 4.0530171394348145, val loss: 4.0696187019348145, ETA in seconds: 12440979.155\n",
      "epoch: 353900, train loss: 4.0471577644348145, val loss: 4.0696187019348145, ETA in seconds: 12442805.528\n",
      "epoch: 354000, train loss: 4.053212451934814, val loss: 4.063564014434815, ETA in seconds: 12444345.995\n",
      "epoch: 354100, train loss: 4.051649951934815, val loss: 4.0725483894348145, ETA in seconds: 12446025.083\n",
      "epoch: 354200, train loss: 4.043837451934815, val loss: 4.074110889434815, ETA in seconds: 12447523.987\n",
      "epoch: 354300, train loss: 4.044814014434815, val loss: 4.066298389434815, ETA in seconds: 12449110.985\n",
      "epoch: 354400, train loss: 4.0530171394348145, val loss: 4.068056201934814, ETA in seconds: 12450713.466\n",
      "epoch: 354500, train loss: 4.0481343269348145, val loss: 4.0696187019348145, ETA in seconds: 12452375.164\n",
      "epoch: 354600, train loss: 4.056532764434815, val loss: 4.065126514434814, ETA in seconds: 12454148.679\n",
      "epoch: 354700, train loss: 4.052626514434815, val loss: 4.069423389434815, ETA in seconds: 12455758.253\n",
      "epoch: 354800, train loss: 4.047743701934815, val loss: 4.072939014434814, ETA in seconds: 12457299.756\n",
      "epoch: 354900, train loss: 4.051649951934815, val loss: 4.067274951934815, ETA in seconds: 12458801.971\n",
      "epoch: 355000, train loss: 4.051845264434815, val loss: 4.061610889434815, ETA in seconds: 12460332.449\n",
      "epoch: 355100, train loss: 4.051845264434815, val loss: 4.066884326934814, ETA in seconds: 12461819.513\n",
      "epoch: 355200, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 12463337.167\n",
      "epoch: 355300, train loss: 4.048524951934814, val loss: 4.065321826934815, ETA in seconds: 12464792.313\n",
      "epoch: 355400, train loss: 4.058681201934815, val loss: 4.0627827644348145, ETA in seconds: 12466395.804\n",
      "epoch: 355500, train loss: 4.052431201934814, val loss: 4.069032764434814, ETA in seconds: 12468009.254\n",
      "epoch: 355600, train loss: 4.045790576934815, val loss: 4.070204639434815, ETA in seconds: 12469513.256\n",
      "epoch: 355700, train loss: 4.050478076934814, val loss: 4.066298389434815, ETA in seconds: 12471048.925\n",
      "epoch: 355800, train loss: 4.045790576934815, val loss: 4.0676655769348145, ETA in seconds: 12472493.081\n",
      "epoch: 355900, train loss: 4.051454639434814, val loss: 4.063173389434814, ETA in seconds: 12474018.942\n",
      "epoch: 356000, train loss: 4.053798389434815, val loss: 4.0676655769348145, ETA in seconds: 12475617.221\n",
      "epoch: 356100, train loss: 4.058095264434814, val loss: 4.069032764434814, ETA in seconds: 12477074.619\n",
      "epoch: 356200, train loss: 4.0569233894348145, val loss: 4.062392139434815, ETA in seconds: 12478682.960\n",
      "epoch: 356300, train loss: 4.050478076934814, val loss: 4.067470264434815, ETA in seconds: 12480162.564\n",
      "epoch: 356400, train loss: 4.055360889434814, val loss: 4.064931201934814, ETA in seconds: 12481723.723\n",
      "epoch: 356500, train loss: 4.0549702644348145, val loss: 4.068837451934814, ETA in seconds: 12483218.690\n",
      "epoch: 356600, train loss: 4.051649951934815, val loss: 4.075087451934815, ETA in seconds: 12484681.010\n",
      "epoch: 356700, train loss: 4.055556201934815, val loss: 4.071376514434815, ETA in seconds: 12486116.537\n",
      "epoch: 356800, train loss: 4.049696826934815, val loss: 4.063564014434815, ETA in seconds: 12487639.653\n",
      "epoch: 356900, train loss: 4.046571826934814, val loss: 4.0715718269348145, ETA in seconds: 12489119.491\n",
      "epoch: 357000, train loss: 4.057509326934815, val loss: 4.065126514434814, ETA in seconds: 12490533.923\n",
      "epoch: 357100, train loss: 4.059071826934814, val loss: 4.065907764434814, ETA in seconds: 12491973.527\n",
      "epoch: 357200, train loss: 4.052821826934815, val loss: 4.066884326934814, ETA in seconds: 12493465.169\n",
      "epoch: 357300, train loss: 4.047548389434814, val loss: 4.0627827644348145, ETA in seconds: 12494972.476\n",
      "epoch: 357400, train loss: 4.046767139434815, val loss: 4.0657124519348145, ETA in seconds: 12496448.670\n",
      "epoch: 357500, train loss: 4.042079639434815, val loss: 4.062587451934815, ETA in seconds: 12497881.073\n",
      "epoch: 357600, train loss: 4.053212451934814, val loss: 4.069814014434814, ETA in seconds: 12499292.582\n",
      "epoch: 357700, train loss: 4.046376514434814, val loss: 4.060048389434814, ETA in seconds: 12500691.473\n",
      "epoch: 357800, train loss: 4.050478076934814, val loss: 4.063368701934815, ETA in seconds: 12502096.327\n",
      "epoch: 357900, train loss: 4.049501514434814, val loss: 4.057314014434814, ETA in seconds: 12503649.076\n",
      "epoch: 358000, train loss: 4.045790576934815, val loss: 4.066493701934815, ETA in seconds: 12505268.497\n",
      "epoch: 358100, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 12506803.579\n",
      "epoch: 358200, train loss: 4.050478076934814, val loss: 4.064149951934814, ETA in seconds: 12508035.053\n",
      "epoch: 358300, train loss: 4.0500874519348145, val loss: 4.067079639434814, ETA in seconds: 12509553.534\n",
      "epoch: 358400, train loss: 4.052431201934814, val loss: 4.0705952644348145, ETA in seconds: 12511172.759\n",
      "epoch: 358500, train loss: 4.056532764434815, val loss: 4.067274951934815, ETA in seconds: 12513037.274\n",
      "epoch: 358600, train loss: 4.053798389434815, val loss: 4.069032764434814, ETA in seconds: 12514985.977\n",
      "epoch: 358700, train loss: 4.048524951934814, val loss: 4.066103076934814, ETA in seconds: 12516838.035\n",
      "epoch: 358800, train loss: 4.059267139434814, val loss: 4.0647358894348145, ETA in seconds: 12518717.222\n",
      "epoch: 358900, train loss: 4.054774951934815, val loss: 4.065126514434814, ETA in seconds: 12520300.925\n",
      "epoch: 359000, train loss: 4.054579639434815, val loss: 4.071181201934815, ETA in seconds: 12521918.172\n",
      "epoch: 359100, train loss: 4.0471577644348145, val loss: 4.0676655769348145, ETA in seconds: 12523503.157\n",
      "epoch: 359200, train loss: 4.051259326934814, val loss: 4.065126514434814, ETA in seconds: 12525058.139\n",
      "epoch: 359300, train loss: 4.049892139434815, val loss: 4.068056201934814, ETA in seconds: 12526599.894\n",
      "epoch: 359400, train loss: 4.045009326934815, val loss: 4.0705952644348145, ETA in seconds: 12528036.089\n",
      "epoch: 359500, train loss: 4.050478076934814, val loss: 4.067470264434815, ETA in seconds: 12529594.862\n",
      "epoch: 359600, train loss: 4.0530171394348145, val loss: 4.0705952644348145, ETA in seconds: 12531218.416\n",
      "epoch: 359700, train loss: 4.057314014434814, val loss: 4.062196826934814, ETA in seconds: 12532699.693\n",
      "epoch: 359800, train loss: 4.051259326934814, val loss: 4.072157764434815, ETA in seconds: 12534120.802\n",
      "epoch: 359900, train loss: 4.053603076934815, val loss: 4.067274951934815, ETA in seconds: 12535652.898\n",
      "epoch: 360000, train loss: 4.051259326934814, val loss: 4.0696187019348145, ETA in seconds: 12537348.482\n",
      "epoch: 360100, train loss: 4.056337451934814, val loss: 4.066103076934814, ETA in seconds: 12538956.949\n",
      "epoch: 360200, train loss: 4.045985889434815, val loss: 4.062392139434815, ETA in seconds: 12540577.054\n",
      "epoch: 360300, train loss: 4.060439014434815, val loss: 4.066493701934815, ETA in seconds: 12541966.178\n",
      "epoch: 360400, train loss: 4.0520405769348145, val loss: 4.0647358894348145, ETA in seconds: 12543516.366\n",
      "epoch: 360500, train loss: 4.0569233894348145, val loss: 4.0676655769348145, ETA in seconds: 12545029.181\n",
      "epoch: 360600, train loss: 4.050673389434815, val loss: 4.0657124519348145, ETA in seconds: 12546700.315\n",
      "epoch: 360700, train loss: 4.0510640144348145, val loss: 4.073720264434814, ETA in seconds: 12548332.985\n",
      "epoch: 360800, train loss: 4.0461812019348145, val loss: 4.069032764434814, ETA in seconds: 12549795.958\n",
      "epoch: 360900, train loss: 4.052626514434815, val loss: 4.067079639434814, ETA in seconds: 12551393.424\n",
      "epoch: 361000, train loss: 4.056142139434814, val loss: 4.065126514434814, ETA in seconds: 12553078.857\n",
      "epoch: 361100, train loss: 4.052431201934814, val loss: 4.0705952644348145, ETA in seconds: 12554656.647\n",
      "epoch: 361200, train loss: 4.047548389434814, val loss: 4.064149951934814, ETA in seconds: 12556119.308\n",
      "epoch: 361300, train loss: 4.053603076934815, val loss: 4.066884326934814, ETA in seconds: 12557508.099\n",
      "epoch: 361400, train loss: 4.051454639434814, val loss: 4.065517139434815, ETA in seconds: 12559023.196\n",
      "epoch: 361500, train loss: 4.058095264434814, val loss: 4.0735249519348145, ETA in seconds: 12560644.637\n",
      "epoch: 361600, train loss: 4.052821826934815, val loss: 4.064931201934814, ETA in seconds: 12562188.280\n",
      "epoch: 361700, train loss: 4.052626514434815, val loss: 4.063173389434814, ETA in seconds: 12564064.510\n",
      "epoch: 361800, train loss: 4.054579639434815, val loss: 4.069228076934815, ETA in seconds: 12565887.865\n",
      "epoch: 361900, train loss: 4.047743701934815, val loss: 4.063954639434814, ETA in seconds: 12567444.118\n",
      "epoch: 362000, train loss: 4.055751514434815, val loss: 4.070790576934814, ETA in seconds: 12568935.227\n",
      "epoch: 362100, train loss: 4.0481343269348145, val loss: 4.062001514434814, ETA in seconds: 12570428.173\n",
      "epoch: 362200, train loss: 4.056532764434815, val loss: 4.069814014434814, ETA in seconds: 12571946.685\n",
      "epoch: 362300, train loss: 4.056142139434814, val loss: 4.0696187019348145, ETA in seconds: 12573333.729\n",
      "epoch: 362400, train loss: 4.0432515144348145, val loss: 4.067860889434814, ETA in seconds: 12574751.460\n",
      "epoch: 362500, train loss: 4.054579639434815, val loss: 4.0696187019348145, ETA in seconds: 12576193.221\n",
      "epoch: 362600, train loss: 4.0559468269348145, val loss: 4.070204639434815, ETA in seconds: 12577608.889\n",
      "epoch: 362700, train loss: 4.045790576934815, val loss: 4.066298389434815, ETA in seconds: 12578982.765\n",
      "epoch: 362800, train loss: 4.054579639434815, val loss: 4.073329639434815, ETA in seconds: 12580543.621\n",
      "epoch: 362900, train loss: 4.059071826934814, val loss: 4.069423389434815, ETA in seconds: 12582015.429\n",
      "epoch: 363000, train loss: 4.0481343269348145, val loss: 4.0666890144348145, ETA in seconds: 12583569.415\n",
      "epoch: 363100, train loss: 4.059267139434814, val loss: 4.066298389434815, ETA in seconds: 12585071.254\n",
      "epoch: 363200, train loss: 4.050478076934814, val loss: 4.065126514434814, ETA in seconds: 12586454.549\n",
      "epoch: 363300, train loss: 4.054384326934814, val loss: 4.063954639434814, ETA in seconds: 12587995.052\n",
      "epoch: 363400, train loss: 4.055165576934814, val loss: 4.065517139434815, ETA in seconds: 12589428.268\n",
      "epoch: 363500, train loss: 4.052626514434815, val loss: 4.060634326934815, ETA in seconds: 12590876.962\n",
      "epoch: 363600, train loss: 4.0500874519348145, val loss: 4.073329639434815, ETA in seconds: 12592333.338\n",
      "epoch: 363700, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 12593707.244\n",
      "epoch: 363800, train loss: 4.051649951934815, val loss: 4.067860889434814, ETA in seconds: 12595247.184\n",
      "epoch: 363900, train loss: 4.053407764434814, val loss: 4.071767139434814, ETA in seconds: 12596690.637\n",
      "epoch: 364000, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 12598433.669\n",
      "epoch: 364100, train loss: 4.049696826934815, val loss: 4.0647358894348145, ETA in seconds: 12599782.102\n",
      "epoch: 364200, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 12601180.572\n",
      "epoch: 364300, train loss: 4.052431201934814, val loss: 4.064345264434815, ETA in seconds: 12602648.616\n",
      "epoch: 364400, train loss: 4.0549702644348145, val loss: 4.064540576934815, ETA in seconds: 12604108.275\n",
      "epoch: 364500, train loss: 4.048524951934814, val loss: 4.068837451934814, ETA in seconds: 12605597.828\n",
      "epoch: 364600, train loss: 4.049696826934815, val loss: 4.070204639434815, ETA in seconds: 12607164.799\n",
      "epoch: 364700, train loss: 4.057509326934815, val loss: 4.065517139434815, ETA in seconds: 12608503.009\n",
      "epoch: 364800, train loss: 4.062587451934815, val loss: 4.067470264434815, ETA in seconds: 12609965.774\n",
      "epoch: 364900, train loss: 4.0520405769348145, val loss: 4.064149951934814, ETA in seconds: 12611418.397\n",
      "epoch: 365000, train loss: 4.0520405769348145, val loss: 4.061415576934815, ETA in seconds: 12612868.574\n",
      "epoch: 365100, train loss: 4.0598530769348145, val loss: 4.064149951934814, ETA in seconds: 12614288.487\n",
      "epoch: 365200, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 12615643.117\n",
      "epoch: 365300, train loss: 4.056728076934815, val loss: 4.060048389434814, ETA in seconds: 12616987.401\n",
      "epoch: 365400, train loss: 4.049306201934814, val loss: 4.061415576934815, ETA in seconds: 12618443.486\n",
      "epoch: 365500, train loss: 4.0491108894348145, val loss: 4.064149951934814, ETA in seconds: 12619885.619\n",
      "epoch: 365600, train loss: 4.048329639434814, val loss: 4.065517139434815, ETA in seconds: 12621469.487\n",
      "epoch: 365700, train loss: 4.0530171394348145, val loss: 4.064149951934814, ETA in seconds: 12622998.572\n",
      "epoch: 365800, train loss: 4.0500874519348145, val loss: 4.071376514434815, ETA in seconds: 12624327.057\n",
      "epoch: 365900, train loss: 4.059071826934814, val loss: 4.066298389434815, ETA in seconds: 12625694.264\n",
      "epoch: 366000, train loss: 4.0549702644348145, val loss: 4.0666890144348145, ETA in seconds: 12627194.813\n",
      "epoch: 366100, train loss: 4.053407764434814, val loss: 4.070985889434814, ETA in seconds: 12628689.144\n",
      "epoch: 366200, train loss: 4.056532764434815, val loss: 4.0705952644348145, ETA in seconds: 12630091.847\n",
      "epoch: 366300, train loss: 4.051845264434815, val loss: 4.065321826934815, ETA in seconds: 12631460.189\n",
      "epoch: 366400, train loss: 4.050673389434815, val loss: 4.066884326934814, ETA in seconds: 12632837.772\n",
      "epoch: 366500, train loss: 4.048329639434814, val loss: 4.070009326934814, ETA in seconds: 12634275.889\n",
      "epoch: 366600, train loss: 4.0452046394348145, val loss: 4.064149951934814, ETA in seconds: 12635655.534\n",
      "epoch: 366700, train loss: 4.056532764434815, val loss: 4.066493701934815, ETA in seconds: 12637133.145\n",
      "epoch: 366800, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 12638558.257\n",
      "epoch: 366900, train loss: 4.0539937019348145, val loss: 4.070009326934814, ETA in seconds: 12640058.186\n",
      "epoch: 367000, train loss: 4.054774951934815, val loss: 4.074696826934814, ETA in seconds: 12641122.637\n",
      "epoch: 367100, train loss: 4.057314014434814, val loss: 4.075282764434815, ETA in seconds: 12642590.361\n",
      "epoch: 367200, train loss: 4.044423389434814, val loss: 4.0686421394348145, ETA in seconds: 12643984.100\n",
      "epoch: 367300, train loss: 4.053798389434815, val loss: 4.0657124519348145, ETA in seconds: 12645377.865\n",
      "epoch: 367400, train loss: 4.053212451934814, val loss: 4.062978076934814, ETA in seconds: 12646999.379\n",
      "epoch: 367500, train loss: 4.050868701934815, val loss: 4.0637593269348145, ETA in seconds: 12648388.222\n",
      "epoch: 367600, train loss: 4.055751514434815, val loss: 4.070204639434815, ETA in seconds: 12649847.355\n",
      "epoch: 367700, train loss: 4.0510640144348145, val loss: 4.064149951934814, ETA in seconds: 12651262.711\n",
      "epoch: 367800, train loss: 4.058485889434815, val loss: 4.069423389434815, ETA in seconds: 12652530.915\n",
      "epoch: 367900, train loss: 4.051259326934814, val loss: 4.0657124519348145, ETA in seconds: 12653961.247\n",
      "epoch: 368000, train loss: 4.051259326934814, val loss: 4.0647358894348145, ETA in seconds: 12655299.872\n",
      "epoch: 368100, train loss: 4.047743701934815, val loss: 4.067470264434815, ETA in seconds: 12656788.634\n",
      "epoch: 368200, train loss: 4.055360889434814, val loss: 4.0627827644348145, ETA in seconds: 12658143.961\n",
      "epoch: 368300, train loss: 4.052235889434814, val loss: 4.070790576934814, ETA in seconds: 12659586.348\n",
      "epoch: 368400, train loss: 4.051649951934815, val loss: 4.064149951934814, ETA in seconds: 12661001.795\n",
      "epoch: 368500, train loss: 4.055556201934815, val loss: 4.0657124519348145, ETA in seconds: 12662414.983\n",
      "epoch: 368600, train loss: 4.056142139434814, val loss: 4.071962451934814, ETA in seconds: 12663781.789\n",
      "epoch: 368700, train loss: 4.048329639434814, val loss: 4.065907764434814, ETA in seconds: 12665246.554\n",
      "epoch: 368800, train loss: 4.052626514434815, val loss: 4.068251514434815, ETA in seconds: 12666751.175\n",
      "epoch: 368900, train loss: 4.048329639434814, val loss: 4.067274951934815, ETA in seconds: 12668152.818\n",
      "epoch: 369000, train loss: 4.054384326934814, val loss: 4.0725483894348145, ETA in seconds: 12669564.627\n",
      "epoch: 369100, train loss: 4.053798389434815, val loss: 4.0637593269348145, ETA in seconds: 12670954.512\n",
      "epoch: 369200, train loss: 4.053603076934815, val loss: 4.070009326934814, ETA in seconds: 12672318.761\n",
      "epoch: 369300, train loss: 4.049892139434815, val loss: 4.070399951934815, ETA in seconds: 12673709.931\n",
      "epoch: 369400, train loss: 4.042470264434814, val loss: 4.067079639434814, ETA in seconds: 12675018.313\n",
      "epoch: 369500, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 12676512.833\n",
      "epoch: 369600, train loss: 4.046571826934814, val loss: 4.066884326934814, ETA in seconds: 12677933.432\n",
      "epoch: 369700, train loss: 4.055360889434814, val loss: 4.069228076934815, ETA in seconds: 12679421.256\n",
      "epoch: 369800, train loss: 4.059071826934814, val loss: 4.062587451934815, ETA in seconds: 12680933.564\n",
      "epoch: 369900, train loss: 4.056337451934814, val loss: 4.075087451934815, ETA in seconds: 12682279.014\n",
      "epoch: 370000, train loss: 4.0491108894348145, val loss: 4.067274951934815, ETA in seconds: 12683691.463\n",
      "epoch: 370100, train loss: 4.056337451934814, val loss: 4.068446826934815, ETA in seconds: 12685198.796\n",
      "epoch: 370200, train loss: 4.0598530769348145, val loss: 4.069814014434814, ETA in seconds: 12686644.205\n",
      "epoch: 370300, train loss: 4.050282764434814, val loss: 4.069423389434815, ETA in seconds: 12687926.637\n",
      "epoch: 370400, train loss: 4.0530171394348145, val loss: 4.066298389434815, ETA in seconds: 12689279.898\n",
      "epoch: 370500, train loss: 4.052626514434815, val loss: 4.073720264434814, ETA in seconds: 12690585.000\n",
      "epoch: 370600, train loss: 4.051259326934814, val loss: 4.070009326934814, ETA in seconds: 12692057.458\n",
      "epoch: 370700, train loss: 4.0559468269348145, val loss: 4.0705952644348145, ETA in seconds: 12693447.617\n",
      "epoch: 370800, train loss: 4.0539937019348145, val loss: 4.066884326934814, ETA in seconds: 12694748.127\n",
      "epoch: 370900, train loss: 4.054579639434815, val loss: 4.068446826934815, ETA in seconds: 12696143.423\n",
      "epoch: 371000, train loss: 4.0491108894348145, val loss: 4.069032764434814, ETA in seconds: 12697401.361\n",
      "epoch: 371100, train loss: 4.045985889434815, val loss: 4.063954639434814, ETA in seconds: 12698929.873\n",
      "epoch: 371200, train loss: 4.0539937019348145, val loss: 4.066884326934814, ETA in seconds: 12700300.462\n",
      "epoch: 371300, train loss: 4.048915576934815, val loss: 4.067274951934815, ETA in seconds: 12701656.813\n",
      "epoch: 371400, train loss: 4.054579639434815, val loss: 4.072353076934815, ETA in seconds: 12703077.391\n",
      "epoch: 371500, train loss: 4.055165576934814, val loss: 4.064931201934814, ETA in seconds: 12704571.472\n",
      "epoch: 371600, train loss: 4.049306201934814, val loss: 4.066103076934814, ETA in seconds: 12706323.717\n",
      "epoch: 371700, train loss: 4.048720264434815, val loss: 4.071181201934815, ETA in seconds: 12708181.639\n",
      "epoch: 371800, train loss: 4.045985889434815, val loss: 4.065126514434814, ETA in seconds: 12709898.801\n",
      "epoch: 371900, train loss: 4.057509326934815, val loss: 4.067860889434814, ETA in seconds: 12711210.321\n",
      "epoch: 372000, train loss: 4.054384326934814, val loss: 4.069814014434814, ETA in seconds: 12712469.632\n",
      "epoch: 372100, train loss: 4.050478076934814, val loss: 4.0637593269348145, ETA in seconds: 12713773.117\n",
      "epoch: 372200, train loss: 4.050282764434814, val loss: 4.071962451934814, ETA in seconds: 12715131.363\n",
      "epoch: 372300, train loss: 4.059462451934815, val loss: 4.0637593269348145, ETA in seconds: 12716531.767\n",
      "epoch: 372400, train loss: 4.047353076934814, val loss: 4.067274951934815, ETA in seconds: 12717890.707\n",
      "epoch: 372500, train loss: 4.0481343269348145, val loss: 4.068251514434815, ETA in seconds: 12719311.090\n",
      "epoch: 372600, train loss: 4.049501514434814, val loss: 4.073329639434815, ETA in seconds: 12720708.562\n",
      "epoch: 372700, train loss: 4.047548389434814, val loss: 4.068056201934814, ETA in seconds: 12722131.404\n",
      "epoch: 372800, train loss: 4.0510640144348145, val loss: 4.0627827644348145, ETA in seconds: 12723563.334\n",
      "epoch: 372900, train loss: 4.054189014434814, val loss: 4.067860889434814, ETA in seconds: 12724989.715\n",
      "epoch: 373000, train loss: 4.058290576934814, val loss: 4.065126514434814, ETA in seconds: 12726437.040\n",
      "epoch: 373100, train loss: 4.047939014434815, val loss: 4.069423389434815, ETA in seconds: 12727924.433\n",
      "epoch: 373200, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 12729472.548\n",
      "epoch: 373300, train loss: 4.056532764434815, val loss: 4.066493701934815, ETA in seconds: 12730929.143\n",
      "epoch: 373400, train loss: 4.057704639434815, val loss: 4.0657124519348145, ETA in seconds: 12732366.503\n",
      "epoch: 373500, train loss: 4.045009326934815, val loss: 4.070399951934815, ETA in seconds: 12733767.707\n",
      "epoch: 373600, train loss: 4.050673389434815, val loss: 4.0618062019348145, ETA in seconds: 12734790.929\n",
      "epoch: 373700, train loss: 4.053212451934814, val loss: 4.071962451934814, ETA in seconds: 12736020.881\n",
      "epoch: 373800, train loss: 4.052235889434814, val loss: 4.062978076934814, ETA in seconds: 12737463.383\n",
      "epoch: 373900, train loss: 4.0569233894348145, val loss: 4.0627827644348145, ETA in seconds: 12738770.226\n",
      "epoch: 374000, train loss: 4.053798389434815, val loss: 4.061415576934815, ETA in seconds: 12740233.699\n",
      "epoch: 374100, train loss: 4.047353076934814, val loss: 4.0686421394348145, ETA in seconds: 12741506.547\n",
      "epoch: 374200, train loss: 4.049696826934815, val loss: 4.065907764434814, ETA in seconds: 12742816.854\n",
      "epoch: 374300, train loss: 4.052626514434815, val loss: 4.067860889434814, ETA in seconds: 12744203.921\n",
      "epoch: 374400, train loss: 4.049892139434815, val loss: 4.063564014434815, ETA in seconds: 12745634.104\n",
      "epoch: 374500, train loss: 4.055165576934814, val loss: 4.072743701934814, ETA in seconds: 12747105.977\n",
      "epoch: 374600, train loss: 4.052626514434815, val loss: 4.063564014434815, ETA in seconds: 12748503.189\n",
      "epoch: 374700, train loss: 4.0510640144348145, val loss: 4.063954639434814, ETA in seconds: 12749733.383\n",
      "epoch: 374800, train loss: 4.053603076934815, val loss: 4.060439014434815, ETA in seconds: 12751068.655\n",
      "epoch: 374900, train loss: 4.053212451934814, val loss: 4.0637593269348145, ETA in seconds: 12752450.482\n",
      "epoch: 375000, train loss: 4.053603076934815, val loss: 4.0676655769348145, ETA in seconds: 12753650.528\n",
      "epoch: 375100, train loss: 4.050478076934814, val loss: 4.0676655769348145, ETA in seconds: 12755038.472\n",
      "epoch: 375200, train loss: 4.056532764434815, val loss: 4.0715718269348145, ETA in seconds: 12756346.520\n",
      "epoch: 375300, train loss: 4.0598530769348145, val loss: 4.075673389434814, ETA in seconds: 12757371.860\n",
      "epoch: 375400, train loss: 4.049501514434814, val loss: 4.072157764434815, ETA in seconds: 12758591.158\n",
      "epoch: 375500, train loss: 4.052431201934814, val loss: 4.0647358894348145, ETA in seconds: 12759782.071\n",
      "epoch: 375600, train loss: 4.052235889434814, val loss: 4.069423389434815, ETA in seconds: 12760203.947\n",
      "epoch: 375700, train loss: 4.052431201934814, val loss: 4.066103076934814, ETA in seconds: 12760972.111\n",
      "epoch: 375800, train loss: 4.060439014434815, val loss: 4.064149951934814, ETA in seconds: 12762197.577\n",
      "epoch: 375900, train loss: 4.0530171394348145, val loss: 4.067860889434814, ETA in seconds: 12763840.787\n",
      "epoch: 376000, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 12765551.032\n",
      "epoch: 376100, train loss: 4.0500874519348145, val loss: 4.0666890144348145, ETA in seconds: 12766904.202\n",
      "epoch: 376200, train loss: 4.047743701934815, val loss: 4.0647358894348145, ETA in seconds: 12768199.232\n",
      "epoch: 376300, train loss: 4.0578999519348145, val loss: 4.064931201934814, ETA in seconds: 12769583.767\n",
      "epoch: 376400, train loss: 4.057314014434814, val loss: 4.067860889434814, ETA in seconds: 12770977.748\n",
      "epoch: 376500, train loss: 4.057118701934814, val loss: 4.065126514434814, ETA in seconds: 12772473.168\n",
      "epoch: 376600, train loss: 4.0510640144348145, val loss: 4.065321826934815, ETA in seconds: 12773733.876\n",
      "epoch: 376700, train loss: 4.047939014434815, val loss: 4.066493701934815, ETA in seconds: 12775037.920\n",
      "epoch: 376800, train loss: 4.0578999519348145, val loss: 4.065126514434814, ETA in seconds: 12776326.149\n",
      "epoch: 376900, train loss: 4.054579639434815, val loss: 4.069814014434814, ETA in seconds: 12777680.767\n",
      "epoch: 377000, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 12779033.911\n",
      "epoch: 377100, train loss: 4.047743701934815, val loss: 4.065517139434815, ETA in seconds: 12780364.746\n",
      "epoch: 377200, train loss: 4.052431201934814, val loss: 4.070204639434815, ETA in seconds: 12781603.579\n",
      "epoch: 377300, train loss: 4.047743701934815, val loss: 4.067470264434815, ETA in seconds: 12782505.352\n",
      "epoch: 377400, train loss: 4.057509326934815, val loss: 4.065907764434814, ETA in seconds: 12783697.472\n",
      "epoch: 377500, train loss: 4.045985889434815, val loss: 4.072157764434815, ETA in seconds: 12784956.259\n",
      "epoch: 377600, train loss: 4.0510640144348145, val loss: 4.071962451934814, ETA in seconds: 12786353.924\n",
      "epoch: 377700, train loss: 4.052821826934815, val loss: 4.065517139434815, ETA in seconds: 12787649.472\n",
      "epoch: 377800, train loss: 4.0442280769348145, val loss: 4.0696187019348145, ETA in seconds: 12788905.509\n",
      "epoch: 377900, train loss: 4.049306201934814, val loss: 4.0647358894348145, ETA in seconds: 12790158.063\n",
      "epoch: 378000, train loss: 4.048915576934815, val loss: 4.069228076934815, ETA in seconds: 12791488.420\n",
      "epoch: 378100, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 12792864.215\n",
      "epoch: 378200, train loss: 4.0510640144348145, val loss: 4.065517139434815, ETA in seconds: 12794215.927\n",
      "epoch: 378300, train loss: 4.045985889434815, val loss: 4.068837451934814, ETA in seconds: 12795601.920\n",
      "epoch: 378400, train loss: 4.055751514434815, val loss: 4.0666890144348145, ETA in seconds: 12797009.313\n",
      "epoch: 378500, train loss: 4.0549702644348145, val loss: 4.067079639434814, ETA in seconds: 12798336.141\n",
      "epoch: 378600, train loss: 4.0500874519348145, val loss: 4.067079639434814, ETA in seconds: 12799652.809\n",
      "epoch: 378700, train loss: 4.049696826934815, val loss: 4.070204639434815, ETA in seconds: 12801010.542\n",
      "epoch: 378800, train loss: 4.0491108894348145, val loss: 4.069423389434815, ETA in seconds: 12801942.573\n",
      "epoch: 378900, train loss: 4.050868701934815, val loss: 4.066298389434815, ETA in seconds: 12803332.433\n",
      "epoch: 379000, train loss: 4.053212451934814, val loss: 4.065907764434814, ETA in seconds: 12804906.750\n",
      "epoch: 379100, train loss: 4.053407764434814, val loss: 4.060048389434814, ETA in seconds: 12806081.616\n",
      "epoch: 379200, train loss: 4.055751514434815, val loss: 4.069423389434815, ETA in seconds: 12807259.051\n",
      "epoch: 379300, train loss: 4.044423389434814, val loss: 4.066493701934815, ETA in seconds: 12807925.455\n",
      "epoch: 379400, train loss: 4.048720264434815, val loss: 4.062587451934815, ETA in seconds: 12809210.923\n",
      "epoch: 379500, train loss: 4.0422749519348145, val loss: 4.061415576934815, ETA in seconds: 12810552.031\n",
      "epoch: 379600, train loss: 4.052626514434815, val loss: 4.067470264434815, ETA in seconds: 12811839.542\n",
      "epoch: 379700, train loss: 4.052431201934814, val loss: 4.064345264434815, ETA in seconds: 12813205.814\n",
      "epoch: 379800, train loss: 4.049892139434815, val loss: 4.064149951934814, ETA in seconds: 12814532.702\n",
      "epoch: 379900, train loss: 4.0549702644348145, val loss: 4.0666890144348145, ETA in seconds: 12815762.317\n",
      "epoch: 380000, train loss: 4.055556201934815, val loss: 4.069423389434815, ETA in seconds: 12817096.137\n",
      "epoch: 380100, train loss: 4.0442280769348145, val loss: 4.0647358894348145, ETA in seconds: 12818402.046\n",
      "epoch: 380200, train loss: 4.0549702644348145, val loss: 4.063954639434814, ETA in seconds: 12819728.719\n",
      "epoch: 380300, train loss: 4.0530171394348145, val loss: 4.0764546394348145, ETA in seconds: 12820914.958\n",
      "epoch: 380400, train loss: 4.051259326934814, val loss: 4.065907764434814, ETA in seconds: 12822127.734\n",
      "epoch: 380500, train loss: 4.056142139434814, val loss: 4.067079639434814, ETA in seconds: 12823431.073\n",
      "epoch: 380600, train loss: 4.048915576934815, val loss: 4.063564014434815, ETA in seconds: 12824603.689\n",
      "epoch: 380700, train loss: 4.0452046394348145, val loss: 4.070790576934814, ETA in seconds: 12825794.976\n",
      "epoch: 380800, train loss: 4.0559468269348145, val loss: 4.067470264434815, ETA in seconds: 12827049.158\n",
      "epoch: 380900, train loss: 4.046376514434814, val loss: 4.063564014434815, ETA in seconds: 12828288.070\n",
      "epoch: 381000, train loss: 4.058290576934814, val loss: 4.0686421394348145, ETA in seconds: 12829683.468\n",
      "epoch: 381100, train loss: 4.046962451934815, val loss: 4.069228076934815, ETA in seconds: 12831116.789\n",
      "epoch: 381200, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 12832464.905\n",
      "epoch: 381300, train loss: 4.053212451934814, val loss: 4.0676655769348145, ETA in seconds: 12833817.143\n",
      "epoch: 381400, train loss: 4.050868701934815, val loss: 4.063173389434814, ETA in seconds: 12835153.031\n",
      "epoch: 381500, train loss: 4.056728076934815, val loss: 4.072939014434814, ETA in seconds: 12836468.961\n",
      "epoch: 381600, train loss: 4.0510640144348145, val loss: 4.063173389434814, ETA in seconds: 12837755.490\n",
      "epoch: 381700, train loss: 4.050478076934814, val loss: 4.069423389434815, ETA in seconds: 12839112.455\n",
      "epoch: 381800, train loss: 4.0549702644348145, val loss: 4.071376514434815, ETA in seconds: 12840427.277\n",
      "epoch: 381900, train loss: 4.050868701934815, val loss: 4.0666890144348145, ETA in seconds: 12841666.067\n",
      "epoch: 382000, train loss: 4.049696826934815, val loss: 4.062978076934814, ETA in seconds: 12842857.999\n",
      "epoch: 382100, train loss: 4.0500874519348145, val loss: 4.069814014434814, ETA in seconds: 12844093.118\n",
      "epoch: 382200, train loss: 4.049892139434815, val loss: 4.067470264434815, ETA in seconds: 12845350.573\n",
      "epoch: 382300, train loss: 4.054774951934815, val loss: 4.066103076934814, ETA in seconds: 12846659.886\n",
      "epoch: 382400, train loss: 4.059657764434815, val loss: 4.070790576934814, ETA in seconds: 12847959.075\n",
      "epoch: 382500, train loss: 4.054579639434815, val loss: 4.070399951934815, ETA in seconds: 12849144.499\n",
      "epoch: 382600, train loss: 4.053798389434815, val loss: 4.0627827644348145, ETA in seconds: 12850352.946\n",
      "epoch: 382700, train loss: 4.047743701934815, val loss: 4.0657124519348145, ETA in seconds: 12851490.756\n",
      "epoch: 382800, train loss: 4.045399951934814, val loss: 4.061610889434815, ETA in seconds: 12852644.990\n",
      "epoch: 382900, train loss: 4.051454639434814, val loss: 4.069814014434814, ETA in seconds: 12854004.654\n",
      "epoch: 383000, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 12855301.818\n",
      "epoch: 383100, train loss: 4.055165576934814, val loss: 4.0676655769348145, ETA in seconds: 12856468.603\n",
      "epoch: 383200, train loss: 4.047939014434815, val loss: 4.069814014434814, ETA in seconds: 12857700.560\n",
      "epoch: 383300, train loss: 4.052235889434814, val loss: 4.072353076934815, ETA in seconds: 12858903.833\n",
      "epoch: 383400, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 12860161.387\n",
      "epoch: 383500, train loss: 4.048524951934814, val loss: 4.0676655769348145, ETA in seconds: 12861427.796\n",
      "epoch: 383600, train loss: 4.0569233894348145, val loss: 4.0618062019348145, ETA in seconds: 12862557.182\n",
      "epoch: 383700, train loss: 4.049696826934815, val loss: 4.066298389434815, ETA in seconds: 12863756.094\n",
      "epoch: 383800, train loss: 4.057509326934815, val loss: 4.061610889434815, ETA in seconds: 12865028.427\n",
      "epoch: 383900, train loss: 4.055165576934814, val loss: 4.067860889434814, ETA in seconds: 12866294.130\n",
      "epoch: 384000, train loss: 4.052821826934815, val loss: 4.069814014434814, ETA in seconds: 12867576.196\n",
      "epoch: 384100, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 12868962.801\n",
      "epoch: 384200, train loss: 4.0481343269348145, val loss: 4.072743701934814, ETA in seconds: 12870337.913\n",
      "epoch: 384300, train loss: 4.042860889434815, val loss: 4.0745015144348145, ETA in seconds: 12871562.426\n",
      "epoch: 384400, train loss: 4.055556201934815, val loss: 4.0647358894348145, ETA in seconds: 12872883.292\n",
      "epoch: 384500, train loss: 4.052431201934814, val loss: 4.074306201934815, ETA in seconds: 12874086.706\n",
      "epoch: 384600, train loss: 4.052235889434814, val loss: 4.068837451934814, ETA in seconds: 12875331.562\n",
      "epoch: 384700, train loss: 4.045985889434815, val loss: 4.063564014434815, ETA in seconds: 12876661.296\n",
      "epoch: 384800, train loss: 4.0461812019348145, val loss: 4.062196826934814, ETA in seconds: 12877960.775\n",
      "epoch: 384900, train loss: 4.0539937019348145, val loss: 4.068056201934814, ETA in seconds: 12879323.541\n",
      "epoch: 385000, train loss: 4.050868701934815, val loss: 4.072353076934815, ETA in seconds: 12880733.679\n",
      "epoch: 385100, train loss: 4.044423389434814, val loss: 4.0666890144348145, ETA in seconds: 12881935.933\n",
      "epoch: 385200, train loss: 4.051649951934815, val loss: 4.075087451934815, ETA in seconds: 12883270.810\n",
      "epoch: 385300, train loss: 4.0491108894348145, val loss: 4.064931201934814, ETA in seconds: 12884439.305\n",
      "epoch: 385400, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 12885788.818\n",
      "epoch: 385500, train loss: 4.057509326934815, val loss: 4.063368701934815, ETA in seconds: 12887203.344\n",
      "epoch: 385600, train loss: 4.050282764434814, val loss: 4.068837451934814, ETA in seconds: 12888711.107\n",
      "epoch: 385700, train loss: 4.0530171394348145, val loss: 4.075282764434815, ETA in seconds: 12890087.357\n",
      "epoch: 385800, train loss: 4.049892139434815, val loss: 4.065126514434814, ETA in seconds: 12891334.042\n",
      "epoch: 385900, train loss: 4.049892139434815, val loss: 4.069423389434815, ETA in seconds: 12892625.753\n",
      "epoch: 386000, train loss: 4.054189014434814, val loss: 4.064345264434815, ETA in seconds: 12893880.983\n",
      "epoch: 386100, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 12895027.985\n",
      "epoch: 386200, train loss: 4.045595264434814, val loss: 4.068056201934814, ETA in seconds: 12896215.629\n",
      "epoch: 386300, train loss: 4.0549702644348145, val loss: 4.0705952644348145, ETA in seconds: 12897501.398\n",
      "epoch: 386400, train loss: 4.053212451934814, val loss: 4.066493701934815, ETA in seconds: 12898686.885\n",
      "epoch: 386500, train loss: 4.048720264434815, val loss: 4.0608296394348145, ETA in seconds: 12900003.248\n",
      "epoch: 386600, train loss: 4.0510640144348145, val loss: 4.0705952644348145, ETA in seconds: 12901337.574\n",
      "epoch: 386700, train loss: 4.052235889434814, val loss: 4.066298389434815, ETA in seconds: 12902550.463\n",
      "epoch: 386800, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 12903837.914\n",
      "epoch: 386900, train loss: 4.0510640144348145, val loss: 4.069228076934815, ETA in seconds: 12905056.792\n",
      "epoch: 387000, train loss: 4.055360889434814, val loss: 4.065517139434815, ETA in seconds: 12906282.182\n",
      "epoch: 387100, train loss: 4.0520405769348145, val loss: 4.063368701934815, ETA in seconds: 12907649.579\n",
      "epoch: 387200, train loss: 4.054189014434814, val loss: 4.067860889434814, ETA in seconds: 12909037.602\n",
      "epoch: 387300, train loss: 4.052821826934815, val loss: 4.066493701934815, ETA in seconds: 12910370.421\n",
      "epoch: 387400, train loss: 4.049501514434814, val loss: 4.064540576934815, ETA in seconds: 12911619.277\n",
      "epoch: 387500, train loss: 4.053212451934814, val loss: 4.068251514434815, ETA in seconds: 12912800.249\n",
      "epoch: 387600, train loss: 4.0510640144348145, val loss: 4.062392139434815, ETA in seconds: 12913937.949\n",
      "epoch: 387700, train loss: 4.051454639434814, val loss: 4.0735249519348145, ETA in seconds: 12915281.945\n",
      "epoch: 387800, train loss: 4.048915576934815, val loss: 4.067274951934815, ETA in seconds: 12916341.447\n",
      "epoch: 387900, train loss: 4.0452046394348145, val loss: 4.063564014434815, ETA in seconds: 12917561.264\n",
      "epoch: 388000, train loss: 4.060439014434815, val loss: 4.0696187019348145, ETA in seconds: 12918760.713\n",
      "epoch: 388100, train loss: 4.054774951934815, val loss: 4.061220264434814, ETA in seconds: 12919929.743\n",
      "epoch: 388200, train loss: 4.050868701934815, val loss: 4.073329639434815, ETA in seconds: 12921146.426\n",
      "epoch: 388300, train loss: 4.0539937019348145, val loss: 4.066103076934814, ETA in seconds: 12922318.125\n",
      "epoch: 388400, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 12923567.547\n",
      "epoch: 388500, train loss: 4.052431201934814, val loss: 4.062978076934814, ETA in seconds: 12924645.837\n",
      "epoch: 388600, train loss: 4.0461812019348145, val loss: 4.0647358894348145, ETA in seconds: 12925821.187\n",
      "epoch: 388700, train loss: 4.050478076934814, val loss: 4.066298389434815, ETA in seconds: 12927044.280\n",
      "epoch: 388800, train loss: 4.052235889434814, val loss: 4.065321826934815, ETA in seconds: 12928211.212\n",
      "epoch: 388900, train loss: 4.061220264434814, val loss: 4.0696187019348145, ETA in seconds: 12929482.824\n",
      "epoch: 389000, train loss: 4.054189014434814, val loss: 4.070009326934814, ETA in seconds: 12930676.877\n",
      "epoch: 389100, train loss: 4.055751514434815, val loss: 4.070009326934814, ETA in seconds: 12931822.262\n",
      "epoch: 389200, train loss: 4.051649951934815, val loss: 4.066493701934815, ETA in seconds: 12933001.042\n",
      "epoch: 389300, train loss: 4.0452046394348145, val loss: 4.0686421394348145, ETA in seconds: 12934171.823\n",
      "epoch: 389400, train loss: 4.053798389434815, val loss: 4.066298389434815, ETA in seconds: 12935328.586\n",
      "epoch: 389500, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 12936546.143\n",
      "epoch: 389600, train loss: 4.043056201934815, val loss: 4.066493701934815, ETA in seconds: 12937748.463\n",
      "epoch: 389700, train loss: 4.053212451934814, val loss: 4.0647358894348145, ETA in seconds: 12938966.240\n",
      "epoch: 389800, train loss: 4.051454639434814, val loss: 4.070399951934815, ETA in seconds: 12940063.075\n",
      "epoch: 389900, train loss: 4.050282764434814, val loss: 4.0666890144348145, ETA in seconds: 12941265.038\n",
      "epoch: 390000, train loss: 4.047353076934814, val loss: 4.065321826934815, ETA in seconds: 12942474.378\n",
      "epoch: 390100, train loss: 4.047939014434815, val loss: 4.072157764434815, ETA in seconds: 12943707.935\n",
      "epoch: 390200, train loss: 4.055556201934815, val loss: 4.062196826934814, ETA in seconds: 12945041.580\n",
      "epoch: 390300, train loss: 4.050282764434814, val loss: 4.065907764434814, ETA in seconds: 12946378.003\n",
      "epoch: 390400, train loss: 4.0588765144348145, val loss: 4.064345264434815, ETA in seconds: 12947613.585\n",
      "epoch: 390500, train loss: 4.049696826934815, val loss: 4.0686421394348145, ETA in seconds: 12948856.790\n",
      "epoch: 390600, train loss: 4.048720264434815, val loss: 4.063564014434815, ETA in seconds: 12950101.527\n",
      "epoch: 390700, train loss: 4.059071826934814, val loss: 4.0666890144348145, ETA in seconds: 12951327.731\n",
      "epoch: 390800, train loss: 4.049696826934815, val loss: 4.062587451934815, ETA in seconds: 12952508.460\n",
      "epoch: 390900, train loss: 4.057704639434815, val loss: 4.071181201934815, ETA in seconds: 12953667.496\n",
      "epoch: 391000, train loss: 4.047548389434814, val loss: 4.0666890144348145, ETA in seconds: 12954873.246\n",
      "epoch: 391100, train loss: 4.0471577644348145, val loss: 4.069814014434814, ETA in seconds: 12956105.067\n",
      "epoch: 391200, train loss: 4.050868701934815, val loss: 4.069228076934815, ETA in seconds: 12957269.134\n",
      "epoch: 391300, train loss: 4.049892139434815, val loss: 4.066103076934814, ETA in seconds: 12958434.413\n",
      "epoch: 391400, train loss: 4.054579639434815, val loss: 4.067470264434815, ETA in seconds: 12959630.156\n",
      "epoch: 391500, train loss: 4.056337451934814, val loss: 4.067079639434814, ETA in seconds: 12960752.737\n",
      "epoch: 391600, train loss: 4.044032764434815, val loss: 4.066298389434815, ETA in seconds: 12961857.347\n",
      "epoch: 391700, train loss: 4.051649951934815, val loss: 4.070399951934815, ETA in seconds: 12962970.666\n",
      "epoch: 391800, train loss: 4.0452046394348145, val loss: 4.0666890144348145, ETA in seconds: 12964105.452\n",
      "epoch: 391900, train loss: 4.046571826934814, val loss: 4.069814014434814, ETA in seconds: 12965005.779\n",
      "epoch: 392000, train loss: 4.0500874519348145, val loss: 4.061024951934814, ETA in seconds: 12965457.027\n",
      "epoch: 392100, train loss: 4.052431201934814, val loss: 4.066493701934815, ETA in seconds: 12966162.176\n",
      "epoch: 392200, train loss: 4.050282764434814, val loss: 4.065126514434814, ETA in seconds: 12967284.922\n",
      "epoch: 392300, train loss: 4.050282764434814, val loss: 4.064540576934815, ETA in seconds: 12968473.991\n",
      "epoch: 392400, train loss: 4.0520405769348145, val loss: 4.066103076934814, ETA in seconds: 12969574.535\n",
      "epoch: 392500, train loss: 4.049696826934815, val loss: 4.071376514434815, ETA in seconds: 12970661.215\n",
      "epoch: 392600, train loss: 4.054384326934814, val loss: 4.072353076934815, ETA in seconds: 12971738.012\n",
      "epoch: 392700, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 12972777.776\n",
      "epoch: 392800, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 12973843.578\n",
      "epoch: 392900, train loss: 4.045009326934815, val loss: 4.067860889434814, ETA in seconds: 12974992.139\n",
      "epoch: 393000, train loss: 4.051454639434814, val loss: 4.063368701934815, ETA in seconds: 12976079.527\n",
      "epoch: 393100, train loss: 4.045595264434814, val loss: 4.072353076934815, ETA in seconds: 12977120.478\n",
      "epoch: 393200, train loss: 4.052821826934815, val loss: 4.062001514434814, ETA in seconds: 12978252.844\n",
      "epoch: 393300, train loss: 4.048524951934814, val loss: 4.067860889434814, ETA in seconds: 12979352.338\n",
      "epoch: 393400, train loss: 4.047939014434815, val loss: 4.0676655769348145, ETA in seconds: 12980400.095\n",
      "epoch: 393500, train loss: 4.047353076934814, val loss: 4.0705952644348145, ETA in seconds: 12981491.256\n",
      "epoch: 393600, train loss: 4.052431201934814, val loss: 4.064931201934814, ETA in seconds: 12982570.916\n",
      "epoch: 393700, train loss: 4.0500874519348145, val loss: 4.064540576934815, ETA in seconds: 12983585.916\n",
      "epoch: 393800, train loss: 4.041689014434814, val loss: 4.064540576934815, ETA in seconds: 12984656.446\n",
      "epoch: 393900, train loss: 4.051259326934814, val loss: 4.063564014434815, ETA in seconds: 12985671.521\n",
      "epoch: 394000, train loss: 4.0530171394348145, val loss: 4.061610889434815, ETA in seconds: 12986730.183\n",
      "epoch: 394100, train loss: 4.054579639434815, val loss: 4.069032764434814, ETA in seconds: 12987765.405\n",
      "epoch: 394200, train loss: 4.052626514434815, val loss: 4.071962451934814, ETA in seconds: 12988810.367\n",
      "epoch: 394300, train loss: 4.0510640144348145, val loss: 4.063173389434814, ETA in seconds: 12989870.709\n",
      "epoch: 394400, train loss: 4.057509326934815, val loss: 4.068056201934814, ETA in seconds: 12990988.517\n",
      "epoch: 394500, train loss: 4.048720264434815, val loss: 4.067274951934815, ETA in seconds: 12992150.336\n",
      "epoch: 394600, train loss: 4.047939014434815, val loss: 4.065126514434814, ETA in seconds: 12993283.146\n",
      "epoch: 394700, train loss: 4.0461812019348145, val loss: 4.071181201934815, ETA in seconds: 12994520.401\n",
      "epoch: 394800, train loss: 4.055556201934815, val loss: 4.069814014434814, ETA in seconds: 12995717.090\n",
      "epoch: 394900, train loss: 4.0539937019348145, val loss: 4.070399951934815, ETA in seconds: 12996898.455\n",
      "epoch: 395000, train loss: 4.057118701934814, val loss: 4.069032764434814, ETA in seconds: 12997954.238\n",
      "epoch: 395100, train loss: 4.050673389434815, val loss: 4.064149951934814, ETA in seconds: 12999091.726\n",
      "epoch: 395200, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 13000315.795\n",
      "epoch: 395300, train loss: 4.055165576934814, val loss: 4.0696187019348145, ETA in seconds: 13001442.571\n",
      "epoch: 395400, train loss: 4.059462451934815, val loss: 4.067274951934815, ETA in seconds: 13002574.246\n",
      "epoch: 395500, train loss: 4.0588765144348145, val loss: 4.0647358894348145, ETA in seconds: 13003686.463\n",
      "epoch: 395600, train loss: 4.051649951934815, val loss: 4.065907764434814, ETA in seconds: 13004717.224\n",
      "epoch: 395700, train loss: 4.055360889434814, val loss: 4.074306201934815, ETA in seconds: 13005757.090\n",
      "epoch: 395800, train loss: 4.0481343269348145, val loss: 4.066884326934814, ETA in seconds: 13006915.360\n",
      "epoch: 395900, train loss: 4.045985889434815, val loss: 4.062196826934814, ETA in seconds: 13007975.550\n",
      "epoch: 396000, train loss: 4.052626514434815, val loss: 4.065321826934815, ETA in seconds: 13009125.420\n",
      "epoch: 396100, train loss: 4.045399951934814, val loss: 4.065517139434815, ETA in seconds: 13010242.462\n",
      "epoch: 396200, train loss: 4.049306201934814, val loss: 4.066103076934814, ETA in seconds: 13011272.903\n",
      "epoch: 396300, train loss: 4.059071826934814, val loss: 4.0715718269348145, ETA in seconds: 13012270.870\n",
      "epoch: 396400, train loss: 4.053798389434815, val loss: 4.063564014434815, ETA in seconds: 13013434.663\n",
      "epoch: 396500, train loss: 4.058485889434815, val loss: 4.069032764434814, ETA in seconds: 13014586.774\n",
      "epoch: 396600, train loss: 4.0530171394348145, val loss: 4.0647358894348145, ETA in seconds: 13015791.580\n",
      "epoch: 396700, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 13016902.691\n",
      "epoch: 396800, train loss: 4.057509326934815, val loss: 4.065517139434815, ETA in seconds: 13018217.141\n",
      "epoch: 396900, train loss: 4.061610889434815, val loss: 4.066884326934814, ETA in seconds: 13019438.534\n",
      "epoch: 397000, train loss: 4.054384326934814, val loss: 4.069814014434814, ETA in seconds: 13020649.031\n",
      "epoch: 397100, train loss: 4.053603076934815, val loss: 4.067079639434814, ETA in seconds: 13021856.679\n",
      "epoch: 397200, train loss: 4.048720264434815, val loss: 4.065126514434814, ETA in seconds: 13022793.140\n",
      "epoch: 397300, train loss: 4.053798389434815, val loss: 4.064931201934814, ETA in seconds: 13023944.578\n",
      "epoch: 397400, train loss: 4.051454639434814, val loss: 4.065321826934815, ETA in seconds: 13025111.961\n",
      "epoch: 397500, train loss: 4.049696826934815, val loss: 4.067470264434815, ETA in seconds: 13026174.736\n",
      "epoch: 397600, train loss: 4.051649951934815, val loss: 4.072939014434814, ETA in seconds: 13027180.514\n",
      "epoch: 397700, train loss: 4.053798389434815, val loss: 4.070790576934814, ETA in seconds: 13028274.788\n",
      "epoch: 397800, train loss: 4.0539937019348145, val loss: 4.071767139434814, ETA in seconds: 13029242.469\n",
      "epoch: 397900, train loss: 4.0588765144348145, val loss: 4.062196826934814, ETA in seconds: 13030281.059\n",
      "epoch: 398000, train loss: 4.054579639434815, val loss: 4.0696187019348145, ETA in seconds: 13031416.004\n",
      "epoch: 398100, train loss: 4.0500874519348145, val loss: 4.0666890144348145, ETA in seconds: 13032470.025\n",
      "epoch: 398200, train loss: 4.046376514434814, val loss: 4.070009326934814, ETA in seconds: 13033454.094\n",
      "epoch: 398300, train loss: 4.0608296394348145, val loss: 4.070399951934815, ETA in seconds: 13034562.298\n",
      "epoch: 398400, train loss: 4.050673389434815, val loss: 4.064149951934814, ETA in seconds: 13035668.016\n",
      "epoch: 398500, train loss: 4.0481343269348145, val loss: 4.069228076934815, ETA in seconds: 13036751.028\n",
      "epoch: 398600, train loss: 4.050282764434814, val loss: 4.070399951934815, ETA in seconds: 13037768.712\n",
      "epoch: 398700, train loss: 4.062978076934814, val loss: 4.068056201934814, ETA in seconds: 13038800.313\n",
      "epoch: 398800, train loss: 4.055751514434815, val loss: 4.064149951934814, ETA in seconds: 13039885.043\n",
      "epoch: 398900, train loss: 4.053407764434814, val loss: 4.068251514434815, ETA in seconds: 13041145.730\n",
      "epoch: 399000, train loss: 4.047548389434814, val loss: 4.0676655769348145, ETA in seconds: 13042354.593\n",
      "epoch: 399100, train loss: 4.052235889434814, val loss: 4.064345264434815, ETA in seconds: 13043530.240\n",
      "epoch: 399200, train loss: 4.059267139434814, val loss: 4.0696187019348145, ETA in seconds: 13044761.285\n",
      "epoch: 399300, train loss: 4.061415576934815, val loss: 4.063564014434815, ETA in seconds: 13045827.667\n",
      "epoch: 399400, train loss: 4.058290576934814, val loss: 4.0618062019348145, ETA in seconds: 13046994.197\n",
      "epoch: 399500, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 13048143.073\n",
      "epoch: 399600, train loss: 4.054774951934815, val loss: 4.069032764434814, ETA in seconds: 13049310.626\n",
      "epoch: 399700, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 13050405.893\n",
      "epoch: 399800, train loss: 4.049306201934814, val loss: 4.069032764434814, ETA in seconds: 13051382.283\n",
      "epoch: 399900, train loss: 4.0520405769348145, val loss: 4.070204639434815, ETA in seconds: 13052495.796\n",
      "epoch: 400000, train loss: 4.045790576934815, val loss: 4.066103076934814, ETA in seconds: 13053630.027\n",
      "epoch: 400100, train loss: 4.050282764434814, val loss: 4.0647358894348145, ETA in seconds: 13054656.011\n",
      "epoch: 400200, train loss: 4.0500874519348145, val loss: 4.067079639434814, ETA in seconds: 13055804.374\n",
      "epoch: 400300, train loss: 4.055165576934814, val loss: 4.065517139434815, ETA in seconds: 13056821.407\n",
      "epoch: 400400, train loss: 4.054579639434815, val loss: 4.0686421394348145, ETA in seconds: 13057858.093\n",
      "epoch: 400500, train loss: 4.049306201934814, val loss: 4.064149951934814, ETA in seconds: 13058967.212\n",
      "epoch: 400600, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 13060131.570\n",
      "epoch: 400700, train loss: 4.054189014434814, val loss: 4.066298389434815, ETA in seconds: 13061436.970\n",
      "epoch: 400800, train loss: 4.056337451934814, val loss: 4.074110889434815, ETA in seconds: 13062482.075\n",
      "epoch: 400900, train loss: 4.052235889434814, val loss: 4.0725483894348145, ETA in seconds: 13063611.542\n",
      "epoch: 401000, train loss: 4.047353076934814, val loss: 4.068056201934814, ETA in seconds: 13064302.165\n",
      "epoch: 401100, train loss: 4.049501514434814, val loss: 4.0637593269348145, ETA in seconds: 13065310.520\n",
      "epoch: 401200, train loss: 4.0500874519348145, val loss: 4.066103076934814, ETA in seconds: 13066414.494\n",
      "epoch: 401300, train loss: 4.0578999519348145, val loss: 4.066103076934814, ETA in seconds: 13067457.754\n",
      "epoch: 401400, train loss: 4.046376514434814, val loss: 4.067860889434814, ETA in seconds: 13068444.119\n",
      "epoch: 401500, train loss: 4.056532764434815, val loss: 4.063954639434814, ETA in seconds: 13069450.966\n",
      "epoch: 401600, train loss: 4.054384326934814, val loss: 4.069032764434814, ETA in seconds: 13070601.526\n",
      "epoch: 401700, train loss: 4.055165576934814, val loss: 4.068056201934814, ETA in seconds: 13071739.949\n",
      "epoch: 401800, train loss: 4.055360889434814, val loss: 4.072353076934815, ETA in seconds: 13072770.545\n",
      "epoch: 401900, train loss: 4.053407764434814, val loss: 4.0647358894348145, ETA in seconds: 13073782.309\n",
      "epoch: 402000, train loss: 4.050282764434814, val loss: 4.067274951934815, ETA in seconds: 13074857.350\n",
      "epoch: 402100, train loss: 4.052821826934815, val loss: 4.063173389434814, ETA in seconds: 13075875.809\n",
      "epoch: 402200, train loss: 4.053407764434814, val loss: 4.067470264434815, ETA in seconds: 13076797.367\n",
      "epoch: 402300, train loss: 4.0520405769348145, val loss: 4.069032764434814, ETA in seconds: 13077847.445\n",
      "epoch: 402400, train loss: 4.062001514434814, val loss: 4.062978076934814, ETA in seconds: 13078902.914\n",
      "epoch: 402500, train loss: 4.053798389434815, val loss: 4.063564014434815, ETA in seconds: 13079602.725\n",
      "epoch: 402600, train loss: 4.0471577644348145, val loss: 4.068837451934814, ETA in seconds: 13080305.375\n",
      "epoch: 402700, train loss: 4.0530171394348145, val loss: 4.063954639434814, ETA in seconds: 13081286.042\n",
      "epoch: 402800, train loss: 4.0549702644348145, val loss: 4.070399951934815, ETA in seconds: 13082399.940\n",
      "epoch: 402900, train loss: 4.056728076934815, val loss: 4.069032764434814, ETA in seconds: 13083628.758\n",
      "epoch: 403000, train loss: 4.052626514434815, val loss: 4.0657124519348145, ETA in seconds: 13084716.667\n",
      "epoch: 403100, train loss: 4.054189014434814, val loss: 4.070399951934815, ETA in seconds: 13085760.428\n",
      "epoch: 403200, train loss: 4.053212451934814, val loss: 4.0627827644348145, ETA in seconds: 13086858.438\n",
      "epoch: 403300, train loss: 4.051454639434814, val loss: 4.065517139434815, ETA in seconds: 13087937.427\n",
      "epoch: 403400, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 13089081.333\n",
      "epoch: 403500, train loss: 4.048915576934815, val loss: 4.065321826934815, ETA in seconds: 13090155.441\n",
      "epoch: 403600, train loss: 4.055360889434814, val loss: 4.066493701934815, ETA in seconds: 13091268.196\n",
      "epoch: 403700, train loss: 4.0461812019348145, val loss: 4.067079639434814, ETA in seconds: 13092317.217\n",
      "epoch: 403800, train loss: 4.047939014434815, val loss: 4.062978076934814, ETA in seconds: 13093330.393\n",
      "epoch: 403900, train loss: 4.051454639434814, val loss: 4.073329639434815, ETA in seconds: 13094410.097\n",
      "epoch: 404000, train loss: 4.053212451934814, val loss: 4.072353076934815, ETA in seconds: 13095613.564\n",
      "epoch: 404100, train loss: 4.0539937019348145, val loss: 4.063173389434814, ETA in seconds: 13096742.040\n",
      "epoch: 404200, train loss: 4.0500874519348145, val loss: 4.063368701934815, ETA in seconds: 13097689.958\n",
      "epoch: 404300, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 13098795.863\n",
      "epoch: 404400, train loss: 4.048329639434814, val loss: 4.070399951934815, ETA in seconds: 13099755.546\n",
      "epoch: 404500, train loss: 4.052821826934815, val loss: 4.0705952644348145, ETA in seconds: 13100945.198\n",
      "epoch: 404600, train loss: 4.0461812019348145, val loss: 4.068056201934814, ETA in seconds: 13102020.754\n",
      "epoch: 404700, train loss: 4.057314014434814, val loss: 4.064540576934815, ETA in seconds: 13103239.673\n",
      "epoch: 404800, train loss: 4.051454639434814, val loss: 4.067860889434814, ETA in seconds: 13104386.248\n",
      "epoch: 404900, train loss: 4.057509326934815, val loss: 4.065126514434814, ETA in seconds: 13105346.946\n",
      "epoch: 405000, train loss: 4.048329639434814, val loss: 4.070204639434815, ETA in seconds: 13106250.853\n",
      "epoch: 405100, train loss: 4.053212451934814, val loss: 4.069032764434814, ETA in seconds: 13107331.594\n",
      "epoch: 405200, train loss: 4.047743701934815, val loss: 4.065907764434814, ETA in seconds: 13108396.115\n",
      "epoch: 405300, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 13109473.109\n",
      "epoch: 405400, train loss: 4.046962451934815, val loss: 4.063368701934815, ETA in seconds: 13110438.106\n",
      "epoch: 405500, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 13111442.503\n",
      "epoch: 405600, train loss: 4.051649951934815, val loss: 4.070985889434814, ETA in seconds: 13112342.606\n",
      "epoch: 405700, train loss: 4.051845264434815, val loss: 4.0666890144348145, ETA in seconds: 13113257.309\n",
      "epoch: 405800, train loss: 4.0539937019348145, val loss: 4.0686421394348145, ETA in seconds: 13114330.784\n",
      "epoch: 405900, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 13115412.872\n",
      "epoch: 406000, train loss: 4.0539937019348145, val loss: 4.0676655769348145, ETA in seconds: 13116478.367\n",
      "epoch: 406100, train loss: 4.055556201934815, val loss: 4.071767139434814, ETA in seconds: 13117539.051\n",
      "epoch: 406200, train loss: 4.053407764434814, val loss: 4.068837451934814, ETA in seconds: 13119152.618\n",
      "epoch: 406300, train loss: 4.051259326934814, val loss: 4.071962451934814, ETA in seconds: 13120571.823\n",
      "epoch: 406400, train loss: 4.0461812019348145, val loss: 4.066884326934814, ETA in seconds: 13121595.135\n",
      "epoch: 406500, train loss: 4.0539937019348145, val loss: 4.066884326934814, ETA in seconds: 13122641.357\n",
      "epoch: 406600, train loss: 4.052235889434814, val loss: 4.068837451934814, ETA in seconds: 13123665.923\n",
      "epoch: 406700, train loss: 4.0491108894348145, val loss: 4.0686421394348145, ETA in seconds: 13124735.178\n",
      "epoch: 406800, train loss: 4.054189014434814, val loss: 4.063564014434815, ETA in seconds: 13125783.347\n",
      "epoch: 406900, train loss: 4.047939014434815, val loss: 4.068251514434815, ETA in seconds: 13126851.601\n",
      "epoch: 407000, train loss: 4.051845264434815, val loss: 4.0627827644348145, ETA in seconds: 13127840.232\n",
      "epoch: 407100, train loss: 4.0559468269348145, val loss: 4.0627827644348145, ETA in seconds: 13128879.706\n",
      "epoch: 407200, train loss: 4.057118701934814, val loss: 4.0666890144348145, ETA in seconds: 13130012.153\n",
      "epoch: 407300, train loss: 4.051259326934814, val loss: 4.073915576934814, ETA in seconds: 13131240.224\n",
      "epoch: 407400, train loss: 4.053603076934815, val loss: 4.0647358894348145, ETA in seconds: 13132256.590\n",
      "epoch: 407500, train loss: 4.0461812019348145, val loss: 4.0676655769348145, ETA in seconds: 13133475.433\n",
      "epoch: 407600, train loss: 4.045595264434814, val loss: 4.071376514434815, ETA in seconds: 13134685.240\n",
      "epoch: 407700, train loss: 4.0510640144348145, val loss: 4.069814014434814, ETA in seconds: 13135796.841\n",
      "epoch: 407800, train loss: 4.050868701934815, val loss: 4.063368701934815, ETA in seconds: 13136904.214\n",
      "epoch: 407900, train loss: 4.053212451934814, val loss: 4.066493701934815, ETA in seconds: 13137898.397\n",
      "epoch: 408000, train loss: 4.053798389434815, val loss: 4.066493701934815, ETA in seconds: 13138932.330\n",
      "epoch: 408100, train loss: 4.053407764434814, val loss: 4.0696187019348145, ETA in seconds: 13140018.344\n",
      "epoch: 408200, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 13140927.447\n",
      "epoch: 408300, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 13141992.906\n",
      "epoch: 408400, train loss: 4.047743701934815, val loss: 4.066298389434815, ETA in seconds: 13143033.014\n",
      "epoch: 408500, train loss: 4.0559468269348145, val loss: 4.069032764434814, ETA in seconds: 13143975.681\n",
      "epoch: 408600, train loss: 4.050868701934815, val loss: 4.070790576934814, ETA in seconds: 13145043.065\n",
      "epoch: 408700, train loss: 4.054189014434814, val loss: 4.064931201934814, ETA in seconds: 13145995.355\n",
      "epoch: 408800, train loss: 4.0539937019348145, val loss: 4.0666890144348145, ETA in seconds: 13146989.545\n",
      "epoch: 408900, train loss: 4.0539937019348145, val loss: 4.064931201934814, ETA in seconds: 13148039.846\n",
      "epoch: 409000, train loss: 4.052626514434815, val loss: 4.062392139434815, ETA in seconds: 13149195.085\n",
      "epoch: 409100, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 13150290.810\n",
      "epoch: 409200, train loss: 4.0549702644348145, val loss: 4.069423389434815, ETA in seconds: 13151406.591\n",
      "epoch: 409300, train loss: 4.0510640144348145, val loss: 4.067079639434814, ETA in seconds: 13152448.148\n",
      "epoch: 409400, train loss: 4.057704639434815, val loss: 4.063954639434814, ETA in seconds: 13153674.986\n",
      "epoch: 409500, train loss: 4.049892139434815, val loss: 4.070399951934815, ETA in seconds: 13154769.961\n",
      "epoch: 409600, train loss: 4.0549702644348145, val loss: 4.068251514434815, ETA in seconds: 13155857.702\n",
      "epoch: 409700, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 13156921.567\n",
      "epoch: 409800, train loss: 4.047548389434814, val loss: 4.0666890144348145, ETA in seconds: 13157847.935\n",
      "epoch: 409900, train loss: 4.0510640144348145, val loss: 4.072353076934815, ETA in seconds: 13158828.257\n",
      "epoch: 410000, train loss: 4.050282764434814, val loss: 4.064149951934814, ETA in seconds: 13159882.698\n",
      "epoch: 410100, train loss: 4.049501514434814, val loss: 4.070985889434814, ETA in seconds: 13160864.450\n",
      "epoch: 410200, train loss: 4.043642139434814, val loss: 4.0696187019348145, ETA in seconds: 13161652.056\n",
      "epoch: 410300, train loss: 4.0452046394348145, val loss: 4.073720264434814, ETA in seconds: 13162766.834\n",
      "epoch: 410400, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 13163673.549\n",
      "epoch: 410500, train loss: 4.049306201934814, val loss: 4.069228076934815, ETA in seconds: 13164473.808\n",
      "epoch: 410600, train loss: 4.061610889434815, val loss: 4.065321826934815, ETA in seconds: 13165200.389\n",
      "epoch: 410700, train loss: 4.053407764434814, val loss: 4.067274951934815, ETA in seconds: 13166272.725\n",
      "epoch: 410800, train loss: 4.049306201934814, val loss: 4.060243701934814, ETA in seconds: 13167328.426\n",
      "epoch: 410900, train loss: 4.0549702644348145, val loss: 4.065126514434814, ETA in seconds: 13168292.768\n",
      "epoch: 411000, train loss: 4.054774951934815, val loss: 4.072353076934815, ETA in seconds: 13169316.699\n",
      "epoch: 411100, train loss: 4.056728076934815, val loss: 4.069423389434815, ETA in seconds: 13170433.199\n",
      "epoch: 411200, train loss: 4.0491108894348145, val loss: 4.067274951934815, ETA in seconds: 13171555.732\n",
      "epoch: 411300, train loss: 4.048720264434815, val loss: 4.064931201934814, ETA in seconds: 13172574.597\n",
      "epoch: 411400, train loss: 4.049892139434815, val loss: 4.064345264434815, ETA in seconds: 13173430.826\n",
      "epoch: 411500, train loss: 4.051649951934815, val loss: 4.065517139434815, ETA in seconds: 13174352.050\n",
      "epoch: 411600, train loss: 4.049501514434814, val loss: 4.066493701934815, ETA in seconds: 13175306.080\n",
      "epoch: 411700, train loss: 4.057704639434815, val loss: 4.070009326934814, ETA in seconds: 13176368.480\n",
      "epoch: 411800, train loss: 4.053212451934814, val loss: 4.066884326934814, ETA in seconds: 13177419.510\n",
      "epoch: 411900, train loss: 4.054579639434815, val loss: 4.063368701934815, ETA in seconds: 13178391.100\n",
      "epoch: 412000, train loss: 4.0500874519348145, val loss: 4.0676655769348145, ETA in seconds: 13179434.482\n",
      "epoch: 412100, train loss: 4.051259326934814, val loss: 4.070009326934814, ETA in seconds: 13180313.057\n",
      "epoch: 412200, train loss: 4.054384326934814, val loss: 4.065126514434814, ETA in seconds: 13181185.693\n",
      "epoch: 412300, train loss: 4.0549702644348145, val loss: 4.065126514434814, ETA in seconds: 13182093.261\n",
      "epoch: 412400, train loss: 4.0471577644348145, val loss: 4.062001514434814, ETA in seconds: 13183037.442\n",
      "epoch: 412500, train loss: 4.049501514434814, val loss: 4.0666890144348145, ETA in seconds: 13183890.843\n",
      "epoch: 412600, train loss: 4.052235889434814, val loss: 4.069814014434814, ETA in seconds: 13184854.504\n",
      "epoch: 412700, train loss: 4.049892139434815, val loss: 4.062001514434814, ETA in seconds: 13185838.518\n",
      "epoch: 412800, train loss: 4.053212451934814, val loss: 4.067079639434814, ETA in seconds: 13186842.923\n",
      "epoch: 412900, train loss: 4.0510640144348145, val loss: 4.069814014434814, ETA in seconds: 13187673.577\n",
      "epoch: 413000, train loss: 4.049306201934814, val loss: 4.062587451934815, ETA in seconds: 13188602.933\n",
      "epoch: 413100, train loss: 4.053603076934815, val loss: 4.0686421394348145, ETA in seconds: 13189362.975\n",
      "epoch: 413200, train loss: 4.053603076934815, val loss: 4.066103076934814, ETA in seconds: 13190442.205\n",
      "epoch: 413300, train loss: 4.053603076934815, val loss: 4.072743701934814, ETA in seconds: 13191278.728\n",
      "epoch: 413400, train loss: 4.053603076934815, val loss: 4.071181201934815, ETA in seconds: 13192151.291\n",
      "epoch: 413500, train loss: 4.046962451934815, val loss: 4.067860889434814, ETA in seconds: 13193185.816\n",
      "epoch: 413600, train loss: 4.052626514434815, val loss: 4.071376514434815, ETA in seconds: 13194080.172\n",
      "epoch: 413700, train loss: 4.045009326934815, val loss: 4.0627827644348145, ETA in seconds: 13194994.326\n",
      "epoch: 413800, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 13195931.364\n",
      "epoch: 413900, train loss: 4.0530171394348145, val loss: 4.068837451934814, ETA in seconds: 13196779.010\n",
      "epoch: 414000, train loss: 4.048720264434815, val loss: 4.0696187019348145, ETA in seconds: 13197624.610\n",
      "epoch: 414100, train loss: 4.052431201934814, val loss: 4.0657124519348145, ETA in seconds: 13198684.906\n",
      "epoch: 414200, train loss: 4.048329639434814, val loss: 4.069228076934815, ETA in seconds: 13199635.964\n",
      "epoch: 414300, train loss: 4.056532764434815, val loss: 4.072353076934815, ETA in seconds: 13200490.161\n",
      "epoch: 414400, train loss: 4.051454639434814, val loss: 4.072353076934815, ETA in seconds: 13201425.451\n",
      "epoch: 414500, train loss: 4.052821826934815, val loss: 4.068837451934814, ETA in seconds: 13202346.294\n",
      "epoch: 414600, train loss: 4.053603076934815, val loss: 4.071962451934814, ETA in seconds: 13203319.613\n",
      "epoch: 414700, train loss: 4.0598530769348145, val loss: 4.063954639434814, ETA in seconds: 13204185.543\n",
      "epoch: 414800, train loss: 4.045985889434815, val loss: 4.065321826934815, ETA in seconds: 13205049.318\n",
      "epoch: 414900, train loss: 4.049696826934815, val loss: 4.066298389434815, ETA in seconds: 13205880.154\n",
      "epoch: 415000, train loss: 4.061610889434815, val loss: 4.074110889434815, ETA in seconds: 13206942.589\n",
      "epoch: 415100, train loss: 4.052431201934814, val loss: 4.067470264434815, ETA in seconds: 13207986.581\n",
      "epoch: 415200, train loss: 4.053603076934815, val loss: 4.057314014434814, ETA in seconds: 13208882.998\n",
      "epoch: 415300, train loss: 4.048524951934814, val loss: 4.071181201934815, ETA in seconds: 13209753.177\n",
      "epoch: 415400, train loss: 4.049696826934815, val loss: 4.068446826934815, ETA in seconds: 13210626.782\n",
      "epoch: 415500, train loss: 4.046376514434814, val loss: 4.062001514434814, ETA in seconds: 13211483.084\n",
      "epoch: 415600, train loss: 4.0539937019348145, val loss: 4.070009326934814, ETA in seconds: 13212356.046\n",
      "epoch: 415700, train loss: 4.0471577644348145, val loss: 4.069032764434814, ETA in seconds: 13213419.971\n",
      "epoch: 415800, train loss: 4.050673389434815, val loss: 4.066103076934814, ETA in seconds: 13214454.346\n",
      "epoch: 415900, train loss: 4.051649951934815, val loss: 4.068446826934815, ETA in seconds: 13215330.179\n",
      "epoch: 416000, train loss: 4.054189014434814, val loss: 4.061415576934815, ETA in seconds: 13216185.974\n",
      "epoch: 416100, train loss: 4.054579639434815, val loss: 4.0666890144348145, ETA in seconds: 13217133.575\n",
      "epoch: 416200, train loss: 4.0500874519348145, val loss: 4.063954639434814, ETA in seconds: 13217606.283\n",
      "epoch: 416300, train loss: 4.0588765144348145, val loss: 4.069032764434814, ETA in seconds: 13218515.460\n",
      "epoch: 416400, train loss: 4.051454639434814, val loss: 4.071181201934815, ETA in seconds: 13219498.391\n",
      "epoch: 416500, train loss: 4.0530171394348145, val loss: 4.066493701934815, ETA in seconds: 13220372.115\n",
      "epoch: 416600, train loss: 4.049892139434815, val loss: 4.068251514434815, ETA in seconds: 13221249.877\n",
      "epoch: 416700, train loss: 4.046767139434815, val loss: 4.068837451934814, ETA in seconds: 13222111.050\n",
      "epoch: 416800, train loss: 4.060048389434814, val loss: 4.066493701934815, ETA in seconds: 13223182.350\n",
      "epoch: 416900, train loss: 4.053603076934815, val loss: 4.0745015144348145, ETA in seconds: 13224127.126\n",
      "epoch: 417000, train loss: 4.047548389434814, val loss: 4.066298389434815, ETA in seconds: 13224920.463\n",
      "epoch: 417100, train loss: 4.049501514434814, val loss: 4.071962451934814, ETA in seconds: 13225900.907\n",
      "epoch: 417200, train loss: 4.047548389434814, val loss: 4.0647358894348145, ETA in seconds: 13226900.714\n",
      "epoch: 417300, train loss: 4.0481343269348145, val loss: 4.063368701934815, ETA in seconds: 13227794.512\n",
      "epoch: 417400, train loss: 4.051259326934814, val loss: 4.069814014434814, ETA in seconds: 13228772.604\n",
      "epoch: 417500, train loss: 4.046376514434814, val loss: 4.067274951934815, ETA in seconds: 13229613.107\n",
      "epoch: 417600, train loss: 4.055556201934815, val loss: 4.064931201934814, ETA in seconds: 13230452.776\n",
      "epoch: 417700, train loss: 4.0520405769348145, val loss: 4.0686421394348145, ETA in seconds: 13231458.175\n",
      "epoch: 417800, train loss: 4.0539937019348145, val loss: 4.068056201934814, ETA in seconds: 13232399.490\n",
      "epoch: 417900, train loss: 4.0500874519348145, val loss: 4.063954639434814, ETA in seconds: 13233274.212\n",
      "epoch: 418000, train loss: 4.049696826934815, val loss: 4.073329639434815, ETA in seconds: 13234069.258\n",
      "epoch: 418100, train loss: 4.057704639434815, val loss: 4.0686421394348145, ETA in seconds: 13234965.585\n",
      "epoch: 418200, train loss: 4.054189014434814, val loss: 4.062392139434815, ETA in seconds: 13235904.564\n",
      "epoch: 418300, train loss: 4.048329639434814, val loss: 4.067860889434814, ETA in seconds: 13236901.939\n",
      "epoch: 418400, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 13237750.337\n",
      "epoch: 418500, train loss: 4.052431201934814, val loss: 4.066884326934814, ETA in seconds: 13238546.490\n",
      "epoch: 418600, train loss: 4.053407764434814, val loss: 4.062196826934814, ETA in seconds: 13239343.103\n",
      "epoch: 418700, train loss: 4.049306201934814, val loss: 4.063368701934815, ETA in seconds: 13240137.725\n",
      "epoch: 418800, train loss: 4.051259326934814, val loss: 4.0618062019348145, ETA in seconds: 13240983.752\n",
      "epoch: 418900, train loss: 4.059462451934815, val loss: 4.065907764434814, ETA in seconds: 13241835.664\n",
      "epoch: 419000, train loss: 4.051649951934815, val loss: 4.062587451934815, ETA in seconds: 13242772.768\n",
      "epoch: 419100, train loss: 4.057509326934815, val loss: 4.071181201934815, ETA in seconds: 13243627.470\n",
      "epoch: 419200, train loss: 4.056337451934814, val loss: 4.068837451934814, ETA in seconds: 13244380.328\n",
      "epoch: 419300, train loss: 4.050282764434814, val loss: 4.0686421394348145, ETA in seconds: 13245151.894\n",
      "epoch: 419400, train loss: 4.049306201934814, val loss: 4.070985889434814, ETA in seconds: 13245970.676\n",
      "epoch: 419500, train loss: 4.0491108894348145, val loss: 4.071376514434815, ETA in seconds: 13246833.294\n",
      "epoch: 419600, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 13247638.383\n",
      "epoch: 419700, train loss: 4.061220264434814, val loss: 4.066493701934815, ETA in seconds: 13248504.538\n",
      "epoch: 419800, train loss: 4.056532764434815, val loss: 4.070399951934815, ETA in seconds: 13249356.350\n",
      "epoch: 419900, train loss: 4.055556201934815, val loss: 4.067079639434814, ETA in seconds: 13250077.464\n",
      "epoch: 420000, train loss: 4.049306201934814, val loss: 4.063954639434814, ETA in seconds: 13250870.332\n",
      "epoch: 420100, train loss: 4.048720264434815, val loss: 4.0715718269348145, ETA in seconds: 13251675.152\n",
      "epoch: 420200, train loss: 4.047743701934815, val loss: 4.069814014434814, ETA in seconds: 13252508.542\n",
      "epoch: 420300, train loss: 4.045595264434814, val loss: 4.068446826934815, ETA in seconds: 13253269.424\n",
      "epoch: 420400, train loss: 4.052626514434815, val loss: 4.062978076934814, ETA in seconds: 13254104.354\n",
      "epoch: 420500, train loss: 4.047353076934814, val loss: 4.0637593269348145, ETA in seconds: 13254949.709\n",
      "epoch: 420600, train loss: 4.054579639434815, val loss: 4.0715718269348145, ETA in seconds: 13255831.250\n",
      "epoch: 420700, train loss: 4.053212451934814, val loss: 4.0666890144348145, ETA in seconds: 13256640.453\n",
      "epoch: 420800, train loss: 4.048915576934815, val loss: 4.0657124519348145, ETA in seconds: 13257820.849\n",
      "epoch: 420900, train loss: 4.047939014434815, val loss: 4.065907764434814, ETA in seconds: 13258795.420\n",
      "epoch: 421000, train loss: 4.050478076934814, val loss: 4.063954639434814, ETA in seconds: 13259678.490\n",
      "epoch: 421100, train loss: 4.060048389434814, val loss: 4.068056201934814, ETA in seconds: 13260555.889\n",
      "epoch: 421200, train loss: 4.056532764434815, val loss: 4.058290576934814, ETA in seconds: 13261396.205\n",
      "epoch: 421300, train loss: 4.051845264434815, val loss: 4.066103076934814, ETA in seconds: 13262252.763\n",
      "epoch: 421400, train loss: 4.053603076934815, val loss: 4.069032764434814, ETA in seconds: 13263063.456\n",
      "epoch: 421500, train loss: 4.052431201934814, val loss: 4.062587451934815, ETA in seconds: 13263843.914\n",
      "epoch: 421600, train loss: 4.053407764434814, val loss: 4.065321826934815, ETA in seconds: 13264590.714\n",
      "epoch: 421700, train loss: 4.053798389434815, val loss: 4.070790576934814, ETA in seconds: 13265273.910\n",
      "epoch: 421800, train loss: 4.0510640144348145, val loss: 4.0627827644348145, ETA in seconds: 13265997.956\n",
      "epoch: 421900, train loss: 4.050868701934815, val loss: 4.065321826934815, ETA in seconds: 13266961.804\n",
      "epoch: 422000, train loss: 4.053407764434814, val loss: 4.068056201934814, ETA in seconds: 13267747.100\n",
      "epoch: 422100, train loss: 4.048329639434814, val loss: 4.071376514434815, ETA in seconds: 13268514.953\n",
      "epoch: 422200, train loss: 4.058290576934814, val loss: 4.072743701934814, ETA in seconds: 13269271.763\n",
      "epoch: 422300, train loss: 4.054189014434814, val loss: 4.063173389434814, ETA in seconds: 13269996.036\n",
      "epoch: 422400, train loss: 4.049306201934814, val loss: 4.056142139434814, ETA in seconds: 13270647.968\n",
      "epoch: 422500, train loss: 4.0481343269348145, val loss: 4.068446826934815, ETA in seconds: 13271480.434\n",
      "epoch: 422600, train loss: 4.046571826934814, val loss: 4.062587451934815, ETA in seconds: 13272199.849\n",
      "epoch: 422700, train loss: 4.046962451934815, val loss: 4.063173389434814, ETA in seconds: 13273171.519\n",
      "epoch: 422800, train loss: 4.049306201934814, val loss: 4.0676655769348145, ETA in seconds: 13274190.562\n",
      "epoch: 422900, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 13274974.140\n",
      "epoch: 423000, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 13275678.924\n",
      "epoch: 423100, train loss: 4.049306201934814, val loss: 4.064345264434815, ETA in seconds: 13276398.779\n",
      "epoch: 423200, train loss: 4.045985889434815, val loss: 4.069032764434814, ETA in seconds: 13277111.225\n",
      "epoch: 423300, train loss: 4.049892139434815, val loss: 4.067470264434815, ETA in seconds: 13277899.333\n",
      "epoch: 423400, train loss: 4.052821826934815, val loss: 4.071376514434815, ETA in seconds: 13278789.750\n",
      "epoch: 423500, train loss: 4.054189014434814, val loss: 4.0715718269348145, ETA in seconds: 13279496.375\n",
      "epoch: 423600, train loss: 4.045595264434814, val loss: 4.059657764434815, ETA in seconds: 13280293.335\n",
      "epoch: 423700, train loss: 4.047548389434814, val loss: 4.0705952644348145, ETA in seconds: 13281057.713\n",
      "epoch: 423800, train loss: 4.0481343269348145, val loss: 4.0637593269348145, ETA in seconds: 13281896.370\n",
      "epoch: 423900, train loss: 4.051649951934815, val loss: 4.074306201934815, ETA in seconds: 13282827.669\n",
      "epoch: 424000, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 13283796.966\n",
      "epoch: 424100, train loss: 4.051259326934814, val loss: 4.068446826934815, ETA in seconds: 13284643.948\n",
      "epoch: 424200, train loss: 4.051454639434814, val loss: 4.065907764434814, ETA in seconds: 13285161.175\n",
      "epoch: 424300, train loss: 4.0539937019348145, val loss: 4.0666890144348145, ETA in seconds: 13286112.238\n",
      "epoch: 424400, train loss: 4.0530171394348145, val loss: 4.068446826934815, ETA in seconds: 13287157.758\n",
      "epoch: 424500, train loss: 4.053212451934814, val loss: 4.070985889434814, ETA in seconds: 13288072.485\n",
      "epoch: 424600, train loss: 4.046767139434815, val loss: 4.070399951934815, ETA in seconds: 13288884.083\n",
      "epoch: 424700, train loss: 4.0452046394348145, val loss: 4.069814014434814, ETA in seconds: 13289784.210\n",
      "epoch: 424800, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 13290656.500\n",
      "epoch: 424900, train loss: 4.049696826934815, val loss: 4.070204639434815, ETA in seconds: 13291440.775\n",
      "epoch: 425000, train loss: 4.0510640144348145, val loss: 4.069228076934815, ETA in seconds: 13292167.660\n",
      "epoch: 425100, train loss: 4.050478076934814, val loss: 4.070009326934814, ETA in seconds: 13292946.401\n",
      "epoch: 425200, train loss: 4.055360889434814, val loss: 4.068251514434815, ETA in seconds: 13293839.321\n",
      "epoch: 425300, train loss: 4.0491108894348145, val loss: 4.0676655769348145, ETA in seconds: 13294597.357\n",
      "epoch: 425400, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 13295431.288\n",
      "epoch: 425500, train loss: 4.054384326934814, val loss: 4.070399951934815, ETA in seconds: 13296271.981\n",
      "epoch: 425600, train loss: 4.043056201934815, val loss: 4.064540576934815, ETA in seconds: 13297066.088\n",
      "epoch: 425700, train loss: 4.057314014434814, val loss: 4.072353076934815, ETA in seconds: 13297880.729\n",
      "epoch: 425800, train loss: 4.062978076934814, val loss: 4.067274951934815, ETA in seconds: 13298653.424\n",
      "epoch: 425900, train loss: 4.053407764434814, val loss: 4.064931201934814, ETA in seconds: 13299487.419\n",
      "epoch: 426000, train loss: 4.0500874519348145, val loss: 4.0618062019348145, ETA in seconds: 13300394.672\n",
      "epoch: 426100, train loss: 4.051845264434815, val loss: 4.063564014434815, ETA in seconds: 13301180.041\n",
      "epoch: 426200, train loss: 4.063564014434815, val loss: 4.067470264434815, ETA in seconds: 13301944.803\n",
      "epoch: 426300, train loss: 4.047353076934814, val loss: 4.0666890144348145, ETA in seconds: 13302730.672\n",
      "epoch: 426400, train loss: 4.047353076934814, val loss: 4.064540576934815, ETA in seconds: 13303510.245\n",
      "epoch: 426500, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 13304282.287\n",
      "epoch: 426600, train loss: 4.054774951934815, val loss: 4.068056201934814, ETA in seconds: 13305070.371\n",
      "epoch: 426700, train loss: 4.0539937019348145, val loss: 4.0676655769348145, ETA in seconds: 13305831.251\n",
      "epoch: 426800, train loss: 4.057509326934815, val loss: 4.071767139434814, ETA in seconds: 13306519.348\n",
      "epoch: 426900, train loss: 4.049892139434815, val loss: 4.067079639434814, ETA in seconds: 13307309.072\n",
      "epoch: 427000, train loss: 4.047939014434815, val loss: 4.067470264434815, ETA in seconds: 13307998.464\n",
      "epoch: 427100, train loss: 4.055751514434815, val loss: 4.063564014434815, ETA in seconds: 13308821.089\n",
      "epoch: 427200, train loss: 4.052821826934815, val loss: 4.070790576934814, ETA in seconds: 13309527.981\n",
      "epoch: 427300, train loss: 4.055165576934814, val loss: 4.0696187019348145, ETA in seconds: 13310366.106\n",
      "epoch: 427400, train loss: 4.050868701934815, val loss: 4.065126514434814, ETA in seconds: 13311145.000\n",
      "epoch: 427500, train loss: 4.057118701934814, val loss: 4.066103076934814, ETA in seconds: 13311780.347\n",
      "epoch: 427600, train loss: 4.048329639434814, val loss: 4.067860889434814, ETA in seconds: 13312542.391\n",
      "epoch: 427700, train loss: 4.051649951934815, val loss: 4.063954639434814, ETA in seconds: 13313247.451\n",
      "epoch: 427800, train loss: 4.059462451934815, val loss: 4.071962451934814, ETA in seconds: 13313927.367\n",
      "epoch: 427900, train loss: 4.051649951934815, val loss: 4.067860889434814, ETA in seconds: 13314662.302\n",
      "epoch: 428000, train loss: 4.051259326934814, val loss: 4.067274951934815, ETA in seconds: 13315387.113\n",
      "epoch: 428100, train loss: 4.046376514434814, val loss: 4.0637593269348145, ETA in seconds: 13316053.345\n",
      "epoch: 428200, train loss: 4.044814014434815, val loss: 4.0657124519348145, ETA in seconds: 13316700.606\n",
      "epoch: 428300, train loss: 4.0491108894348145, val loss: 4.0657124519348145, ETA in seconds: 13317446.742\n",
      "epoch: 428400, train loss: 4.050673389434815, val loss: 4.069814014434814, ETA in seconds: 13318354.586\n",
      "epoch: 428500, train loss: 4.049306201934814, val loss: 4.070985889434814, ETA in seconds: 13319181.954\n",
      "epoch: 428600, train loss: 4.047548389434814, val loss: 4.070009326934814, ETA in seconds: 13320057.572\n",
      "epoch: 428700, train loss: 4.054774951934815, val loss: 4.066103076934814, ETA in seconds: 13320792.749\n",
      "epoch: 428800, train loss: 4.0500874519348145, val loss: 4.070204639434815, ETA in seconds: 13321668.340\n",
      "epoch: 428900, train loss: 4.0461812019348145, val loss: 4.0657124519348145, ETA in seconds: 13322526.198\n",
      "epoch: 429000, train loss: 4.054774951934815, val loss: 4.0657124519348145, ETA in seconds: 13323484.675\n",
      "epoch: 429100, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 13324283.268\n",
      "epoch: 429200, train loss: 4.054384326934814, val loss: 4.067860889434814, ETA in seconds: 13325052.797\n",
      "epoch: 429300, train loss: 4.0471577644348145, val loss: 4.069814014434814, ETA in seconds: 13325755.546\n",
      "epoch: 429400, train loss: 4.0578999519348145, val loss: 4.067079639434814, ETA in seconds: 13326470.831\n",
      "epoch: 429500, train loss: 4.045985889434815, val loss: 4.0705952644348145, ETA in seconds: 13327346.281\n",
      "epoch: 429600, train loss: 4.0520405769348145, val loss: 4.069814014434814, ETA in seconds: 13328237.418\n",
      "epoch: 429700, train loss: 4.049892139434815, val loss: 4.059462451934815, ETA in seconds: 13328927.714\n",
      "epoch: 429800, train loss: 4.049696826934815, val loss: 4.066884326934814, ETA in seconds: 13329707.728\n",
      "epoch: 429900, train loss: 4.057509326934815, val loss: 4.065907764434814, ETA in seconds: 13330365.744\n",
      "epoch: 430000, train loss: 4.0559468269348145, val loss: 4.0637593269348145, ETA in seconds: 13331116.471\n",
      "epoch: 430100, train loss: 4.049306201934814, val loss: 4.070009326934814, ETA in seconds: 13331929.131\n",
      "epoch: 430200, train loss: 4.046571826934814, val loss: 4.066103076934814, ETA in seconds: 13332791.169\n",
      "epoch: 430300, train loss: 4.050282764434814, val loss: 4.0637593269348145, ETA in seconds: 13333499.593\n",
      "epoch: 430400, train loss: 4.046571826934814, val loss: 4.069228076934815, ETA in seconds: 13334247.702\n",
      "epoch: 430500, train loss: 4.0510640144348145, val loss: 4.070790576934814, ETA in seconds: 13334970.190\n",
      "epoch: 430600, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 13335773.386\n",
      "epoch: 430700, train loss: 4.0520405769348145, val loss: 4.065321826934815, ETA in seconds: 13336611.358\n",
      "epoch: 430800, train loss: 4.049892139434815, val loss: 4.068446826934815, ETA in seconds: 13337441.131\n",
      "epoch: 430900, train loss: 4.0481343269348145, val loss: 4.068251514434815, ETA in seconds: 13338334.560\n",
      "epoch: 431000, train loss: 4.053603076934815, val loss: 4.064345264434815, ETA in seconds: 13339320.501\n",
      "epoch: 431100, train loss: 4.057118701934814, val loss: 4.070204639434815, ETA in seconds: 13340221.071\n",
      "epoch: 431200, train loss: 4.050673389434815, val loss: 4.066884326934814, ETA in seconds: 13341044.476\n",
      "epoch: 431300, train loss: 4.0491108894348145, val loss: 4.070009326934814, ETA in seconds: 13341729.287\n",
      "epoch: 431400, train loss: 4.051649951934815, val loss: 4.0676655769348145, ETA in seconds: 13342358.901\n",
      "epoch: 431500, train loss: 4.0510640144348145, val loss: 4.068251514434815, ETA in seconds: 13343009.193\n",
      "epoch: 431600, train loss: 4.048720264434815, val loss: 4.069228076934815, ETA in seconds: 13343914.021\n",
      "epoch: 431700, train loss: 4.051845264434815, val loss: 4.0686421394348145, ETA in seconds: 13344765.820\n",
      "epoch: 431800, train loss: 4.0471577644348145, val loss: 4.070009326934814, ETA in seconds: 13345579.491\n",
      "epoch: 431900, train loss: 4.0549702644348145, val loss: 4.0686421394348145, ETA in seconds: 13346355.551\n",
      "epoch: 432000, train loss: 4.048524951934814, val loss: 4.071181201934815, ETA in seconds: 13347186.772\n",
      "epoch: 432100, train loss: 4.058681201934815, val loss: 4.062978076934814, ETA in seconds: 13347902.532\n",
      "epoch: 432200, train loss: 4.048329639434814, val loss: 4.063954639434814, ETA in seconds: 13348595.096\n",
      "epoch: 432300, train loss: 4.057314014434814, val loss: 4.070204639434815, ETA in seconds: 13349271.849\n",
      "epoch: 432400, train loss: 4.056532764434815, val loss: 4.063173389434814, ETA in seconds: 13350025.049\n",
      "epoch: 432500, train loss: 4.057314014434814, val loss: 4.069423389434815, ETA in seconds: 13350784.985\n",
      "epoch: 432600, train loss: 4.053407764434814, val loss: 4.070204639434815, ETA in seconds: 13351548.112\n",
      "epoch: 432700, train loss: 4.051649951934815, val loss: 4.061415576934815, ETA in seconds: 13352488.766\n",
      "epoch: 432800, train loss: 4.060048389434814, val loss: 4.0618062019348145, ETA in seconds: 13353460.115\n",
      "epoch: 432900, train loss: 4.053212451934814, val loss: 4.067079639434814, ETA in seconds: 13354598.644\n",
      "epoch: 433000, train loss: 4.058485889434815, val loss: 4.063954639434814, ETA in seconds: 13355699.962\n",
      "epoch: 433100, train loss: 4.047548389434814, val loss: 4.069032764434814, ETA in seconds: 13356607.976\n",
      "epoch: 433200, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 13357319.197\n",
      "epoch: 433300, train loss: 4.051649951934815, val loss: 4.070790576934814, ETA in seconds: 13358095.938\n",
      "epoch: 433400, train loss: 4.047743701934815, val loss: 4.068251514434815, ETA in seconds: 13358750.664\n",
      "epoch: 433500, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 13359492.317\n",
      "epoch: 433600, train loss: 4.0510640144348145, val loss: 4.072939014434814, ETA in seconds: 13360308.681\n",
      "epoch: 433700, train loss: 4.0452046394348145, val loss: 4.062587451934815, ETA in seconds: 13361038.014\n",
      "epoch: 433800, train loss: 4.052235889434814, val loss: 4.0686421394348145, ETA in seconds: 13361303.315\n",
      "epoch: 433900, train loss: 4.050868701934815, val loss: 4.074696826934814, ETA in seconds: 13361665.916\n",
      "epoch: 434000, train loss: 4.051454639434814, val loss: 4.060634326934815, ETA in seconds: 13362084.218\n",
      "epoch: 434100, train loss: 4.047548389434814, val loss: 4.069423389434815, ETA in seconds: 13362360.825\n",
      "epoch: 434200, train loss: 4.046571826934814, val loss: 4.071962451934814, ETA in seconds: 13362589.336\n",
      "epoch: 434300, train loss: 4.0520405769348145, val loss: 4.066493701934815, ETA in seconds: 13363231.863\n",
      "epoch: 434400, train loss: 4.0578999519348145, val loss: 4.068056201934814, ETA in seconds: 13363883.745\n",
      "epoch: 434500, train loss: 4.054189014434814, val loss: 4.0657124519348145, ETA in seconds: 13364534.330\n",
      "epoch: 434600, train loss: 4.049306201934814, val loss: 4.0657124519348145, ETA in seconds: 13365315.485\n",
      "epoch: 434700, train loss: 4.057509326934815, val loss: 4.068837451934814, ETA in seconds: 13366081.356\n",
      "epoch: 434800, train loss: 4.047939014434815, val loss: 4.069228076934815, ETA in seconds: 13366850.865\n",
      "epoch: 434900, train loss: 4.051845264434815, val loss: 4.0647358894348145, ETA in seconds: 13367607.576\n",
      "epoch: 435000, train loss: 4.049892139434815, val loss: 4.072353076934815, ETA in seconds: 13368349.801\n",
      "epoch: 435100, train loss: 4.052235889434814, val loss: 4.070790576934814, ETA in seconds: 13369145.929\n",
      "epoch: 435200, train loss: 4.055556201934815, val loss: 4.067860889434814, ETA in seconds: 13369970.025\n",
      "epoch: 435300, train loss: 4.053798389434815, val loss: 4.0686421394348145, ETA in seconds: 13370733.473\n",
      "epoch: 435400, train loss: 4.0549702644348145, val loss: 4.0666890144348145, ETA in seconds: 13371445.794\n",
      "epoch: 435500, train loss: 4.052235889434814, val loss: 4.070790576934814, ETA in seconds: 13372137.850\n",
      "epoch: 435600, train loss: 4.049306201934814, val loss: 4.0647358894348145, ETA in seconds: 13372836.038\n",
      "epoch: 435700, train loss: 4.052235889434814, val loss: 4.0725483894348145, ETA in seconds: 13373644.496\n",
      "epoch: 435800, train loss: 4.050282764434814, val loss: 4.069814014434814, ETA in seconds: 13374460.367\n",
      "epoch: 435900, train loss: 4.0549702644348145, val loss: 4.063564014434815, ETA in seconds: 13375105.703\n",
      "epoch: 436000, train loss: 4.0471577644348145, val loss: 4.060634326934815, ETA in seconds: 13375944.908\n",
      "epoch: 436100, train loss: 4.054579639434815, val loss: 4.068251514434815, ETA in seconds: 13376724.467\n",
      "epoch: 436200, train loss: 4.055360889434814, val loss: 4.067274951934815, ETA in seconds: 13377394.130\n",
      "epoch: 436300, train loss: 4.049892139434815, val loss: 4.0666890144348145, ETA in seconds: 13378153.600\n",
      "epoch: 436400, train loss: 4.052626514434815, val loss: 4.0696187019348145, ETA in seconds: 13378981.659\n",
      "epoch: 436500, train loss: 4.044423389434814, val loss: 4.066298389434815, ETA in seconds: 13379744.202\n",
      "epoch: 436600, train loss: 4.051259326934814, val loss: 4.067274951934815, ETA in seconds: 13380514.685\n",
      "epoch: 436700, train loss: 4.057314014434814, val loss: 4.067470264434815, ETA in seconds: 13381264.678\n",
      "epoch: 436800, train loss: 4.0412983894348145, val loss: 4.068251514434815, ETA in seconds: 13381912.423\n",
      "epoch: 436900, train loss: 4.051259326934814, val loss: 4.0657124519348145, ETA in seconds: 13382535.930\n",
      "epoch: 437000, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 13383133.690\n",
      "epoch: 437100, train loss: 4.056142139434814, val loss: 4.066493701934815, ETA in seconds: 13383769.825\n",
      "epoch: 437200, train loss: 4.053798389434815, val loss: 4.068837451934814, ETA in seconds: 13384277.019\n",
      "epoch: 437300, train loss: 4.0549702644348145, val loss: 4.0715718269348145, ETA in seconds: 13385045.072\n",
      "epoch: 437400, train loss: 4.053212451934814, val loss: 4.0725483894348145, ETA in seconds: 13385656.750\n",
      "epoch: 437500, train loss: 4.047548389434814, val loss: 4.072353076934815, ETA in seconds: 13386316.309\n",
      "epoch: 437600, train loss: 4.048915576934815, val loss: 4.067470264434815, ETA in seconds: 13386936.863\n",
      "epoch: 437700, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 13387496.336\n",
      "epoch: 437800, train loss: 4.0471577644348145, val loss: 4.065907764434814, ETA in seconds: 13388098.924\n",
      "epoch: 437900, train loss: 4.0569233894348145, val loss: 4.067274951934815, ETA in seconds: 13388688.109\n",
      "epoch: 438000, train loss: 4.048329639434814, val loss: 4.065126514434814, ETA in seconds: 13389307.969\n",
      "epoch: 438100, train loss: 4.045399951934814, val loss: 4.071181201934815, ETA in seconds: 13390030.637\n",
      "epoch: 438200, train loss: 4.053212451934814, val loss: 4.067470264434815, ETA in seconds: 13390895.684\n",
      "epoch: 438300, train loss: 4.045985889434815, val loss: 4.070985889434814, ETA in seconds: 13391352.023\n",
      "epoch: 438400, train loss: 4.049696826934815, val loss: 4.0618062019348145, ETA in seconds: 13391919.111\n",
      "epoch: 438500, train loss: 4.055751514434815, val loss: 4.071181201934815, ETA in seconds: 13392603.454\n",
      "epoch: 438600, train loss: 4.055165576934814, val loss: 4.0666890144348145, ETA in seconds: 13393632.782\n",
      "epoch: 438700, train loss: 4.0520405769348145, val loss: 4.063368701934815, ETA in seconds: 13394623.721\n",
      "epoch: 438800, train loss: 4.045009326934815, val loss: 4.068056201934814, ETA in seconds: 13395317.338\n",
      "epoch: 438900, train loss: 4.057314014434814, val loss: 4.072353076934815, ETA in seconds: 13396098.715\n",
      "epoch: 439000, train loss: 4.051649951934815, val loss: 4.067860889434814, ETA in seconds: 13396731.547\n",
      "epoch: 439100, train loss: 4.0569233894348145, val loss: 4.070790576934814, ETA in seconds: 13397466.968\n",
      "epoch: 439200, train loss: 4.052821826934815, val loss: 4.064540576934815, ETA in seconds: 13398166.694\n",
      "epoch: 439300, train loss: 4.049306201934814, val loss: 4.068056201934814, ETA in seconds: 13398841.781\n",
      "epoch: 439400, train loss: 4.052235889434814, val loss: 4.066493701934815, ETA in seconds: 13399563.300\n",
      "epoch: 439500, train loss: 4.051845264434815, val loss: 4.068837451934814, ETA in seconds: 13400339.962\n",
      "epoch: 439600, train loss: 4.052626514434815, val loss: 4.069032764434814, ETA in seconds: 13401303.226\n",
      "epoch: 439700, train loss: 4.053212451934814, val loss: 4.064345264434815, ETA in seconds: 13402144.525\n",
      "epoch: 439800, train loss: 4.0461812019348145, val loss: 4.062196826934814, ETA in seconds: 13402960.290\n",
      "epoch: 439900, train loss: 4.056337451934814, val loss: 4.069032764434814, ETA in seconds: 13403685.389\n",
      "epoch: 440000, train loss: 4.048720264434815, val loss: 4.0686421394348145, ETA in seconds: 13404385.604\n",
      "epoch: 440100, train loss: 4.049696826934815, val loss: 4.071767139434814, ETA in seconds: 13405088.659\n",
      "epoch: 440200, train loss: 4.048329639434814, val loss: 4.065907764434814, ETA in seconds: 13405839.534\n",
      "epoch: 440300, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 13406681.395\n",
      "epoch: 440400, train loss: 4.050673389434815, val loss: 4.066493701934815, ETA in seconds: 13407315.813\n",
      "epoch: 440500, train loss: 4.055360889434814, val loss: 4.069228076934815, ETA in seconds: 13408005.316\n",
      "epoch: 440600, train loss: 4.059657764434815, val loss: 4.068446826934815, ETA in seconds: 13408667.604\n",
      "epoch: 440700, train loss: 4.051845264434815, val loss: 4.064149951934814, ETA in seconds: 13409407.119\n",
      "epoch: 440800, train loss: 4.050673389434815, val loss: 4.0696187019348145, ETA in seconds: 13410091.010\n",
      "epoch: 440900, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 13410833.227\n",
      "epoch: 441000, train loss: 4.0500874519348145, val loss: 4.067274951934815, ETA in seconds: 13411323.494\n",
      "epoch: 441100, train loss: 4.051845264434815, val loss: 4.070399951934815, ETA in seconds: 13411741.715\n",
      "epoch: 441200, train loss: 4.053407764434814, val loss: 4.072353076934815, ETA in seconds: 13412185.353\n",
      "epoch: 441300, train loss: 4.0491108894348145, val loss: 4.066884326934814, ETA in seconds: 13412806.700\n",
      "epoch: 441400, train loss: 4.053212451934814, val loss: 4.0725483894348145, ETA in seconds: 13413519.604\n",
      "epoch: 441500, train loss: 4.046962451934815, val loss: 4.068251514434815, ETA in seconds: 13414347.021\n",
      "epoch: 441600, train loss: 4.0481343269348145, val loss: 4.070985889434814, ETA in seconds: 13415072.998\n",
      "epoch: 441700, train loss: 4.046571826934814, val loss: 4.069423389434815, ETA in seconds: 13415668.477\n",
      "epoch: 441800, train loss: 4.0500874519348145, val loss: 4.069814014434814, ETA in seconds: 13416492.279\n",
      "epoch: 441900, train loss: 4.0559468269348145, val loss: 4.067860889434814, ETA in seconds: 13417126.670\n",
      "epoch: 442000, train loss: 4.0569233894348145, val loss: 4.063564014434815, ETA in seconds: 13417751.145\n",
      "epoch: 442100, train loss: 4.056728076934815, val loss: 4.064149951934814, ETA in seconds: 13418362.815\n",
      "epoch: 442200, train loss: 4.044618701934814, val loss: 4.069423389434815, ETA in seconds: 13418983.073\n",
      "epoch: 442300, train loss: 4.052235889434814, val loss: 4.0676655769348145, ETA in seconds: 13419576.824\n",
      "epoch: 442400, train loss: 4.058681201934815, val loss: 4.061220264434814, ETA in seconds: 13420277.857\n",
      "epoch: 442500, train loss: 4.056532764434815, val loss: 4.069423389434815, ETA in seconds: 13420919.699\n",
      "epoch: 442600, train loss: 4.057314014434814, val loss: 4.0676655769348145, ETA in seconds: 13421586.785\n",
      "epoch: 442700, train loss: 4.055165576934814, val loss: 4.070204639434815, ETA in seconds: 13422211.931\n",
      "epoch: 442800, train loss: 4.048524951934814, val loss: 4.065907764434814, ETA in seconds: 13422819.748\n",
      "epoch: 442900, train loss: 4.047353076934814, val loss: 4.063368701934815, ETA in seconds: 13423439.651\n",
      "epoch: 443000, train loss: 4.0491108894348145, val loss: 4.065321826934815, ETA in seconds: 13424073.893\n",
      "epoch: 443100, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 13424644.202\n",
      "epoch: 443200, train loss: 4.046767139434815, val loss: 4.070204639434815, ETA in seconds: 13425278.173\n",
      "epoch: 443300, train loss: 4.048524951934814, val loss: 4.067470264434815, ETA in seconds: 13425829.866\n",
      "epoch: 443400, train loss: 4.052626514434815, val loss: 4.069032764434814, ETA in seconds: 13426359.282\n",
      "epoch: 443500, train loss: 4.0559468269348145, val loss: 4.065517139434815, ETA in seconds: 13426968.446\n",
      "epoch: 443600, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 13427519.277\n",
      "epoch: 443700, train loss: 4.050282764434814, val loss: 4.068837451934814, ETA in seconds: 13428135.496\n",
      "epoch: 443800, train loss: 4.050478076934814, val loss: 4.070204639434815, ETA in seconds: 13428786.339\n",
      "epoch: 443900, train loss: 4.0461812019348145, val loss: 4.066493701934815, ETA in seconds: 13429352.837\n",
      "epoch: 444000, train loss: 4.0491108894348145, val loss: 4.072743701934814, ETA in seconds: 13430018.993\n",
      "epoch: 444100, train loss: 4.0530171394348145, val loss: 4.070399951934815, ETA in seconds: 13430606.723\n",
      "epoch: 444200, train loss: 4.051845264434815, val loss: 4.060048389434814, ETA in seconds: 13431131.410\n",
      "epoch: 444300, train loss: 4.055360889434814, val loss: 4.065126514434814, ETA in seconds: 13431640.353\n",
      "epoch: 444400, train loss: 4.054189014434814, val loss: 4.068251514434815, ETA in seconds: 13432323.232\n",
      "epoch: 444500, train loss: 4.0549702644348145, val loss: 4.067079639434814, ETA in seconds: 13432896.801\n",
      "epoch: 444600, train loss: 4.051649951934815, val loss: 4.065321826934815, ETA in seconds: 13433642.078\n",
      "epoch: 444700, train loss: 4.050868701934815, val loss: 4.070009326934814, ETA in seconds: 13434247.995\n",
      "epoch: 444800, train loss: 4.056142139434814, val loss: 4.0666890144348145, ETA in seconds: 13434774.166\n",
      "epoch: 444900, train loss: 4.048720264434815, val loss: 4.064931201934814, ETA in seconds: 13435391.746\n",
      "epoch: 445000, train loss: 4.047939014434815, val loss: 4.063368701934815, ETA in seconds: 13435973.709\n",
      "epoch: 445100, train loss: 4.054189014434814, val loss: 4.0705952644348145, ETA in seconds: 13436524.585\n",
      "epoch: 445200, train loss: 4.050868701934815, val loss: 4.072157764434815, ETA in seconds: 13437097.564\n",
      "epoch: 445300, train loss: 4.050478076934814, val loss: 4.068251514434815, ETA in seconds: 13437591.615\n",
      "epoch: 445400, train loss: 4.051259326934814, val loss: 4.072743701934814, ETA in seconds: 13438095.236\n",
      "epoch: 445500, train loss: 4.055360889434814, val loss: 4.072353076934815, ETA in seconds: 13438524.143\n",
      "epoch: 445600, train loss: 4.044814014434815, val loss: 4.066103076934814, ETA in seconds: 13439176.800\n",
      "epoch: 445700, train loss: 4.047548389434814, val loss: 4.070985889434814, ETA in seconds: 13439889.272\n",
      "epoch: 445800, train loss: 4.045985889434815, val loss: 4.0705952644348145, ETA in seconds: 13440467.299\n",
      "epoch: 445900, train loss: 4.051259326934814, val loss: 4.0686421394348145, ETA in seconds: 13440988.522\n",
      "epoch: 446000, train loss: 4.0520405769348145, val loss: 4.067860889434814, ETA in seconds: 13441462.108\n",
      "epoch: 446100, train loss: 4.058485889434815, val loss: 4.063954639434814, ETA in seconds: 13441962.553\n",
      "epoch: 446200, train loss: 4.0500874519348145, val loss: 4.0696187019348145, ETA in seconds: 13442504.979\n",
      "epoch: 446300, train loss: 4.045790576934815, val loss: 4.064345264434815, ETA in seconds: 13443006.896\n",
      "epoch: 446400, train loss: 4.053798389434815, val loss: 4.0657124519348145, ETA in seconds: 13443594.196\n",
      "epoch: 446500, train loss: 4.049306201934814, val loss: 4.064931201934814, ETA in seconds: 13444186.999\n",
      "epoch: 446600, train loss: 4.052235889434814, val loss: 4.065321826934815, ETA in seconds: 13444757.247\n",
      "epoch: 446700, train loss: 4.046571826934814, val loss: 4.064149951934814, ETA in seconds: 13445329.519\n",
      "epoch: 446800, train loss: 4.048329639434814, val loss: 4.058290576934814, ETA in seconds: 13445735.047\n",
      "epoch: 446900, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 13446459.991\n",
      "epoch: 447000, train loss: 4.049696826934815, val loss: 4.066103076934814, ETA in seconds: 13447196.067\n",
      "epoch: 447100, train loss: 4.051649951934815, val loss: 4.067470264434815, ETA in seconds: 13447856.852\n",
      "epoch: 447200, train loss: 4.049696826934815, val loss: 4.070204639434815, ETA in seconds: 13448443.096\n",
      "epoch: 447300, train loss: 4.051259326934814, val loss: 4.070009326934814, ETA in seconds: 13449000.320\n",
      "epoch: 447400, train loss: 4.0530171394348145, val loss: 4.066493701934815, ETA in seconds: 13449598.861\n",
      "epoch: 447500, train loss: 4.051259326934814, val loss: 4.071376514434815, ETA in seconds: 13450114.112\n",
      "epoch: 447600, train loss: 4.052626514434815, val loss: 4.068056201934814, ETA in seconds: 13450614.740\n",
      "epoch: 447700, train loss: 4.051454639434814, val loss: 4.067860889434814, ETA in seconds: 13451432.194\n",
      "epoch: 447800, train loss: 4.053603076934815, val loss: 4.070790576934814, ETA in seconds: 13452081.902\n",
      "epoch: 447900, train loss: 4.051845264434815, val loss: 4.065321826934815, ETA in seconds: 13452519.367\n",
      "epoch: 448000, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 13453057.593\n",
      "epoch: 448100, train loss: 4.053798389434815, val loss: 4.069423389434815, ETA in seconds: 13453736.238\n",
      "epoch: 448200, train loss: 4.045790576934815, val loss: 4.069814014434814, ETA in seconds: 13454297.124\n",
      "epoch: 448300, train loss: 4.059071826934814, val loss: 4.067470264434815, ETA in seconds: 13454988.389\n",
      "epoch: 448400, train loss: 4.043837451934815, val loss: 4.069032764434814, ETA in seconds: 13455670.843\n",
      "epoch: 448500, train loss: 4.046571826934814, val loss: 4.0705952644348145, ETA in seconds: 13456288.632\n",
      "epoch: 448600, train loss: 4.053798389434815, val loss: 4.068251514434815, ETA in seconds: 13456845.545\n",
      "epoch: 448700, train loss: 4.044618701934814, val loss: 4.063173389434814, ETA in seconds: 13457360.383\n",
      "epoch: 448800, train loss: 4.0559468269348145, val loss: 4.0686421394348145, ETA in seconds: 13457906.550\n",
      "epoch: 448900, train loss: 4.0539937019348145, val loss: 4.060243701934814, ETA in seconds: 13458476.103\n",
      "epoch: 449000, train loss: 4.059462451934815, val loss: 4.070204639434815, ETA in seconds: 13458953.229\n",
      "epoch: 449100, train loss: 4.050868701934815, val loss: 4.072743701934814, ETA in seconds: 13459429.483\n",
      "epoch: 449200, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 13459907.900\n",
      "epoch: 449300, train loss: 4.0539937019348145, val loss: 4.072939014434814, ETA in seconds: 13460481.250\n",
      "epoch: 449400, train loss: 4.049501514434814, val loss: 4.069423389434815, ETA in seconds: 13460991.226\n",
      "epoch: 449500, train loss: 4.054384326934814, val loss: 4.064149951934814, ETA in seconds: 13461487.476\n",
      "epoch: 449600, train loss: 4.052235889434814, val loss: 4.071376514434815, ETA in seconds: 13461987.589\n",
      "epoch: 449700, train loss: 4.048329639434814, val loss: 4.066298389434815, ETA in seconds: 13462614.863\n",
      "epoch: 449800, train loss: 4.051649951934815, val loss: 4.070790576934814, ETA in seconds: 13463296.482\n",
      "epoch: 449900, train loss: 4.049892139434815, val loss: 4.069423389434815, ETA in seconds: 13463981.325\n",
      "epoch: 450000, train loss: 4.046962451934815, val loss: 4.064931201934814, ETA in seconds: 13464576.818\n",
      "epoch: 450100, train loss: 4.0588765144348145, val loss: 4.0647358894348145, ETA in seconds: 13464843.683\n",
      "epoch: 450200, train loss: 4.057314014434814, val loss: 4.0627827644348145, ETA in seconds: 13465045.569\n",
      "epoch: 450300, train loss: 4.052626514434815, val loss: 4.065517139434815, ETA in seconds: 13465822.506\n",
      "epoch: 450400, train loss: 4.0549702644348145, val loss: 4.065517139434815, ETA in seconds: 13466512.298\n",
      "epoch: 450500, train loss: 4.0539937019348145, val loss: 4.069228076934815, ETA in seconds: 13467159.981\n",
      "epoch: 450600, train loss: 4.0500874519348145, val loss: 4.0705952644348145, ETA in seconds: 13467102.374\n",
      "epoch: 450700, train loss: 4.051259326934814, val loss: 4.073134326934815, ETA in seconds: 13467068.553\n",
      "epoch: 450800, train loss: 4.055165576934814, val loss: 4.067274951934815, ETA in seconds: 13467621.914\n",
      "epoch: 450900, train loss: 4.051845264434815, val loss: 4.069032764434814, ETA in seconds: 13468091.028\n",
      "epoch: 451000, train loss: 4.051454639434814, val loss: 4.0647358894348145, ETA in seconds: 13468698.947\n",
      "epoch: 451100, train loss: 4.0569233894348145, val loss: 4.069814014434814, ETA in seconds: 13469401.424\n",
      "epoch: 451200, train loss: 4.0452046394348145, val loss: 4.067470264434815, ETA in seconds: 13469953.960\n",
      "epoch: 451300, train loss: 4.053798389434815, val loss: 4.067274951934815, ETA in seconds: 13470434.428\n",
      "epoch: 451400, train loss: 4.055751514434815, val loss: 4.065907764434814, ETA in seconds: 13470986.949\n",
      "epoch: 451500, train loss: 4.055751514434815, val loss: 4.070204639434815, ETA in seconds: 13471464.186\n",
      "epoch: 451600, train loss: 4.053603076934815, val loss: 4.064345264434815, ETA in seconds: 13471961.755\n",
      "epoch: 451700, train loss: 4.057314014434814, val loss: 4.068251514434815, ETA in seconds: 13472848.550\n",
      "epoch: 451800, train loss: 4.051845264434815, val loss: 4.066493701934815, ETA in seconds: 13473532.153\n",
      "epoch: 451900, train loss: 4.041103076934815, val loss: 4.064540576934815, ETA in seconds: 13474091.510\n",
      "epoch: 452000, train loss: 4.0530171394348145, val loss: 4.069814014434814, ETA in seconds: 13474697.411\n",
      "epoch: 452100, train loss: 4.054579639434815, val loss: 4.065321826934815, ETA in seconds: 13475340.364\n",
      "epoch: 452200, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 13475857.558\n",
      "epoch: 452300, train loss: 4.054774951934815, val loss: 4.067860889434814, ETA in seconds: 13476509.503\n",
      "epoch: 452400, train loss: 4.0491108894348145, val loss: 4.066103076934814, ETA in seconds: 13477183.524\n",
      "epoch: 452500, train loss: 4.053212451934814, val loss: 4.0647358894348145, ETA in seconds: 13477749.678\n",
      "epoch: 452600, train loss: 4.056142139434814, val loss: 4.067079639434814, ETA in seconds: 13478370.936\n",
      "epoch: 452700, train loss: 4.0432515144348145, val loss: 4.066493701934815, ETA in seconds: 13478954.814\n",
      "epoch: 452800, train loss: 4.052431201934814, val loss: 4.0725483894348145, ETA in seconds: 13479547.519\n",
      "epoch: 452900, train loss: 4.049696826934815, val loss: 4.067079639434814, ETA in seconds: 13479997.961\n",
      "epoch: 453000, train loss: 4.0471577644348145, val loss: 4.066493701934815, ETA in seconds: 13480601.950\n",
      "epoch: 453100, train loss: 4.0530171394348145, val loss: 4.071376514434815, ETA in seconds: 13481276.019\n",
      "epoch: 453200, train loss: 4.059462451934815, val loss: 4.068837451934814, ETA in seconds: 13481721.040\n",
      "epoch: 453300, train loss: 4.0500874519348145, val loss: 4.068446826934815, ETA in seconds: 13482218.680\n",
      "epoch: 453400, train loss: 4.047939014434815, val loss: 4.0666890144348145, ETA in seconds: 13482683.503\n",
      "epoch: 453500, train loss: 4.0481343269348145, val loss: 4.064931201934814, ETA in seconds: 13483116.649\n",
      "epoch: 453600, train loss: 4.054189014434814, val loss: 4.068837451934814, ETA in seconds: 13483664.688\n",
      "epoch: 453700, train loss: 4.0442280769348145, val loss: 4.064931201934814, ETA in seconds: 13484131.804\n",
      "epoch: 453800, train loss: 4.046376514434814, val loss: 4.064345264434815, ETA in seconds: 13484501.311\n",
      "epoch: 453900, train loss: 4.047939014434815, val loss: 4.071181201934815, ETA in seconds: 13485038.577\n",
      "epoch: 454000, train loss: 4.055751514434815, val loss: 4.065517139434815, ETA in seconds: 13485659.331\n",
      "epoch: 454100, train loss: 4.049501514434814, val loss: 4.068837451934814, ETA in seconds: 13486115.315\n",
      "epoch: 454200, train loss: 4.0578999519348145, val loss: 4.065907764434814, ETA in seconds: 13486577.401\n",
      "epoch: 454300, train loss: 4.0539937019348145, val loss: 4.068837451934814, ETA in seconds: 13487190.747\n",
      "epoch: 454400, train loss: 4.055360889434814, val loss: 4.069423389434815, ETA in seconds: 13487701.805\n",
      "epoch: 454500, train loss: 4.0530171394348145, val loss: 4.0627827644348145, ETA in seconds: 13488233.234\n",
      "epoch: 454600, train loss: 4.049696826934815, val loss: 4.065517139434815, ETA in seconds: 13488819.304\n",
      "epoch: 454700, train loss: 4.0491108894348145, val loss: 4.066493701934815, ETA in seconds: 13489446.154\n",
      "epoch: 454800, train loss: 4.046571826934814, val loss: 4.068837451934814, ETA in seconds: 13490134.037\n",
      "epoch: 454900, train loss: 4.0481343269348145, val loss: 4.070009326934814, ETA in seconds: 13490732.509\n",
      "epoch: 455000, train loss: 4.053603076934815, val loss: 4.072353076934815, ETA in seconds: 13491227.882\n",
      "epoch: 455100, train loss: 4.050282764434814, val loss: 4.069814014434814, ETA in seconds: 13491708.351\n",
      "epoch: 455200, train loss: 4.050478076934814, val loss: 4.064540576934815, ETA in seconds: 13492302.243\n",
      "epoch: 455300, train loss: 4.056337451934814, val loss: 4.064149951934814, ETA in seconds: 13492825.174\n",
      "epoch: 455400, train loss: 4.050868701934815, val loss: 4.0647358894348145, ETA in seconds: 13493266.876\n",
      "epoch: 455500, train loss: 4.053798389434815, val loss: 4.0657124519348145, ETA in seconds: 13493824.511\n",
      "epoch: 455600, train loss: 4.0549702644348145, val loss: 4.065126514434814, ETA in seconds: 13494200.290\n",
      "epoch: 455700, train loss: 4.0530171394348145, val loss: 4.071181201934815, ETA in seconds: 13494526.754\n",
      "epoch: 455800, train loss: 4.0618062019348145, val loss: 4.065517139434815, ETA in seconds: 13495018.895\n",
      "epoch: 455900, train loss: 4.048329639434814, val loss: 4.070790576934814, ETA in seconds: 13495450.156\n",
      "epoch: 456000, train loss: 4.056532764434815, val loss: 4.0666890144348145, ETA in seconds: 13495952.547\n",
      "epoch: 456100, train loss: 4.0520405769348145, val loss: 4.0666890144348145, ETA in seconds: 13496811.110\n",
      "epoch: 456200, train loss: 4.054774951934815, val loss: 4.0715718269348145, ETA in seconds: 13497624.485\n",
      "epoch: 456300, train loss: 4.053407764434814, val loss: 4.066493701934815, ETA in seconds: 13498107.172\n",
      "epoch: 456400, train loss: 4.059071826934814, val loss: 4.066493701934815, ETA in seconds: 13498477.195\n",
      "epoch: 456500, train loss: 4.054774951934815, val loss: 4.0696187019348145, ETA in seconds: 13498928.069\n",
      "epoch: 456600, train loss: 4.048720264434815, val loss: 4.065126514434814, ETA in seconds: 13499416.818\n",
      "epoch: 456700, train loss: 4.050868701934815, val loss: 4.068251514434815, ETA in seconds: 13499688.598\n",
      "epoch: 456800, train loss: 4.056142139434814, val loss: 4.064345264434815, ETA in seconds: 13500367.452\n",
      "epoch: 456900, train loss: 4.054384326934814, val loss: 4.067860889434814, ETA in seconds: 13500779.526\n",
      "epoch: 457000, train loss: 4.048720264434815, val loss: 4.067470264434815, ETA in seconds: 13501236.787\n",
      "epoch: 457100, train loss: 4.048915576934815, val loss: 4.067470264434815, ETA in seconds: 13501690.195\n",
      "epoch: 457200, train loss: 4.054189014434814, val loss: 4.069228076934815, ETA in seconds: 13502133.262\n",
      "epoch: 457300, train loss: 4.058095264434814, val loss: 4.069814014434814, ETA in seconds: 13502643.187\n",
      "epoch: 457400, train loss: 4.049306201934814, val loss: 4.069228076934815, ETA in seconds: 13503196.726\n",
      "epoch: 457500, train loss: 4.048720264434815, val loss: 4.068251514434815, ETA in seconds: 13503874.749\n",
      "epoch: 457600, train loss: 4.050868701934815, val loss: 4.067470264434815, ETA in seconds: 13504462.788\n",
      "epoch: 457700, train loss: 4.045985889434815, val loss: 4.071767139434814, ETA in seconds: 13504953.392\n",
      "epoch: 457800, train loss: 4.051845264434815, val loss: 4.071181201934815, ETA in seconds: 13505400.043\n",
      "epoch: 457900, train loss: 4.054774951934815, val loss: 4.068837451934814, ETA in seconds: 13505908.587\n",
      "epoch: 458000, train loss: 4.0481343269348145, val loss: 4.065126514434814, ETA in seconds: 13506300.499\n",
      "epoch: 458100, train loss: 4.0559468269348145, val loss: 4.068251514434815, ETA in seconds: 13506734.333\n",
      "epoch: 458200, train loss: 4.057314014434814, val loss: 4.066884326934814, ETA in seconds: 13507131.301\n",
      "epoch: 458300, train loss: 4.0520405769348145, val loss: 4.071962451934814, ETA in seconds: 13507541.031\n",
      "epoch: 458400, train loss: 4.054189014434814, val loss: 4.070985889434814, ETA in seconds: 13508014.286\n",
      "epoch: 458500, train loss: 4.045595264434814, val loss: 4.074696826934814, ETA in seconds: 13508430.293\n",
      "epoch: 458600, train loss: 4.047743701934815, val loss: 4.070790576934814, ETA in seconds: 13508885.996\n",
      "epoch: 458700, train loss: 4.053212451934814, val loss: 4.0608296394348145, ETA in seconds: 13509364.041\n",
      "epoch: 458800, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 13509894.298\n",
      "epoch: 458900, train loss: 4.046571826934814, val loss: 4.069032764434814, ETA in seconds: 13510412.056\n",
      "epoch: 459000, train loss: 4.050282764434814, val loss: 4.062978076934814, ETA in seconds: 13510878.563\n",
      "epoch: 459100, train loss: 4.054579639434815, val loss: 4.065321826934815, ETA in seconds: 13511284.337\n",
      "epoch: 459200, train loss: 4.058290576934814, val loss: 4.069423389434815, ETA in seconds: 13511761.333\n",
      "epoch: 459300, train loss: 4.056337451934814, val loss: 4.065126514434814, ETA in seconds: 13512132.500\n",
      "epoch: 459400, train loss: 4.058485889434815, val loss: 4.070009326934814, ETA in seconds: 13512956.144\n",
      "epoch: 459500, train loss: 4.051649951934815, val loss: 4.065126514434814, ETA in seconds: 13513658.439\n",
      "epoch: 459600, train loss: 4.0481343269348145, val loss: 4.0715718269348145, ETA in seconds: 13514079.825\n",
      "epoch: 459700, train loss: 4.041103076934815, val loss: 4.070399951934815, ETA in seconds: 13514644.017\n",
      "epoch: 459800, train loss: 4.0491108894348145, val loss: 4.068056201934814, ETA in seconds: 13514783.555\n",
      "epoch: 459900, train loss: 4.0491108894348145, val loss: 4.0647358894348145, ETA in seconds: 13514953.642\n",
      "epoch: 460000, train loss: 4.055165576934814, val loss: 4.066493701934815, ETA in seconds: 13515356.482\n",
      "epoch: 460100, train loss: 4.045985889434815, val loss: 4.059657764434815, ETA in seconds: 13515777.802\n",
      "epoch: 460200, train loss: 4.054189014434814, val loss: 4.074110889434815, ETA in seconds: 13516193.657\n",
      "epoch: 460300, train loss: 4.054579639434815, val loss: 4.064931201934814, ETA in seconds: 13516130.520\n",
      "epoch: 460400, train loss: 4.056728076934815, val loss: 4.0666890144348145, ETA in seconds: 13516626.884\n",
      "epoch: 460500, train loss: 4.053212451934814, val loss: 4.0666890144348145, ETA in seconds: 13517095.590\n",
      "epoch: 460600, train loss: 4.059071826934814, val loss: 4.063368701934815, ETA in seconds: 13517542.263\n",
      "epoch: 460700, train loss: 4.053798389434815, val loss: 4.064149951934814, ETA in seconds: 13517918.418\n",
      "epoch: 460800, train loss: 4.052821826934815, val loss: 4.065126514434814, ETA in seconds: 13518349.314\n",
      "epoch: 460900, train loss: 4.045009326934815, val loss: 4.065126514434814, ETA in seconds: 13518666.309\n",
      "epoch: 461000, train loss: 4.0481343269348145, val loss: 4.071962451934814, ETA in seconds: 13519054.700\n",
      "epoch: 461100, train loss: 4.0559468269348145, val loss: 4.073134326934815, ETA in seconds: 13519465.995\n",
      "epoch: 461200, train loss: 4.050673389434815, val loss: 4.069032764434814, ETA in seconds: 13519882.209\n",
      "epoch: 461300, train loss: 4.054774951934815, val loss: 4.0666890144348145, ETA in seconds: 13520327.506\n",
      "epoch: 461400, train loss: 4.052626514434815, val loss: 4.064931201934814, ETA in seconds: 13520848.006\n",
      "epoch: 461500, train loss: 4.0530171394348145, val loss: 4.0657124519348145, ETA in seconds: 13521231.830\n",
      "epoch: 461600, train loss: 4.055360889434814, val loss: 4.067079639434814, ETA in seconds: 13521650.216\n",
      "epoch: 461700, train loss: 4.047743701934815, val loss: 4.064540576934815, ETA in seconds: 13522169.730\n",
      "epoch: 461800, train loss: 4.046962451934815, val loss: 4.0666890144348145, ETA in seconds: 13522734.421\n",
      "epoch: 461900, train loss: 4.050478076934814, val loss: 4.065517139434815, ETA in seconds: 13523313.962\n",
      "epoch: 462000, train loss: 4.051845264434815, val loss: 4.0686421394348145, ETA in seconds: 13523723.301\n",
      "epoch: 462100, train loss: 4.051649951934815, val loss: 4.071962451934814, ETA in seconds: 13524024.639\n",
      "epoch: 462200, train loss: 4.050868701934815, val loss: 4.063564014434815, ETA in seconds: 13524354.128\n",
      "epoch: 462300, train loss: 4.056337451934814, val loss: 4.062392139434815, ETA in seconds: 13524715.814\n",
      "epoch: 462400, train loss: 4.0510640144348145, val loss: 4.072353076934815, ETA in seconds: 13525191.682\n",
      "epoch: 462500, train loss: 4.0510640144348145, val loss: 4.068251514434815, ETA in seconds: 13525615.195\n",
      "epoch: 462600, train loss: 4.050673389434815, val loss: 4.069814014434814, ETA in seconds: 13526163.082\n",
      "epoch: 462700, train loss: 4.049306201934814, val loss: 4.067274951934815, ETA in seconds: 13526603.789\n",
      "epoch: 462800, train loss: 4.0461812019348145, val loss: 4.067274951934815, ETA in seconds: 13527028.556\n",
      "epoch: 462900, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 13527474.251\n",
      "epoch: 463000, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 13528020.651\n",
      "epoch: 463100, train loss: 4.050868701934815, val loss: 4.066493701934815, ETA in seconds: 13528382.508\n",
      "epoch: 463200, train loss: 4.051649951934815, val loss: 4.067079639434814, ETA in seconds: 13529050.009\n",
      "epoch: 463300, train loss: 4.0510640144348145, val loss: 4.072157764434815, ETA in seconds: 13529605.826\n",
      "epoch: 463400, train loss: 4.050282764434814, val loss: 4.071767139434814, ETA in seconds: 13530196.249\n",
      "epoch: 463500, train loss: 4.055165576934814, val loss: 4.062978076934814, ETA in seconds: 13530490.626\n",
      "epoch: 463600, train loss: 4.047939014434815, val loss: 4.066884326934814, ETA in seconds: 13530796.860\n",
      "epoch: 463700, train loss: 4.051649951934815, val loss: 4.075673389434814, ETA in seconds: 13531261.062\n",
      "epoch: 463800, train loss: 4.056337451934814, val loss: 4.067274951934815, ETA in seconds: 13531576.628\n",
      "epoch: 463900, train loss: 4.047548389434814, val loss: 4.066493701934815, ETA in seconds: 13531963.607\n",
      "epoch: 464000, train loss: 4.056728076934815, val loss: 4.066103076934814, ETA in seconds: 13532364.614\n",
      "epoch: 464100, train loss: 4.048524951934814, val loss: 4.068251514434815, ETA in seconds: 13532840.670\n",
      "epoch: 464200, train loss: 4.0491108894348145, val loss: 4.070790576934814, ETA in seconds: 13533184.119\n",
      "epoch: 464300, train loss: 4.053407764434814, val loss: 4.064931201934814, ETA in seconds: 13533507.378\n",
      "epoch: 464400, train loss: 4.052431201934814, val loss: 4.070790576934814, ETA in seconds: 13533824.202\n",
      "epoch: 464500, train loss: 4.050282764434814, val loss: 4.069228076934815, ETA in seconds: 13534129.120\n",
      "epoch: 464600, train loss: 4.046376514434814, val loss: 4.069228076934815, ETA in seconds: 13534466.840\n",
      "epoch: 464700, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 13534844.798\n",
      "epoch: 464800, train loss: 4.040712451934814, val loss: 4.065126514434814, ETA in seconds: 13535154.789\n",
      "epoch: 464900, train loss: 4.056337451934814, val loss: 4.070009326934814, ETA in seconds: 13535537.338\n",
      "epoch: 465000, train loss: 4.054579639434815, val loss: 4.068056201934814, ETA in seconds: 13535942.241\n",
      "epoch: 465100, train loss: 4.056142139434814, val loss: 4.066103076934814, ETA in seconds: 13536346.503\n",
      "epoch: 465200, train loss: 4.058485889434815, val loss: 4.069228076934815, ETA in seconds: 13536713.796\n",
      "epoch: 465300, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 13537159.670\n",
      "epoch: 465400, train loss: 4.0539937019348145, val loss: 4.065517139434815, ETA in seconds: 13537534.463\n",
      "epoch: 465500, train loss: 4.049696826934815, val loss: 4.064931201934814, ETA in seconds: 13537904.377\n",
      "epoch: 465600, train loss: 4.048915576934815, val loss: 4.072157764434815, ETA in seconds: 13538262.176\n",
      "epoch: 465700, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 13538836.929\n",
      "epoch: 465800, train loss: 4.053212451934814, val loss: 4.062587451934815, ETA in seconds: 13539516.867\n",
      "epoch: 465900, train loss: 4.0500874519348145, val loss: 4.062978076934814, ETA in seconds: 13540152.608\n",
      "epoch: 466000, train loss: 4.051454639434814, val loss: 4.0696187019348145, ETA in seconds: 13540473.404\n",
      "epoch: 466100, train loss: 4.053798389434815, val loss: 4.067274951934815, ETA in seconds: 13540767.961\n",
      "epoch: 466200, train loss: 4.054774951934815, val loss: 4.0647358894348145, ETA in seconds: 13540975.589\n",
      "epoch: 466300, train loss: 4.046962451934815, val loss: 4.066103076934814, ETA in seconds: 13541277.330\n",
      "epoch: 466400, train loss: 4.0510640144348145, val loss: 4.0686421394348145, ETA in seconds: 13541645.385\n",
      "epoch: 466500, train loss: 4.051649951934815, val loss: 4.068446826934815, ETA in seconds: 13541974.029\n",
      "epoch: 466600, train loss: 4.042860889434815, val loss: 4.070985889434814, ETA in seconds: 13542258.583\n",
      "epoch: 466700, train loss: 4.055556201934815, val loss: 4.065321826934815, ETA in seconds: 13542549.497\n",
      "epoch: 466800, train loss: 4.057704639434815, val loss: 4.069814014434814, ETA in seconds: 13542842.204\n",
      "epoch: 466900, train loss: 4.046376514434814, val loss: 4.061220264434814, ETA in seconds: 13543134.900\n",
      "epoch: 467000, train loss: 4.0510640144348145, val loss: 4.0618062019348145, ETA in seconds: 13543408.643\n",
      "epoch: 467100, train loss: 4.058681201934815, val loss: 4.063368701934815, ETA in seconds: 13543786.827\n",
      "epoch: 467200, train loss: 4.044423389434814, val loss: 4.069814014434814, ETA in seconds: 13544099.519\n",
      "epoch: 467300, train loss: 4.050673389434815, val loss: 4.071767139434814, ETA in seconds: 13544423.182\n",
      "epoch: 467400, train loss: 4.045399951934814, val loss: 4.067470264434815, ETA in seconds: 13544757.954\n",
      "epoch: 467500, train loss: 4.053212451934814, val loss: 4.069423389434815, ETA in seconds: 13545123.830\n",
      "epoch: 467600, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 13545495.779\n",
      "epoch: 467700, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 13545858.056\n",
      "epoch: 467800, train loss: 4.052821826934815, val loss: 4.071181201934815, ETA in seconds: 13546163.692\n",
      "epoch: 467900, train loss: 4.051649951934815, val loss: 4.065517139434815, ETA in seconds: 13546477.406\n",
      "epoch: 468000, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 13546774.126\n",
      "epoch: 468100, train loss: 4.0491108894348145, val loss: 4.066103076934814, ETA in seconds: 13547215.648\n",
      "epoch: 468200, train loss: 4.049892139434815, val loss: 4.070009326934814, ETA in seconds: 13547653.712\n",
      "epoch: 468300, train loss: 4.051454639434814, val loss: 4.068837451934814, ETA in seconds: 13547921.017\n",
      "epoch: 468400, train loss: 4.053212451934814, val loss: 4.066884326934814, ETA in seconds: 13548283.744\n",
      "epoch: 468500, train loss: 4.047939014434815, val loss: 4.065907764434814, ETA in seconds: 13548670.238\n",
      "epoch: 468600, train loss: 4.0539937019348145, val loss: 4.072157764434815, ETA in seconds: 13549041.049\n",
      "epoch: 468700, train loss: 4.0569233894348145, val loss: 4.0686421394348145, ETA in seconds: 13549409.119\n",
      "epoch: 468800, train loss: 4.052235889434814, val loss: 4.070399951934815, ETA in seconds: 13549759.128\n",
      "epoch: 468900, train loss: 4.054189014434814, val loss: 4.062978076934814, ETA in seconds: 13550289.930\n",
      "epoch: 469000, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 13550776.207\n",
      "epoch: 469100, train loss: 4.050868701934815, val loss: 4.069814014434814, ETA in seconds: 13551215.540\n",
      "epoch: 469200, train loss: 4.0549702644348145, val loss: 4.060634326934815, ETA in seconds: 13551503.435\n",
      "epoch: 469300, train loss: 4.057314014434814, val loss: 4.0637593269348145, ETA in seconds: 13551762.926\n",
      "epoch: 469400, train loss: 4.047939014434815, val loss: 4.072157764434815, ETA in seconds: 13552141.245\n",
      "epoch: 469500, train loss: 4.0500874519348145, val loss: 4.073134326934815, ETA in seconds: 13552483.841\n",
      "epoch: 469600, train loss: 4.045009326934815, val loss: 4.0676655769348145, ETA in seconds: 13552871.978\n",
      "epoch: 469700, train loss: 4.047548389434814, val loss: 4.0686421394348145, ETA in seconds: 13553243.265\n",
      "epoch: 469800, train loss: 4.047353076934814, val loss: 4.072939014434814, ETA in seconds: 13553594.665\n",
      "epoch: 469900, train loss: 4.056728076934815, val loss: 4.065907764434814, ETA in seconds: 13553901.028\n",
      "epoch: 470000, train loss: 4.054579639434815, val loss: 4.062392139434815, ETA in seconds: 13554238.382\n",
      "epoch: 470100, train loss: 4.048524951934814, val loss: 4.0647358894348145, ETA in seconds: 13554586.436\n",
      "epoch: 470200, train loss: 4.050282764434814, val loss: 4.0657124519348145, ETA in seconds: 13554974.857\n",
      "epoch: 470300, train loss: 4.046376514434814, val loss: 4.065321826934815, ETA in seconds: 13555295.293\n",
      "epoch: 470400, train loss: 4.054579639434815, val loss: 4.069032764434814, ETA in seconds: 13555670.171\n",
      "epoch: 470500, train loss: 4.050282764434814, val loss: 4.068446826934815, ETA in seconds: 13556140.327\n",
      "epoch: 470600, train loss: 4.048720264434815, val loss: 4.065126514434814, ETA in seconds: 13556498.765\n",
      "epoch: 470700, train loss: 4.055165576934814, val loss: 4.064345264434815, ETA in seconds: 13556707.989\n",
      "epoch: 470800, train loss: 4.046767139434815, val loss: 4.064931201934814, ETA in seconds: 13556933.292\n",
      "epoch: 470900, train loss: 4.0491108894348145, val loss: 4.065517139434815, ETA in seconds: 13557270.524\n",
      "epoch: 471000, train loss: 4.0510640144348145, val loss: 4.068837451934814, ETA in seconds: 13557590.726\n",
      "epoch: 471100, train loss: 4.052431201934814, val loss: 4.070985889434814, ETA in seconds: 13557967.185\n",
      "epoch: 471200, train loss: 4.049696826934815, val loss: 4.064345264434815, ETA in seconds: 13558297.926\n",
      "epoch: 471300, train loss: 4.046571826934814, val loss: 4.066298389434815, ETA in seconds: 13558604.002\n",
      "epoch: 471400, train loss: 4.0510640144348145, val loss: 4.066493701934815, ETA in seconds: 13558915.189\n",
      "epoch: 471500, train loss: 4.052821826934815, val loss: 4.063564014434815, ETA in seconds: 13559312.939\n",
      "epoch: 471600, train loss: 4.057509326934815, val loss: 4.073329639434815, ETA in seconds: 13559635.552\n",
      "epoch: 471700, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 13559930.044\n",
      "epoch: 471800, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 13560252.146\n",
      "epoch: 471900, train loss: 4.047939014434815, val loss: 4.063564014434815, ETA in seconds: 13560503.747\n",
      "epoch: 472000, train loss: 4.048329639434814, val loss: 4.064931201934814, ETA in seconds: 13560844.534\n",
      "epoch: 472100, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 13561100.046\n",
      "epoch: 472200, train loss: 4.050478076934814, val loss: 4.073720264434814, ETA in seconds: 13561335.477\n",
      "epoch: 472300, train loss: 4.057509326934815, val loss: 4.0676655769348145, ETA in seconds: 13561654.377\n",
      "epoch: 472400, train loss: 4.048524951934814, val loss: 4.064345264434815, ETA in seconds: 13561944.266\n",
      "epoch: 472500, train loss: 4.051454639434814, val loss: 4.061415576934815, ETA in seconds: 13562223.784\n",
      "epoch: 472600, train loss: 4.046376514434814, val loss: 4.074696826934814, ETA in seconds: 13562503.533\n",
      "epoch: 472700, train loss: 4.054774951934815, val loss: 4.066493701934815, ETA in seconds: 13562734.232\n",
      "epoch: 472800, train loss: 4.057314014434814, val loss: 4.064540576934815, ETA in seconds: 13563077.988\n",
      "epoch: 472900, train loss: 4.051845264434815, val loss: 4.064149951934814, ETA in seconds: 13563319.393\n",
      "epoch: 473000, train loss: 4.051259326934814, val loss: 4.071376514434815, ETA in seconds: 13563647.077\n",
      "epoch: 473100, train loss: 4.056532764434815, val loss: 4.069032764434814, ETA in seconds: 13563911.319\n",
      "epoch: 473200, train loss: 4.047939014434815, val loss: 4.063954639434814, ETA in seconds: 13564264.254\n",
      "epoch: 473300, train loss: 4.047743701934815, val loss: 4.071376514434815, ETA in seconds: 13564702.969\n",
      "epoch: 473400, train loss: 4.055556201934815, val loss: 4.064149951934814, ETA in seconds: 13565222.571\n",
      "epoch: 473500, train loss: 4.046376514434814, val loss: 4.064931201934814, ETA in seconds: 13565645.999\n",
      "epoch: 473600, train loss: 4.054774951934815, val loss: 4.0705952644348145, ETA in seconds: 13566007.142\n",
      "epoch: 473700, train loss: 4.046376514434814, val loss: 4.067079639434814, ETA in seconds: 13566341.255\n",
      "epoch: 473800, train loss: 4.0500874519348145, val loss: 4.0637593269348145, ETA in seconds: 13566614.475\n",
      "epoch: 473900, train loss: 4.054384326934814, val loss: 4.069228076934815, ETA in seconds: 13566914.059\n",
      "epoch: 474000, train loss: 4.043446826934814, val loss: 4.064540576934815, ETA in seconds: 13567209.435\n",
      "epoch: 474100, train loss: 4.045009326934815, val loss: 4.070985889434814, ETA in seconds: 13567365.073\n",
      "epoch: 474200, train loss: 4.052431201934814, val loss: 4.064345264434815, ETA in seconds: 13567567.685\n",
      "epoch: 474300, train loss: 4.051454639434814, val loss: 4.066493701934815, ETA in seconds: 13567839.030\n",
      "epoch: 474400, train loss: 4.051649951934815, val loss: 4.068251514434815, ETA in seconds: 13568082.942\n",
      "epoch: 474500, train loss: 4.045790576934815, val loss: 4.068056201934814, ETA in seconds: 13568379.324\n",
      "epoch: 474600, train loss: 4.047939014434815, val loss: 4.068251514434815, ETA in seconds: 13568691.816\n",
      "epoch: 474700, train loss: 4.054189014434814, val loss: 4.067079639434814, ETA in seconds: 13568922.521\n",
      "epoch: 474800, train loss: 4.055751514434815, val loss: 4.070009326934814, ETA in seconds: 13569108.754\n",
      "epoch: 474900, train loss: 4.0539937019348145, val loss: 4.063173389434814, ETA in seconds: 13569347.128\n",
      "epoch: 475000, train loss: 4.050868701934815, val loss: 4.068251514434815, ETA in seconds: 13569643.625\n",
      "epoch: 475100, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 13570113.206\n",
      "epoch: 475200, train loss: 4.050478076934814, val loss: 4.0686421394348145, ETA in seconds: 13570495.625\n",
      "epoch: 475300, train loss: 4.050282764434814, val loss: 4.070204639434815, ETA in seconds: 13570885.797\n",
      "epoch: 475400, train loss: 4.0491108894348145, val loss: 4.065321826934815, ETA in seconds: 13571225.440\n",
      "epoch: 475500, train loss: 4.045595264434814, val loss: 4.062978076934814, ETA in seconds: 13571550.163\n",
      "epoch: 475600, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 13571764.319\n",
      "epoch: 475700, train loss: 4.0481343269348145, val loss: 4.0666890144348145, ETA in seconds: 13572030.803\n",
      "epoch: 475800, train loss: 4.050282764434814, val loss: 4.071767139434814, ETA in seconds: 13572382.777\n",
      "epoch: 475900, train loss: 4.057509326934815, val loss: 4.0696187019348145, ETA in seconds: 13572735.442\n",
      "epoch: 476000, train loss: 4.057118701934814, val loss: 4.065907764434814, ETA in seconds: 13572989.158\n",
      "epoch: 476100, train loss: 4.052626514434815, val loss: 4.065126514434814, ETA in seconds: 13573207.335\n",
      "epoch: 476200, train loss: 4.048329639434814, val loss: 4.068251514434815, ETA in seconds: 13573369.975\n",
      "epoch: 476300, train loss: 4.048720264434815, val loss: 4.071962451934814, ETA in seconds: 13573733.975\n",
      "epoch: 476400, train loss: 4.055751514434815, val loss: 4.063564014434815, ETA in seconds: 13573992.691\n",
      "epoch: 476500, train loss: 4.0549702644348145, val loss: 4.062587451934815, ETA in seconds: 13574153.261\n",
      "epoch: 476600, train loss: 4.0539937019348145, val loss: 4.066103076934814, ETA in seconds: 13574416.302\n",
      "epoch: 476700, train loss: 4.051454639434814, val loss: 4.068251514434815, ETA in seconds: 13574714.263\n",
      "epoch: 476800, train loss: 4.054384326934814, val loss: 4.073720264434814, ETA in seconds: 13574970.341\n",
      "epoch: 476900, train loss: 4.055165576934814, val loss: 4.064149951934814, ETA in seconds: 13575249.564\n",
      "epoch: 477000, train loss: 4.054189014434814, val loss: 4.066103076934814, ETA in seconds: 13575445.780\n",
      "epoch: 477100, train loss: 4.055556201934815, val loss: 4.064345264434815, ETA in seconds: 13575567.405\n",
      "epoch: 477200, train loss: 4.050673389434815, val loss: 4.066884326934814, ETA in seconds: 13575752.963\n",
      "epoch: 477300, train loss: 4.048720264434815, val loss: 4.0715718269348145, ETA in seconds: 13575954.321\n",
      "epoch: 477400, train loss: 4.057118701934814, val loss: 4.066493701934815, ETA in seconds: 13576324.541\n",
      "epoch: 477500, train loss: 4.0452046394348145, val loss: 4.066884326934814, ETA in seconds: 13576680.038\n",
      "epoch: 477600, train loss: 4.054774951934815, val loss: 4.065907764434814, ETA in seconds: 13576936.643\n",
      "epoch: 477700, train loss: 4.051454639434814, val loss: 4.073915576934814, ETA in seconds: 13577049.343\n",
      "epoch: 477800, train loss: 4.053212451934814, val loss: 4.061415576934815, ETA in seconds: 13577286.067\n",
      "epoch: 477900, train loss: 4.051259326934814, val loss: 4.0715718269348145, ETA in seconds: 13577474.775\n",
      "epoch: 478000, train loss: 4.051259326934814, val loss: 4.063368701934815, ETA in seconds: 13577652.011\n",
      "epoch: 478100, train loss: 4.046962451934815, val loss: 4.062196826934814, ETA in seconds: 13577862.784\n",
      "epoch: 478200, train loss: 4.042665576934814, val loss: 4.069814014434814, ETA in seconds: 13578191.381\n",
      "epoch: 478300, train loss: 4.049306201934814, val loss: 4.070790576934814, ETA in seconds: 13578517.842\n",
      "epoch: 478400, train loss: 4.046571826934814, val loss: 4.066103076934814, ETA in seconds: 13578790.128\n",
      "epoch: 478500, train loss: 4.0559468269348145, val loss: 4.071376514434815, ETA in seconds: 13579049.563\n",
      "epoch: 478600, train loss: 4.055165576934814, val loss: 4.068446826934815, ETA in seconds: 13578994.284\n",
      "epoch: 478700, train loss: 4.052821826934815, val loss: 4.0676655769348145, ETA in seconds: 13579304.444\n",
      "epoch: 478800, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 13579693.286\n",
      "epoch: 478900, train loss: 4.048524951934814, val loss: 4.067079639434814, ETA in seconds: 13579910.602\n",
      "epoch: 479000, train loss: 4.054774951934815, val loss: 4.0608296394348145, ETA in seconds: 13580092.779\n",
      "epoch: 479100, train loss: 4.0530171394348145, val loss: 4.0647358894348145, ETA in seconds: 13580319.020\n",
      "epoch: 479200, train loss: 4.046767139434815, val loss: 4.0666890144348145, ETA in seconds: 13580567.220\n",
      "epoch: 479300, train loss: 4.049696826934815, val loss: 4.067079639434814, ETA in seconds: 13580762.494\n",
      "epoch: 479400, train loss: 4.054189014434814, val loss: 4.068251514434815, ETA in seconds: 13580999.280\n",
      "epoch: 479500, train loss: 4.048524951934814, val loss: 4.069423389434815, ETA in seconds: 13581357.908\n",
      "epoch: 479600, train loss: 4.054189014434814, val loss: 4.067860889434814, ETA in seconds: 13581409.276\n",
      "epoch: 479700, train loss: 4.053798389434815, val loss: 4.064345264434815, ETA in seconds: 13581465.050\n",
      "epoch: 479800, train loss: 4.051845264434815, val loss: 4.066103076934814, ETA in seconds: 13581677.929\n",
      "epoch: 479900, train loss: 4.048915576934815, val loss: 4.062196826934814, ETA in seconds: 13582021.167\n",
      "epoch: 480000, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 13582317.042\n",
      "epoch: 480100, train loss: 4.0481343269348145, val loss: 4.069228076934815, ETA in seconds: 13582587.605\n",
      "epoch: 480200, train loss: 4.051845264434815, val loss: 4.069228076934815, ETA in seconds: 13582754.657\n",
      "epoch: 480300, train loss: 4.0530171394348145, val loss: 4.0715718269348145, ETA in seconds: 13583054.171\n",
      "epoch: 480400, train loss: 4.051454639434814, val loss: 4.0686421394348145, ETA in seconds: 13583363.761\n",
      "epoch: 480500, train loss: 4.045790576934815, val loss: 4.0696187019348145, ETA in seconds: 13583567.235\n",
      "epoch: 480600, train loss: 4.0530171394348145, val loss: 4.065126514434814, ETA in seconds: 13583748.386\n",
      "epoch: 480700, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 13583735.750\n",
      "epoch: 480800, train loss: 4.055751514434815, val loss: 4.0745015144348145, ETA in seconds: 13583777.426\n",
      "epoch: 480900, train loss: 4.059462451934815, val loss: 4.065517139434815, ETA in seconds: 13583958.650\n",
      "epoch: 481000, train loss: 4.051845264434815, val loss: 4.060634326934815, ETA in seconds: 13584287.553\n",
      "epoch: 481100, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 13584487.174\n",
      "epoch: 481200, train loss: 4.048329639434814, val loss: 4.065517139434815, ETA in seconds: 13584708.706\n",
      "epoch: 481300, train loss: 4.0481343269348145, val loss: 4.069228076934815, ETA in seconds: 13584893.812\n",
      "epoch: 481400, train loss: 4.054774951934815, val loss: 4.064931201934814, ETA in seconds: 13585106.929\n",
      "epoch: 481500, train loss: 4.048524951934814, val loss: 4.0657124519348145, ETA in seconds: 13585294.887\n",
      "epoch: 481600, train loss: 4.047353076934814, val loss: 4.071767139434814, ETA in seconds: 13585414.073\n",
      "epoch: 481700, train loss: 4.055556201934815, val loss: 4.063368701934815, ETA in seconds: 13585528.888\n",
      "epoch: 481800, train loss: 4.047548389434814, val loss: 4.065321826934815, ETA in seconds: 13585666.025\n",
      "epoch: 481900, train loss: 4.052626514434815, val loss: 4.072157764434815, ETA in seconds: 13585773.766\n",
      "epoch: 482000, train loss: 4.049306201934814, val loss: 4.066493701934815, ETA in seconds: 13585896.916\n",
      "epoch: 482100, train loss: 4.0539937019348145, val loss: 4.069814014434814, ETA in seconds: 13586080.941\n",
      "epoch: 482200, train loss: 4.053603076934815, val loss: 4.067470264434815, ETA in seconds: 13586206.780\n",
      "epoch: 482300, train loss: 4.054189014434814, val loss: 4.073134326934815, ETA in seconds: 13586344.090\n",
      "epoch: 482400, train loss: 4.055556201934815, val loss: 4.068837451934814, ETA in seconds: 13586580.119\n",
      "epoch: 482500, train loss: 4.053603076934815, val loss: 4.065126514434814, ETA in seconds: 13586751.390\n",
      "epoch: 482600, train loss: 4.0520405769348145, val loss: 4.063173389434814, ETA in seconds: 13586894.714\n",
      "epoch: 482700, train loss: 4.050673389434815, val loss: 4.072939014434814, ETA in seconds: 13587141.713\n",
      "epoch: 482800, train loss: 4.0530171394348145, val loss: 4.069228076934815, ETA in seconds: 13587408.625\n",
      "epoch: 482900, train loss: 4.054579639434815, val loss: 4.070985889434814, ETA in seconds: 13587509.965\n",
      "epoch: 483000, train loss: 4.0481343269348145, val loss: 4.060439014434815, ETA in seconds: 13587737.210\n",
      "epoch: 483100, train loss: 4.049501514434814, val loss: 4.0637593269348145, ETA in seconds: 13587916.818\n",
      "epoch: 483200, train loss: 4.049306201934814, val loss: 4.070790576934814, ETA in seconds: 13588107.010\n",
      "epoch: 483300, train loss: 4.048524951934814, val loss: 4.0608296394348145, ETA in seconds: 13588376.472\n",
      "epoch: 483400, train loss: 4.046571826934814, val loss: 4.062587451934815, ETA in seconds: 13588566.798\n",
      "epoch: 483500, train loss: 4.0530171394348145, val loss: 4.069423389434815, ETA in seconds: 13588660.833\n",
      "epoch: 483600, train loss: 4.053603076934815, val loss: 4.0657124519348145, ETA in seconds: 13588801.276\n",
      "epoch: 483700, train loss: 4.0530171394348145, val loss: 4.066103076934814, ETA in seconds: 13588936.451\n",
      "epoch: 483800, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 13589099.170\n",
      "epoch: 483900, train loss: 4.055360889434814, val loss: 4.070790576934814, ETA in seconds: 13589213.516\n",
      "epoch: 484000, train loss: 4.051259326934814, val loss: 4.0696187019348145, ETA in seconds: 13589396.561\n",
      "epoch: 484100, train loss: 4.048329639434814, val loss: 4.062392139434815, ETA in seconds: 13589487.787\n",
      "epoch: 484200, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 13589634.176\n",
      "epoch: 484300, train loss: 4.052235889434814, val loss: 4.066298389434815, ETA in seconds: 13589708.216\n",
      "epoch: 484400, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 13589776.819\n",
      "epoch: 484500, train loss: 4.058290576934814, val loss: 4.069814014434814, ETA in seconds: 13589965.573\n",
      "epoch: 484600, train loss: 4.043837451934815, val loss: 4.066493701934815, ETA in seconds: 13590099.642\n",
      "epoch: 484700, train loss: 4.0549702644348145, val loss: 4.067079639434814, ETA in seconds: 13590285.208\n",
      "epoch: 484800, train loss: 4.0510640144348145, val loss: 4.065126514434814, ETA in seconds: 13590552.531\n",
      "epoch: 484900, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 13590706.596\n",
      "epoch: 485000, train loss: 4.052626514434815, val loss: 4.0735249519348145, ETA in seconds: 13590856.163\n",
      "epoch: 485100, train loss: 4.051454639434814, val loss: 4.069814014434814, ETA in seconds: 13591018.281\n",
      "epoch: 485200, train loss: 4.0481343269348145, val loss: 4.0647358894348145, ETA in seconds: 13591074.925\n",
      "epoch: 485300, train loss: 4.053603076934815, val loss: 4.0705952644348145, ETA in seconds: 13591234.685\n",
      "epoch: 485400, train loss: 4.054384326934814, val loss: 4.064540576934815, ETA in seconds: 13591290.764\n",
      "epoch: 485500, train loss: 4.0520405769348145, val loss: 4.0686421394348145, ETA in seconds: 13591383.239\n",
      "epoch: 485600, train loss: 4.0520405769348145, val loss: 4.069423389434815, ETA in seconds: 13591549.134\n",
      "epoch: 485700, train loss: 4.0530171394348145, val loss: 4.066298389434815, ETA in seconds: 13591746.236\n",
      "epoch: 485800, train loss: 4.056142139434814, val loss: 4.0657124519348145, ETA in seconds: 13591905.370\n",
      "epoch: 485900, train loss: 4.0491108894348145, val loss: 4.066103076934814, ETA in seconds: 13592063.875\n",
      "epoch: 486000, train loss: 4.046962451934815, val loss: 4.067860889434814, ETA in seconds: 13592138.045\n",
      "epoch: 486100, train loss: 4.048329639434814, val loss: 4.069814014434814, ETA in seconds: 13592238.997\n",
      "epoch: 486200, train loss: 4.056142139434814, val loss: 4.069423389434815, ETA in seconds: 13592349.167\n",
      "epoch: 486300, train loss: 4.049501514434814, val loss: 4.064345264434815, ETA in seconds: 13592528.089\n",
      "epoch: 486400, train loss: 4.057314014434814, val loss: 4.065321826934815, ETA in seconds: 13592649.379\n",
      "epoch: 486500, train loss: 4.052431201934814, val loss: 4.0647358894348145, ETA in seconds: 13592804.543\n",
      "epoch: 486600, train loss: 4.053603076934815, val loss: 4.062978076934814, ETA in seconds: 13592988.610\n",
      "epoch: 486700, train loss: 4.0471577644348145, val loss: 4.067470264434815, ETA in seconds: 13593113.137\n",
      "epoch: 486800, train loss: 4.0520405769348145, val loss: 4.074110889434815, ETA in seconds: 13593318.004\n",
      "epoch: 486900, train loss: 4.050478076934814, val loss: 4.061024951934814, ETA in seconds: 13593407.848\n",
      "epoch: 487000, train loss: 4.053212451934814, val loss: 4.065517139434815, ETA in seconds: 13593566.416\n",
      "epoch: 487100, train loss: 4.0442280769348145, val loss: 4.0627827644348145, ETA in seconds: 13593652.559\n",
      "epoch: 487200, train loss: 4.051454639434814, val loss: 4.0676655769348145, ETA in seconds: 13593779.969\n",
      "epoch: 487300, train loss: 4.053407764434814, val loss: 4.068251514434815, ETA in seconds: 13593895.052\n",
      "epoch: 487400, train loss: 4.048915576934815, val loss: 4.067470264434815, ETA in seconds: 13593930.381\n",
      "epoch: 487500, train loss: 4.057118701934814, val loss: 4.0657124519348145, ETA in seconds: 13594078.097\n",
      "epoch: 487600, train loss: 4.052626514434815, val loss: 4.0686421394348145, ETA in seconds: 13594266.203\n",
      "epoch: 487700, train loss: 4.051259326934814, val loss: 4.068251514434815, ETA in seconds: 13594346.684\n",
      "epoch: 487800, train loss: 4.050478076934814, val loss: 4.071962451934814, ETA in seconds: 13594466.131\n",
      "epoch: 487900, train loss: 4.059267139434814, val loss: 4.0637593269348145, ETA in seconds: 13594676.287\n",
      "epoch: 488000, train loss: 4.048720264434815, val loss: 4.068446826934815, ETA in seconds: 13594702.064\n",
      "epoch: 488100, train loss: 4.054384326934814, val loss: 4.0588765144348145, ETA in seconds: 13594743.200\n",
      "epoch: 488200, train loss: 4.056728076934815, val loss: 4.069032764434814, ETA in seconds: 13594783.853\n",
      "epoch: 488300, train loss: 4.047743701934815, val loss: 4.063368701934815, ETA in seconds: 13594833.172\n",
      "epoch: 488400, train loss: 4.047939014434815, val loss: 4.065126514434814, ETA in seconds: 13594960.167\n",
      "epoch: 488500, train loss: 4.045399951934814, val loss: 4.069423389434815, ETA in seconds: 13595087.291\n",
      "epoch: 488600, train loss: 4.051259326934814, val loss: 4.066884326934814, ETA in seconds: 13595265.619\n",
      "epoch: 488700, train loss: 4.0491108894348145, val loss: 4.070985889434814, ETA in seconds: 13595376.523\n",
      "epoch: 488800, train loss: 4.0520405769348145, val loss: 4.060634326934815, ETA in seconds: 13595459.941\n",
      "epoch: 488900, train loss: 4.049501514434814, val loss: 4.068056201934814, ETA in seconds: 13595610.942\n",
      "epoch: 489000, train loss: 4.0491108894348145, val loss: 4.0715718269348145, ETA in seconds: 13595798.735\n",
      "epoch: 489100, train loss: 4.0549702644348145, val loss: 4.068056201934814, ETA in seconds: 13595961.949\n",
      "epoch: 489200, train loss: 4.051845264434815, val loss: 4.071376514434815, ETA in seconds: 13596238.017\n",
      "epoch: 489300, train loss: 4.049892139434815, val loss: 4.064931201934814, ETA in seconds: 13596380.469\n",
      "epoch: 489400, train loss: 4.049306201934814, val loss: 4.065321826934815, ETA in seconds: 13596467.840\n",
      "epoch: 489500, train loss: 4.055165576934814, val loss: 4.0647358894348145, ETA in seconds: 13596619.985\n",
      "epoch: 489600, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 13596697.418\n",
      "epoch: 489700, train loss: 4.053212451934814, val loss: 4.064540576934815, ETA in seconds: 13596801.219\n",
      "epoch: 489800, train loss: 4.050673389434815, val loss: 4.069814014434814, ETA in seconds: 13596840.798\n",
      "epoch: 489900, train loss: 4.0559468269348145, val loss: 4.068446826934815, ETA in seconds: 13596927.711\n",
      "epoch: 490000, train loss: 4.055556201934815, val loss: 4.068446826934815, ETA in seconds: 13596975.905\n",
      "epoch: 490100, train loss: 4.048915576934815, val loss: 4.066493701934815, ETA in seconds: 13597015.646\n",
      "epoch: 490200, train loss: 4.047939014434815, val loss: 4.068837451934814, ETA in seconds: 13597074.126\n",
      "epoch: 490300, train loss: 4.058681201934815, val loss: 4.065126514434814, ETA in seconds: 13597170.697\n",
      "epoch: 490400, train loss: 4.0549702644348145, val loss: 4.074110889434815, ETA in seconds: 13597252.799\n",
      "epoch: 490500, train loss: 4.045790576934815, val loss: 4.067274951934815, ETA in seconds: 13597292.767\n",
      "epoch: 490600, train loss: 4.054774951934815, val loss: 4.066298389434815, ETA in seconds: 13597332.856\n",
      "epoch: 490700, train loss: 4.0539937019348145, val loss: 4.067470264434815, ETA in seconds: 13597334.153\n",
      "epoch: 490800, train loss: 4.056142139434814, val loss: 4.065907764434814, ETA in seconds: 13597393.555\n",
      "epoch: 490900, train loss: 4.055360889434814, val loss: 4.067470264434815, ETA in seconds: 13597501.640\n",
      "epoch: 491000, train loss: 4.045985889434815, val loss: 4.068446826934815, ETA in seconds: 13597582.763\n",
      "epoch: 491100, train loss: 4.054189014434814, val loss: 4.064540576934815, ETA in seconds: 13597667.041\n",
      "epoch: 491200, train loss: 4.053603076934815, val loss: 4.068837451934814, ETA in seconds: 13597717.329\n",
      "epoch: 491300, train loss: 4.051454639434814, val loss: 4.063954639434814, ETA in seconds: 13597853.722\n",
      "epoch: 491400, train loss: 4.053212451934814, val loss: 4.066884326934814, ETA in seconds: 13597875.497\n",
      "epoch: 491500, train loss: 4.055360889434814, val loss: 4.066884326934814, ETA in seconds: 13598021.493\n",
      "epoch: 491600, train loss: 4.048329639434814, val loss: 4.065126514434814, ETA in seconds: 13598078.051\n",
      "epoch: 491700, train loss: 4.0530171394348145, val loss: 4.071181201934815, ETA in seconds: 13598188.806\n",
      "epoch: 491800, train loss: 4.0500874519348145, val loss: 4.062196826934814, ETA in seconds: 13598389.625\n",
      "epoch: 491900, train loss: 4.052821826934815, val loss: 4.063173389434814, ETA in seconds: 13598448.545\n",
      "epoch: 492000, train loss: 4.052235889434814, val loss: 4.066884326934814, ETA in seconds: 13598663.492\n",
      "epoch: 492100, train loss: 4.0539937019348145, val loss: 4.065126514434814, ETA in seconds: 13598684.760\n",
      "epoch: 492200, train loss: 4.048915576934815, val loss: 4.069423389434815, ETA in seconds: 13598628.594\n",
      "epoch: 492300, train loss: 4.049306201934814, val loss: 4.065907764434814, ETA in seconds: 13598655.894\n",
      "epoch: 492400, train loss: 4.058485889434815, val loss: 4.065907764434814, ETA in seconds: 13598847.054\n",
      "epoch: 492500, train loss: 4.049696826934815, val loss: 4.070985889434814, ETA in seconds: 13598937.023\n",
      "epoch: 492600, train loss: 4.0520405769348145, val loss: 4.069423389434815, ETA in seconds: 13598935.785\n",
      "epoch: 492700, train loss: 4.053798389434815, val loss: 4.063173389434814, ETA in seconds: 13599014.897\n",
      "epoch: 492800, train loss: 4.056142139434814, val loss: 4.068251514434815, ETA in seconds: 13599110.351\n",
      "epoch: 492900, train loss: 4.052821826934815, val loss: 4.068251514434815, ETA in seconds: 13599168.272\n",
      "epoch: 493000, train loss: 4.0491108894348145, val loss: 4.068251514434815, ETA in seconds: 13599202.283\n",
      "epoch: 493100, train loss: 4.051649951934815, val loss: 4.070985889434814, ETA in seconds: 13599182.924\n",
      "epoch: 493200, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 13599163.955\n",
      "epoch: 493300, train loss: 4.0510640144348145, val loss: 4.065321826934815, ETA in seconds: 13599263.532\n",
      "epoch: 493400, train loss: 4.0461812019348145, val loss: 4.064931201934814, ETA in seconds: 13599473.571\n",
      "epoch: 493500, train loss: 4.0510640144348145, val loss: 4.070985889434814, ETA in seconds: 13599555.237\n",
      "epoch: 493600, train loss: 4.048329639434814, val loss: 4.0647358894348145, ETA in seconds: 13599665.023\n",
      "epoch: 493700, train loss: 4.051259326934814, val loss: 4.0696187019348145, ETA in seconds: 13599811.586\n",
      "epoch: 493800, train loss: 4.047353076934814, val loss: 4.067470264434815, ETA in seconds: 13599944.856\n",
      "epoch: 493900, train loss: 4.054189014434814, val loss: 4.065321826934815, ETA in seconds: 13600104.676\n",
      "epoch: 494000, train loss: 4.055751514434815, val loss: 4.067079639434814, ETA in seconds: 13600330.892\n",
      "epoch: 494100, train loss: 4.049501514434814, val loss: 4.0705952644348145, ETA in seconds: 13600458.618\n",
      "epoch: 494200, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 13600504.890\n",
      "epoch: 494300, train loss: 4.0578999519348145, val loss: 4.0676655769348145, ETA in seconds: 13600383.338\n",
      "epoch: 494400, train loss: 4.051259326934814, val loss: 4.064149951934814, ETA in seconds: 13600441.020\n",
      "epoch: 494500, train loss: 4.055360889434814, val loss: 4.062978076934814, ETA in seconds: 13600579.777\n",
      "epoch: 494600, train loss: 4.0569233894348145, val loss: 4.065321826934815, ETA in seconds: 13600738.106\n",
      "epoch: 494700, train loss: 4.050868701934815, val loss: 4.063564014434815, ETA in seconds: 13600824.636\n",
      "epoch: 494800, train loss: 4.056728076934815, val loss: 4.0666890144348145, ETA in seconds: 13600899.270\n",
      "epoch: 494900, train loss: 4.051259326934814, val loss: 4.067470264434815, ETA in seconds: 13600938.961\n",
      "epoch: 495000, train loss: 4.048915576934815, val loss: 4.066493701934815, ETA in seconds: 13601063.569\n",
      "epoch: 495100, train loss: 4.053212451934814, val loss: 4.065321826934815, ETA in seconds: 13601191.069\n",
      "epoch: 495200, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 13601283.885\n",
      "epoch: 495300, train loss: 4.053603076934815, val loss: 4.0676655769348145, ETA in seconds: 13601412.786\n",
      "epoch: 495400, train loss: 4.055751514434815, val loss: 4.070985889434814, ETA in seconds: 13601497.271\n",
      "epoch: 495500, train loss: 4.050868701934815, val loss: 4.069814014434814, ETA in seconds: 13601590.314\n",
      "epoch: 495600, train loss: 4.053407764434814, val loss: 4.065321826934815, ETA in seconds: 13601703.685\n",
      "epoch: 495700, train loss: 4.0461812019348145, val loss: 4.067274951934815, ETA in seconds: 13601733.542\n",
      "epoch: 495800, train loss: 4.045399951934814, val loss: 4.0666890144348145, ETA in seconds: 13601257.328\n",
      "epoch: 495900, train loss: 4.048329639434814, val loss: 4.068446826934815, ETA in seconds: 13601156.812\n",
      "epoch: 496000, train loss: 4.052235889434814, val loss: 4.066103076934814, ETA in seconds: 13601207.113\n",
      "epoch: 496100, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 13600969.402\n",
      "epoch: 496200, train loss: 4.052235889434814, val loss: 4.065126514434814, ETA in seconds: 13600920.354\n",
      "epoch: 496300, train loss: 4.056142139434814, val loss: 4.077235889434815, ETA in seconds: 13601033.418\n",
      "epoch: 496400, train loss: 4.050478076934814, val loss: 4.064149951934814, ETA in seconds: 13601136.320\n",
      "epoch: 496500, train loss: 4.050478076934814, val loss: 4.069423389434815, ETA in seconds: 13601262.539\n",
      "epoch: 496600, train loss: 4.0530171394348145, val loss: 4.066884326934814, ETA in seconds: 13601436.916\n",
      "epoch: 496700, train loss: 4.0539937019348145, val loss: 4.070399951934815, ETA in seconds: 13601567.803\n",
      "epoch: 496800, train loss: 4.052821826934815, val loss: 4.064931201934814, ETA in seconds: 13601726.653\n",
      "epoch: 496900, train loss: 4.056337451934814, val loss: 4.066298389434815, ETA in seconds: 13601879.589\n",
      "epoch: 497000, train loss: 4.054384326934814, val loss: 4.065517139434815, ETA in seconds: 13602003.735\n",
      "epoch: 497100, train loss: 4.0559468269348145, val loss: 4.067860889434814, ETA in seconds: 13602048.973\n",
      "epoch: 497200, train loss: 4.057118701934814, val loss: 4.070790576934814, ETA in seconds: 13602190.210\n",
      "epoch: 497300, train loss: 4.058095264434814, val loss: 4.067470264434815, ETA in seconds: 13602383.993\n",
      "epoch: 497400, train loss: 4.053212451934814, val loss: 4.063564014434815, ETA in seconds: 13602415.058\n",
      "epoch: 497500, train loss: 4.0510640144348145, val loss: 4.068837451934814, ETA in seconds: 13602561.547\n",
      "epoch: 497600, train loss: 4.051259326934814, val loss: 4.068056201934814, ETA in seconds: 13602463.379\n",
      "epoch: 497700, train loss: 4.048915576934815, val loss: 4.073329639434815, ETA in seconds: 13602177.678\n",
      "epoch: 497800, train loss: 4.050868701934815, val loss: 4.068837451934814, ETA in seconds: 13601877.068\n",
      "epoch: 497900, train loss: 4.052235889434814, val loss: 4.0657124519348145, ETA in seconds: 13601957.402\n",
      "epoch: 498000, train loss: 4.050282764434814, val loss: 4.070204639434815, ETA in seconds: 13602141.889\n",
      "epoch: 498100, train loss: 4.057118701934814, val loss: 4.066493701934815, ETA in seconds: 13602265.194\n",
      "epoch: 498200, train loss: 4.044618701934814, val loss: 4.0647358894348145, ETA in seconds: 13602418.175\n",
      "epoch: 498300, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 13602452.775\n",
      "epoch: 498400, train loss: 4.051649951934815, val loss: 4.0745015144348145, ETA in seconds: 13602054.032\n",
      "epoch: 498500, train loss: 4.056728076934815, val loss: 4.065517139434815, ETA in seconds: 13601664.787\n",
      "epoch: 498600, train loss: 4.056728076934815, val loss: 4.0725483894348145, ETA in seconds: 13601561.100\n",
      "epoch: 498700, train loss: 4.047548389434814, val loss: 4.0666890144348145, ETA in seconds: 13601533.503\n",
      "epoch: 498800, train loss: 4.052626514434815, val loss: 4.0657124519348145, ETA in seconds: 13601509.641\n",
      "epoch: 498900, train loss: 4.047548389434814, val loss: 4.065517139434815, ETA in seconds: 13601470.697\n",
      "epoch: 499000, train loss: 4.053798389434815, val loss: 4.065321826934815, ETA in seconds: 13601387.320\n",
      "epoch: 499100, train loss: 4.045595264434814, val loss: 4.061220264434814, ETA in seconds: 13601413.514\n",
      "epoch: 499200, train loss: 4.0510640144348145, val loss: 4.0637593269348145, ETA in seconds: 13601472.302\n",
      "epoch: 499300, train loss: 4.048329639434814, val loss: 4.0686421394348145, ETA in seconds: 13601481.493\n",
      "epoch: 499400, train loss: 4.049306201934814, val loss: 4.068056201934814, ETA in seconds: 13601466.833\n",
      "epoch: 499500, train loss: 4.0481343269348145, val loss: 4.073134326934815, ETA in seconds: 13601411.398\n",
      "epoch: 499600, train loss: 4.048329639434814, val loss: 4.066103076934814, ETA in seconds: 13601628.618\n",
      "epoch: 499700, train loss: 4.052626514434815, val loss: 4.064345264434815, ETA in seconds: 13601810.587\n",
      "epoch: 499800, train loss: 4.051649951934815, val loss: 4.0666890144348145, ETA in seconds: 13601686.438\n",
      "epoch: 499900, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 13601426.654\n",
      "epoch: 500000, train loss: 4.048329639434814, val loss: 4.062587451934815, ETA in seconds: 13601416.893\n",
      "epoch: 500100, train loss: 4.0452046394348145, val loss: 4.066884326934814, ETA in seconds: 13601488.547\n",
      "epoch: 500200, train loss: 4.048915576934815, val loss: 4.061024951934814, ETA in seconds: 13601518.786\n",
      "epoch: 500300, train loss: 4.052821826934815, val loss: 4.0686421394348145, ETA in seconds: 13601563.383\n",
      "epoch: 500400, train loss: 4.054579639434815, val loss: 4.061415576934815, ETA in seconds: 13601578.818\n",
      "epoch: 500500, train loss: 4.053798389434815, val loss: 4.0657124519348145, ETA in seconds: 13601552.699\n",
      "epoch: 500600, train loss: 4.055360889434814, val loss: 4.0666890144348145, ETA in seconds: 13601489.227\n",
      "epoch: 500700, train loss: 4.0530171394348145, val loss: 4.065517139434815, ETA in seconds: 13601518.245\n",
      "epoch: 500800, train loss: 4.053212451934814, val loss: 4.070009326934814, ETA in seconds: 13601478.541\n",
      "epoch: 500900, train loss: 4.051649951934815, val loss: 4.0696187019348145, ETA in seconds: 13601438.005\n",
      "epoch: 501000, train loss: 4.054774951934815, val loss: 4.066103076934814, ETA in seconds: 13601363.620\n",
      "epoch: 501100, train loss: 4.0452046394348145, val loss: 4.0666890144348145, ETA in seconds: 13601392.437\n",
      "epoch: 501200, train loss: 4.048329639434814, val loss: 4.066884326934814, ETA in seconds: 13601309.089\n",
      "epoch: 501300, train loss: 4.0559468269348145, val loss: 4.0686421394348145, ETA in seconds: 13601254.267\n",
      "epoch: 501400, train loss: 4.0549702644348145, val loss: 4.069032764434814, ETA in seconds: 13601301.756\n",
      "epoch: 501500, train loss: 4.051845264434815, val loss: 4.062587451934815, ETA in seconds: 13601397.619\n",
      "epoch: 501600, train loss: 4.049696826934815, val loss: 4.070204639434815, ETA in seconds: 13601408.576\n",
      "epoch: 501700, train loss: 4.055556201934815, val loss: 4.062196826934814, ETA in seconds: 13601182.938\n",
      "epoch: 501800, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 13601145.040\n",
      "epoch: 501900, train loss: 4.0520405769348145, val loss: 4.065517139434815, ETA in seconds: 13601136.045\n",
      "epoch: 502000, train loss: 4.050673389434815, val loss: 4.075282764434815, ETA in seconds: 13601149.588\n",
      "epoch: 502100, train loss: 4.043446826934814, val loss: 4.0647358894348145, ETA in seconds: 13601155.113\n",
      "epoch: 502200, train loss: 4.053798389434815, val loss: 4.064931201934814, ETA in seconds: 13601103.932\n",
      "epoch: 502300, train loss: 4.054774951934815, val loss: 4.0666890144348145, ETA in seconds: 13601165.115\n",
      "epoch: 502400, train loss: 4.054774951934815, val loss: 4.0666890144348145, ETA in seconds: 13601064.286\n",
      "epoch: 502500, train loss: 4.054579639434815, val loss: 4.0627827644348145, ETA in seconds: 13600782.233\n",
      "epoch: 502600, train loss: 4.0510640144348145, val loss: 4.067860889434814, ETA in seconds: 13600625.138\n",
      "epoch: 502700, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 13600478.422\n",
      "epoch: 502800, train loss: 4.052431201934814, val loss: 4.0637593269348145, ETA in seconds: 13600286.937\n",
      "epoch: 502900, train loss: 4.0491108894348145, val loss: 4.067274951934815, ETA in seconds: 13600069.041\n",
      "epoch: 503000, train loss: 4.047939014434815, val loss: 4.067470264434815, ETA in seconds: 13600092.252\n",
      "epoch: 503100, train loss: 4.049892139434815, val loss: 4.070009326934814, ETA in seconds: 13600021.509\n",
      "epoch: 503200, train loss: 4.047353076934814, val loss: 4.064345264434815, ETA in seconds: 13600021.357\n",
      "epoch: 503300, train loss: 4.046767139434815, val loss: 4.0637593269348145, ETA in seconds: 13599990.046\n",
      "epoch: 503400, train loss: 4.050868701934815, val loss: 4.064540576934815, ETA in seconds: 13599870.361\n",
      "epoch: 503500, train loss: 4.047353076934814, val loss: 4.072157764434815, ETA in seconds: 13599767.644\n",
      "epoch: 503600, train loss: 4.047353076934814, val loss: 4.070790576934814, ETA in seconds: 13599628.093\n",
      "epoch: 503700, train loss: 4.049501514434814, val loss: 4.067860889434814, ETA in seconds: 13599628.676\n",
      "epoch: 503800, train loss: 4.048915576934815, val loss: 4.068251514434815, ETA in seconds: 13599536.964\n",
      "epoch: 503900, train loss: 4.051649951934815, val loss: 4.0676655769348145, ETA in seconds: 13599420.995\n",
      "epoch: 504000, train loss: 4.049306201934814, val loss: 4.067860889434814, ETA in seconds: 13599562.717\n",
      "epoch: 504100, train loss: 4.054579639434815, val loss: 4.065907764434814, ETA in seconds: 13599645.182\n",
      "epoch: 504200, train loss: 4.053603076934815, val loss: 4.072157764434815, ETA in seconds: 13599728.646\n",
      "epoch: 504300, train loss: 4.0461812019348145, val loss: 4.069814014434814, ETA in seconds: 13599594.554\n",
      "epoch: 504400, train loss: 4.046571826934814, val loss: 4.069423389434815, ETA in seconds: 13599476.727\n",
      "epoch: 504500, train loss: 4.055556201934815, val loss: 4.0676655769348145, ETA in seconds: 13599414.826\n",
      "epoch: 504600, train loss: 4.047548389434814, val loss: 4.067274951934815, ETA in seconds: 13599424.440\n",
      "epoch: 504700, train loss: 4.055556201934815, val loss: 4.068446826934815, ETA in seconds: 13599358.593\n",
      "epoch: 504800, train loss: 4.051845264434815, val loss: 4.0666890144348145, ETA in seconds: 13599188.815\n",
      "epoch: 504900, train loss: 4.0500874519348145, val loss: 4.066298389434815, ETA in seconds: 13599144.539\n",
      "epoch: 505000, train loss: 4.051259326934814, val loss: 4.066103076934814, ETA in seconds: 13599118.621\n",
      "epoch: 505100, train loss: 4.050478076934814, val loss: 4.066298389434815, ETA in seconds: 13599041.779\n",
      "epoch: 505200, train loss: 4.053798389434815, val loss: 4.0666890144348145, ETA in seconds: 13599091.375\n",
      "epoch: 505300, train loss: 4.048915576934815, val loss: 4.071376514434815, ETA in seconds: 13599224.629\n",
      "epoch: 505400, train loss: 4.054384326934814, val loss: 4.078212451934815, ETA in seconds: 13599307.275\n",
      "epoch: 505500, train loss: 4.054189014434814, val loss: 4.072157764434815, ETA in seconds: 13599390.035\n",
      "epoch: 505600, train loss: 4.054384326934814, val loss: 4.0608296394348145, ETA in seconds: 13599308.068\n",
      "epoch: 505700, train loss: 4.044032764434815, val loss: 4.067860889434814, ETA in seconds: 13599270.452\n",
      "epoch: 505800, train loss: 4.055556201934815, val loss: 4.063173389434814, ETA in seconds: 13599245.966\n",
      "epoch: 505900, train loss: 4.0549702644348145, val loss: 4.067079639434814, ETA in seconds: 13599265.603\n",
      "epoch: 506000, train loss: 4.051259326934814, val loss: 4.066103076934814, ETA in seconds: 13599157.751\n",
      "epoch: 506100, train loss: 4.047548389434814, val loss: 4.066493701934815, ETA in seconds: 13599135.541\n",
      "epoch: 506200, train loss: 4.045985889434815, val loss: 4.0676655769348145, ETA in seconds: 13598993.201\n",
      "epoch: 506300, train loss: 4.054774951934815, val loss: 4.067470264434815, ETA in seconds: 13598916.077\n",
      "epoch: 506400, train loss: 4.0559468269348145, val loss: 4.066884326934814, ETA in seconds: 13598803.983\n",
      "epoch: 506500, train loss: 4.048720264434815, val loss: 4.0676655769348145, ETA in seconds: 13598833.442\n",
      "epoch: 506600, train loss: 4.050478076934814, val loss: 4.066298389434815, ETA in seconds: 13598690.878\n",
      "epoch: 506700, train loss: 4.052431201934814, val loss: 4.069032764434814, ETA in seconds: 13598679.544\n",
      "epoch: 506800, train loss: 4.056532764434815, val loss: 4.064540576934815, ETA in seconds: 13598683.215\n",
      "epoch: 506900, train loss: 4.0500874519348145, val loss: 4.070009326934814, ETA in seconds: 13598606.446\n",
      "epoch: 507000, train loss: 4.045399951934814, val loss: 4.070399951934815, ETA in seconds: 13598554.610\n",
      "epoch: 507100, train loss: 4.052821826934815, val loss: 4.0657124519348145, ETA in seconds: 13598696.198\n",
      "epoch: 507200, train loss: 4.053407764434814, val loss: 4.0627827644348145, ETA in seconds: 13598726.762\n",
      "epoch: 507300, train loss: 4.052235889434814, val loss: 4.065907764434814, ETA in seconds: 13598616.692\n",
      "epoch: 507400, train loss: 4.0452046394348145, val loss: 4.071767139434814, ETA in seconds: 13598526.895\n",
      "epoch: 507500, train loss: 4.054189014434814, val loss: 4.070790576934814, ETA in seconds: 13598487.235\n",
      "epoch: 507600, train loss: 4.049306201934814, val loss: 4.064149951934814, ETA in seconds: 13598505.433\n",
      "epoch: 507700, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 13598435.957\n",
      "epoch: 507800, train loss: 4.041884326934815, val loss: 4.074696826934814, ETA in seconds: 13598215.826\n",
      "epoch: 507900, train loss: 4.052431201934814, val loss: 4.070009326934814, ETA in seconds: 13597931.542\n",
      "epoch: 508000, train loss: 4.059462451934815, val loss: 4.063954639434814, ETA in seconds: 13597851.787\n",
      "epoch: 508100, train loss: 4.045790576934815, val loss: 4.066103076934814, ETA in seconds: 13597712.867\n",
      "epoch: 508200, train loss: 4.0452046394348145, val loss: 4.073915576934814, ETA in seconds: 13597605.507\n",
      "epoch: 508300, train loss: 4.0530171394348145, val loss: 4.068446826934815, ETA in seconds: 13597500.775\n",
      "epoch: 508400, train loss: 4.050868701934815, val loss: 4.068446826934815, ETA in seconds: 13597520.339\n",
      "epoch: 508500, train loss: 4.051649951934815, val loss: 4.067470264434815, ETA in seconds: 13597364.844\n",
      "epoch: 508600, train loss: 4.0520405769348145, val loss: 4.067079639434814, ETA in seconds: 13597062.392\n",
      "epoch: 508700, train loss: 4.0530171394348145, val loss: 4.070985889434814, ETA in seconds: 13596856.800\n",
      "epoch: 508800, train loss: 4.056728076934815, val loss: 4.066298389434815, ETA in seconds: 13596571.982\n",
      "epoch: 508900, train loss: 4.049696826934815, val loss: 4.070009326934814, ETA in seconds: 13596098.047\n",
      "epoch: 509000, train loss: 4.048524951934814, val loss: 4.0676655769348145, ETA in seconds: 13595693.963\n",
      "epoch: 509100, train loss: 4.056337451934814, val loss: 4.0666890144348145, ETA in seconds: 13595283.693\n",
      "epoch: 509200, train loss: 4.052626514434815, val loss: 4.065126514434814, ETA in seconds: 13594864.540\n",
      "epoch: 509300, train loss: 4.0432515144348145, val loss: 4.065321826934815, ETA in seconds: 13594536.867\n",
      "epoch: 509400, train loss: 4.058290576934814, val loss: 4.068056201934814, ETA in seconds: 13594273.605\n",
      "epoch: 509500, train loss: 4.046962451934815, val loss: 4.0627827644348145, ETA in seconds: 13594008.153\n",
      "epoch: 509600, train loss: 4.050478076934814, val loss: 4.062001514434814, ETA in seconds: 13593692.883\n",
      "epoch: 509700, train loss: 4.056337451934814, val loss: 4.065907764434814, ETA in seconds: 13593505.178\n",
      "epoch: 509800, train loss: 4.048915576934815, val loss: 4.067274951934815, ETA in seconds: 13593389.129\n",
      "epoch: 509900, train loss: 4.055751514434815, val loss: 4.073329639434815, ETA in seconds: 13593335.348\n",
      "epoch: 510000, train loss: 4.0539937019348145, val loss: 4.065517139434815, ETA in seconds: 13593319.122\n",
      "epoch: 510100, train loss: 4.058681201934815, val loss: 4.071376514434815, ETA in seconds: 13593152.529\n",
      "epoch: 510200, train loss: 4.056142139434814, val loss: 4.072743701934814, ETA in seconds: 13593001.122\n",
      "epoch: 510300, train loss: 4.046962451934815, val loss: 4.064149951934814, ETA in seconds: 13592986.660\n",
      "epoch: 510400, train loss: 4.053407764434814, val loss: 4.063954639434814, ETA in seconds: 13592825.408\n",
      "epoch: 510500, train loss: 4.049696826934815, val loss: 4.062978076934814, ETA in seconds: 13592657.422\n",
      "epoch: 510600, train loss: 4.049696826934815, val loss: 4.065517139434815, ETA in seconds: 13592439.347\n",
      "epoch: 510700, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 13592359.313\n",
      "epoch: 510800, train loss: 4.0510640144348145, val loss: 4.0686421394348145, ETA in seconds: 13592348.929\n",
      "epoch: 510900, train loss: 4.060439014434815, val loss: 4.067860889434814, ETA in seconds: 13592357.553\n",
      "epoch: 511000, train loss: 4.059267139434814, val loss: 4.066884326934814, ETA in seconds: 13592200.839\n",
      "epoch: 511100, train loss: 4.058095264434814, val loss: 4.067079639434814, ETA in seconds: 13592103.948\n",
      "epoch: 511200, train loss: 4.047743701934815, val loss: 4.063173389434814, ETA in seconds: 13591928.958\n",
      "epoch: 511300, train loss: 4.047353076934814, val loss: 4.0676655769348145, ETA in seconds: 13591715.784\n",
      "epoch: 511400, train loss: 4.045790576934815, val loss: 4.067274951934815, ETA in seconds: 13591426.698\n",
      "epoch: 511500, train loss: 4.046571826934814, val loss: 4.073915576934814, ETA in seconds: 13591296.412\n",
      "epoch: 511600, train loss: 4.0471577644348145, val loss: 4.069423389434815, ETA in seconds: 13591116.740\n",
      "epoch: 511700, train loss: 4.046767139434815, val loss: 4.066103076934814, ETA in seconds: 13590902.561\n",
      "epoch: 511800, train loss: 4.047548389434814, val loss: 4.067079639434814, ETA in seconds: 13590817.409\n",
      "epoch: 511900, train loss: 4.051454639434814, val loss: 4.066493701934815, ETA in seconds: 13590720.297\n",
      "epoch: 512000, train loss: 4.0491108894348145, val loss: 4.071767139434814, ETA in seconds: 13590488.548\n",
      "epoch: 512100, train loss: 4.0549702644348145, val loss: 4.069423389434815, ETA in seconds: 13590296.008\n",
      "epoch: 512200, train loss: 4.052626514434815, val loss: 4.0637593269348145, ETA in seconds: 13590201.254\n",
      "epoch: 512300, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 13590160.475\n",
      "epoch: 512400, train loss: 4.0500874519348145, val loss: 4.0666890144348145, ETA in seconds: 13590013.781\n",
      "epoch: 512500, train loss: 4.061220264434814, val loss: 4.064540576934815, ETA in seconds: 13589895.696\n",
      "epoch: 512600, train loss: 4.0500874519348145, val loss: 4.062978076934814, ETA in seconds: 13589421.772\n",
      "epoch: 512700, train loss: 4.046767139434815, val loss: 4.070204639434815, ETA in seconds: 13588899.058\n",
      "epoch: 512800, train loss: 4.051649951934815, val loss: 4.0618062019348145, ETA in seconds: 13588295.929\n",
      "epoch: 512900, train loss: 4.050478076934814, val loss: 4.074306201934815, ETA in seconds: 13587772.361\n",
      "epoch: 513000, train loss: 4.0500874519348145, val loss: 4.065517139434815, ETA in seconds: 13587144.334\n",
      "epoch: 513100, train loss: 4.053212451934814, val loss: 4.069423389434815, ETA in seconds: 13586523.684\n",
      "epoch: 513200, train loss: 4.051649951934815, val loss: 4.0715718269348145, ETA in seconds: 13585942.397\n",
      "epoch: 513300, train loss: 4.048524951934814, val loss: 4.065517139434815, ETA in seconds: 13585368.889\n",
      "epoch: 513400, train loss: 4.053212451934814, val loss: 4.065517139434815, ETA in seconds: 13584982.263\n",
      "epoch: 513500, train loss: 4.0520405769348145, val loss: 4.062978076934814, ETA in seconds: 13584621.981\n",
      "epoch: 513600, train loss: 4.049892139434815, val loss: 4.062978076934814, ETA in seconds: 13584461.993\n",
      "epoch: 513700, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 13584249.246\n",
      "epoch: 513800, train loss: 4.0539937019348145, val loss: 4.068837451934814, ETA in seconds: 13584044.900\n",
      "epoch: 513900, train loss: 4.050868701934815, val loss: 4.068446826934815, ETA in seconds: 13583868.015\n",
      "epoch: 514000, train loss: 4.048329639434814, val loss: 4.062392139434815, ETA in seconds: 13583668.799\n",
      "epoch: 514100, train loss: 4.050673389434815, val loss: 4.067274951934815, ETA in seconds: 13583455.656\n",
      "epoch: 514200, train loss: 4.053798389434815, val loss: 4.066298389434815, ETA in seconds: 13583265.916\n",
      "epoch: 514300, train loss: 4.053212451934814, val loss: 4.065126514434814, ETA in seconds: 13583050.667\n",
      "epoch: 514400, train loss: 4.055751514434815, val loss: 4.064931201934814, ETA in seconds: 13582850.651\n",
      "epoch: 514500, train loss: 4.054774951934815, val loss: 4.066298389434815, ETA in seconds: 13582832.339\n",
      "epoch: 514600, train loss: 4.054579639434815, val loss: 4.066884326934814, ETA in seconds: 13582801.418\n",
      "epoch: 514700, train loss: 4.054579639434815, val loss: 4.068056201934814, ETA in seconds: 13582671.009\n",
      "epoch: 514800, train loss: 4.051649951934815, val loss: 4.0666890144348145, ETA in seconds: 13582529.542\n",
      "epoch: 514900, train loss: 4.050673389434815, val loss: 4.0696187019348145, ETA in seconds: 13582441.589\n",
      "epoch: 515000, train loss: 4.051845264434815, val loss: 4.067470264434815, ETA in seconds: 13582316.796\n",
      "epoch: 515100, train loss: 4.047743701934815, val loss: 4.064540576934815, ETA in seconds: 13582271.129\n",
      "epoch: 515200, train loss: 4.056728076934815, val loss: 4.067860889434814, ETA in seconds: 13582178.968\n",
      "epoch: 515300, train loss: 4.050673389434815, val loss: 4.072157764434815, ETA in seconds: 13582047.377\n",
      "epoch: 515400, train loss: 4.051649951934815, val loss: 4.068251514434815, ETA in seconds: 13581688.228\n",
      "epoch: 515500, train loss: 4.051845264434815, val loss: 4.072353076934815, ETA in seconds: 13581615.331\n",
      "epoch: 515600, train loss: 4.054189014434814, val loss: 4.065321826934815, ETA in seconds: 13581425.605\n",
      "epoch: 515700, train loss: 4.0530171394348145, val loss: 4.068446826934815, ETA in seconds: 13581225.991\n",
      "epoch: 515800, train loss: 4.048524951934814, val loss: 4.068251514434815, ETA in seconds: 13581123.040\n",
      "epoch: 515900, train loss: 4.054189014434814, val loss: 4.062196826934814, ETA in seconds: 13580998.898\n",
      "epoch: 516000, train loss: 4.052626514434815, val loss: 4.0657124519348145, ETA in seconds: 13580889.940\n",
      "epoch: 516100, train loss: 4.0530171394348145, val loss: 4.0637593269348145, ETA in seconds: 13580457.656\n",
      "epoch: 516200, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 13579952.869\n",
      "epoch: 516300, train loss: 4.0549702644348145, val loss: 4.065321826934815, ETA in seconds: 13579616.587\n",
      "epoch: 516400, train loss: 4.054189014434814, val loss: 4.067860889434814, ETA in seconds: 13579389.930\n",
      "epoch: 516500, train loss: 4.050868701934815, val loss: 4.0637593269348145, ETA in seconds: 13579110.742\n",
      "epoch: 516600, train loss: 4.050478076934814, val loss: 4.066493701934815, ETA in seconds: 13578873.135\n",
      "epoch: 516700, train loss: 4.051649951934815, val loss: 4.070204639434815, ETA in seconds: 13578657.199\n",
      "epoch: 516800, train loss: 4.047939014434815, val loss: 4.0725483894348145, ETA in seconds: 13578445.614\n",
      "epoch: 516900, train loss: 4.045399951934814, val loss: 4.066884326934814, ETA in seconds: 13578284.428\n",
      "epoch: 517000, train loss: 4.046571826934814, val loss: 4.070790576934814, ETA in seconds: 13578383.689\n",
      "epoch: 517100, train loss: 4.055751514434815, val loss: 4.070399951934815, ETA in seconds: 13578515.183\n",
      "epoch: 517200, train loss: 4.050282764434814, val loss: 4.066493701934815, ETA in seconds: 13578409.585\n",
      "epoch: 517300, train loss: 4.050478076934814, val loss: 4.0715718269348145, ETA in seconds: 13578135.011\n",
      "epoch: 517400, train loss: 4.051454639434814, val loss: 4.0608296394348145, ETA in seconds: 13577880.123\n",
      "epoch: 517500, train loss: 4.049696826934815, val loss: 4.073329639434815, ETA in seconds: 13577687.086\n",
      "epoch: 517600, train loss: 4.056728076934815, val loss: 4.072743701934814, ETA in seconds: 13577521.940\n",
      "epoch: 517700, train loss: 4.050282764434814, val loss: 4.067079639434814, ETA in seconds: 13577423.322\n",
      "epoch: 517800, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 13577212.093\n",
      "epoch: 517900, train loss: 4.048720264434815, val loss: 4.069228076934815, ETA in seconds: 13576992.720\n",
      "epoch: 518000, train loss: 4.051649951934815, val loss: 4.071181201934815, ETA in seconds: 13576780.398\n",
      "epoch: 518100, train loss: 4.046571826934814, val loss: 4.070009326934814, ETA in seconds: 13576536.534\n",
      "epoch: 518200, train loss: 4.055165576934814, val loss: 4.0696187019348145, ETA in seconds: 13576334.055\n",
      "epoch: 518300, train loss: 4.043446826934814, val loss: 4.070204639434815, ETA in seconds: 13576114.036\n",
      "epoch: 518400, train loss: 4.0539937019348145, val loss: 4.077821826934814, ETA in seconds: 13575874.060\n",
      "epoch: 518500, train loss: 4.051259326934814, val loss: 4.072353076934815, ETA in seconds: 13575689.257\n",
      "epoch: 518600, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 13575432.916\n",
      "epoch: 518700, train loss: 4.049306201934814, val loss: 4.068446826934815, ETA in seconds: 13575286.222\n",
      "epoch: 518800, train loss: 4.0520405769348145, val loss: 4.066493701934815, ETA in seconds: 13575002.092\n",
      "epoch: 518900, train loss: 4.048524951934814, val loss: 4.065321826934815, ETA in seconds: 13574716.573\n",
      "epoch: 519000, train loss: 4.0520405769348145, val loss: 4.063954639434814, ETA in seconds: 13574532.995\n",
      "epoch: 519100, train loss: 4.051259326934814, val loss: 4.067079639434814, ETA in seconds: 13574311.024\n",
      "epoch: 519200, train loss: 4.051845264434815, val loss: 4.071962451934814, ETA in seconds: 13574006.647\n",
      "epoch: 519300, train loss: 4.053212451934814, val loss: 4.063564014434815, ETA in seconds: 13573833.521\n",
      "epoch: 519400, train loss: 4.053603076934815, val loss: 4.072353076934815, ETA in seconds: 13573531.701\n",
      "epoch: 519500, train loss: 4.053603076934815, val loss: 4.071767139434814, ETA in seconds: 13573244.941\n",
      "epoch: 519600, train loss: 4.056142139434814, val loss: 4.067860889434814, ETA in seconds: 13572983.011\n",
      "epoch: 519700, train loss: 4.052626514434815, val loss: 4.068251514434815, ETA in seconds: 13572715.968\n",
      "epoch: 519800, train loss: 4.046962451934815, val loss: 4.069032764434814, ETA in seconds: 13572402.753\n",
      "epoch: 519900, train loss: 4.0510640144348145, val loss: 4.066103076934814, ETA in seconds: 13572211.622\n",
      "epoch: 520000, train loss: 4.053798389434815, val loss: 4.067079639434814, ETA in seconds: 13571927.546\n",
      "epoch: 520100, train loss: 4.0530171394348145, val loss: 4.071962451934814, ETA in seconds: 13571598.492\n",
      "epoch: 520200, train loss: 4.047743701934815, val loss: 4.069032764434814, ETA in seconds: 13571292.356\n",
      "epoch: 520300, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 13570950.546\n",
      "epoch: 520400, train loss: 4.0539937019348145, val loss: 4.063564014434815, ETA in seconds: 13570702.362\n",
      "epoch: 520500, train loss: 4.051649951934815, val loss: 4.0627827644348145, ETA in seconds: 13570454.353\n",
      "epoch: 520600, train loss: 4.050673389434815, val loss: 4.059462451934815, ETA in seconds: 13570280.461\n",
      "epoch: 520700, train loss: 4.052235889434814, val loss: 4.066103076934814, ETA in seconds: 13570127.465\n",
      "epoch: 520800, train loss: 4.0539937019348145, val loss: 4.070985889434814, ETA in seconds: 13569902.355\n",
      "epoch: 520900, train loss: 4.052431201934814, val loss: 4.070399951934815, ETA in seconds: 13569689.771\n",
      "epoch: 521000, train loss: 4.055360889434814, val loss: 4.066884326934814, ETA in seconds: 13569370.961\n",
      "epoch: 521100, train loss: 4.0520405769348145, val loss: 4.070204639434815, ETA in seconds: 13569187.814\n",
      "epoch: 521200, train loss: 4.0530171394348145, val loss: 4.0715718269348145, ETA in seconds: 13568946.967\n",
      "epoch: 521300, train loss: 4.051649951934815, val loss: 4.069032764434814, ETA in seconds: 13568810.336\n",
      "epoch: 521400, train loss: 4.046962451934815, val loss: 4.069228076934815, ETA in seconds: 13568546.008\n",
      "epoch: 521500, train loss: 4.045790576934815, val loss: 4.064931201934814, ETA in seconds: 13568358.597\n",
      "epoch: 521600, train loss: 4.050673389434815, val loss: 4.068446826934815, ETA in seconds: 13568188.905\n",
      "epoch: 521700, train loss: 4.049306201934814, val loss: 4.062001514434814, ETA in seconds: 13567949.341\n",
      "epoch: 521800, train loss: 4.050282764434814, val loss: 4.062978076934814, ETA in seconds: 13567858.096\n",
      "epoch: 521900, train loss: 4.048329639434814, val loss: 4.068056201934814, ETA in seconds: 13567743.754\n",
      "epoch: 522000, train loss: 4.055751514434815, val loss: 4.065321826934815, ETA in seconds: 13567516.570\n",
      "epoch: 522100, train loss: 4.045399951934814, val loss: 4.0676655769348145, ETA in seconds: 13567310.466\n",
      "epoch: 522200, train loss: 4.052821826934815, val loss: 4.068056201934814, ETA in seconds: 13567174.287\n",
      "epoch: 522300, train loss: 4.0520405769348145, val loss: 4.0686421394348145, ETA in seconds: 13566959.798\n",
      "epoch: 522400, train loss: 4.050868701934815, val loss: 4.072743701934814, ETA in seconds: 13566761.180\n",
      "epoch: 522500, train loss: 4.054579639434815, val loss: 4.067860889434814, ETA in seconds: 13566552.378\n",
      "epoch: 522600, train loss: 4.048524951934814, val loss: 4.070204639434815, ETA in seconds: 13566361.349\n",
      "epoch: 522700, train loss: 4.0481343269348145, val loss: 4.063564014434815, ETA in seconds: 13566147.159\n",
      "epoch: 522800, train loss: 4.0559468269348145, val loss: 4.060634326934815, ETA in seconds: 13565963.442\n",
      "epoch: 522900, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 13565739.877\n",
      "epoch: 523000, train loss: 4.055360889434814, val loss: 4.0637593269348145, ETA in seconds: 13565512.219\n",
      "epoch: 523100, train loss: 4.055360889434814, val loss: 4.071181201934815, ETA in seconds: 13565306.377\n",
      "epoch: 523200, train loss: 4.056337451934814, val loss: 4.062978076934814, ETA in seconds: 13565020.787\n",
      "epoch: 523300, train loss: 4.052431201934814, val loss: 4.064149951934814, ETA in seconds: 13564351.483\n",
      "epoch: 523400, train loss: 4.055165576934814, val loss: 4.065517139434815, ETA in seconds: 13563578.970\n",
      "epoch: 523500, train loss: 4.055556201934815, val loss: 4.0627827644348145, ETA in seconds: 13563316.379\n",
      "epoch: 523600, train loss: 4.049696826934815, val loss: 4.064540576934815, ETA in seconds: 13563136.367\n",
      "epoch: 523700, train loss: 4.060048389434814, val loss: 4.066884326934814, ETA in seconds: 13562961.650\n",
      "epoch: 523800, train loss: 4.051259326934814, val loss: 4.0705952644348145, ETA in seconds: 13562602.235\n",
      "epoch: 523900, train loss: 4.046767139434815, val loss: 4.068446826934815, ETA in seconds: 13562393.907\n",
      "epoch: 524000, train loss: 4.051845264434815, val loss: 4.066298389434815, ETA in seconds: 13562108.698\n",
      "epoch: 524100, train loss: 4.0559468269348145, val loss: 4.069228076934815, ETA in seconds: 13561848.936\n",
      "epoch: 524200, train loss: 4.051845264434815, val loss: 4.067860889434814, ETA in seconds: 13561550.350\n",
      "epoch: 524300, train loss: 4.057704639434815, val loss: 4.060048389434814, ETA in seconds: 13561237.149\n",
      "epoch: 524400, train loss: 4.0549702644348145, val loss: 4.069814014434814, ETA in seconds: 13560990.799\n",
      "epoch: 524500, train loss: 4.053603076934815, val loss: 4.073134326934815, ETA in seconds: 13560676.228\n",
      "epoch: 524600, train loss: 4.048720264434815, val loss: 4.067079639434814, ETA in seconds: 13560394.220\n",
      "epoch: 524700, train loss: 4.047548389434814, val loss: 4.065517139434815, ETA in seconds: 13560105.988\n",
      "epoch: 524800, train loss: 4.048524951934814, val loss: 4.068837451934814, ETA in seconds: 13559827.406\n",
      "epoch: 524900, train loss: 4.039931201934815, val loss: 4.059267139434814, ETA in seconds: 13559611.277\n",
      "epoch: 525000, train loss: 4.048524951934814, val loss: 4.0686421394348145, ETA in seconds: 13559301.002\n",
      "epoch: 525100, train loss: 4.049892139434815, val loss: 4.072743701934814, ETA in seconds: 13558984.506\n",
      "epoch: 525200, train loss: 4.0598530769348145, val loss: 4.067860889434814, ETA in seconds: 13558702.247\n",
      "epoch: 525300, train loss: 4.042860889434815, val loss: 4.0647358894348145, ETA in seconds: 13558417.689\n",
      "epoch: 525400, train loss: 4.049501514434814, val loss: 4.068251514434815, ETA in seconds: 13558094.439\n",
      "epoch: 525500, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 13557748.463\n",
      "epoch: 525600, train loss: 4.055556201934815, val loss: 4.071376514434815, ETA in seconds: 13557517.378\n",
      "epoch: 525700, train loss: 4.046767139434815, val loss: 4.064540576934815, ETA in seconds: 13557266.536\n",
      "epoch: 525800, train loss: 4.049892139434815, val loss: 4.0676655769348145, ETA in seconds: 13556942.874\n",
      "epoch: 525900, train loss: 4.0520405769348145, val loss: 4.065321826934815, ETA in seconds: 13556605.168\n",
      "epoch: 526000, train loss: 4.054579639434815, val loss: 4.067860889434814, ETA in seconds: 13556243.796\n",
      "epoch: 526100, train loss: 4.0510640144348145, val loss: 4.0637593269348145, ETA in seconds: 13555951.528\n",
      "epoch: 526200, train loss: 4.051845264434815, val loss: 4.0696187019348145, ETA in seconds: 13555692.381\n",
      "epoch: 526300, train loss: 4.050673389434815, val loss: 4.0725483894348145, ETA in seconds: 13555383.282\n",
      "epoch: 526400, train loss: 4.049306201934814, val loss: 4.065907764434814, ETA in seconds: 13555093.691\n",
      "epoch: 526500, train loss: 4.056728076934815, val loss: 4.064149951934814, ETA in seconds: 13554925.911\n",
      "epoch: 526600, train loss: 4.049696826934815, val loss: 4.071767139434814, ETA in seconds: 13554641.284\n",
      "epoch: 526700, train loss: 4.054579639434815, val loss: 4.066103076934814, ETA in seconds: 13554336.428\n",
      "epoch: 526800, train loss: 4.055165576934814, val loss: 4.066884326934814, ETA in seconds: 13553976.162\n",
      "epoch: 526900, train loss: 4.0559468269348145, val loss: 4.066298389434815, ETA in seconds: 13553627.680\n",
      "epoch: 527000, train loss: 4.049501514434814, val loss: 4.070204639434815, ETA in seconds: 13553311.835\n",
      "epoch: 527100, train loss: 4.051845264434815, val loss: 4.060243701934814, ETA in seconds: 13552881.984\n",
      "epoch: 527200, train loss: 4.0530171394348145, val loss: 4.065126514434814, ETA in seconds: 13552587.407\n",
      "epoch: 527300, train loss: 4.051845264434815, val loss: 4.073915576934814, ETA in seconds: 13552234.282\n",
      "epoch: 527400, train loss: 4.0520405769348145, val loss: 4.071767139434814, ETA in seconds: 13551877.383\n",
      "epoch: 527500, train loss: 4.047548389434814, val loss: 4.071181201934815, ETA in seconds: 13551520.616\n",
      "epoch: 527600, train loss: 4.049696826934815, val loss: 4.065517139434815, ETA in seconds: 13551180.101\n",
      "epoch: 527700, train loss: 4.045790576934815, val loss: 4.066884326934814, ETA in seconds: 13550882.374\n",
      "epoch: 527800, train loss: 4.055751514434815, val loss: 4.068251514434815, ETA in seconds: 13550602.032\n",
      "epoch: 527900, train loss: 4.0520405769348145, val loss: 4.064149951934814, ETA in seconds: 13550274.281\n",
      "epoch: 528000, train loss: 4.049696826934815, val loss: 4.067274951934815, ETA in seconds: 13550029.156\n",
      "epoch: 528100, train loss: 4.0510640144348145, val loss: 4.067274951934815, ETA in seconds: 13549892.098\n",
      "epoch: 528200, train loss: 4.0520405769348145, val loss: 4.063173389434814, ETA in seconds: 13549804.752\n",
      "epoch: 528300, train loss: 4.057509326934815, val loss: 4.065126514434814, ETA in seconds: 13549471.182\n",
      "epoch: 528400, train loss: 4.054189014434814, val loss: 4.065321826934815, ETA in seconds: 13549093.471\n",
      "epoch: 528500, train loss: 4.051259326934814, val loss: 4.068446826934815, ETA in seconds: 13548727.322\n",
      "epoch: 528600, train loss: 4.052626514434815, val loss: 4.0686421394348145, ETA in seconds: 13548317.987\n",
      "epoch: 528700, train loss: 4.0481343269348145, val loss: 4.063368701934815, ETA in seconds: 13547936.966\n",
      "epoch: 528800, train loss: 4.050282764434814, val loss: 4.067079639434814, ETA in seconds: 13547541.449\n",
      "epoch: 528900, train loss: 4.0520405769348145, val loss: 4.0647358894348145, ETA in seconds: 13547166.859\n",
      "epoch: 529000, train loss: 4.048524951934814, val loss: 4.064540576934815, ETA in seconds: 13546758.171\n",
      "epoch: 529100, train loss: 4.0539937019348145, val loss: 4.0676655769348145, ETA in seconds: 13546458.378\n",
      "epoch: 529200, train loss: 4.0559468269348145, val loss: 4.0647358894348145, ETA in seconds: 13546154.023\n",
      "epoch: 529300, train loss: 4.0539937019348145, val loss: 4.070790576934814, ETA in seconds: 13545913.635\n",
      "epoch: 529400, train loss: 4.048720264434815, val loss: 4.066493701934815, ETA in seconds: 13545541.514\n",
      "epoch: 529500, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 13545133.053\n",
      "epoch: 529600, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 13544733.776\n",
      "epoch: 529700, train loss: 4.0530171394348145, val loss: 4.069032764434814, ETA in seconds: 13544372.010\n",
      "epoch: 529800, train loss: 4.048720264434815, val loss: 4.066298389434815, ETA in seconds: 13544008.812\n",
      "epoch: 529900, train loss: 4.0539937019348145, val loss: 4.0666890144348145, ETA in seconds: 13543568.332\n",
      "epoch: 530000, train loss: 4.050478076934814, val loss: 4.067274951934815, ETA in seconds: 13543135.536\n",
      "epoch: 530100, train loss: 4.043446826934814, val loss: 4.068837451934814, ETA in seconds: 13542836.376\n",
      "epoch: 530200, train loss: 4.045399951934814, val loss: 4.0725483894348145, ETA in seconds: 13542615.725\n",
      "epoch: 530300, train loss: 4.0481343269348145, val loss: 4.066884326934814, ETA in seconds: 13542319.454\n",
      "epoch: 530400, train loss: 4.051454639434814, val loss: 4.067470264434815, ETA in seconds: 13542019.984\n",
      "epoch: 530500, train loss: 4.046767139434815, val loss: 4.0696187019348145, ETA in seconds: 13541679.617\n",
      "epoch: 530600, train loss: 4.0539937019348145, val loss: 4.067860889434814, ETA in seconds: 13541305.124\n",
      "epoch: 530700, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 13540976.484\n",
      "epoch: 530800, train loss: 4.051454639434814, val loss: 4.071181201934815, ETA in seconds: 13540715.018\n",
      "epoch: 530900, train loss: 4.050868701934815, val loss: 4.071767139434814, ETA in seconds: 13540360.991\n",
      "epoch: 531000, train loss: 4.050478076934814, val loss: 4.066493701934815, ETA in seconds: 13540010.523\n",
      "epoch: 531100, train loss: 4.053212451934814, val loss: 4.0647358894348145, ETA in seconds: 13539684.741\n",
      "epoch: 531200, train loss: 4.053798389434815, val loss: 4.0676655769348145, ETA in seconds: 13539302.015\n",
      "epoch: 531300, train loss: 4.0452046394348145, val loss: 4.065126514434814, ETA in seconds: 13538856.134\n",
      "epoch: 531400, train loss: 4.055360889434814, val loss: 4.060634326934815, ETA in seconds: 13538345.417\n",
      "epoch: 531500, train loss: 4.046571826934814, val loss: 4.069814014434814, ETA in seconds: 13537829.059\n",
      "epoch: 531600, train loss: 4.052626514434815, val loss: 4.073329639434815, ETA in seconds: 13537528.384\n",
      "epoch: 531700, train loss: 4.047939014434815, val loss: 4.078017139434815, ETA in seconds: 13537253.441\n",
      "epoch: 531800, train loss: 4.053212451934814, val loss: 4.064931201934814, ETA in seconds: 13536823.514\n",
      "epoch: 531900, train loss: 4.051649951934815, val loss: 4.063954639434814, ETA in seconds: 13536383.733\n",
      "epoch: 532000, train loss: 4.050868701934815, val loss: 4.071767139434814, ETA in seconds: 13536104.126\n",
      "epoch: 532100, train loss: 4.053798389434815, val loss: 4.072353076934815, ETA in seconds: 13535741.696\n",
      "epoch: 532200, train loss: 4.059071826934814, val loss: 4.063173389434814, ETA in seconds: 13535340.832\n",
      "epoch: 532300, train loss: 4.058290576934814, val loss: 4.069228076934815, ETA in seconds: 13534973.803\n",
      "epoch: 532400, train loss: 4.048720264434815, val loss: 4.070985889434814, ETA in seconds: 13534631.312\n",
      "epoch: 532500, train loss: 4.057509326934815, val loss: 4.067470264434815, ETA in seconds: 13534240.702\n",
      "epoch: 532600, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 13533987.944\n",
      "epoch: 532700, train loss: 4.046962451934815, val loss: 4.070790576934814, ETA in seconds: 13533705.396\n",
      "epoch: 532800, train loss: 4.0481343269348145, val loss: 4.068446826934815, ETA in seconds: 13533311.614\n",
      "epoch: 532900, train loss: 4.051454639434814, val loss: 4.070009326934814, ETA in seconds: 13532908.192\n",
      "epoch: 533000, train loss: 4.053407764434814, val loss: 4.0735249519348145, ETA in seconds: 13532474.170\n",
      "epoch: 533100, train loss: 4.0559468269348145, val loss: 4.066103076934814, ETA in seconds: 13532162.319\n",
      "epoch: 533200, train loss: 4.048524951934814, val loss: 4.072939014434814, ETA in seconds: 13531661.176\n",
      "epoch: 533300, train loss: 4.057314014434814, val loss: 4.068446826934815, ETA in seconds: 13531197.511\n",
      "epoch: 533400, train loss: 4.050673389434815, val loss: 4.073329639434815, ETA in seconds: 13530808.585\n",
      "epoch: 533500, train loss: 4.049696826934815, val loss: 4.067860889434814, ETA in seconds: 13530348.137\n",
      "epoch: 533600, train loss: 4.0530171394348145, val loss: 4.064345264434815, ETA in seconds: 13529906.451\n",
      "epoch: 533700, train loss: 4.055556201934815, val loss: 4.070204639434815, ETA in seconds: 13529394.585\n",
      "epoch: 533800, train loss: 4.049306201934814, val loss: 4.070790576934814, ETA in seconds: 13528860.091\n",
      "epoch: 533900, train loss: 4.055360889434814, val loss: 4.063368701934815, ETA in seconds: 13528446.388\n",
      "epoch: 534000, train loss: 4.055556201934815, val loss: 4.0637593269348145, ETA in seconds: 13528064.149\n",
      "epoch: 534100, train loss: 4.049306201934814, val loss: 4.068056201934814, ETA in seconds: 13527692.489\n",
      "epoch: 534200, train loss: 4.061024951934814, val loss: 4.069814014434814, ETA in seconds: 13527375.529\n",
      "epoch: 534300, train loss: 4.058095264434814, val loss: 4.0657124519348145, ETA in seconds: 13527303.807\n",
      "epoch: 534400, train loss: 4.050868701934815, val loss: 4.0637593269348145, ETA in seconds: 13527046.718\n",
      "epoch: 534500, train loss: 4.050673389434815, val loss: 4.066493701934815, ETA in seconds: 13526606.120\n",
      "epoch: 534600, train loss: 4.051649951934815, val loss: 4.070985889434814, ETA in seconds: 13526125.296\n",
      "epoch: 534700, train loss: 4.048915576934815, val loss: 4.066298389434815, ETA in seconds: 13525694.795\n",
      "epoch: 534800, train loss: 4.054579639434815, val loss: 4.0686421394348145, ETA in seconds: 13525341.161\n",
      "epoch: 534900, train loss: 4.053407764434814, val loss: 4.067860889434814, ETA in seconds: 13525018.014\n",
      "epoch: 535000, train loss: 4.051845264434815, val loss: 4.069814014434814, ETA in seconds: 13524741.299\n",
      "epoch: 535100, train loss: 4.0491108894348145, val loss: 4.066103076934814, ETA in seconds: 13524486.155\n",
      "epoch: 535200, train loss: 4.0530171394348145, val loss: 4.064149951934814, ETA in seconds: 13524161.920\n",
      "epoch: 535300, train loss: 4.054774951934815, val loss: 4.0608296394348145, ETA in seconds: 13523828.697\n",
      "epoch: 535400, train loss: 4.054189014434814, val loss: 4.064540576934815, ETA in seconds: 13523545.918\n",
      "epoch: 535500, train loss: 4.0481343269348145, val loss: 4.068446826934815, ETA in seconds: 13523110.876\n",
      "epoch: 535600, train loss: 4.058290576934814, val loss: 4.070204639434815, ETA in seconds: 13522728.615\n",
      "epoch: 535700, train loss: 4.050868701934815, val loss: 4.062587451934815, ETA in seconds: 13522378.842\n",
      "epoch: 535800, train loss: 4.054579639434815, val loss: 4.077040576934815, ETA in seconds: 13522002.951\n",
      "epoch: 535900, train loss: 4.047939014434815, val loss: 4.069228076934815, ETA in seconds: 13521615.701\n",
      "epoch: 536000, train loss: 4.045595264434814, val loss: 4.0686421394348145, ETA in seconds: 13521187.792\n",
      "epoch: 536100, train loss: 4.050282764434814, val loss: 4.063564014434815, ETA in seconds: 13520754.575\n",
      "epoch: 536200, train loss: 4.056337451934814, val loss: 4.065907764434814, ETA in seconds: 13520281.997\n",
      "epoch: 536300, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 13519805.676\n",
      "epoch: 536400, train loss: 4.049892139434815, val loss: 4.065321826934815, ETA in seconds: 13519419.264\n",
      "epoch: 536500, train loss: 4.056532764434815, val loss: 4.069423389434815, ETA in seconds: 13519015.354\n",
      "epoch: 536600, train loss: 4.044032764434815, val loss: 4.070009326934814, ETA in seconds: 13518560.746\n",
      "epoch: 536700, train loss: 4.057314014434814, val loss: 4.063173389434814, ETA in seconds: 13518121.649\n",
      "epoch: 536800, train loss: 4.0520405769348145, val loss: 4.069032764434814, ETA in seconds: 13517720.738\n",
      "epoch: 536900, train loss: 4.057314014434814, val loss: 4.069228076934815, ETA in seconds: 13517246.191\n",
      "epoch: 537000, train loss: 4.056728076934815, val loss: 4.062392139434815, ETA in seconds: 13516822.557\n",
      "epoch: 537100, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 13516430.542\n",
      "epoch: 537200, train loss: 4.052235889434814, val loss: 4.0725483894348145, ETA in seconds: 13516055.606\n",
      "epoch: 537300, train loss: 4.061610889434815, val loss: 4.060243701934814, ETA in seconds: 13515731.904\n",
      "epoch: 537400, train loss: 4.046767139434815, val loss: 4.064931201934814, ETA in seconds: 13515403.816\n",
      "epoch: 537500, train loss: 4.049892139434815, val loss: 4.066493701934815, ETA in seconds: 13515028.422\n",
      "epoch: 537600, train loss: 4.051845264434815, val loss: 4.0578999519348145, ETA in seconds: 13514646.296\n",
      "epoch: 537700, train loss: 4.0598530769348145, val loss: 4.071181201934815, ETA in seconds: 13514271.752\n",
      "epoch: 537800, train loss: 4.050868701934815, val loss: 4.071181201934815, ETA in seconds: 13513869.557\n",
      "epoch: 537900, train loss: 4.061220264434814, val loss: 4.068056201934814, ETA in seconds: 13513421.149\n",
      "epoch: 538000, train loss: 4.058485889434815, val loss: 4.069032764434814, ETA in seconds: 13512966.392\n",
      "epoch: 538100, train loss: 4.051649951934815, val loss: 4.066103076934814, ETA in seconds: 13512565.761\n",
      "epoch: 538200, train loss: 4.045985889434815, val loss: 4.069032764434814, ETA in seconds: 13512143.638\n",
      "epoch: 538300, train loss: 4.053798389434815, val loss: 4.062587451934815, ETA in seconds: 13511719.960\n",
      "epoch: 538400, train loss: 4.053603076934815, val loss: 4.069423389434815, ETA in seconds: 13511137.038\n",
      "epoch: 538500, train loss: 4.052821826934815, val loss: 4.065126514434814, ETA in seconds: 13510620.136\n",
      "epoch: 538600, train loss: 4.047353076934814, val loss: 4.067860889434814, ETA in seconds: 13510033.200\n",
      "epoch: 538700, train loss: 4.053603076934815, val loss: 4.069423389434815, ETA in seconds: 13509526.982\n",
      "epoch: 538800, train loss: 4.0500874519348145, val loss: 4.073329639434815, ETA in seconds: 13509237.799\n",
      "epoch: 538900, train loss: 4.051259326934814, val loss: 4.0725483894348145, ETA in seconds: 13508954.157\n",
      "epoch: 539000, train loss: 4.045009326934815, val loss: 4.067470264434815, ETA in seconds: 13508625.207\n",
      "epoch: 539100, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 13508249.887\n",
      "epoch: 539200, train loss: 4.053212451934814, val loss: 4.067274951934815, ETA in seconds: 13507824.234\n",
      "epoch: 539300, train loss: 4.045399951934814, val loss: 4.0647358894348145, ETA in seconds: 13507457.960\n",
      "epoch: 539400, train loss: 4.047939014434815, val loss: 4.071181201934815, ETA in seconds: 13506996.864\n",
      "epoch: 539500, train loss: 4.055165576934814, val loss: 4.068056201934814, ETA in seconds: 13506523.788\n",
      "epoch: 539600, train loss: 4.055360889434814, val loss: 4.072353076934815, ETA in seconds: 13506045.538\n",
      "epoch: 539700, train loss: 4.056337451934814, val loss: 4.0686421394348145, ETA in seconds: 13505545.058\n",
      "epoch: 539800, train loss: 4.052431201934814, val loss: 4.068251514434815, ETA in seconds: 13505132.272\n",
      "epoch: 539900, train loss: 4.048915576934815, val loss: 4.070985889434814, ETA in seconds: 13504826.333\n",
      "epoch: 540000, train loss: 4.047743701934815, val loss: 4.0657124519348145, ETA in seconds: 13504379.234\n",
      "epoch: 540100, train loss: 4.0481343269348145, val loss: 4.070985889434814, ETA in seconds: 13504069.783\n",
      "epoch: 540200, train loss: 4.048720264434815, val loss: 4.0637593269348145, ETA in seconds: 13503714.671\n",
      "epoch: 540300, train loss: 4.047743701934815, val loss: 4.068446826934815, ETA in seconds: 13503281.012\n",
      "epoch: 540400, train loss: 4.049696826934815, val loss: 4.063954639434814, ETA in seconds: 13502797.530\n",
      "epoch: 540500, train loss: 4.0569233894348145, val loss: 4.069423389434815, ETA in seconds: 13502330.411\n",
      "epoch: 540600, train loss: 4.052235889434814, val loss: 4.069814014434814, ETA in seconds: 13501859.442\n",
      "epoch: 540700, train loss: 4.047743701934815, val loss: 4.067274951934815, ETA in seconds: 13501420.292\n",
      "epoch: 540800, train loss: 4.060048389434814, val loss: 4.067274951934815, ETA in seconds: 13501006.340\n",
      "epoch: 540900, train loss: 4.051259326934814, val loss: 4.064149951934814, ETA in seconds: 13500638.321\n",
      "epoch: 541000, train loss: 4.052431201934814, val loss: 4.070204639434815, ETA in seconds: 13500200.299\n",
      "epoch: 541100, train loss: 4.056728076934815, val loss: 4.063564014434815, ETA in seconds: 13499670.623\n",
      "epoch: 541200, train loss: 4.049306201934814, val loss: 4.063173389434814, ETA in seconds: 13499184.525\n",
      "epoch: 541300, train loss: 4.052821826934815, val loss: 4.067079639434814, ETA in seconds: 13498742.199\n",
      "epoch: 541400, train loss: 4.048329639434814, val loss: 4.063954639434814, ETA in seconds: 13498313.161\n",
      "epoch: 541500, train loss: 4.051845264434815, val loss: 4.066103076934814, ETA in seconds: 13497949.748\n",
      "epoch: 541600, train loss: 4.045595264434814, val loss: 4.0637593269348145, ETA in seconds: 13497540.689\n",
      "epoch: 541700, train loss: 4.0491108894348145, val loss: 4.064345264434815, ETA in seconds: 13497118.689\n",
      "epoch: 541800, train loss: 4.056728076934815, val loss: 4.067079639434814, ETA in seconds: 13496653.772\n",
      "epoch: 541900, train loss: 4.0500874519348145, val loss: 4.067274951934815, ETA in seconds: 13496180.271\n",
      "epoch: 542000, train loss: 4.049892139434815, val loss: 4.070399951934815, ETA in seconds: 13495623.254\n",
      "epoch: 542100, train loss: 4.052821826934815, val loss: 4.068446826934815, ETA in seconds: 13494947.792\n",
      "epoch: 542200, train loss: 4.0471577644348145, val loss: 4.0696187019348145, ETA in seconds: 13494574.118\n",
      "epoch: 542300, train loss: 4.049306201934814, val loss: 4.062978076934814, ETA in seconds: 13494295.157\n",
      "epoch: 542400, train loss: 4.0549702644348145, val loss: 4.068837451934814, ETA in seconds: 13493978.641\n",
      "epoch: 542500, train loss: 4.054774951934815, val loss: 4.062001514434814, ETA in seconds: 13493661.578\n",
      "epoch: 542600, train loss: 4.051649951934815, val loss: 4.0715718269348145, ETA in seconds: 13493218.361\n",
      "epoch: 542700, train loss: 4.054579639434815, val loss: 4.063173389434814, ETA in seconds: 13492792.370\n",
      "epoch: 542800, train loss: 4.043837451934815, val loss: 4.065321826934815, ETA in seconds: 13492352.408\n",
      "epoch: 542900, train loss: 4.049892139434815, val loss: 4.061220264434814, ETA in seconds: 13491877.325\n",
      "epoch: 543000, train loss: 4.051259326934814, val loss: 4.0637593269348145, ETA in seconds: 13491423.165\n",
      "epoch: 543100, train loss: 4.054384326934814, val loss: 4.071962451934814, ETA in seconds: 13490963.765\n",
      "epoch: 543200, train loss: 4.050868701934815, val loss: 4.068837451934814, ETA in seconds: 13490507.660\n",
      "epoch: 543300, train loss: 4.050282764434814, val loss: 4.065517139434815, ETA in seconds: 13490093.154\n",
      "epoch: 543400, train loss: 4.054579639434815, val loss: 4.0735249519348145, ETA in seconds: 13489533.008\n",
      "epoch: 543500, train loss: 4.054189014434814, val loss: 4.068251514434815, ETA in seconds: 13488928.487\n",
      "epoch: 543600, train loss: 4.0520405769348145, val loss: 4.0666890144348145, ETA in seconds: 13488217.978\n",
      "epoch: 543700, train loss: 4.054384326934814, val loss: 4.0618062019348145, ETA in seconds: 13487832.749\n",
      "epoch: 543800, train loss: 4.0500874519348145, val loss: 4.0705952644348145, ETA in seconds: 13487410.507\n",
      "epoch: 543900, train loss: 4.048915576934815, val loss: 4.069423389434815, ETA in seconds: 13486927.002\n",
      "epoch: 544000, train loss: 4.052431201934814, val loss: 4.061024951934814, ETA in seconds: 13486433.723\n",
      "epoch: 544100, train loss: 4.048524951934814, val loss: 4.065126514434814, ETA in seconds: 13486011.019\n",
      "epoch: 544200, train loss: 4.0530171394348145, val loss: 4.066298389434815, ETA in seconds: 13485632.097\n",
      "epoch: 544300, train loss: 4.058290576934814, val loss: 4.064149951934814, ETA in seconds: 13485250.417\n",
      "epoch: 544400, train loss: 4.055751514434815, val loss: 4.063954639434814, ETA in seconds: 13484786.249\n",
      "epoch: 544500, train loss: 4.055360889434814, val loss: 4.063564014434815, ETA in seconds: 13484297.286\n",
      "epoch: 544600, train loss: 4.0510640144348145, val loss: 4.067860889434814, ETA in seconds: 13483799.854\n",
      "epoch: 544700, train loss: 4.045595264434814, val loss: 4.0657124519348145, ETA in seconds: 13483257.284\n",
      "epoch: 544800, train loss: 4.048329639434814, val loss: 4.0647358894348145, ETA in seconds: 13482710.933\n",
      "epoch: 544900, train loss: 4.057704639434815, val loss: 4.0666890144348145, ETA in seconds: 13482188.478\n",
      "epoch: 545000, train loss: 4.052235889434814, val loss: 4.065907764434814, ETA in seconds: 13481749.798\n",
      "epoch: 545100, train loss: 4.046571826934814, val loss: 4.0666890144348145, ETA in seconds: 13481339.452\n",
      "epoch: 545200, train loss: 4.046962451934815, val loss: 4.064540576934815, ETA in seconds: 13480813.740\n",
      "epoch: 545300, train loss: 4.049306201934814, val loss: 4.0696187019348145, ETA in seconds: 13480373.030\n",
      "epoch: 545400, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 13479931.489\n",
      "epoch: 545500, train loss: 4.048329639434814, val loss: 4.072939014434814, ETA in seconds: 13479321.065\n",
      "epoch: 545600, train loss: 4.059267139434814, val loss: 4.071181201934815, ETA in seconds: 13478831.617\n",
      "epoch: 545700, train loss: 4.048915576934815, val loss: 4.064149951934814, ETA in seconds: 13478537.373\n",
      "epoch: 545800, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 13478024.225\n",
      "epoch: 545900, train loss: 4.058681201934815, val loss: 4.067274951934815, ETA in seconds: 13477459.570\n",
      "epoch: 546000, train loss: 4.049306201934814, val loss: 4.067079639434814, ETA in seconds: 13476849.573\n",
      "epoch: 546100, train loss: 4.0491108894348145, val loss: 4.065907764434814, ETA in seconds: 13476313.707\n",
      "epoch: 546200, train loss: 4.049501514434814, val loss: 4.065907764434814, ETA in seconds: 13475833.061\n",
      "epoch: 546300, train loss: 4.049501514434814, val loss: 4.062978076934814, ETA in seconds: 13475250.710\n",
      "epoch: 546400, train loss: 4.052626514434815, val loss: 4.065321826934815, ETA in seconds: 13474691.808\n",
      "epoch: 546500, train loss: 4.048329639434814, val loss: 4.073329639434815, ETA in seconds: 13474132.863\n",
      "epoch: 546600, train loss: 4.040712451934814, val loss: 4.068251514434815, ETA in seconds: 13473674.885\n",
      "epoch: 546700, train loss: 4.053212451934814, val loss: 4.067860889434814, ETA in seconds: 13473120.622\n",
      "epoch: 546800, train loss: 4.050282764434814, val loss: 4.065517139434815, ETA in seconds: 13472543.633\n",
      "epoch: 546900, train loss: 4.050868701934815, val loss: 4.070399951934815, ETA in seconds: 13471984.342\n",
      "epoch: 547000, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 13471415.588\n",
      "epoch: 547100, train loss: 4.055165576934814, val loss: 4.064540576934815, ETA in seconds: 13470687.657\n",
      "epoch: 547200, train loss: 4.049306201934814, val loss: 4.065517139434815, ETA in seconds: 13470154.659\n",
      "epoch: 547300, train loss: 4.0539937019348145, val loss: 4.062587451934815, ETA in seconds: 13469608.232\n",
      "epoch: 547400, train loss: 4.054384326934814, val loss: 4.064931201934814, ETA in seconds: 13469088.952\n",
      "epoch: 547500, train loss: 4.048524951934814, val loss: 4.064931201934814, ETA in seconds: 13468630.908\n",
      "epoch: 547600, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 13468092.231\n",
      "epoch: 547700, train loss: 4.0588765144348145, val loss: 4.063173389434814, ETA in seconds: 13467534.065\n",
      "epoch: 547800, train loss: 4.050673389434815, val loss: 4.063954639434814, ETA in seconds: 13466992.253\n",
      "epoch: 547900, train loss: 4.051454639434814, val loss: 4.066884326934814, ETA in seconds: 13466525.981\n",
      "epoch: 548000, train loss: 4.048720264434815, val loss: 4.066884326934814, ETA in seconds: 13465931.869\n",
      "epoch: 548100, train loss: 4.051259326934814, val loss: 4.064540576934815, ETA in seconds: 13465622.848\n",
      "epoch: 548200, train loss: 4.054579639434815, val loss: 4.066884326934814, ETA in seconds: 13465255.172\n",
      "epoch: 548300, train loss: 4.0452046394348145, val loss: 4.059267139434814, ETA in seconds: 13464720.997\n",
      "epoch: 548400, train loss: 4.053603076934815, val loss: 4.067470264434815, ETA in seconds: 13464187.153\n",
      "epoch: 548500, train loss: 4.051845264434815, val loss: 4.069228076934815, ETA in seconds: 13463608.426\n",
      "epoch: 548600, train loss: 4.053212451934814, val loss: 4.067079639434814, ETA in seconds: 13462992.098\n",
      "epoch: 548700, train loss: 4.050673389434815, val loss: 4.0686421394348145, ETA in seconds: 13462455.147\n",
      "epoch: 548800, train loss: 4.051454639434814, val loss: 4.0715718269348145, ETA in seconds: 13461826.532\n",
      "epoch: 548900, train loss: 4.0461812019348145, val loss: 4.068251514434815, ETA in seconds: 13461243.056\n",
      "epoch: 549000, train loss: 4.050282764434814, val loss: 4.065126514434814, ETA in seconds: 13460726.968\n",
      "epoch: 549100, train loss: 4.048915576934815, val loss: 4.068056201934814, ETA in seconds: 13460155.288\n",
      "epoch: 549200, train loss: 4.055556201934815, val loss: 4.061220264434814, ETA in seconds: 13459604.531\n",
      "epoch: 549300, train loss: 4.055165576934814, val loss: 4.066884326934814, ETA in seconds: 13459043.472\n",
      "epoch: 549400, train loss: 4.051454639434814, val loss: 4.064149951934814, ETA in seconds: 13458550.282\n",
      "epoch: 549500, train loss: 4.050478076934814, val loss: 4.069814014434814, ETA in seconds: 13457984.493\n",
      "epoch: 549600, train loss: 4.0520405769348145, val loss: 4.065126514434814, ETA in seconds: 13457508.357\n",
      "epoch: 549700, train loss: 4.050868701934815, val loss: 4.0676655769348145, ETA in seconds: 13456968.627\n",
      "epoch: 549800, train loss: 4.052821826934815, val loss: 4.0705952644348145, ETA in seconds: 13456432.216\n",
      "epoch: 549900, train loss: 4.0481343269348145, val loss: 4.067470264434815, ETA in seconds: 13455919.977\n",
      "epoch: 550000, train loss: 4.053798389434815, val loss: 4.067079639434814, ETA in seconds: 13455444.456\n",
      "epoch: 550100, train loss: 4.058485889434815, val loss: 4.067274951934815, ETA in seconds: 13454982.309\n",
      "epoch: 550200, train loss: 4.045399951934814, val loss: 4.061415576934815, ETA in seconds: 13454422.113\n",
      "epoch: 550300, train loss: 4.049501514434814, val loss: 4.063173389434814, ETA in seconds: 13453883.795\n",
      "epoch: 550400, train loss: 4.049306201934814, val loss: 4.075282764434815, ETA in seconds: 13453355.851\n",
      "epoch: 550500, train loss: 4.0520405769348145, val loss: 4.069814014434814, ETA in seconds: 13452819.016\n",
      "epoch: 550600, train loss: 4.053212451934814, val loss: 4.073329639434815, ETA in seconds: 13452477.978\n",
      "epoch: 550700, train loss: 4.054774951934815, val loss: 4.070985889434814, ETA in seconds: 13451970.335\n",
      "epoch: 550800, train loss: 4.0520405769348145, val loss: 4.068837451934814, ETA in seconds: 13451468.666\n",
      "epoch: 550900, train loss: 4.045009326934815, val loss: 4.0637593269348145, ETA in seconds: 13451036.773\n",
      "epoch: 551000, train loss: 4.054189014434814, val loss: 4.071962451934814, ETA in seconds: 13450585.150\n",
      "epoch: 551100, train loss: 4.047548389434814, val loss: 4.0686421394348145, ETA in seconds: 13449977.631\n",
      "epoch: 551200, train loss: 4.053603076934815, val loss: 4.068446826934815, ETA in seconds: 13449442.669\n",
      "epoch: 551300, train loss: 4.0549702644348145, val loss: 4.068446826934815, ETA in seconds: 13448881.779\n",
      "epoch: 551400, train loss: 4.0500874519348145, val loss: 4.068446826934815, ETA in seconds: 13448332.694\n",
      "epoch: 551500, train loss: 4.050282764434814, val loss: 4.070399951934815, ETA in seconds: 13447792.948\n",
      "epoch: 551600, train loss: 4.045595264434814, val loss: 4.069423389434815, ETA in seconds: 13447213.984\n",
      "epoch: 551700, train loss: 4.058290576934814, val loss: 4.070985889434814, ETA in seconds: 13446669.885\n",
      "epoch: 551800, train loss: 4.049696826934815, val loss: 4.067274951934815, ETA in seconds: 13446095.781\n",
      "epoch: 551900, train loss: 4.0539937019348145, val loss: 4.070399951934815, ETA in seconds: 13445513.486\n",
      "epoch: 552000, train loss: 4.042079639434815, val loss: 4.067274951934815, ETA in seconds: 13444993.605\n",
      "epoch: 552100, train loss: 4.055751514434815, val loss: 4.066884326934814, ETA in seconds: 13444493.249\n",
      "epoch: 552200, train loss: 4.052431201934814, val loss: 4.065126514434814, ETA in seconds: 13443901.732\n",
      "epoch: 552300, train loss: 4.049501514434814, val loss: 4.062587451934815, ETA in seconds: 13443479.550\n",
      "epoch: 552400, train loss: 4.051649951934815, val loss: 4.059657764434815, ETA in seconds: 13442975.588\n",
      "epoch: 552500, train loss: 4.047548389434814, val loss: 4.067079639434814, ETA in seconds: 13442511.956\n",
      "epoch: 552600, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 13442002.377\n",
      "epoch: 552700, train loss: 4.052626514434815, val loss: 4.065517139434815, ETA in seconds: 13441432.820\n",
      "epoch: 552800, train loss: 4.055751514434815, val loss: 4.0676655769348145, ETA in seconds: 13440904.061\n",
      "epoch: 552900, train loss: 4.051454639434814, val loss: 4.070009326934814, ETA in seconds: 13440319.832\n",
      "epoch: 553000, train loss: 4.051259326934814, val loss: 4.071962451934814, ETA in seconds: 13439682.989\n",
      "epoch: 553100, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 13439053.556\n",
      "epoch: 553200, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 13438491.594\n",
      "epoch: 553300, train loss: 4.048524951934814, val loss: 4.071962451934814, ETA in seconds: 13437883.581\n",
      "epoch: 553400, train loss: 4.051845264434815, val loss: 4.066884326934814, ETA in seconds: 13437251.014\n",
      "epoch: 553500, train loss: 4.049306201934814, val loss: 4.065517139434815, ETA in seconds: 13436674.022\n",
      "epoch: 553600, train loss: 4.047548389434814, val loss: 4.0637593269348145, ETA in seconds: 13436092.323\n",
      "epoch: 553700, train loss: 4.054189014434814, val loss: 4.063368701934815, ETA in seconds: 13435492.033\n",
      "epoch: 553800, train loss: 4.052235889434814, val loss: 4.0627827644348145, ETA in seconds: 13434856.226\n",
      "epoch: 553900, train loss: 4.057118701934814, val loss: 4.066298389434815, ETA in seconds: 13434282.012\n",
      "epoch: 554000, train loss: 4.0549702644348145, val loss: 4.068056201934814, ETA in seconds: 13433679.898\n",
      "epoch: 554100, train loss: 4.0559468269348145, val loss: 4.0715718269348145, ETA in seconds: 13433053.049\n",
      "epoch: 554200, train loss: 4.053212451934814, val loss: 4.0618062019348145, ETA in seconds: 13432428.087\n",
      "epoch: 554300, train loss: 4.049696826934815, val loss: 4.073720264434814, ETA in seconds: 13431810.618\n",
      "epoch: 554400, train loss: 4.046767139434815, val loss: 4.065321826934815, ETA in seconds: 13431231.719\n",
      "epoch: 554500, train loss: 4.051454639434814, val loss: 4.071376514434815, ETA in seconds: 13430695.802\n",
      "epoch: 554600, train loss: 4.053798389434815, val loss: 4.067470264434815, ETA in seconds: 13430081.796\n",
      "epoch: 554700, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 13429452.066\n",
      "epoch: 554800, train loss: 4.051259326934814, val loss: 4.0686421394348145, ETA in seconds: 13428817.982\n",
      "epoch: 554900, train loss: 4.047353076934814, val loss: 4.0705952644348145, ETA in seconds: 13428250.052\n",
      "epoch: 555000, train loss: 4.0549702644348145, val loss: 4.067470264434815, ETA in seconds: 13427616.176\n",
      "epoch: 555100, train loss: 4.053603076934815, val loss: 4.063564014434815, ETA in seconds: 13427033.784\n",
      "epoch: 555200, train loss: 4.049892139434815, val loss: 4.065907764434814, ETA in seconds: 13426280.538\n",
      "epoch: 555300, train loss: 4.050673389434815, val loss: 4.0715718269348145, ETA in seconds: 13425320.164\n",
      "epoch: 555400, train loss: 4.048329639434814, val loss: 4.064540576934815, ETA in seconds: 13424575.971\n",
      "epoch: 555500, train loss: 4.055751514434815, val loss: 4.062196826934814, ETA in seconds: 13423996.703\n",
      "epoch: 555600, train loss: 4.050282764434814, val loss: 4.0647358894348145, ETA in seconds: 13423434.346\n",
      "epoch: 555700, train loss: 4.050478076934814, val loss: 4.070204639434815, ETA in seconds: 13422938.118\n",
      "epoch: 555800, train loss: 4.0500874519348145, val loss: 4.068446826934815, ETA in seconds: 13422340.068\n",
      "epoch: 555900, train loss: 4.051649951934815, val loss: 4.0627827644348145, ETA in seconds: 13421745.946\n",
      "epoch: 556000, train loss: 4.050282764434814, val loss: 4.068837451934814, ETA in seconds: 13421157.313\n",
      "epoch: 556100, train loss: 4.054189014434814, val loss: 4.068251514434815, ETA in seconds: 13420487.965\n",
      "epoch: 556200, train loss: 4.052431201934814, val loss: 4.061024951934814, ETA in seconds: 13419902.871\n",
      "epoch: 556300, train loss: 4.047939014434815, val loss: 4.065907764434814, ETA in seconds: 13419369.392\n",
      "epoch: 556400, train loss: 4.057509326934815, val loss: 4.0618062019348145, ETA in seconds: 13418723.658\n",
      "epoch: 556500, train loss: 4.045985889434815, val loss: 4.062001514434814, ETA in seconds: 13418042.588\n",
      "epoch: 556600, train loss: 4.051259326934814, val loss: 4.068251514434815, ETA in seconds: 13417399.938\n",
      "epoch: 556700, train loss: 4.051845264434815, val loss: 4.070204639434815, ETA in seconds: 13416779.807\n",
      "epoch: 556800, train loss: 4.050868701934815, val loss: 4.067470264434815, ETA in seconds: 13416114.033\n",
      "epoch: 556900, train loss: 4.049892139434815, val loss: 4.064931201934814, ETA in seconds: 13415421.493\n",
      "epoch: 557000, train loss: 4.053407764434814, val loss: 4.068251514434815, ETA in seconds: 13414729.394\n",
      "epoch: 557100, train loss: 4.047548389434814, val loss: 4.0666890144348145, ETA in seconds: 13414057.945\n",
      "epoch: 557200, train loss: 4.052821826934815, val loss: 4.0647358894348145, ETA in seconds: 13413371.689\n",
      "epoch: 557300, train loss: 4.045399951934814, val loss: 4.073134326934815, ETA in seconds: 13412721.704\n",
      "epoch: 557400, train loss: 4.051454639434814, val loss: 4.066884326934814, ETA in seconds: 13412078.082\n",
      "epoch: 557500, train loss: 4.056337451934814, val loss: 4.065321826934815, ETA in seconds: 13411448.106\n",
      "epoch: 557600, train loss: 4.044814014434815, val loss: 4.067470264434815, ETA in seconds: 13410808.669\n",
      "epoch: 557700, train loss: 4.049306201934814, val loss: 4.064931201934814, ETA in seconds: 13410169.088\n",
      "epoch: 557800, train loss: 4.0530171394348145, val loss: 4.064345264434815, ETA in seconds: 13409580.430\n",
      "epoch: 557900, train loss: 4.0471577644348145, val loss: 4.068056201934814, ETA in seconds: 13408970.633\n",
      "epoch: 558000, train loss: 4.055556201934815, val loss: 4.069814014434814, ETA in seconds: 13408291.787\n",
      "epoch: 558100, train loss: 4.045790576934815, val loss: 4.066884326934814, ETA in seconds: 13407790.762\n",
      "epoch: 558200, train loss: 4.047353076934814, val loss: 4.064149951934814, ETA in seconds: 13407151.457\n",
      "epoch: 558300, train loss: 4.055556201934815, val loss: 4.071181201934815, ETA in seconds: 13406520.951\n",
      "epoch: 558400, train loss: 4.0559468269348145, val loss: 4.068446826934815, ETA in seconds: 13405955.474\n",
      "epoch: 558500, train loss: 4.044423389434814, val loss: 4.067274951934815, ETA in seconds: 13405210.469\n",
      "epoch: 558600, train loss: 4.0559468269348145, val loss: 4.067274951934815, ETA in seconds: 13404599.161\n",
      "epoch: 558700, train loss: 4.060243701934814, val loss: 4.069423389434815, ETA in seconds: 13404022.699\n",
      "epoch: 558800, train loss: 4.053407764434814, val loss: 4.072353076934815, ETA in seconds: 13403490.537\n",
      "epoch: 558900, train loss: 4.045985889434815, val loss: 4.072353076934815, ETA in seconds: 13402873.397\n",
      "epoch: 559000, train loss: 4.058681201934815, val loss: 4.065321826934815, ETA in seconds: 13402260.763\n",
      "epoch: 559100, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 13401571.586\n",
      "epoch: 559200, train loss: 4.051454639434814, val loss: 4.067274951934815, ETA in seconds: 13400860.667\n",
      "epoch: 559300, train loss: 4.053212451934814, val loss: 4.066493701934815, ETA in seconds: 13400192.203\n",
      "epoch: 559400, train loss: 4.0598530769348145, val loss: 4.063954639434814, ETA in seconds: 13399616.866\n",
      "epoch: 559500, train loss: 4.0510640144348145, val loss: 4.068446826934815, ETA in seconds: 13399052.047\n",
      "epoch: 559600, train loss: 4.0481343269348145, val loss: 4.060048389434814, ETA in seconds: 13398401.382\n",
      "epoch: 559700, train loss: 4.050868701934815, val loss: 4.0715718269348145, ETA in seconds: 13397417.917\n",
      "epoch: 559800, train loss: 4.053603076934815, val loss: 4.069423389434815, ETA in seconds: 13396763.349\n",
      "epoch: 559900, train loss: 4.056728076934815, val loss: 4.0686421394348145, ETA in seconds: 13396127.052\n",
      "epoch: 560000, train loss: 4.052626514434815, val loss: 4.062587451934815, ETA in seconds: 13395419.328\n",
      "epoch: 560100, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 13394698.151\n",
      "epoch: 560200, train loss: 4.047743701934815, val loss: 4.066103076934814, ETA in seconds: 13394026.678\n",
      "epoch: 560300, train loss: 4.0530171394348145, val loss: 4.069423389434815, ETA in seconds: 13393388.755\n",
      "epoch: 560400, train loss: 4.049501514434814, val loss: 4.066884326934814, ETA in seconds: 13392632.319\n",
      "epoch: 560500, train loss: 4.051454639434814, val loss: 4.0705952644348145, ETA in seconds: 13392101.643\n",
      "epoch: 560600, train loss: 4.0530171394348145, val loss: 4.0696187019348145, ETA in seconds: 13391410.799\n",
      "epoch: 560700, train loss: 4.051259326934814, val loss: 4.066493701934815, ETA in seconds: 13390753.801\n",
      "epoch: 560800, train loss: 4.054384326934814, val loss: 4.066298389434815, ETA in seconds: 13390072.835\n",
      "epoch: 560900, train loss: 4.050478076934814, val loss: 4.0637593269348145, ETA in seconds: 13389355.944\n",
      "epoch: 561000, train loss: 4.051259326934814, val loss: 4.061415576934815, ETA in seconds: 13388661.932\n",
      "epoch: 561100, train loss: 4.055165576934814, val loss: 4.068837451934814, ETA in seconds: 13388221.547\n",
      "epoch: 561200, train loss: 4.051454639434814, val loss: 4.062587451934815, ETA in seconds: 13387578.230\n",
      "epoch: 561300, train loss: 4.056728076934815, val loss: 4.070009326934814, ETA in seconds: 13386926.989\n",
      "epoch: 561400, train loss: 4.049501514434814, val loss: 4.070399951934815, ETA in seconds: 13386288.154\n",
      "epoch: 561500, train loss: 4.056532764434815, val loss: 4.068446826934815, ETA in seconds: 13385605.006\n",
      "epoch: 561600, train loss: 4.050478076934814, val loss: 4.064931201934814, ETA in seconds: 13384864.356\n",
      "epoch: 561700, train loss: 4.053603076934815, val loss: 4.0686421394348145, ETA in seconds: 13384159.397\n",
      "epoch: 561800, train loss: 4.053407764434814, val loss: 4.062978076934814, ETA in seconds: 13383535.956\n",
      "epoch: 561900, train loss: 4.049892139434815, val loss: 4.069032764434814, ETA in seconds: 13382831.760\n",
      "epoch: 562000, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 13381830.637\n",
      "epoch: 562100, train loss: 4.058485889434815, val loss: 4.063954639434814, ETA in seconds: 13381103.975\n",
      "epoch: 562200, train loss: 4.051649951934815, val loss: 4.065517139434815, ETA in seconds: 13380387.092\n",
      "epoch: 562300, train loss: 4.048329639434814, val loss: 4.063564014434815, ETA in seconds: 13379708.511\n",
      "epoch: 562400, train loss: 4.052626514434815, val loss: 4.067860889434814, ETA in seconds: 13379045.080\n",
      "epoch: 562500, train loss: 4.052821826934815, val loss: 4.062978076934814, ETA in seconds: 13378368.431\n",
      "epoch: 562600, train loss: 4.052431201934814, val loss: 4.066103076934814, ETA in seconds: 13377745.378\n",
      "epoch: 562700, train loss: 4.053603076934815, val loss: 4.067860889434814, ETA in seconds: 13377030.735\n",
      "epoch: 562800, train loss: 4.0539937019348145, val loss: 4.062978076934814, ETA in seconds: 13376321.188\n",
      "epoch: 562900, train loss: 4.046962451934815, val loss: 4.069228076934815, ETA in seconds: 13375699.244\n",
      "epoch: 563000, train loss: 4.0461812019348145, val loss: 4.064540576934815, ETA in seconds: 13374996.340\n",
      "epoch: 563100, train loss: 4.054774951934815, val loss: 4.069423389434815, ETA in seconds: 13374322.192\n",
      "epoch: 563200, train loss: 4.048720264434815, val loss: 4.064931201934814, ETA in seconds: 13373614.926\n",
      "epoch: 563300, train loss: 4.053407764434814, val loss: 4.068251514434815, ETA in seconds: 13372928.081\n",
      "epoch: 563400, train loss: 4.058681201934815, val loss: 4.0647358894348145, ETA in seconds: 13372209.697\n",
      "epoch: 563500, train loss: 4.0442280769348145, val loss: 4.067274951934815, ETA in seconds: 13371558.135\n",
      "epoch: 563600, train loss: 4.058681201934815, val loss: 4.065321826934815, ETA in seconds: 13370847.209\n",
      "epoch: 563700, train loss: 4.044032764434815, val loss: 4.068251514434815, ETA in seconds: 13370132.619\n",
      "epoch: 563800, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 13369395.425\n",
      "epoch: 563900, train loss: 4.047548389434814, val loss: 4.062978076934814, ETA in seconds: 13368570.453\n",
      "epoch: 564000, train loss: 4.0559468269348145, val loss: 4.064931201934814, ETA in seconds: 13367788.039\n",
      "epoch: 564100, train loss: 4.046962451934815, val loss: 4.066884326934814, ETA in seconds: 13366997.381\n",
      "epoch: 564200, train loss: 4.047939014434815, val loss: 4.067470264434815, ETA in seconds: 13366205.831\n",
      "epoch: 564300, train loss: 4.0569233894348145, val loss: 4.060243701934814, ETA in seconds: 13365398.833\n",
      "epoch: 564400, train loss: 4.052235889434814, val loss: 4.066298389434815, ETA in seconds: 13364623.467\n",
      "epoch: 564500, train loss: 4.054384326934814, val loss: 4.063368701934815, ETA in seconds: 13363807.542\n",
      "epoch: 564600, train loss: 4.0442280769348145, val loss: 4.065126514434814, ETA in seconds: 13362995.187\n",
      "epoch: 564700, train loss: 4.051845264434815, val loss: 4.066493701934815, ETA in seconds: 13362234.168\n",
      "epoch: 564800, train loss: 4.053798389434815, val loss: 4.064540576934815, ETA in seconds: 13361476.700\n",
      "epoch: 564900, train loss: 4.055751514434815, val loss: 4.072157764434815, ETA in seconds: 13360703.096\n",
      "epoch: 565000, train loss: 4.050282764434814, val loss: 4.067079639434814, ETA in seconds: 13359987.026\n",
      "epoch: 565100, train loss: 4.046962451934815, val loss: 4.065126514434814, ETA in seconds: 13359208.783\n",
      "epoch: 565200, train loss: 4.051649951934815, val loss: 4.071181201934815, ETA in seconds: 13358441.557\n",
      "epoch: 565300, train loss: 4.052626514434815, val loss: 4.067470264434815, ETA in seconds: 13357637.348\n",
      "epoch: 565400, train loss: 4.0500874519348145, val loss: 4.063173389434814, ETA in seconds: 13356644.054\n",
      "epoch: 565500, train loss: 4.049306201934814, val loss: 4.066103076934814, ETA in seconds: 13355929.658\n",
      "epoch: 565600, train loss: 4.054189014434814, val loss: 4.072157764434815, ETA in seconds: 13355145.240\n",
      "epoch: 565700, train loss: 4.051454639434814, val loss: 4.070985889434814, ETA in seconds: 13354330.455\n",
      "epoch: 565800, train loss: 4.049892139434815, val loss: 4.067274951934815, ETA in seconds: 13353564.735\n",
      "epoch: 565900, train loss: 4.050868701934815, val loss: 4.065907764434814, ETA in seconds: 13352746.443\n",
      "epoch: 566000, train loss: 4.052431201934814, val loss: 4.061415576934815, ETA in seconds: 13351992.044\n",
      "epoch: 566100, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 13351247.604\n",
      "epoch: 566200, train loss: 4.050478076934814, val loss: 4.0666890144348145, ETA in seconds: 13350491.915\n",
      "epoch: 566300, train loss: 4.057704639434815, val loss: 4.071181201934815, ETA in seconds: 13349687.152\n",
      "epoch: 566400, train loss: 4.053798389434815, val loss: 4.070009326934814, ETA in seconds: 13348896.249\n",
      "epoch: 566500, train loss: 4.0471577644348145, val loss: 4.0666890144348145, ETA in seconds: 13348128.100\n",
      "epoch: 566600, train loss: 4.055556201934815, val loss: 4.064149951934814, ETA in seconds: 13347348.133\n",
      "epoch: 566700, train loss: 4.054774951934815, val loss: 4.065907764434814, ETA in seconds: 13346560.979\n",
      "epoch: 566800, train loss: 4.053212451934814, val loss: 4.061024951934814, ETA in seconds: 13345891.086\n",
      "epoch: 566900, train loss: 4.058290576934814, val loss: 4.0705952644348145, ETA in seconds: 13345072.204\n",
      "epoch: 567000, train loss: 4.051454639434814, val loss: 4.062392139434815, ETA in seconds: 13344284.962\n",
      "epoch: 567100, train loss: 4.053798389434815, val loss: 4.063954639434814, ETA in seconds: 13343508.377\n",
      "epoch: 567200, train loss: 4.0442280769348145, val loss: 4.061610889434815, ETA in seconds: 13342705.604\n",
      "epoch: 567300, train loss: 4.053212451934814, val loss: 4.068446826934815, ETA in seconds: 13341915.791\n",
      "epoch: 567400, train loss: 4.048524951934814, val loss: 4.070790576934814, ETA in seconds: 13341113.374\n",
      "epoch: 567500, train loss: 4.057314014434814, val loss: 4.067079639434814, ETA in seconds: 13340294.190\n",
      "epoch: 567600, train loss: 4.052431201934814, val loss: 4.063954639434814, ETA in seconds: 13339517.219\n",
      "epoch: 567700, train loss: 4.053603076934815, val loss: 4.063173389434814, ETA in seconds: 13338717.909\n",
      "epoch: 567800, train loss: 4.053212451934814, val loss: 4.0647358894348145, ETA in seconds: 13337904.498\n",
      "epoch: 567900, train loss: 4.057704639434815, val loss: 4.068251514434815, ETA in seconds: 13337108.185\n",
      "epoch: 568000, train loss: 4.054189014434814, val loss: 4.062587451934815, ETA in seconds: 13336322.513\n",
      "epoch: 568100, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 13335557.555\n",
      "epoch: 568200, train loss: 4.049696826934815, val loss: 4.066103076934814, ETA in seconds: 13334813.419\n",
      "epoch: 568300, train loss: 4.054189014434814, val loss: 4.066493701934815, ETA in seconds: 13334041.763\n",
      "epoch: 568400, train loss: 4.0539937019348145, val loss: 4.070009326934814, ETA in seconds: 13333216.626\n",
      "epoch: 568500, train loss: 4.0481343269348145, val loss: 4.065907764434814, ETA in seconds: 13332522.093\n",
      "epoch: 568600, train loss: 4.0578999519348145, val loss: 4.064345264434815, ETA in seconds: 13331777.212\n",
      "epoch: 568700, train loss: 4.0452046394348145, val loss: 4.067860889434814, ETA in seconds: 13330912.221\n",
      "epoch: 568800, train loss: 4.055556201934815, val loss: 4.067274951934815, ETA in seconds: 13330130.544\n",
      "epoch: 568900, train loss: 4.056142139434814, val loss: 4.072157764434815, ETA in seconds: 13329283.531\n",
      "epoch: 569000, train loss: 4.052431201934814, val loss: 4.066493701934815, ETA in seconds: 13328445.656\n",
      "epoch: 569100, train loss: 4.047939014434815, val loss: 4.0764546394348145, ETA in seconds: 13327701.988\n",
      "epoch: 569200, train loss: 4.0539937019348145, val loss: 4.0627827644348145, ETA in seconds: 13326941.120\n",
      "epoch: 569300, train loss: 4.0549702644348145, val loss: 4.066103076934814, ETA in seconds: 13326151.207\n",
      "epoch: 569400, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 13325437.506\n",
      "epoch: 569500, train loss: 4.052821826934815, val loss: 4.069423389434815, ETA in seconds: 13324768.241\n",
      "epoch: 569600, train loss: 4.0461812019348145, val loss: 4.070399951934815, ETA in seconds: 13323915.094\n",
      "epoch: 569700, train loss: 4.0549702644348145, val loss: 4.065321826934815, ETA in seconds: 13323054.034\n",
      "epoch: 569800, train loss: 4.059267139434814, val loss: 4.0676655769348145, ETA in seconds: 13322236.515\n",
      "epoch: 569900, train loss: 4.051649951934815, val loss: 4.063954639434814, ETA in seconds: 13321380.994\n",
      "epoch: 570000, train loss: 4.047743701934815, val loss: 4.0666890144348145, ETA in seconds: 13320562.778\n",
      "epoch: 570100, train loss: 4.054189014434814, val loss: 4.064345264434815, ETA in seconds: 13319974.764\n",
      "epoch: 570200, train loss: 4.052235889434814, val loss: 4.064345264434815, ETA in seconds: 13319315.107\n",
      "epoch: 570300, train loss: 4.046571826934814, val loss: 4.070790576934814, ETA in seconds: 13318555.627\n",
      "epoch: 570400, train loss: 4.052431201934814, val loss: 4.0637593269348145, ETA in seconds: 13317722.322\n",
      "epoch: 570500, train loss: 4.054774951934815, val loss: 4.069423389434815, ETA in seconds: 13316891.088\n",
      "epoch: 570600, train loss: 4.053798389434815, val loss: 4.064149951934814, ETA in seconds: 13316061.275\n",
      "epoch: 570700, train loss: 4.051454639434814, val loss: 4.066103076934814, ETA in seconds: 13315222.612\n",
      "epoch: 570800, train loss: 4.0569233894348145, val loss: 4.069814014434814, ETA in seconds: 13314352.391\n",
      "epoch: 570900, train loss: 4.056142139434814, val loss: 4.065907764434814, ETA in seconds: 13313531.218\n",
      "epoch: 571000, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 13312696.001\n",
      "epoch: 571100, train loss: 4.0549702644348145, val loss: 4.071962451934814, ETA in seconds: 13311872.539\n",
      "epoch: 571200, train loss: 4.052235889434814, val loss: 4.065126514434814, ETA in seconds: 13311055.857\n",
      "epoch: 571300, train loss: 4.057509326934815, val loss: 4.061415576934815, ETA in seconds: 13310255.489\n",
      "epoch: 571400, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 13309399.297\n",
      "epoch: 571500, train loss: 4.0520405769348145, val loss: 4.059267139434814, ETA in seconds: 13308580.537\n",
      "epoch: 571600, train loss: 4.054774951934815, val loss: 4.063368701934815, ETA in seconds: 13307787.332\n",
      "epoch: 571700, train loss: 4.046376514434814, val loss: 4.066493701934815, ETA in seconds: 13307012.646\n",
      "epoch: 571800, train loss: 4.059071826934814, val loss: 4.0696187019348145, ETA in seconds: 13306251.726\n",
      "epoch: 571900, train loss: 4.046571826934814, val loss: 4.067274951934815, ETA in seconds: 13305443.855\n",
      "epoch: 572000, train loss: 4.049501514434814, val loss: 4.064931201934814, ETA in seconds: 13304625.051\n",
      "epoch: 572100, train loss: 4.0530171394348145, val loss: 4.068056201934814, ETA in seconds: 13303857.835\n",
      "epoch: 572200, train loss: 4.045985889434815, val loss: 4.070009326934814, ETA in seconds: 13303035.333\n",
      "epoch: 572300, train loss: 4.056337451934814, val loss: 4.0647358894348145, ETA in seconds: 13302262.195\n",
      "epoch: 572400, train loss: 4.0539937019348145, val loss: 4.066103076934814, ETA in seconds: 13301598.246\n",
      "epoch: 572500, train loss: 4.052235889434814, val loss: 4.071181201934815, ETA in seconds: 13300809.309\n",
      "epoch: 572600, train loss: 4.056728076934815, val loss: 4.0657124519348145, ETA in seconds: 13299961.697\n",
      "epoch: 572700, train loss: 4.051259326934814, val loss: 4.068056201934814, ETA in seconds: 13299146.510\n",
      "epoch: 572800, train loss: 4.061024951934814, val loss: 4.065321826934815, ETA in seconds: 13298333.223\n",
      "epoch: 572900, train loss: 4.044618701934814, val loss: 4.0657124519348145, ETA in seconds: 13297492.202\n",
      "epoch: 573000, train loss: 4.054774951934815, val loss: 4.065321826934815, ETA in seconds: 13296612.372\n",
      "epoch: 573100, train loss: 4.0491108894348145, val loss: 4.064149951934814, ETA in seconds: 13295751.724\n",
      "epoch: 573200, train loss: 4.047548389434814, val loss: 4.072157764434815, ETA in seconds: 13294929.438\n",
      "epoch: 573300, train loss: 4.052431201934814, val loss: 4.068446826934815, ETA in seconds: 13294029.966\n",
      "epoch: 573400, train loss: 4.050282764434814, val loss: 4.073915576934814, ETA in seconds: 13293233.154\n",
      "epoch: 573500, train loss: 4.047743701934815, val loss: 4.0647358894348145, ETA in seconds: 13292379.644\n",
      "epoch: 573600, train loss: 4.059657764434815, val loss: 4.070790576934814, ETA in seconds: 13291521.758\n",
      "epoch: 573700, train loss: 4.049892139434815, val loss: 4.066493701934815, ETA in seconds: 13290633.151\n",
      "epoch: 573800, train loss: 4.054384326934814, val loss: 4.064345264434815, ETA in seconds: 13289752.508\n",
      "epoch: 573900, train loss: 4.0491108894348145, val loss: 4.066298389434815, ETA in seconds: 13288919.832\n",
      "epoch: 574000, train loss: 4.052626514434815, val loss: 4.067860889434814, ETA in seconds: 13288067.522\n",
      "epoch: 574100, train loss: 4.047743701934815, val loss: 4.063368701934815, ETA in seconds: 13287202.408\n",
      "epoch: 574200, train loss: 4.053407764434814, val loss: 4.064345264434815, ETA in seconds: 13286345.373\n",
      "epoch: 574300, train loss: 4.0578999519348145, val loss: 4.070204639434815, ETA in seconds: 13285814.387\n",
      "epoch: 574400, train loss: 4.044618701934814, val loss: 4.067470264434815, ETA in seconds: 13285237.386\n",
      "epoch: 574500, train loss: 4.048915576934815, val loss: 4.070790576934814, ETA in seconds: 13284752.918\n",
      "epoch: 574600, train loss: 4.0500874519348145, val loss: 4.062001514434814, ETA in seconds: 13284144.443\n",
      "epoch: 574700, train loss: 4.049501514434814, val loss: 4.070009326934814, ETA in seconds: 13283254.923\n",
      "epoch: 574800, train loss: 4.046767139434815, val loss: 4.065517139434815, ETA in seconds: 13282386.980\n",
      "epoch: 574900, train loss: 4.048524951934814, val loss: 4.0686421394348145, ETA in seconds: 13281520.799\n",
      "epoch: 575000, train loss: 4.0539937019348145, val loss: 4.0705952644348145, ETA in seconds: 13280674.983\n",
      "epoch: 575100, train loss: 4.0530171394348145, val loss: 4.066493701934815, ETA in seconds: 13279793.988\n",
      "epoch: 575200, train loss: 4.057704639434815, val loss: 4.061024951934814, ETA in seconds: 13278886.198\n",
      "epoch: 575300, train loss: 4.056728076934815, val loss: 4.069814014434814, ETA in seconds: 13278034.374\n",
      "epoch: 575400, train loss: 4.054579639434815, val loss: 4.066884326934814, ETA in seconds: 13277171.639\n",
      "epoch: 575500, train loss: 4.0510640144348145, val loss: 4.0647358894348145, ETA in seconds: 13276342.157\n",
      "epoch: 575600, train loss: 4.052235889434814, val loss: 4.067079639434814, ETA in seconds: 13275452.449\n",
      "epoch: 575700, train loss: 4.049892139434815, val loss: 4.069032764434814, ETA in seconds: 13274514.577\n",
      "epoch: 575800, train loss: 4.0608296394348145, val loss: 4.069228076934815, ETA in seconds: 13273635.756\n",
      "epoch: 575900, train loss: 4.051845264434815, val loss: 4.063954639434814, ETA in seconds: 13272753.400\n",
      "epoch: 576000, train loss: 4.054189014434814, val loss: 4.068056201934814, ETA in seconds: 13271855.215\n",
      "epoch: 576100, train loss: 4.058290576934814, val loss: 4.063368701934815, ETA in seconds: 13270966.027\n",
      "epoch: 576200, train loss: 4.054384326934814, val loss: 4.062587451934815, ETA in seconds: 13270064.319\n",
      "epoch: 576300, train loss: 4.053212451934814, val loss: 4.062001514434814, ETA in seconds: 13269155.569\n",
      "epoch: 576400, train loss: 4.049501514434814, val loss: 4.0725483894348145, ETA in seconds: 13268242.638\n",
      "epoch: 576500, train loss: 4.052235889434814, val loss: 4.066103076934814, ETA in seconds: 13267329.165\n",
      "epoch: 576600, train loss: 4.0559468269348145, val loss: 4.0627827644348145, ETA in seconds: 13266472.146\n",
      "epoch: 576700, train loss: 4.054579639434815, val loss: 4.060634326934815, ETA in seconds: 13265598.910\n",
      "epoch: 576800, train loss: 4.056142139434814, val loss: 4.061610889434815, ETA in seconds: 13264678.162\n",
      "epoch: 576900, train loss: 4.048720264434815, val loss: 4.069814014434814, ETA in seconds: 13263775.363\n",
      "epoch: 577000, train loss: 4.050673389434815, val loss: 4.069032764434814, ETA in seconds: 13262922.721\n",
      "epoch: 577100, train loss: 4.0530171394348145, val loss: 4.069032764434814, ETA in seconds: 13262050.887\n",
      "epoch: 577200, train loss: 4.050282764434814, val loss: 4.0666890144348145, ETA in seconds: 13261203.053\n",
      "epoch: 577300, train loss: 4.043056201934815, val loss: 4.065126514434814, ETA in seconds: 13260330.999\n",
      "epoch: 577400, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 13259536.868\n",
      "epoch: 577500, train loss: 4.0500874519348145, val loss: 4.073720264434814, ETA in seconds: 13258746.897\n",
      "epoch: 577600, train loss: 4.053407764434814, val loss: 4.065126514434814, ETA in seconds: 13257990.340\n",
      "epoch: 577700, train loss: 4.042470264434814, val loss: 4.069228076934815, ETA in seconds: 13257293.554\n",
      "epoch: 577800, train loss: 4.043056201934815, val loss: 4.0715718269348145, ETA in seconds: 13256589.452\n",
      "epoch: 577900, train loss: 4.053603076934815, val loss: 4.076064014434815, ETA in seconds: 13255836.171\n",
      "epoch: 578000, train loss: 4.0491108894348145, val loss: 4.071376514434815, ETA in seconds: 13254925.077\n",
      "epoch: 578100, train loss: 4.051454639434814, val loss: 4.070009326934814, ETA in seconds: 13254032.094\n",
      "epoch: 578200, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 13252763.129\n",
      "epoch: 578300, train loss: 4.0520405769348145, val loss: 4.060439014434815, ETA in seconds: 13251471.327\n",
      "epoch: 578400, train loss: 4.051454639434814, val loss: 4.066298389434815, ETA in seconds: 13250149.073\n",
      "epoch: 578500, train loss: 4.047743701934815, val loss: 4.071962451934814, ETA in seconds: 13249009.802\n",
      "epoch: 578600, train loss: 4.0500874519348145, val loss: 4.065517139434815, ETA in seconds: 13248039.265\n",
      "epoch: 578700, train loss: 4.046767139434815, val loss: 4.063954639434814, ETA in seconds: 13247279.395\n",
      "epoch: 578800, train loss: 4.048720264434815, val loss: 4.066298389434815, ETA in seconds: 13246343.619\n",
      "epoch: 578900, train loss: 4.049892139434815, val loss: 4.068056201934814, ETA in seconds: 13245407.474\n",
      "epoch: 579000, train loss: 4.050478076934814, val loss: 4.0676655769348145, ETA in seconds: 13244478.145\n",
      "epoch: 579100, train loss: 4.054384326934814, val loss: 4.0637593269348145, ETA in seconds: 13243542.016\n",
      "epoch: 579200, train loss: 4.050478076934814, val loss: 4.0657124519348145, ETA in seconds: 13242670.781\n",
      "epoch: 579300, train loss: 4.054579639434815, val loss: 4.066493701934815, ETA in seconds: 13241438.266\n",
      "epoch: 579400, train loss: 4.0618062019348145, val loss: 4.068446826934815, ETA in seconds: 13240494.850\n",
      "epoch: 579500, train loss: 4.043837451934815, val loss: 4.067274951934815, ETA in seconds: 13239566.491\n",
      "epoch: 579600, train loss: 4.047939014434815, val loss: 4.066298389434815, ETA in seconds: 13238625.705\n",
      "epoch: 579700, train loss: 4.0530171394348145, val loss: 4.070009326934814, ETA in seconds: 13237731.326\n",
      "epoch: 579800, train loss: 4.049306201934814, val loss: 4.070204639434815, ETA in seconds: 13236815.118\n",
      "epoch: 579900, train loss: 4.055751514434815, val loss: 4.0686421394348145, ETA in seconds: 13235907.293\n",
      "epoch: 580000, train loss: 4.044814014434815, val loss: 4.0696187019348145, ETA in seconds: 13234994.252\n",
      "epoch: 580100, train loss: 4.052821826934815, val loss: 4.067079639434814, ETA in seconds: 13234075.361\n",
      "epoch: 580200, train loss: 4.051845264434815, val loss: 4.064149951934814, ETA in seconds: 13233116.090\n",
      "epoch: 580300, train loss: 4.053603076934815, val loss: 4.068251514434815, ETA in seconds: 13232147.961\n",
      "epoch: 580400, train loss: 4.049892139434815, val loss: 4.0598530769348145, ETA in seconds: 13231194.922\n",
      "epoch: 580500, train loss: 4.049306201934814, val loss: 4.066298389434815, ETA in seconds: 13230191.986\n",
      "epoch: 580600, train loss: 4.051845264434815, val loss: 4.065517139434815, ETA in seconds: 13229280.750\n",
      "epoch: 580700, train loss: 4.057118701934814, val loss: 4.070985889434814, ETA in seconds: 13228512.625\n",
      "epoch: 580800, train loss: 4.048915576934815, val loss: 4.072157764434815, ETA in seconds: 13227708.048\n",
      "epoch: 580900, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 13226771.556\n",
      "epoch: 581000, train loss: 4.052235889434814, val loss: 4.065126514434814, ETA in seconds: 13225871.904\n",
      "epoch: 581100, train loss: 4.052235889434814, val loss: 4.068056201934814, ETA in seconds: 13224905.951\n",
      "epoch: 581200, train loss: 4.050282764434814, val loss: 4.064931201934814, ETA in seconds: 13223990.046\n",
      "epoch: 581300, train loss: 4.0530171394348145, val loss: 4.070204639434815, ETA in seconds: 13223042.625\n",
      "epoch: 581400, train loss: 4.046376514434814, val loss: 4.064149951934814, ETA in seconds: 13222092.747\n",
      "epoch: 581500, train loss: 4.0539937019348145, val loss: 4.066298389434815, ETA in seconds: 13221158.846\n",
      "epoch: 581600, train loss: 4.048720264434815, val loss: 4.065907764434814, ETA in seconds: 13220190.161\n",
      "epoch: 581700, train loss: 4.0559468269348145, val loss: 4.0696187019348145, ETA in seconds: 13219264.299\n",
      "epoch: 581800, train loss: 4.0500874519348145, val loss: 4.070399951934815, ETA in seconds: 13218307.039\n",
      "epoch: 581900, train loss: 4.045985889434815, val loss: 4.067860889434814, ETA in seconds: 13217291.060\n",
      "epoch: 582000, train loss: 4.050478076934814, val loss: 4.0676655769348145, ETA in seconds: 13216101.082\n",
      "epoch: 582100, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 13215095.461\n",
      "epoch: 582200, train loss: 4.041689014434814, val loss: 4.0647358894348145, ETA in seconds: 13214143.638\n",
      "epoch: 582300, train loss: 4.0432515144348145, val loss: 4.066298389434815, ETA in seconds: 13213218.341\n",
      "epoch: 582400, train loss: 4.057314014434814, val loss: 4.065907764434814, ETA in seconds: 13212353.618\n",
      "epoch: 582500, train loss: 4.055165576934814, val loss: 4.065907764434814, ETA in seconds: 13211412.387\n",
      "epoch: 582600, train loss: 4.053798389434815, val loss: 4.067470264434815, ETA in seconds: 13210445.618\n",
      "epoch: 582700, train loss: 4.045009326934815, val loss: 4.064345264434815, ETA in seconds: 13209451.985\n",
      "epoch: 582800, train loss: 4.049306201934814, val loss: 4.067860889434814, ETA in seconds: 13208499.345\n",
      "epoch: 582900, train loss: 4.052626514434815, val loss: 4.063954639434814, ETA in seconds: 13207491.963\n",
      "epoch: 583000, train loss: 4.0559468269348145, val loss: 4.068446826934815, ETA in seconds: 13206534.301\n",
      "epoch: 583100, train loss: 4.055751514434815, val loss: 4.071962451934814, ETA in seconds: 13205619.679\n",
      "epoch: 583200, train loss: 4.0422749519348145, val loss: 4.060243701934814, ETA in seconds: 13204742.319\n",
      "epoch: 583300, train loss: 4.055360889434814, val loss: 4.068446826934815, ETA in seconds: 13203827.259\n",
      "epoch: 583400, train loss: 4.052235889434814, val loss: 4.069032764434814, ETA in seconds: 13202884.252\n",
      "epoch: 583500, train loss: 4.045595264434814, val loss: 4.070204639434815, ETA in seconds: 13202024.932\n",
      "epoch: 583600, train loss: 4.056337451934814, val loss: 4.067860889434814, ETA in seconds: 13201068.279\n",
      "epoch: 583700, train loss: 4.052821826934815, val loss: 4.0676655769348145, ETA in seconds: 13200151.373\n",
      "epoch: 583800, train loss: 4.050868701934815, val loss: 4.064345264434815, ETA in seconds: 13199219.467\n",
      "epoch: 583900, train loss: 4.047939014434815, val loss: 4.067860889434814, ETA in seconds: 13198322.138\n",
      "epoch: 584000, train loss: 4.0520405769348145, val loss: 4.060243701934814, ETA in seconds: 13197376.014\n",
      "epoch: 584100, train loss: 4.057509326934815, val loss: 4.066103076934814, ETA in seconds: 13196451.160\n",
      "epoch: 584200, train loss: 4.050282764434814, val loss: 4.066493701934815, ETA in seconds: 13195533.163\n",
      "epoch: 584300, train loss: 4.052821826934815, val loss: 4.064931201934814, ETA in seconds: 13194595.531\n",
      "epoch: 584400, train loss: 4.0588765144348145, val loss: 4.070790576934814, ETA in seconds: 13193681.180\n",
      "epoch: 584500, train loss: 4.052821826934815, val loss: 4.067470264434815, ETA in seconds: 13192733.318\n",
      "epoch: 584600, train loss: 4.055751514434815, val loss: 4.070985889434814, ETA in seconds: 13191758.728\n",
      "epoch: 584700, train loss: 4.054384326934814, val loss: 4.0657124519348145, ETA in seconds: 13190904.945\n",
      "epoch: 584800, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 13190009.195\n",
      "epoch: 584900, train loss: 4.0530171394348145, val loss: 4.062196826934814, ETA in seconds: 13189125.106\n",
      "epoch: 585000, train loss: 4.048720264434815, val loss: 4.071181201934815, ETA in seconds: 13188093.729\n",
      "epoch: 585100, train loss: 4.052821826934815, val loss: 4.069228076934815, ETA in seconds: 13187095.767\n",
      "epoch: 585200, train loss: 4.054774951934815, val loss: 4.064149951934814, ETA in seconds: 13186079.378\n",
      "epoch: 585300, train loss: 4.0471577644348145, val loss: 4.0676655769348145, ETA in seconds: 13185071.555\n",
      "epoch: 585400, train loss: 4.055360889434814, val loss: 4.066298389434815, ETA in seconds: 13184059.638\n",
      "epoch: 585500, train loss: 4.055751514434815, val loss: 4.069228076934815, ETA in seconds: 13183106.161\n",
      "epoch: 585600, train loss: 4.0491108894348145, val loss: 4.067274951934815, ETA in seconds: 13182084.067\n",
      "epoch: 585700, train loss: 4.0569233894348145, val loss: 4.067274951934815, ETA in seconds: 13181075.572\n",
      "epoch: 585800, train loss: 4.050673389434815, val loss: 4.0696187019348145, ETA in seconds: 13180081.968\n",
      "epoch: 585900, train loss: 4.0491108894348145, val loss: 4.064931201934814, ETA in seconds: 13179133.321\n",
      "epoch: 586000, train loss: 4.0559468269348145, val loss: 4.068446826934815, ETA in seconds: 13178122.042\n",
      "epoch: 586100, train loss: 4.054189014434814, val loss: 4.064540576934815, ETA in seconds: 13177142.391\n",
      "epoch: 586200, train loss: 4.050868701934815, val loss: 4.064345264434815, ETA in seconds: 13176150.369\n",
      "epoch: 586300, train loss: 4.056337451934814, val loss: 4.068056201934814, ETA in seconds: 13175143.678\n",
      "epoch: 586400, train loss: 4.045009326934815, val loss: 4.0666890144348145, ETA in seconds: 13174138.338\n",
      "epoch: 586500, train loss: 4.0559468269348145, val loss: 4.071181201934815, ETA in seconds: 13173136.869\n",
      "epoch: 586600, train loss: 4.052626514434815, val loss: 4.064540576934815, ETA in seconds: 13172146.494\n",
      "epoch: 586700, train loss: 4.062196826934814, val loss: 4.0666890144348145, ETA in seconds: 13171181.168\n",
      "epoch: 586800, train loss: 4.0520405769348145, val loss: 4.063564014434815, ETA in seconds: 13170181.131\n",
      "epoch: 586900, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 13169202.883\n",
      "epoch: 587000, train loss: 4.057509326934815, val loss: 4.063564014434815, ETA in seconds: 13168237.073\n",
      "epoch: 587100, train loss: 4.051259326934814, val loss: 4.0637593269348145, ETA in seconds: 13167216.900\n",
      "epoch: 587200, train loss: 4.0539937019348145, val loss: 4.071767139434814, ETA in seconds: 13166239.544\n",
      "epoch: 587300, train loss: 4.055165576934814, val loss: 4.064931201934814, ETA in seconds: 13165281.163\n",
      "epoch: 587400, train loss: 4.055556201934815, val loss: 4.059071826934814, ETA in seconds: 13164251.812\n",
      "epoch: 587500, train loss: 4.049306201934814, val loss: 4.0686421394348145, ETA in seconds: 13163198.056\n",
      "epoch: 587600, train loss: 4.051454639434814, val loss: 4.060439014434815, ETA in seconds: 13162215.658\n",
      "epoch: 587700, train loss: 4.055556201934815, val loss: 4.067274951934815, ETA in seconds: 13161185.492\n",
      "epoch: 587800, train loss: 4.0578999519348145, val loss: 4.067274951934815, ETA in seconds: 13160203.302\n",
      "epoch: 587900, train loss: 4.049501514434814, val loss: 4.062587451934815, ETA in seconds: 13159207.642\n",
      "epoch: 588000, train loss: 4.053798389434815, val loss: 4.070985889434814, ETA in seconds: 13158201.878\n",
      "epoch: 588100, train loss: 4.051454639434814, val loss: 4.0657124519348145, ETA in seconds: 13157222.432\n",
      "epoch: 588200, train loss: 4.049501514434814, val loss: 4.068251514434815, ETA in seconds: 13156252.430\n",
      "epoch: 588300, train loss: 4.058485889434815, val loss: 4.069814014434814, ETA in seconds: 13155248.277\n",
      "epoch: 588400, train loss: 4.050478076934814, val loss: 4.068446826934815, ETA in seconds: 13154212.690\n",
      "epoch: 588500, train loss: 4.047548389434814, val loss: 4.072353076934815, ETA in seconds: 13153255.659\n",
      "epoch: 588600, train loss: 4.058485889434815, val loss: 4.065321826934815, ETA in seconds: 13152264.259\n",
      "epoch: 588700, train loss: 4.0500874519348145, val loss: 4.065517139434815, ETA in seconds: 13151243.236\n",
      "epoch: 588800, train loss: 4.053798389434815, val loss: 4.068251514434815, ETA in seconds: 13150243.415\n",
      "epoch: 588900, train loss: 4.047353076934814, val loss: 4.062001514434814, ETA in seconds: 13149258.373\n",
      "epoch: 589000, train loss: 4.062196826934814, val loss: 4.071962451934814, ETA in seconds: 13148240.991\n",
      "epoch: 589100, train loss: 4.049306201934814, val loss: 4.070009326934814, ETA in seconds: 13147231.743\n",
      "epoch: 589200, train loss: 4.058485889434815, val loss: 4.0696187019348145, ETA in seconds: 13146260.070\n",
      "epoch: 589300, train loss: 4.050868701934815, val loss: 4.065126514434814, ETA in seconds: 13145281.711\n",
      "epoch: 589400, train loss: 4.0539937019348145, val loss: 4.063564014434815, ETA in seconds: 13144275.432\n",
      "epoch: 589500, train loss: 4.047743701934815, val loss: 4.066103076934814, ETA in seconds: 13143310.251\n",
      "epoch: 589600, train loss: 4.057704639434815, val loss: 4.0666890144348145, ETA in seconds: 13142337.838\n",
      "epoch: 589700, train loss: 4.046571826934814, val loss: 4.063954639434814, ETA in seconds: 13141320.893\n",
      "epoch: 589800, train loss: 4.055165576934814, val loss: 4.064345264434815, ETA in seconds: 13140274.365\n",
      "epoch: 589900, train loss: 4.0530171394348145, val loss: 4.063368701934815, ETA in seconds: 13139312.035\n",
      "epoch: 590000, train loss: 4.046571826934814, val loss: 4.0696187019348145, ETA in seconds: 13138336.253\n",
      "epoch: 590100, train loss: 4.050282764434814, val loss: 4.0676655769348145, ETA in seconds: 13137329.263\n",
      "epoch: 590200, train loss: 4.053407764434814, val loss: 4.073329639434815, ETA in seconds: 13136297.289\n",
      "epoch: 590300, train loss: 4.049892139434815, val loss: 4.061610889434815, ETA in seconds: 13135288.791\n",
      "epoch: 590400, train loss: 4.046962451934815, val loss: 4.0696187019348145, ETA in seconds: 13134267.663\n",
      "epoch: 590500, train loss: 4.050868701934815, val loss: 4.069814014434814, ETA in seconds: 13133276.470\n",
      "epoch: 590600, train loss: 4.051649951934815, val loss: 4.061415576934815, ETA in seconds: 13132231.515\n",
      "epoch: 590700, train loss: 4.052431201934814, val loss: 4.0647358894348145, ETA in seconds: 13131185.684\n",
      "epoch: 590800, train loss: 4.0481343269348145, val loss: 4.060439014434815, ETA in seconds: 13130154.062\n",
      "epoch: 590900, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 13129135.035\n",
      "epoch: 591000, train loss: 4.056142139434814, val loss: 4.068056201934814, ETA in seconds: 13128118.945\n",
      "epoch: 591100, train loss: 4.049696826934815, val loss: 4.071181201934815, ETA in seconds: 13127116.317\n",
      "epoch: 591200, train loss: 4.0510640144348145, val loss: 4.065907764434814, ETA in seconds: 13126084.158\n",
      "epoch: 591300, train loss: 4.049892139434815, val loss: 4.071767139434814, ETA in seconds: 13125044.015\n",
      "epoch: 591400, train loss: 4.0471577644348145, val loss: 4.067470264434815, ETA in seconds: 13124031.464\n",
      "epoch: 591500, train loss: 4.048524951934814, val loss: 4.069814014434814, ETA in seconds: 13123015.919\n",
      "epoch: 591600, train loss: 4.044618701934814, val loss: 4.0657124519348145, ETA in seconds: 13121945.479\n",
      "epoch: 591700, train loss: 4.052821826934815, val loss: 4.068251514434815, ETA in seconds: 13120894.418\n",
      "epoch: 591800, train loss: 4.050478076934814, val loss: 4.070985889434814, ETA in seconds: 13119839.264\n",
      "epoch: 591900, train loss: 4.043446826934814, val loss: 4.063564014434815, ETA in seconds: 13118815.701\n",
      "epoch: 592000, train loss: 4.0510640144348145, val loss: 4.069228076934815, ETA in seconds: 13117856.434\n",
      "epoch: 592100, train loss: 4.0491108894348145, val loss: 4.0637593269348145, ETA in seconds: 13116864.491\n",
      "epoch: 592200, train loss: 4.054579639434815, val loss: 4.068837451934814, ETA in seconds: 13115812.338\n",
      "epoch: 592300, train loss: 4.045595264434814, val loss: 4.0647358894348145, ETA in seconds: 13114770.256\n",
      "epoch: 592400, train loss: 4.051454639434814, val loss: 4.074306201934815, ETA in seconds: 13113700.359\n",
      "epoch: 592500, train loss: 4.057118701934814, val loss: 4.069814014434814, ETA in seconds: 13112656.759\n",
      "epoch: 592600, train loss: 4.047548389434814, val loss: 4.0686421394348145, ETA in seconds: 13111627.800\n",
      "epoch: 592700, train loss: 4.048524951934814, val loss: 4.064540576934815, ETA in seconds: 13110549.603\n",
      "epoch: 592800, train loss: 4.046962451934815, val loss: 4.062978076934814, ETA in seconds: 13109492.585\n",
      "epoch: 592900, train loss: 4.051649951934815, val loss: 4.068446826934815, ETA in seconds: 13108360.829\n",
      "epoch: 593000, train loss: 4.055751514434815, val loss: 4.066298389434815, ETA in seconds: 13107312.406\n",
      "epoch: 593100, train loss: 4.047939014434815, val loss: 4.069032764434814, ETA in seconds: 13106229.738\n",
      "epoch: 593200, train loss: 4.0569233894348145, val loss: 4.0715718269348145, ETA in seconds: 13105164.451\n",
      "epoch: 593300, train loss: 4.051259326934814, val loss: 4.064540576934815, ETA in seconds: 13104133.607\n",
      "epoch: 593400, train loss: 4.0539937019348145, val loss: 4.073329639434815, ETA in seconds: 13103093.971\n",
      "epoch: 593500, train loss: 4.058290576934814, val loss: 4.066493701934815, ETA in seconds: 13102068.632\n",
      "epoch: 593600, train loss: 4.049696826934815, val loss: 4.067274951934815, ETA in seconds: 13101084.817\n",
      "epoch: 593700, train loss: 4.050282764434814, val loss: 4.064540576934815, ETA in seconds: 13100005.848\n",
      "epoch: 593800, train loss: 4.049501514434814, val loss: 4.0666890144348145, ETA in seconds: 13098930.050\n",
      "epoch: 593900, train loss: 4.0500874519348145, val loss: 4.0647358894348145, ETA in seconds: 13097854.900\n",
      "epoch: 594000, train loss: 4.054384326934814, val loss: 4.062196826934814, ETA in seconds: 13096796.382\n",
      "epoch: 594100, train loss: 4.053603076934815, val loss: 4.0745015144348145, ETA in seconds: 13095708.195\n",
      "epoch: 594200, train loss: 4.046376514434814, val loss: 4.069423389434815, ETA in seconds: 13094632.175\n",
      "epoch: 594300, train loss: 4.0569233894348145, val loss: 4.0657124519348145, ETA in seconds: 13093548.645\n",
      "epoch: 594400, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 13092474.161\n",
      "epoch: 594500, train loss: 4.051845264434815, val loss: 4.069032764434814, ETA in seconds: 13091371.800\n",
      "epoch: 594600, train loss: 4.0559468269348145, val loss: 4.0686421394348145, ETA in seconds: 13090302.764\n",
      "epoch: 594700, train loss: 4.047353076934814, val loss: 4.066884326934814, ETA in seconds: 13089235.297\n",
      "epoch: 594800, train loss: 4.057509326934815, val loss: 4.065907764434814, ETA in seconds: 13088174.713\n",
      "epoch: 594900, train loss: 4.0559468269348145, val loss: 4.0676655769348145, ETA in seconds: 13087058.559\n",
      "epoch: 595000, train loss: 4.0530171394348145, val loss: 4.070204639434815, ETA in seconds: 13085965.598\n",
      "epoch: 595100, train loss: 4.0588765144348145, val loss: 4.063368701934815, ETA in seconds: 13084851.552\n",
      "epoch: 595200, train loss: 4.0500874519348145, val loss: 4.0696187019348145, ETA in seconds: 13083755.124\n",
      "epoch: 595300, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 13082586.137\n",
      "epoch: 595400, train loss: 4.056142139434814, val loss: 4.062196826934814, ETA in seconds: 13081442.010\n",
      "epoch: 595500, train loss: 4.052821826934815, val loss: 4.067860889434814, ETA in seconds: 13080349.328\n",
      "epoch: 595600, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 13079334.204\n",
      "epoch: 595700, train loss: 4.054579639434815, val loss: 4.063173389434814, ETA in seconds: 13078306.382\n",
      "epoch: 595800, train loss: 4.0442280769348145, val loss: 4.063173389434814, ETA in seconds: 13077210.426\n",
      "epoch: 595900, train loss: 4.0530171394348145, val loss: 4.069423389434815, ETA in seconds: 13076128.652\n",
      "epoch: 596000, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 13075089.146\n",
      "epoch: 596100, train loss: 4.049892139434815, val loss: 4.067860889434814, ETA in seconds: 13074014.296\n",
      "epoch: 596200, train loss: 4.0520405769348145, val loss: 4.066103076934814, ETA in seconds: 13072930.600\n",
      "epoch: 596300, train loss: 4.0530171394348145, val loss: 4.065907764434814, ETA in seconds: 13071847.304\n",
      "epoch: 596400, train loss: 4.051845264434815, val loss: 4.0705952644348145, ETA in seconds: 13070767.129\n",
      "epoch: 596500, train loss: 4.0549702644348145, val loss: 4.063954639434814, ETA in seconds: 13069704.005\n",
      "epoch: 596600, train loss: 4.050282764434814, val loss: 4.065126514434814, ETA in seconds: 13068603.863\n",
      "epoch: 596700, train loss: 4.051649951934815, val loss: 4.071376514434815, ETA in seconds: 13067483.718\n",
      "epoch: 596800, train loss: 4.054189014434814, val loss: 4.064540576934815, ETA in seconds: 13066400.543\n",
      "epoch: 596900, train loss: 4.053407764434814, val loss: 4.0696187019348145, ETA in seconds: 13065299.129\n",
      "epoch: 597000, train loss: 4.049696826934815, val loss: 4.0666890144348145, ETA in seconds: 13064234.404\n",
      "epoch: 597100, train loss: 4.055360889434814, val loss: 4.063173389434814, ETA in seconds: 13063144.960\n",
      "epoch: 597200, train loss: 4.053798389434815, val loss: 4.0657124519348145, ETA in seconds: 13062035.436\n",
      "epoch: 597300, train loss: 4.055751514434815, val loss: 4.0676655769348145, ETA in seconds: 13060894.114\n",
      "epoch: 597400, train loss: 4.0481343269348145, val loss: 4.0647358894348145, ETA in seconds: 13059810.685\n",
      "epoch: 597500, train loss: 4.055556201934815, val loss: 4.072353076934815, ETA in seconds: 13058737.189\n",
      "epoch: 597600, train loss: 4.053798389434815, val loss: 4.064149951934814, ETA in seconds: 13057683.430\n",
      "epoch: 597700, train loss: 4.0461812019348145, val loss: 4.061415576934815, ETA in seconds: 13056560.341\n",
      "epoch: 597800, train loss: 4.051454639434814, val loss: 4.062587451934815, ETA in seconds: 13055447.823\n",
      "epoch: 597900, train loss: 4.049306201934814, val loss: 4.066103076934814, ETA in seconds: 13054339.260\n",
      "epoch: 598000, train loss: 4.0549702644348145, val loss: 4.062392139434815, ETA in seconds: 13053226.826\n",
      "epoch: 598100, train loss: 4.058095264434814, val loss: 4.066493701934815, ETA in seconds: 13052104.142\n",
      "epoch: 598200, train loss: 4.046571826934814, val loss: 4.072353076934815, ETA in seconds: 13050993.218\n",
      "epoch: 598300, train loss: 4.0442280769348145, val loss: 4.074892139434814, ETA in seconds: 13049869.058\n",
      "epoch: 598400, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 13048745.928\n",
      "epoch: 598500, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 13047642.867\n",
      "epoch: 598600, train loss: 4.049892139434815, val loss: 4.067860889434814, ETA in seconds: 13046525.527\n",
      "epoch: 598700, train loss: 4.052235889434814, val loss: 4.072157764434815, ETA in seconds: 13045442.633\n",
      "epoch: 598800, train loss: 4.048720264434815, val loss: 4.065907764434814, ETA in seconds: 13044363.177\n",
      "epoch: 598900, train loss: 4.050673389434815, val loss: 4.070399951934815, ETA in seconds: 13043252.074\n",
      "epoch: 599000, train loss: 4.050478076934814, val loss: 4.063564014434815, ETA in seconds: 13042133.410\n",
      "epoch: 599100, train loss: 4.0530171394348145, val loss: 4.068056201934814, ETA in seconds: 13041037.315\n",
      "epoch: 599200, train loss: 4.053407764434814, val loss: 4.069814014434814, ETA in seconds: 13039880.388\n",
      "epoch: 599300, train loss: 4.0491108894348145, val loss: 4.066493701934815, ETA in seconds: 13038787.536\n",
      "epoch: 599400, train loss: 4.046571826934814, val loss: 4.069814014434814, ETA in seconds: 13037658.478\n",
      "epoch: 599500, train loss: 4.052235889434814, val loss: 4.0686421394348145, ETA in seconds: 13036500.567\n",
      "epoch: 599600, train loss: 4.049696826934815, val loss: 4.064345264434815, ETA in seconds: 13035343.697\n",
      "epoch: 599700, train loss: 4.054384326934814, val loss: 4.068446826934815, ETA in seconds: 13034221.050\n",
      "epoch: 599800, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 13033094.009\n",
      "epoch: 599900, train loss: 4.051649951934815, val loss: 4.0666890144348145, ETA in seconds: 13031964.828\n",
      "epoch: 600000, train loss: 4.050282764434814, val loss: 4.067274951934815, ETA in seconds: 13030899.980\n",
      "epoch: 600100, train loss: 4.051259326934814, val loss: 4.069814014434814, ETA in seconds: 13029766.432\n",
      "epoch: 600200, train loss: 4.053798389434815, val loss: 4.066298389434815, ETA in seconds: 13028629.225\n",
      "epoch: 600300, train loss: 4.059267139434814, val loss: 4.063954639434814, ETA in seconds: 13027456.463\n",
      "epoch: 600400, train loss: 4.053407764434814, val loss: 4.0696187019348145, ETA in seconds: 13026294.931\n",
      "epoch: 600500, train loss: 4.0520405769348145, val loss: 4.065126514434814, ETA in seconds: 13025142.419\n",
      "epoch: 600600, train loss: 4.050673389434815, val loss: 4.0686421394348145, ETA in seconds: 13024009.506\n",
      "epoch: 600700, train loss: 4.0520405769348145, val loss: 4.059657764434815, ETA in seconds: 13022871.221\n",
      "epoch: 600800, train loss: 4.056142139434814, val loss: 4.070985889434814, ETA in seconds: 13021734.319\n",
      "epoch: 600900, train loss: 4.052235889434814, val loss: 4.070399951934815, ETA in seconds: 13020526.642\n",
      "epoch: 601000, train loss: 4.055751514434815, val loss: 4.0696187019348145, ETA in seconds: 13019383.035\n",
      "epoch: 601100, train loss: 4.052626514434815, val loss: 4.065126514434814, ETA in seconds: 13018280.104\n",
      "epoch: 601200, train loss: 4.0510640144348145, val loss: 4.060243701934814, ETA in seconds: 13017118.294\n",
      "epoch: 601300, train loss: 4.051845264434815, val loss: 4.069423389434815, ETA in seconds: 13015948.217\n",
      "epoch: 601400, train loss: 4.052821826934815, val loss: 4.064540576934815, ETA in seconds: 13014796.230\n",
      "epoch: 601500, train loss: 4.0627827644348145, val loss: 4.0735249519348145, ETA in seconds: 13013649.150\n",
      "epoch: 601600, train loss: 4.048720264434815, val loss: 4.063954639434814, ETA in seconds: 13012489.382\n",
      "epoch: 601700, train loss: 4.058095264434814, val loss: 4.065126514434814, ETA in seconds: 13011297.203\n",
      "epoch: 601800, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 13010231.605\n",
      "epoch: 601900, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 13009111.924\n",
      "epoch: 602000, train loss: 4.055165576934814, val loss: 4.070985889434814, ETA in seconds: 13007923.487\n",
      "epoch: 602100, train loss: 4.056337451934814, val loss: 4.063173389434814, ETA in seconds: 13006748.441\n",
      "epoch: 602200, train loss: 4.0510640144348145, val loss: 4.0696187019348145, ETA in seconds: 13005568.463\n",
      "epoch: 602300, train loss: 4.052235889434814, val loss: 4.069032764434814, ETA in seconds: 13004426.358\n",
      "epoch: 602400, train loss: 4.0491108894348145, val loss: 4.0666890144348145, ETA in seconds: 13003263.085\n",
      "epoch: 602500, train loss: 4.050868701934815, val loss: 4.068837451934814, ETA in seconds: 13002130.723\n",
      "epoch: 602600, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 13000971.278\n",
      "epoch: 602700, train loss: 4.053603076934815, val loss: 4.062392139434815, ETA in seconds: 12999834.388\n",
      "epoch: 602800, train loss: 4.048329639434814, val loss: 4.070790576934814, ETA in seconds: 12998682.706\n",
      "epoch: 602900, train loss: 4.045985889434815, val loss: 4.0666890144348145, ETA in seconds: 12997545.557\n",
      "epoch: 603000, train loss: 4.050868701934815, val loss: 4.0637593269348145, ETA in seconds: 12996399.789\n",
      "epoch: 603100, train loss: 4.049696826934815, val loss: 4.064540576934815, ETA in seconds: 12995244.626\n",
      "epoch: 603200, train loss: 4.0491108894348145, val loss: 4.0647358894348145, ETA in seconds: 12994084.096\n",
      "epoch: 603300, train loss: 4.046571826934814, val loss: 4.068837451934814, ETA in seconds: 12992921.510\n",
      "epoch: 603400, train loss: 4.052235889434814, val loss: 4.066298389434815, ETA in seconds: 12991771.329\n",
      "epoch: 603500, train loss: 4.053603076934815, val loss: 4.0657124519348145, ETA in seconds: 12990610.950\n",
      "epoch: 603600, train loss: 4.056142139434814, val loss: 4.067079639434814, ETA in seconds: 12989423.324\n",
      "epoch: 603700, train loss: 4.057118701934814, val loss: 4.0666890144348145, ETA in seconds: 12988354.936\n",
      "epoch: 603800, train loss: 4.0530171394348145, val loss: 4.071376514434815, ETA in seconds: 12987246.842\n",
      "epoch: 603900, train loss: 4.049696826934815, val loss: 4.071376514434815, ETA in seconds: 12986147.137\n",
      "epoch: 604000, train loss: 4.0559468269348145, val loss: 4.069423389434815, ETA in seconds: 12985103.122\n",
      "epoch: 604100, train loss: 4.0598530769348145, val loss: 4.0657124519348145, ETA in seconds: 12984126.313\n",
      "epoch: 604200, train loss: 4.055360889434814, val loss: 4.077626514434814, ETA in seconds: 12983005.154\n",
      "epoch: 604300, train loss: 4.052235889434814, val loss: 4.071962451934814, ETA in seconds: 12981846.199\n",
      "epoch: 604400, train loss: 4.051259326934814, val loss: 4.0647358894348145, ETA in seconds: 12980680.945\n",
      "epoch: 604500, train loss: 4.045009326934815, val loss: 4.0657124519348145, ETA in seconds: 12979488.090\n",
      "epoch: 604600, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 12978351.645\n",
      "epoch: 604700, train loss: 4.0471577644348145, val loss: 4.067470264434815, ETA in seconds: 12977137.533\n",
      "epoch: 604800, train loss: 4.046962451934815, val loss: 4.063368701934815, ETA in seconds: 12975929.814\n",
      "epoch: 604900, train loss: 4.050282764434814, val loss: 4.072939014434814, ETA in seconds: 12974755.381\n",
      "epoch: 605000, train loss: 4.0510640144348145, val loss: 4.071962451934814, ETA in seconds: 12973577.077\n",
      "epoch: 605100, train loss: 4.0510640144348145, val loss: 4.070204639434815, ETA in seconds: 12972395.957\n",
      "epoch: 605200, train loss: 4.049306201934814, val loss: 4.065126514434814, ETA in seconds: 12971197.323\n",
      "epoch: 605300, train loss: 4.059657764434815, val loss: 4.066103076934814, ETA in seconds: 12970022.167\n",
      "epoch: 605400, train loss: 4.042860889434815, val loss: 4.069423389434815, ETA in seconds: 12968834.741\n",
      "epoch: 605500, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 12967649.497\n",
      "epoch: 605600, train loss: 4.051649951934815, val loss: 4.064931201934814, ETA in seconds: 12966438.608\n",
      "epoch: 605700, train loss: 4.054384326934814, val loss: 4.0676655769348145, ETA in seconds: 12965255.088\n",
      "epoch: 605800, train loss: 4.046767139434815, val loss: 4.068837451934814, ETA in seconds: 12964073.496\n",
      "epoch: 605900, train loss: 4.056142139434814, val loss: 4.063954639434814, ETA in seconds: 12962875.962\n",
      "epoch: 606000, train loss: 4.049892139434815, val loss: 4.063564014434815, ETA in seconds: 12961673.263\n",
      "epoch: 606100, train loss: 4.0520405769348145, val loss: 4.0627827644348145, ETA in seconds: 12960523.852\n",
      "epoch: 606200, train loss: 4.048524951934814, val loss: 4.0696187019348145, ETA in seconds: 12959335.113\n",
      "epoch: 606300, train loss: 4.0530171394348145, val loss: 4.066884326934814, ETA in seconds: 12958145.187\n",
      "epoch: 606400, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 12956951.664\n",
      "epoch: 606500, train loss: 4.052431201934814, val loss: 4.067470264434815, ETA in seconds: 12955753.546\n",
      "epoch: 606600, train loss: 4.043837451934815, val loss: 4.0686421394348145, ETA in seconds: 12954556.071\n",
      "epoch: 606700, train loss: 4.0452046394348145, val loss: 4.0647358894348145, ETA in seconds: 12953385.952\n",
      "epoch: 606800, train loss: 4.054189014434814, val loss: 4.061024951934814, ETA in seconds: 12952196.615\n",
      "epoch: 606900, train loss: 4.053407764434814, val loss: 4.0666890144348145, ETA in seconds: 12951025.388\n",
      "epoch: 607000, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 12949831.756\n",
      "epoch: 607100, train loss: 4.058095264434814, val loss: 4.072157764434815, ETA in seconds: 12948635.003\n",
      "epoch: 607200, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 12947371.597\n",
      "epoch: 607300, train loss: 4.0481343269348145, val loss: 4.067470264434815, ETA in seconds: 12946175.378\n",
      "epoch: 607400, train loss: 4.050282764434814, val loss: 4.070009326934814, ETA in seconds: 12944968.012\n",
      "epoch: 607500, train loss: 4.050868701934815, val loss: 4.071767139434814, ETA in seconds: 12943782.004\n",
      "epoch: 607600, train loss: 4.051454639434814, val loss: 4.071962451934814, ETA in seconds: 12942563.698\n",
      "epoch: 607700, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 12941350.296\n",
      "epoch: 607800, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 12940129.949\n",
      "epoch: 607900, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 12938885.172\n",
      "epoch: 608000, train loss: 4.048329639434814, val loss: 4.0686421394348145, ETA in seconds: 12937694.513\n",
      "epoch: 608100, train loss: 4.049892139434815, val loss: 4.066298389434815, ETA in seconds: 12936514.785\n",
      "epoch: 608200, train loss: 4.0530171394348145, val loss: 4.065126514434814, ETA in seconds: 12935313.700\n",
      "epoch: 608300, train loss: 4.052235889434814, val loss: 4.067079639434814, ETA in seconds: 12934130.490\n",
      "epoch: 608400, train loss: 4.056728076934815, val loss: 4.067079639434814, ETA in seconds: 12933110.154\n",
      "epoch: 608500, train loss: 4.055751514434815, val loss: 4.062196826934814, ETA in seconds: 12932027.155\n",
      "epoch: 608600, train loss: 4.059462451934815, val loss: 4.063954639434814, ETA in seconds: 12930949.048\n",
      "epoch: 608700, train loss: 4.053407764434814, val loss: 4.070985889434814, ETA in seconds: 12929889.020\n",
      "epoch: 608800, train loss: 4.050478076934814, val loss: 4.065517139434815, ETA in seconds: 12928787.930\n",
      "epoch: 608900, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 12927693.558\n",
      "epoch: 609000, train loss: 4.049892139434815, val loss: 4.0686421394348145, ETA in seconds: 12926488.276\n",
      "epoch: 609100, train loss: 4.051845264434815, val loss: 4.069423389434815, ETA in seconds: 12925261.940\n",
      "epoch: 609200, train loss: 4.048524951934814, val loss: 4.069814014434814, ETA in seconds: 12924019.203\n",
      "epoch: 609300, train loss: 4.047743701934815, val loss: 4.067860889434814, ETA in seconds: 12922782.551\n",
      "epoch: 609400, train loss: 4.050282764434814, val loss: 4.065126514434814, ETA in seconds: 12921550.134\n",
      "epoch: 609500, train loss: 4.0500874519348145, val loss: 4.0676655769348145, ETA in seconds: 12920342.732\n",
      "epoch: 609600, train loss: 4.050868701934815, val loss: 4.0686421394348145, ETA in seconds: 12919116.095\n",
      "epoch: 609700, train loss: 4.046962451934815, val loss: 4.069228076934815, ETA in seconds: 12917901.934\n",
      "epoch: 609800, train loss: 4.052626514434815, val loss: 4.070985889434814, ETA in seconds: 12916680.215\n",
      "epoch: 609900, train loss: 4.049696826934815, val loss: 4.066884326934814, ETA in seconds: 12915496.160\n",
      "epoch: 610000, train loss: 4.057509326934815, val loss: 4.062587451934815, ETA in seconds: 12914224.353\n",
      "epoch: 610100, train loss: 4.050478076934814, val loss: 4.063954639434814, ETA in seconds: 12912978.276\n",
      "epoch: 610200, train loss: 4.050673389434815, val loss: 4.0686421394348145, ETA in seconds: 12911700.999\n",
      "epoch: 610300, train loss: 4.051259326934814, val loss: 4.069032764434814, ETA in seconds: 12910422.052\n",
      "epoch: 610400, train loss: 4.048524951934814, val loss: 4.066884326934814, ETA in seconds: 12909145.477\n",
      "epoch: 610500, train loss: 4.046767139434815, val loss: 4.061610889434815, ETA in seconds: 12907888.363\n",
      "epoch: 610600, train loss: 4.048524951934814, val loss: 4.068251514434815, ETA in seconds: 12906608.365\n",
      "epoch: 610700, train loss: 4.054774951934815, val loss: 4.063954639434814, ETA in seconds: 12905326.704\n",
      "epoch: 610800, train loss: 4.049892139434815, val loss: 4.0676655769348145, ETA in seconds: 12904052.764\n",
      "epoch: 610900, train loss: 4.052821826934815, val loss: 4.062978076934814, ETA in seconds: 12902837.859\n",
      "epoch: 611000, train loss: 4.052626514434815, val loss: 4.068056201934814, ETA in seconds: 12901555.840\n",
      "epoch: 611100, train loss: 4.056142139434814, val loss: 4.065907764434814, ETA in seconds: 12900283.121\n",
      "epoch: 611200, train loss: 4.049696826934815, val loss: 4.0637593269348145, ETA in seconds: 12899218.341\n",
      "epoch: 611300, train loss: 4.051259326934814, val loss: 4.074110889434815, ETA in seconds: 12898181.814\n",
      "epoch: 611400, train loss: 4.0569233894348145, val loss: 4.066493701934815, ETA in seconds: 12897145.731\n",
      "epoch: 611500, train loss: 4.0510640144348145, val loss: 4.0657124519348145, ETA in seconds: 12895918.231\n",
      "epoch: 611600, train loss: 4.053407764434814, val loss: 4.070009326934814, ETA in seconds: 12894712.604\n",
      "epoch: 611700, train loss: 4.0500874519348145, val loss: 4.069814014434814, ETA in seconds: 12893527.005\n",
      "epoch: 611800, train loss: 4.054189014434814, val loss: 4.0657124519348145, ETA in seconds: 12892320.836\n",
      "epoch: 611900, train loss: 4.049306201934814, val loss: 4.066103076934814, ETA in seconds: 12891092.863\n",
      "epoch: 612000, train loss: 4.0530171394348145, val loss: 4.063954639434814, ETA in seconds: 12889819.242\n",
      "epoch: 612100, train loss: 4.051845264434815, val loss: 4.067860889434814, ETA in seconds: 12888528.503\n",
      "epoch: 612200, train loss: 4.054384326934814, val loss: 4.064540576934815, ETA in seconds: 12887242.664\n",
      "epoch: 612300, train loss: 4.0452046394348145, val loss: 4.070204639434815, ETA in seconds: 12885928.097\n",
      "epoch: 612400, train loss: 4.051649951934815, val loss: 4.066103076934814, ETA in seconds: 12884609.834\n",
      "epoch: 612500, train loss: 4.055751514434815, val loss: 4.0705952644348145, ETA in seconds: 12883362.632\n",
      "epoch: 612600, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 12882116.708\n",
      "epoch: 612700, train loss: 4.0510640144348145, val loss: 4.065126514434814, ETA in seconds: 12880832.192\n",
      "epoch: 612800, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 12879546.324\n",
      "epoch: 612900, train loss: 4.055751514434815, val loss: 4.067274951934815, ETA in seconds: 12878284.502\n",
      "epoch: 613000, train loss: 4.0530171394348145, val loss: 4.071376514434815, ETA in seconds: 12876986.928\n",
      "epoch: 613100, train loss: 4.055751514434815, val loss: 4.068837451934814, ETA in seconds: 12875679.246\n",
      "epoch: 613200, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 12874380.507\n",
      "epoch: 613300, train loss: 4.0471577644348145, val loss: 4.070790576934814, ETA in seconds: 12873075.106\n",
      "epoch: 613400, train loss: 4.046376514434814, val loss: 4.070985889434814, ETA in seconds: 12871766.680\n",
      "epoch: 613500, train loss: 4.047548389434814, val loss: 4.068446826934815, ETA in seconds: 12870459.465\n",
      "epoch: 613600, train loss: 4.052626514434815, val loss: 4.064540576934815, ETA in seconds: 12869185.125\n",
      "epoch: 613700, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 12867918.906\n",
      "epoch: 613800, train loss: 4.052431201934814, val loss: 4.065321826934815, ETA in seconds: 12866630.096\n",
      "epoch: 613900, train loss: 4.0530171394348145, val loss: 4.066103076934814, ETA in seconds: 12865331.589\n",
      "epoch: 614000, train loss: 4.045595264434814, val loss: 4.067860889434814, ETA in seconds: 12864033.671\n",
      "epoch: 614100, train loss: 4.0539937019348145, val loss: 4.0676655769348145, ETA in seconds: 12862746.572\n",
      "epoch: 614200, train loss: 4.0559468269348145, val loss: 4.064149951934814, ETA in seconds: 12861419.432\n",
      "epoch: 614300, train loss: 4.051649951934815, val loss: 4.067079639434814, ETA in seconds: 12860108.589\n",
      "epoch: 614400, train loss: 4.062978076934814, val loss: 4.070009326934814, ETA in seconds: 12858822.221\n",
      "epoch: 614500, train loss: 4.0510640144348145, val loss: 4.069423389434815, ETA in seconds: 12857554.624\n",
      "epoch: 614600, train loss: 4.051454639434814, val loss: 4.0657124519348145, ETA in seconds: 12856283.785\n",
      "epoch: 614700, train loss: 4.054384326934814, val loss: 4.063368701934815, ETA in seconds: 12854998.358\n",
      "epoch: 614800, train loss: 4.0500874519348145, val loss: 4.067470264434815, ETA in seconds: 12853722.595\n",
      "epoch: 614900, train loss: 4.051845264434815, val loss: 4.067860889434814, ETA in seconds: 12852409.315\n",
      "epoch: 615000, train loss: 4.045009326934815, val loss: 4.0676655769348145, ETA in seconds: 12851117.422\n",
      "epoch: 615100, train loss: 4.0491108894348145, val loss: 4.067470264434815, ETA in seconds: 12849869.052\n",
      "epoch: 615200, train loss: 4.045399951934814, val loss: 4.067860889434814, ETA in seconds: 12848546.841\n",
      "epoch: 615300, train loss: 4.050478076934814, val loss: 4.062001514434814, ETA in seconds: 12847250.183\n",
      "epoch: 615400, train loss: 4.051259326934814, val loss: 4.067470264434815, ETA in seconds: 12845967.834\n",
      "epoch: 615500, train loss: 4.053603076934815, val loss: 4.068251514434815, ETA in seconds: 12844653.648\n",
      "epoch: 615600, train loss: 4.049306201934814, val loss: 4.068446826934815, ETA in seconds: 12843541.105\n",
      "epoch: 615700, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 12842420.460\n",
      "epoch: 615800, train loss: 4.0491108894348145, val loss: 4.063954639434814, ETA in seconds: 12841179.243\n",
      "epoch: 615900, train loss: 4.051649951934815, val loss: 4.064149951934814, ETA in seconds: 12839821.209\n",
      "epoch: 616000, train loss: 4.0471577644348145, val loss: 4.0627827644348145, ETA in seconds: 12838486.891\n",
      "epoch: 616100, train loss: 4.051259326934814, val loss: 4.069228076934815, ETA in seconds: 12837158.639\n",
      "epoch: 616200, train loss: 4.043056201934815, val loss: 4.062978076934814, ETA in seconds: 12835827.385\n",
      "epoch: 616300, train loss: 4.0510640144348145, val loss: 4.069814014434814, ETA in seconds: 12834499.806\n",
      "epoch: 616400, train loss: 4.049306201934814, val loss: 4.0666890144348145, ETA in seconds: 12833234.104\n",
      "epoch: 616500, train loss: 4.052431201934814, val loss: 4.069032764434814, ETA in seconds: 12831910.315\n",
      "epoch: 616600, train loss: 4.048524951934814, val loss: 4.063564014434815, ETA in seconds: 12830655.081\n",
      "epoch: 616700, train loss: 4.047743701934815, val loss: 4.0676655769348145, ETA in seconds: 12829514.257\n",
      "epoch: 616800, train loss: 4.047353076934814, val loss: 4.0666890144348145, ETA in seconds: 12828244.478\n",
      "epoch: 616900, train loss: 4.049501514434814, val loss: 4.070985889434814, ETA in seconds: 12827029.144\n",
      "epoch: 617000, train loss: 4.045985889434815, val loss: 4.0705952644348145, ETA in seconds: 12825710.805\n",
      "epoch: 617100, train loss: 4.046571826934814, val loss: 4.0666890144348145, ETA in seconds: 12824392.834\n",
      "epoch: 617200, train loss: 4.0500874519348145, val loss: 4.064149951934814, ETA in seconds: 12823080.110\n",
      "epoch: 617300, train loss: 4.052431201934814, val loss: 4.070399951934815, ETA in seconds: 12821747.532\n",
      "epoch: 617400, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 12820585.823\n",
      "epoch: 617500, train loss: 4.047353076934814, val loss: 4.068251514434815, ETA in seconds: 12819327.200\n",
      "epoch: 617600, train loss: 4.054774951934815, val loss: 4.057509326934815, ETA in seconds: 12818066.262\n",
      "epoch: 617700, train loss: 4.050673389434815, val loss: 4.068837451934814, ETA in seconds: 12816718.304\n",
      "epoch: 617800, train loss: 4.053212451934814, val loss: 4.063368701934815, ETA in seconds: 12815376.062\n",
      "epoch: 617900, train loss: 4.0510640144348145, val loss: 4.075282764434815, ETA in seconds: 12814002.855\n",
      "epoch: 618000, train loss: 4.048329639434814, val loss: 4.066884326934814, ETA in seconds: 12812618.178\n",
      "epoch: 618100, train loss: 4.050282764434814, val loss: 4.062978076934814, ETA in seconds: 12811234.897\n",
      "epoch: 618200, train loss: 4.055556201934815, val loss: 4.068056201934814, ETA in seconds: 12809900.239\n",
      "epoch: 618300, train loss: 4.054774951934815, val loss: 4.070204639434815, ETA in seconds: 12808559.741\n",
      "epoch: 618400, train loss: 4.056142139434814, val loss: 4.069423389434815, ETA in seconds: 12807227.571\n",
      "epoch: 618500, train loss: 4.057118701934814, val loss: 4.063368701934815, ETA in seconds: 12805848.725\n",
      "epoch: 618600, train loss: 4.054774951934815, val loss: 4.0735249519348145, ETA in seconds: 12804515.846\n",
      "epoch: 618700, train loss: 4.056728076934815, val loss: 4.065126514434814, ETA in seconds: 12803182.944\n",
      "epoch: 618800, train loss: 4.050868701934815, val loss: 4.072157764434815, ETA in seconds: 12801849.867\n",
      "epoch: 618900, train loss: 4.045985889434815, val loss: 4.072939014434814, ETA in seconds: 12800450.568\n",
      "epoch: 619000, train loss: 4.050673389434815, val loss: 4.073134326934815, ETA in seconds: 12799078.182\n",
      "epoch: 619100, train loss: 4.050478076934814, val loss: 4.0666890144348145, ETA in seconds: 12797726.654\n",
      "epoch: 619200, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 12796439.997\n",
      "epoch: 619300, train loss: 4.046376514434814, val loss: 4.062392139434815, ETA in seconds: 12795092.697\n",
      "epoch: 619400, train loss: 4.053212451934814, val loss: 4.068446826934815, ETA in seconds: 12793805.783\n",
      "epoch: 619500, train loss: 4.050673389434815, val loss: 4.063368701934815, ETA in seconds: 12792433.172\n",
      "epoch: 619600, train loss: 4.0578999519348145, val loss: 4.062392139434815, ETA in seconds: 12791204.837\n",
      "epoch: 619700, train loss: 4.050478076934814, val loss: 4.0705952644348145, ETA in seconds: 12790000.601\n",
      "epoch: 619800, train loss: 4.0520405769348145, val loss: 4.063173389434814, ETA in seconds: 12788749.196\n",
      "epoch: 619900, train loss: 4.046767139434815, val loss: 4.067470264434815, ETA in seconds: 12787495.163\n",
      "epoch: 620000, train loss: 4.053603076934815, val loss: 4.0696187019348145, ETA in seconds: 12786152.956\n",
      "epoch: 620100, train loss: 4.055556201934815, val loss: 4.0676655769348145, ETA in seconds: 12784769.243\n",
      "epoch: 620200, train loss: 4.052626514434815, val loss: 4.068446826934815, ETA in seconds: 12783436.660\n",
      "epoch: 620300, train loss: 4.049306201934814, val loss: 4.070790576934814, ETA in seconds: 12782078.815\n",
      "epoch: 620400, train loss: 4.054189014434814, val loss: 4.0666890144348145, ETA in seconds: 12780758.956\n",
      "epoch: 620500, train loss: 4.050478076934814, val loss: 4.0657124519348145, ETA in seconds: 12779441.801\n",
      "epoch: 620600, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 12778146.995\n",
      "epoch: 620700, train loss: 4.058485889434815, val loss: 4.062001514434814, ETA in seconds: 12776832.326\n",
      "epoch: 620800, train loss: 4.048915576934815, val loss: 4.070204639434815, ETA in seconds: 12775493.907\n",
      "epoch: 620900, train loss: 4.055751514434815, val loss: 4.070009326934814, ETA in seconds: 12774096.668\n",
      "epoch: 621000, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 12772712.765\n",
      "epoch: 621100, train loss: 4.0481343269348145, val loss: 4.064149951934814, ETA in seconds: 12771311.816\n",
      "epoch: 621200, train loss: 4.057509326934815, val loss: 4.0696187019348145, ETA in seconds: 12769880.269\n",
      "epoch: 621300, train loss: 4.049696826934815, val loss: 4.070009326934814, ETA in seconds: 12768478.095\n",
      "epoch: 621400, train loss: 4.050282764434814, val loss: 4.068056201934814, ETA in seconds: 12767114.895\n",
      "epoch: 621500, train loss: 4.0539937019348145, val loss: 4.067079639434814, ETA in seconds: 12765762.190\n",
      "epoch: 621600, train loss: 4.054189014434814, val loss: 4.065321826934815, ETA in seconds: 12764370.212\n",
      "epoch: 621700, train loss: 4.045985889434815, val loss: 4.063954639434814, ETA in seconds: 12762999.112\n",
      "epoch: 621800, train loss: 4.0559468269348145, val loss: 4.070985889434814, ETA in seconds: 12761584.140\n",
      "epoch: 621900, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 12760197.075\n",
      "epoch: 622000, train loss: 4.047548389434814, val loss: 4.062392139434815, ETA in seconds: 12758796.480\n",
      "epoch: 622100, train loss: 4.052431201934814, val loss: 4.0696187019348145, ETA in seconds: 12757424.807\n",
      "epoch: 622200, train loss: 4.056532764434815, val loss: 4.069423389434815, ETA in seconds: 12756057.636\n",
      "epoch: 622300, train loss: 4.053798389434815, val loss: 4.0715718269348145, ETA in seconds: 12754696.336\n",
      "epoch: 622400, train loss: 4.051845264434815, val loss: 4.0725483894348145, ETA in seconds: 12753369.886\n",
      "epoch: 622500, train loss: 4.0539937019348145, val loss: 4.069814014434814, ETA in seconds: 12752062.962\n",
      "epoch: 622600, train loss: 4.052821826934815, val loss: 4.0647358894348145, ETA in seconds: 12750741.302\n",
      "epoch: 622700, train loss: 4.048524951934814, val loss: 4.0715718269348145, ETA in seconds: 12749377.473\n",
      "epoch: 622800, train loss: 4.049696826934815, val loss: 4.065517139434815, ETA in seconds: 12748050.788\n",
      "epoch: 622900, train loss: 4.053407764434814, val loss: 4.0627827644348145, ETA in seconds: 12746736.067\n",
      "epoch: 623000, train loss: 4.055556201934815, val loss: 4.066493701934815, ETA in seconds: 12745440.074\n",
      "epoch: 623100, train loss: 4.048524951934814, val loss: 4.073720264434814, ETA in seconds: 12744193.415\n",
      "epoch: 623200, train loss: 4.051649951934815, val loss: 4.069814014434814, ETA in seconds: 12742811.371\n",
      "epoch: 623300, train loss: 4.047548389434814, val loss: 4.067274951934815, ETA in seconds: 12741491.481\n",
      "epoch: 623400, train loss: 4.048915576934815, val loss: 4.069423389434815, ETA in seconds: 12740149.089\n",
      "epoch: 623500, train loss: 4.0500874519348145, val loss: 4.061220264434814, ETA in seconds: 12738838.663\n",
      "epoch: 623600, train loss: 4.049892139434815, val loss: 4.068251514434815, ETA in seconds: 12737446.027\n",
      "epoch: 623700, train loss: 4.054774951934815, val loss: 4.068837451934814, ETA in seconds: 12736006.233\n",
      "epoch: 623800, train loss: 4.047939014434815, val loss: 4.067079639434814, ETA in seconds: 12734581.980\n",
      "epoch: 623900, train loss: 4.055360889434814, val loss: 4.066103076934814, ETA in seconds: 12733158.175\n",
      "epoch: 624000, train loss: 4.054579639434815, val loss: 4.064149951934814, ETA in seconds: 12731770.493\n",
      "epoch: 624100, train loss: 4.052821826934815, val loss: 4.065907764434814, ETA in seconds: 12730345.844\n",
      "epoch: 624200, train loss: 4.054189014434814, val loss: 4.064931201934814, ETA in seconds: 12729004.949\n",
      "epoch: 624300, train loss: 4.0510640144348145, val loss: 4.066493701934815, ETA in seconds: 12727596.105\n",
      "epoch: 624400, train loss: 4.0578999519348145, val loss: 4.068837451934814, ETA in seconds: 12726199.105\n",
      "epoch: 624500, train loss: 4.047939014434815, val loss: 4.0647358894348145, ETA in seconds: 12724781.117\n",
      "epoch: 624600, train loss: 4.051649951934815, val loss: 4.067274951934815, ETA in seconds: 12723344.307\n",
      "epoch: 624700, train loss: 4.047939014434815, val loss: 4.068446826934815, ETA in seconds: 12721999.259\n",
      "epoch: 624800, train loss: 4.0500874519348145, val loss: 4.063954639434814, ETA in seconds: 12720623.065\n",
      "epoch: 624900, train loss: 4.053603076934815, val loss: 4.067274951934815, ETA in seconds: 12719224.027\n",
      "epoch: 625000, train loss: 4.044814014434815, val loss: 4.0696187019348145, ETA in seconds: 12717808.594\n",
      "epoch: 625100, train loss: 4.053212451934814, val loss: 4.065321826934815, ETA in seconds: 12716438.904\n",
      "epoch: 625200, train loss: 4.055751514434815, val loss: 4.069032764434814, ETA in seconds: 12715047.770\n",
      "epoch: 625300, train loss: 4.0539937019348145, val loss: 4.064540576934815, ETA in seconds: 12713648.967\n",
      "epoch: 625400, train loss: 4.052821826934815, val loss: 4.068446826934815, ETA in seconds: 12712272.386\n",
      "epoch: 625500, train loss: 4.049696826934815, val loss: 4.064931201934814, ETA in seconds: 12710816.507\n",
      "epoch: 625600, train loss: 4.0500874519348145, val loss: 4.070009326934814, ETA in seconds: 12709363.653\n",
      "epoch: 625700, train loss: 4.049306201934814, val loss: 4.0666890144348145, ETA in seconds: 12707948.103\n",
      "epoch: 625800, train loss: 4.048915576934815, val loss: 4.066493701934815, ETA in seconds: 12706498.870\n",
      "epoch: 625900, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 12705082.889\n",
      "epoch: 626000, train loss: 4.0530171394348145, val loss: 4.0715718269348145, ETA in seconds: 12703866.505\n",
      "epoch: 626100, train loss: 4.051454639434814, val loss: 4.065321826934815, ETA in seconds: 12702491.709\n",
      "epoch: 626200, train loss: 4.058681201934815, val loss: 4.066298389434815, ETA in seconds: 12701103.997\n",
      "epoch: 626300, train loss: 4.057314014434814, val loss: 4.070790576934814, ETA in seconds: 12699691.444\n",
      "epoch: 626400, train loss: 4.050282764434814, val loss: 4.063564014434815, ETA in seconds: 12698277.463\n",
      "epoch: 626500, train loss: 4.0530171394348145, val loss: 4.0666890144348145, ETA in seconds: 12696837.299\n",
      "epoch: 626600, train loss: 4.054189014434814, val loss: 4.066884326934814, ETA in seconds: 12695403.445\n",
      "epoch: 626700, train loss: 4.051649951934815, val loss: 4.0715718269348145, ETA in seconds: 12693938.566\n",
      "epoch: 626800, train loss: 4.049696826934815, val loss: 4.068251514434815, ETA in seconds: 12692583.787\n",
      "epoch: 626900, train loss: 4.055360889434814, val loss: 4.067274951934815, ETA in seconds: 12691261.827\n",
      "epoch: 627000, train loss: 4.052235889434814, val loss: 4.0696187019348145, ETA in seconds: 12689922.941\n",
      "epoch: 627100, train loss: 4.048329639434814, val loss: 4.064345264434815, ETA in seconds: 12688535.263\n",
      "epoch: 627200, train loss: 4.055165576934814, val loss: 4.059462451934815, ETA in seconds: 12687147.633\n",
      "epoch: 627300, train loss: 4.0510640144348145, val loss: 4.068251514434815, ETA in seconds: 12685713.245\n",
      "epoch: 627400, train loss: 4.046767139434815, val loss: 4.059462451934815, ETA in seconds: 12684375.021\n",
      "epoch: 627500, train loss: 4.054579639434815, val loss: 4.068056201934814, ETA in seconds: 12683069.829\n",
      "epoch: 627600, train loss: 4.055556201934815, val loss: 4.066298389434815, ETA in seconds: 12681695.305\n",
      "epoch: 627700, train loss: 4.049501514434814, val loss: 4.0657124519348145, ETA in seconds: 12680275.367\n",
      "epoch: 627800, train loss: 4.057314014434814, val loss: 4.061610889434815, ETA in seconds: 12678810.623\n",
      "epoch: 627900, train loss: 4.052626514434815, val loss: 4.0686421394348145, ETA in seconds: 12677401.242\n",
      "epoch: 628000, train loss: 4.051649951934815, val loss: 4.0608296394348145, ETA in seconds: 12676101.282\n",
      "epoch: 628100, train loss: 4.053212451934814, val loss: 4.065517139434815, ETA in seconds: 12674871.002\n",
      "epoch: 628200, train loss: 4.049306201934814, val loss: 4.071376514434815, ETA in seconds: 12673562.308\n",
      "epoch: 628300, train loss: 4.0520405769348145, val loss: 4.068251514434815, ETA in seconds: 12672385.388\n",
      "epoch: 628400, train loss: 4.0491108894348145, val loss: 4.064345264434815, ETA in seconds: 12671027.465\n",
      "epoch: 628500, train loss: 4.057118701934814, val loss: 4.064345264434815, ETA in seconds: 12669545.518\n",
      "epoch: 628600, train loss: 4.051845264434815, val loss: 4.0696187019348145, ETA in seconds: 12668068.627\n",
      "epoch: 628700, train loss: 4.0491108894348145, val loss: 4.064540576934815, ETA in seconds: 12666627.697\n",
      "epoch: 628800, train loss: 4.054189014434814, val loss: 4.063954639434814, ETA in seconds: 12665133.528\n",
      "epoch: 628900, train loss: 4.057118701934814, val loss: 4.0627827644348145, ETA in seconds: 12663678.664\n",
      "epoch: 629000, train loss: 4.054384326934814, val loss: 4.064149951934814, ETA in seconds: 12662213.978\n",
      "epoch: 629100, train loss: 4.048915576934815, val loss: 4.0657124519348145, ETA in seconds: 12660779.017\n",
      "epoch: 629200, train loss: 4.050282764434814, val loss: 4.066884326934814, ETA in seconds: 12659346.343\n",
      "epoch: 629300, train loss: 4.058485889434815, val loss: 4.069228076934815, ETA in seconds: 12657863.703\n",
      "epoch: 629400, train loss: 4.051649951934815, val loss: 4.068446826934815, ETA in seconds: 12656467.778\n",
      "epoch: 629500, train loss: 4.044032764434815, val loss: 4.065126514434814, ETA in seconds: 12655011.992\n",
      "epoch: 629600, train loss: 4.049501514434814, val loss: 4.0676655769348145, ETA in seconds: 12653617.846\n",
      "epoch: 629700, train loss: 4.049696826934815, val loss: 4.074892139434814, ETA in seconds: 12652167.365\n",
      "epoch: 629800, train loss: 4.053798389434815, val loss: 4.068446826934815, ETA in seconds: 12650715.983\n",
      "epoch: 629900, train loss: 4.051454639434814, val loss: 4.067274951934815, ETA in seconds: 12649247.213\n",
      "epoch: 630000, train loss: 4.050282764434814, val loss: 4.0647358894348145, ETA in seconds: 12647804.622\n",
      "epoch: 630100, train loss: 4.048329639434814, val loss: 4.068056201934814, ETA in seconds: 12646399.773\n",
      "epoch: 630200, train loss: 4.049892139434815, val loss: 4.070399951934815, ETA in seconds: 12644918.119\n",
      "epoch: 630300, train loss: 4.048524951934814, val loss: 4.068446826934815, ETA in seconds: 12643447.835\n",
      "epoch: 630400, train loss: 4.0500874519348145, val loss: 4.067860889434814, ETA in seconds: 12641952.025\n",
      "epoch: 630500, train loss: 4.0569233894348145, val loss: 4.0676655769348145, ETA in seconds: 12640441.024\n",
      "epoch: 630600, train loss: 4.049892139434815, val loss: 4.071767139434814, ETA in seconds: 12638985.186\n",
      "epoch: 630700, train loss: 4.0549702644348145, val loss: 4.069032764434814, ETA in seconds: 12637493.224\n",
      "epoch: 630800, train loss: 4.047939014434815, val loss: 4.072353076934815, ETA in seconds: 12636025.001\n",
      "epoch: 630900, train loss: 4.054189014434814, val loss: 4.067274951934815, ETA in seconds: 12634577.425\n",
      "epoch: 631000, train loss: 4.055556201934815, val loss: 4.064149951934814, ETA in seconds: 12633133.361\n",
      "epoch: 631100, train loss: 4.045595264434814, val loss: 4.071181201934815, ETA in seconds: 12631704.672\n",
      "epoch: 631200, train loss: 4.048329639434814, val loss: 4.068251514434815, ETA in seconds: 12630203.669\n",
      "epoch: 631300, train loss: 4.0491108894348145, val loss: 4.0676655769348145, ETA in seconds: 12628718.622\n",
      "epoch: 631400, train loss: 4.053407764434814, val loss: 4.064345264434815, ETA in seconds: 12627209.633\n",
      "epoch: 631500, train loss: 4.0520405769348145, val loss: 4.070985889434814, ETA in seconds: 12625725.661\n",
      "epoch: 631600, train loss: 4.0491108894348145, val loss: 4.0647358894348145, ETA in seconds: 12624253.628\n",
      "epoch: 631700, train loss: 4.047353076934814, val loss: 4.063954639434814, ETA in seconds: 12622747.919\n",
      "epoch: 631800, train loss: 4.056142139434814, val loss: 4.0676655769348145, ETA in seconds: 12621279.309\n",
      "epoch: 631900, train loss: 4.055165576934814, val loss: 4.064540576934815, ETA in seconds: 12619778.294\n",
      "epoch: 632000, train loss: 4.053603076934815, val loss: 4.064149951934814, ETA in seconds: 12618344.401\n",
      "epoch: 632100, train loss: 4.050282764434814, val loss: 4.0637593269348145, ETA in seconds: 12616928.386\n",
      "epoch: 632200, train loss: 4.046962451934815, val loss: 4.066493701934815, ETA in seconds: 12615453.269\n",
      "epoch: 632300, train loss: 4.055360889434814, val loss: 4.069032764434814, ETA in seconds: 12613992.375\n",
      "epoch: 632400, train loss: 4.056532764434815, val loss: 4.0666890144348145, ETA in seconds: 12612482.805\n",
      "epoch: 632500, train loss: 4.059071826934814, val loss: 4.0686421394348145, ETA in seconds: 12611005.695\n",
      "epoch: 632600, train loss: 4.0569233894348145, val loss: 4.072939014434814, ETA in seconds: 12609535.682\n",
      "epoch: 632700, train loss: 4.046376514434814, val loss: 4.071376514434815, ETA in seconds: 12608043.907\n",
      "epoch: 632800, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 12606536.079\n",
      "epoch: 632900, train loss: 4.053407764434814, val loss: 4.065321826934815, ETA in seconds: 12605053.668\n",
      "epoch: 633000, train loss: 4.0500874519348145, val loss: 4.065517139434815, ETA in seconds: 12603524.291\n",
      "epoch: 633100, train loss: 4.053407764434814, val loss: 4.0686421394348145, ETA in seconds: 12602015.028\n",
      "epoch: 633200, train loss: 4.0500874519348145, val loss: 4.067274951934815, ETA in seconds: 12600516.970\n",
      "epoch: 633300, train loss: 4.0520405769348145, val loss: 4.070790576934814, ETA in seconds: 12598992.959\n",
      "epoch: 633400, train loss: 4.052821826934815, val loss: 4.067079639434814, ETA in seconds: 12597473.152\n",
      "epoch: 633500, train loss: 4.048329639434814, val loss: 4.070985889434814, ETA in seconds: 12596010.448\n",
      "epoch: 633600, train loss: 4.055165576934814, val loss: 4.065907764434814, ETA in seconds: 12594518.250\n",
      "epoch: 633700, train loss: 4.047353076934814, val loss: 4.0686421394348145, ETA in seconds: 12593084.326\n",
      "epoch: 633800, train loss: 4.0588765144348145, val loss: 4.068251514434815, ETA in seconds: 12591773.934\n",
      "epoch: 633900, train loss: 4.057118701934814, val loss: 4.070204639434815, ETA in seconds: 12590300.173\n",
      "epoch: 634000, train loss: 4.047939014434815, val loss: 4.068251514434815, ETA in seconds: 12588847.369\n",
      "epoch: 634100, train loss: 4.053798389434815, val loss: 4.0715718269348145, ETA in seconds: 12587348.578\n",
      "epoch: 634200, train loss: 4.0500874519348145, val loss: 4.071962451934814, ETA in seconds: 12585854.150\n",
      "epoch: 634300, train loss: 4.053603076934815, val loss: 4.062196826934814, ETA in seconds: 12584347.054\n",
      "epoch: 634400, train loss: 4.0588765144348145, val loss: 4.067274951934815, ETA in seconds: 12582859.602\n",
      "epoch: 634500, train loss: 4.058485889434815, val loss: 4.069814014434814, ETA in seconds: 12581341.428\n",
      "epoch: 634600, train loss: 4.058095264434814, val loss: 4.0676655769348145, ETA in seconds: 12579835.507\n",
      "epoch: 634700, train loss: 4.053798389434815, val loss: 4.064931201934814, ETA in seconds: 12578310.202\n",
      "epoch: 634800, train loss: 4.051649951934815, val loss: 4.056337451934814, ETA in seconds: 12576790.676\n",
      "epoch: 634900, train loss: 4.053212451934814, val loss: 4.068446826934815, ETA in seconds: 12575228.066\n",
      "epoch: 635000, train loss: 4.053212451934814, val loss: 4.068837451934814, ETA in seconds: 12573719.065\n",
      "epoch: 635100, train loss: 4.052431201934814, val loss: 4.068446826934815, ETA in seconds: 12572169.276\n",
      "epoch: 635200, train loss: 4.055360889434814, val loss: 4.066884326934814, ETA in seconds: 12570664.765\n",
      "epoch: 635300, train loss: 4.050478076934814, val loss: 4.065907764434814, ETA in seconds: 12569143.004\n",
      "epoch: 635400, train loss: 4.049696826934815, val loss: 4.068446826934815, ETA in seconds: 12567651.129\n",
      "epoch: 635500, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 12566111.326\n",
      "epoch: 635600, train loss: 4.051845264434815, val loss: 4.0657124519348145, ETA in seconds: 12564574.229\n",
      "epoch: 635700, train loss: 4.048720264434815, val loss: 4.062001514434814, ETA in seconds: 12563019.089\n",
      "epoch: 635800, train loss: 4.0539937019348145, val loss: 4.070790576934814, ETA in seconds: 12561509.095\n",
      "epoch: 635900, train loss: 4.045009326934815, val loss: 4.072743701934814, ETA in seconds: 12559968.246\n",
      "epoch: 636000, train loss: 4.053407764434814, val loss: 4.0725483894348145, ETA in seconds: 12558408.235\n",
      "epoch: 636100, train loss: 4.050478076934814, val loss: 4.072939014434814, ETA in seconds: 12556881.987\n",
      "epoch: 636200, train loss: 4.049306201934814, val loss: 4.071181201934815, ETA in seconds: 12555300.755\n",
      "epoch: 636300, train loss: 4.048915576934815, val loss: 4.065321826934815, ETA in seconds: 12553804.166\n",
      "epoch: 636400, train loss: 4.054579639434815, val loss: 4.065126514434814, ETA in seconds: 12552303.681\n",
      "epoch: 636500, train loss: 4.053603076934815, val loss: 4.071181201934815, ETA in seconds: 12550761.532\n",
      "epoch: 636600, train loss: 4.0422749519348145, val loss: 4.069228076934815, ETA in seconds: 12549236.531\n",
      "epoch: 636700, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 12547661.377\n",
      "epoch: 636800, train loss: 4.049696826934815, val loss: 4.070009326934814, ETA in seconds: 12546090.409\n",
      "epoch: 636900, train loss: 4.045009326934815, val loss: 4.067860889434814, ETA in seconds: 12544529.665\n",
      "epoch: 637000, train loss: 4.049696826934815, val loss: 4.071962451934814, ETA in seconds: 12543030.856\n",
      "epoch: 637100, train loss: 4.054189014434814, val loss: 4.068251514434815, ETA in seconds: 12541554.140\n",
      "epoch: 637200, train loss: 4.0598530769348145, val loss: 4.068837451934814, ETA in seconds: 12540023.509\n",
      "epoch: 637300, train loss: 4.052821826934815, val loss: 4.0735249519348145, ETA in seconds: 12538465.049\n",
      "epoch: 637400, train loss: 4.051454639434814, val loss: 4.063173389434814, ETA in seconds: 12536869.223\n",
      "epoch: 637500, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 12535338.265\n",
      "epoch: 637600, train loss: 4.048524951934814, val loss: 4.0715718269348145, ETA in seconds: 12533770.637\n",
      "epoch: 637700, train loss: 4.0578999519348145, val loss: 4.065126514434814, ETA in seconds: 12532229.354\n",
      "epoch: 637800, train loss: 4.0510640144348145, val loss: 4.069032764434814, ETA in seconds: 12530694.796\n",
      "epoch: 637900, train loss: 4.049696826934815, val loss: 4.066298389434815, ETA in seconds: 12529177.924\n",
      "epoch: 638000, train loss: 4.051259326934814, val loss: 4.069032764434814, ETA in seconds: 12527639.722\n",
      "epoch: 638100, train loss: 4.0530171394348145, val loss: 4.065517139434815, ETA in seconds: 12526091.885\n",
      "epoch: 638200, train loss: 4.047353076934814, val loss: 4.0696187019348145, ETA in seconds: 12524520.753\n",
      "epoch: 638300, train loss: 4.046571826934814, val loss: 4.068056201934814, ETA in seconds: 12522991.589\n",
      "epoch: 638400, train loss: 4.053212451934814, val loss: 4.063368701934815, ETA in seconds: 12521415.002\n",
      "epoch: 638500, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 12519862.581\n",
      "epoch: 638600, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 12518283.032\n",
      "epoch: 638700, train loss: 4.044032764434815, val loss: 4.063954639434814, ETA in seconds: 12516790.883\n",
      "epoch: 638800, train loss: 4.0481343269348145, val loss: 4.069228076934815, ETA in seconds: 12515270.749\n",
      "epoch: 638900, train loss: 4.057509326934815, val loss: 4.067470264434815, ETA in seconds: 12513719.157\n",
      "epoch: 639000, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 12512163.325\n",
      "epoch: 639100, train loss: 4.052431201934814, val loss: 4.064345264434815, ETA in seconds: 12510580.388\n",
      "epoch: 639200, train loss: 4.053212451934814, val loss: 4.071962451934814, ETA in seconds: 12508997.750\n",
      "epoch: 639300, train loss: 4.043642139434814, val loss: 4.061610889434815, ETA in seconds: 12507448.659\n",
      "epoch: 639400, train loss: 4.047353076934814, val loss: 4.0676655769348145, ETA in seconds: 12505895.214\n",
      "epoch: 639500, train loss: 4.049892139434815, val loss: 4.0657124519348145, ETA in seconds: 12504508.594\n",
      "epoch: 639600, train loss: 4.047743701934815, val loss: 4.067470264434815, ETA in seconds: 12503081.068\n",
      "epoch: 639700, train loss: 4.0520405769348145, val loss: 4.065517139434815, ETA in seconds: 12501531.670\n",
      "epoch: 639800, train loss: 4.051649951934815, val loss: 4.071376514434815, ETA in seconds: 12499942.914\n",
      "epoch: 639900, train loss: 4.054774951934815, val loss: 4.067860889434814, ETA in seconds: 12498372.126\n",
      "epoch: 640000, train loss: 4.046571826934814, val loss: 4.067860889434814, ETA in seconds: 12496809.813\n",
      "epoch: 640100, train loss: 4.052821826934815, val loss: 4.0696187019348145, ETA in seconds: 12495232.417\n",
      "epoch: 640200, train loss: 4.051845264434815, val loss: 4.069814014434814, ETA in seconds: 12493657.163\n",
      "epoch: 640300, train loss: 4.049306201934814, val loss: 4.064931201934814, ETA in seconds: 12492097.558\n",
      "epoch: 640400, train loss: 4.044423389434814, val loss: 4.0647358894348145, ETA in seconds: 12490507.044\n",
      "epoch: 640500, train loss: 4.055556201934815, val loss: 4.0637593269348145, ETA in seconds: 12488958.571\n",
      "epoch: 640600, train loss: 4.0500874519348145, val loss: 4.067079639434814, ETA in seconds: 12487318.830\n",
      "epoch: 640700, train loss: 4.0461812019348145, val loss: 4.069032764434814, ETA in seconds: 12485756.675\n",
      "epoch: 640800, train loss: 4.056728076934815, val loss: 4.066103076934814, ETA in seconds: 12484180.402\n",
      "epoch: 640900, train loss: 4.0510640144348145, val loss: 4.0715718269348145, ETA in seconds: 12482604.752\n",
      "epoch: 641000, train loss: 4.0559468269348145, val loss: 4.065321826934815, ETA in seconds: 12481069.052\n",
      "epoch: 641100, train loss: 4.055751514434815, val loss: 4.0686421394348145, ETA in seconds: 12479432.489\n",
      "epoch: 641200, train loss: 4.045595264434814, val loss: 4.060634326934815, ETA in seconds: 12477892.994\n",
      "epoch: 641300, train loss: 4.051845264434815, val loss: 4.0637593269348145, ETA in seconds: 12476278.822\n",
      "epoch: 641400, train loss: 4.055165576934814, val loss: 4.0686421394348145, ETA in seconds: 12474671.407\n",
      "epoch: 641500, train loss: 4.049501514434814, val loss: 4.065517139434815, ETA in seconds: 12473077.081\n",
      "epoch: 641600, train loss: 4.056337451934814, val loss: 4.067860889434814, ETA in seconds: 12471539.520\n",
      "epoch: 641700, train loss: 4.049696826934815, val loss: 4.060048389434814, ETA in seconds: 12470001.162\n",
      "epoch: 641800, train loss: 4.045595264434814, val loss: 4.0627827644348145, ETA in seconds: 12468470.731\n",
      "epoch: 641900, train loss: 4.0559468269348145, val loss: 4.0637593269348145, ETA in seconds: 12466928.611\n",
      "epoch: 642000, train loss: 4.052431201934814, val loss: 4.0647358894348145, ETA in seconds: 12465378.009\n",
      "epoch: 642100, train loss: 4.0481343269348145, val loss: 4.071181201934815, ETA in seconds: 12463820.824\n",
      "epoch: 642200, train loss: 4.048329639434814, val loss: 4.069228076934815, ETA in seconds: 12462234.590\n",
      "epoch: 642300, train loss: 4.050282764434814, val loss: 4.066298389434815, ETA in seconds: 12460707.435\n",
      "epoch: 642400, train loss: 4.051259326934814, val loss: 4.064931201934814, ETA in seconds: 12459140.698\n",
      "epoch: 642500, train loss: 4.048720264434815, val loss: 4.062978076934814, ETA in seconds: 12457559.010\n",
      "epoch: 642600, train loss: 4.056532764434815, val loss: 4.067079639434814, ETA in seconds: 12455958.186\n",
      "epoch: 642700, train loss: 4.050868701934815, val loss: 4.068251514434815, ETA in seconds: 12454402.220\n",
      "epoch: 642800, train loss: 4.059657764434815, val loss: 4.070009326934814, ETA in seconds: 12452807.446\n",
      "epoch: 642900, train loss: 4.051454639434814, val loss: 4.070009326934814, ETA in seconds: 12451216.556\n",
      "epoch: 643000, train loss: 4.0481343269348145, val loss: 4.068837451934814, ETA in seconds: 12449622.975\n",
      "epoch: 643100, train loss: 4.046767139434815, val loss: 4.064345264434815, ETA in seconds: 12447999.417\n",
      "epoch: 643200, train loss: 4.048915576934815, val loss: 4.065321826934815, ETA in seconds: 12446409.792\n",
      "epoch: 643300, train loss: 4.051845264434815, val loss: 4.0647358894348145, ETA in seconds: 12444807.228\n",
      "epoch: 643400, train loss: 4.054189014434814, val loss: 4.066298389434815, ETA in seconds: 12443191.733\n",
      "epoch: 643500, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 12441604.914\n",
      "epoch: 643600, train loss: 4.050868701934815, val loss: 4.071376514434815, ETA in seconds: 12439957.614\n",
      "epoch: 643700, train loss: 4.041884326934815, val loss: 4.066298389434815, ETA in seconds: 12438369.171\n",
      "epoch: 643800, train loss: 4.049306201934814, val loss: 4.0745015144348145, ETA in seconds: 12436767.720\n",
      "epoch: 643900, train loss: 4.052821826934815, val loss: 4.072353076934815, ETA in seconds: 12435143.482\n",
      "epoch: 644000, train loss: 4.055360889434814, val loss: 4.0686421394348145, ETA in seconds: 12433534.730\n",
      "epoch: 644100, train loss: 4.0491108894348145, val loss: 4.0608296394348145, ETA in seconds: 12431951.279\n",
      "epoch: 644200, train loss: 4.051845264434815, val loss: 4.066103076934814, ETA in seconds: 12430379.323\n",
      "epoch: 644300, train loss: 4.056532764434815, val loss: 4.062001514434814, ETA in seconds: 12428737.585\n",
      "epoch: 644400, train loss: 4.050868701934815, val loss: 4.067860889434814, ETA in seconds: 12427171.945\n",
      "epoch: 644500, train loss: 4.046376514434814, val loss: 4.066298389434815, ETA in seconds: 12425590.059\n",
      "epoch: 644600, train loss: 4.050282764434814, val loss: 4.065321826934815, ETA in seconds: 12424034.713\n",
      "epoch: 644700, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 12422440.646\n",
      "epoch: 644800, train loss: 4.045399951934814, val loss: 4.066298389434815, ETA in seconds: 12420851.907\n",
      "epoch: 644900, train loss: 4.054384326934814, val loss: 4.0745015144348145, ETA in seconds: 12419225.354\n",
      "epoch: 645000, train loss: 4.050478076934814, val loss: 4.073134326934815, ETA in seconds: 12417552.735\n",
      "epoch: 645100, train loss: 4.046962451934815, val loss: 4.063368701934815, ETA in seconds: 12415941.835\n",
      "epoch: 645200, train loss: 4.056142139434814, val loss: 4.0637593269348145, ETA in seconds: 12414322.688\n",
      "epoch: 645300, train loss: 4.0539937019348145, val loss: 4.065126514434814, ETA in seconds: 12412713.940\n",
      "epoch: 645400, train loss: 4.054579639434815, val loss: 4.065907764434814, ETA in seconds: 12411102.370\n",
      "epoch: 645500, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 12409517.977\n",
      "epoch: 645600, train loss: 4.054384326934814, val loss: 4.069814014434814, ETA in seconds: 12407894.529\n",
      "epoch: 645700, train loss: 4.053798389434815, val loss: 4.0686421394348145, ETA in seconds: 12406298.623\n",
      "epoch: 645800, train loss: 4.054189014434814, val loss: 4.063173389434814, ETA in seconds: 12404691.185\n",
      "epoch: 645900, train loss: 4.052821826934815, val loss: 4.062392139434815, ETA in seconds: 12403075.223\n",
      "epoch: 646000, train loss: 4.0530171394348145, val loss: 4.066298389434815, ETA in seconds: 12401460.335\n",
      "epoch: 646100, train loss: 4.048720264434815, val loss: 4.064540576934815, ETA in seconds: 12399827.663\n",
      "epoch: 646200, train loss: 4.048720264434815, val loss: 4.071181201934815, ETA in seconds: 12398235.229\n",
      "epoch: 646300, train loss: 4.052431201934814, val loss: 4.068837451934814, ETA in seconds: 12396589.240\n",
      "epoch: 646400, train loss: 4.048329639434814, val loss: 4.067274951934815, ETA in seconds: 12394957.221\n",
      "epoch: 646500, train loss: 4.052431201934814, val loss: 4.068446826934815, ETA in seconds: 12393347.497\n",
      "epoch: 646600, train loss: 4.049306201934814, val loss: 4.070985889434814, ETA in seconds: 12391682.300\n",
      "epoch: 646700, train loss: 4.048915576934815, val loss: 4.066103076934814, ETA in seconds: 12390173.285\n",
      "epoch: 646800, train loss: 4.046767139434815, val loss: 4.066298389434815, ETA in seconds: 12388704.162\n",
      "epoch: 646900, train loss: 4.0520405769348145, val loss: 4.070204639434815, ETA in seconds: 12387115.988\n",
      "epoch: 647000, train loss: 4.047353076934814, val loss: 4.068056201934814, ETA in seconds: 12385629.468\n",
      "epoch: 647100, train loss: 4.0491108894348145, val loss: 4.061610889434815, ETA in seconds: 12384104.430\n",
      "epoch: 647200, train loss: 4.049696826934815, val loss: 4.068446826934815, ETA in seconds: 12382534.677\n",
      "epoch: 647300, train loss: 4.039735889434814, val loss: 4.070204639434815, ETA in seconds: 12380898.012\n",
      "epoch: 647400, train loss: 4.057704639434815, val loss: 4.0676655769348145, ETA in seconds: 12379273.423\n",
      "epoch: 647500, train loss: 4.055165576934814, val loss: 4.067470264434815, ETA in seconds: 12377631.499\n",
      "epoch: 647600, train loss: 4.052821826934815, val loss: 4.069423389434815, ETA in seconds: 12375989.881\n",
      "epoch: 647700, train loss: 4.052235889434814, val loss: 4.069423389434815, ETA in seconds: 12374327.655\n",
      "epoch: 647800, train loss: 4.0471577644348145, val loss: 4.069032764434814, ETA in seconds: 12372709.144\n",
      "epoch: 647900, train loss: 4.049892139434815, val loss: 4.071181201934815, ETA in seconds: 12371047.513\n",
      "epoch: 648000, train loss: 4.046376514434814, val loss: 4.075087451934815, ETA in seconds: 12369425.122\n",
      "epoch: 648100, train loss: 4.048915576934815, val loss: 4.066298389434815, ETA in seconds: 12367776.425\n",
      "epoch: 648200, train loss: 4.042665576934814, val loss: 4.064931201934814, ETA in seconds: 12366168.471\n",
      "epoch: 648300, train loss: 4.046767139434815, val loss: 4.070985889434814, ETA in seconds: 12364537.997\n",
      "epoch: 648400, train loss: 4.0608296394348145, val loss: 4.062978076934814, ETA in seconds: 12362883.870\n",
      "epoch: 648500, train loss: 4.045790576934815, val loss: 4.070204639434815, ETA in seconds: 12361189.787\n",
      "epoch: 648600, train loss: 4.048720264434815, val loss: 4.072353076934815, ETA in seconds: 12359511.965\n",
      "epoch: 648700, train loss: 4.051845264434815, val loss: 4.0686421394348145, ETA in seconds: 12357905.626\n",
      "epoch: 648800, train loss: 4.054774951934815, val loss: 4.070985889434814, ETA in seconds: 12356244.216\n",
      "epoch: 648900, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 12354608.597\n",
      "epoch: 649000, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 12352981.014\n",
      "epoch: 649100, train loss: 4.053407764434814, val loss: 4.064149951934814, ETA in seconds: 12351324.290\n",
      "epoch: 649200, train loss: 4.055751514434815, val loss: 4.067079639434814, ETA in seconds: 12349624.833\n",
      "epoch: 649300, train loss: 4.0510640144348145, val loss: 4.068446826934815, ETA in seconds: 12347999.187\n",
      "epoch: 649400, train loss: 4.051649951934815, val loss: 4.072353076934815, ETA in seconds: 12346340.486\n",
      "epoch: 649500, train loss: 4.049696826934815, val loss: 4.065321826934815, ETA in seconds: 12344638.640\n",
      "epoch: 649600, train loss: 4.051845264434815, val loss: 4.067470264434815, ETA in seconds: 12342957.088\n",
      "epoch: 649700, train loss: 4.052431201934814, val loss: 4.072353076934815, ETA in seconds: 12341310.020\n",
      "epoch: 649800, train loss: 4.0530171394348145, val loss: 4.068446826934815, ETA in seconds: 12339623.292\n",
      "epoch: 649900, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 12337974.040\n",
      "epoch: 650000, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 12336287.227\n",
      "epoch: 650100, train loss: 4.046376514434814, val loss: 4.0618062019348145, ETA in seconds: 12334603.414\n",
      "epoch: 650200, train loss: 4.049306201934814, val loss: 4.069228076934815, ETA in seconds: 12332947.121\n",
      "epoch: 650300, train loss: 4.049696826934815, val loss: 4.0657124519348145, ETA in seconds: 12331388.033\n",
      "epoch: 650400, train loss: 4.055165576934814, val loss: 4.063368701934815, ETA in seconds: 12329737.536\n",
      "epoch: 650500, train loss: 4.056728076934815, val loss: 4.0696187019348145, ETA in seconds: 12328071.989\n",
      "epoch: 650600, train loss: 4.047548389434814, val loss: 4.065517139434815, ETA in seconds: 12326381.886\n",
      "epoch: 650700, train loss: 4.0471577644348145, val loss: 4.069814014434814, ETA in seconds: 12324729.664\n",
      "epoch: 650800, train loss: 4.053603076934815, val loss: 4.0676655769348145, ETA in seconds: 12323056.726\n",
      "epoch: 650900, train loss: 4.047353076934814, val loss: 4.064149951934814, ETA in seconds: 12321414.865\n",
      "epoch: 651000, train loss: 4.0569233894348145, val loss: 4.067860889434814, ETA in seconds: 12319753.128\n",
      "epoch: 651100, train loss: 4.046376514434814, val loss: 4.067079639434814, ETA in seconds: 12318079.860\n",
      "epoch: 651200, train loss: 4.057704639434815, val loss: 4.060439014434815, ETA in seconds: 12316498.165\n",
      "epoch: 651300, train loss: 4.053798389434815, val loss: 4.066884326934814, ETA in seconds: 12314831.706\n",
      "epoch: 651400, train loss: 4.050282764434814, val loss: 4.065321826934815, ETA in seconds: 12313162.725\n",
      "epoch: 651500, train loss: 4.045790576934815, val loss: 4.064149951934814, ETA in seconds: 12311484.573\n",
      "epoch: 651600, train loss: 4.054774951934815, val loss: 4.0676655769348145, ETA in seconds: 12309772.297\n",
      "epoch: 651700, train loss: 4.049306201934814, val loss: 4.0725483894348145, ETA in seconds: 12308046.385\n",
      "epoch: 651800, train loss: 4.055751514434815, val loss: 4.069228076934815, ETA in seconds: 12306337.510\n",
      "epoch: 651900, train loss: 4.054384326934814, val loss: 4.079189014434815, ETA in seconds: 12304658.170\n",
      "epoch: 652000, train loss: 4.050868701934815, val loss: 4.0705952644348145, ETA in seconds: 12302979.384\n",
      "epoch: 652100, train loss: 4.049892139434815, val loss: 4.067274951934815, ETA in seconds: 12301302.343\n",
      "epoch: 652200, train loss: 4.046571826934814, val loss: 4.0676655769348145, ETA in seconds: 12299642.448\n",
      "epoch: 652300, train loss: 4.0549702644348145, val loss: 4.070009326934814, ETA in seconds: 12297935.740\n",
      "epoch: 652400, train loss: 4.053798389434815, val loss: 4.059462451934815, ETA in seconds: 12296274.964\n",
      "epoch: 652500, train loss: 4.058095264434814, val loss: 4.070204639434815, ETA in seconds: 12294712.182\n",
      "epoch: 652600, train loss: 4.055360889434814, val loss: 4.064345264434815, ETA in seconds: 12293171.566\n",
      "epoch: 652700, train loss: 4.053212451934814, val loss: 4.0618062019348145, ETA in seconds: 12291575.345\n",
      "epoch: 652800, train loss: 4.055556201934815, val loss: 4.069228076934815, ETA in seconds: 12290008.546\n",
      "epoch: 652900, train loss: 4.053603076934815, val loss: 4.067470264434815, ETA in seconds: 12288450.871\n",
      "epoch: 653000, train loss: 4.052626514434815, val loss: 4.068251514434815, ETA in seconds: 12286720.589\n",
      "epoch: 653100, train loss: 4.0520405769348145, val loss: 4.062001514434814, ETA in seconds: 12285127.079\n",
      "epoch: 653200, train loss: 4.054579639434815, val loss: 4.0686421394348145, ETA in seconds: 12283538.738\n",
      "epoch: 653300, train loss: 4.052821826934815, val loss: 4.070790576934814, ETA in seconds: 12281874.680\n",
      "epoch: 653400, train loss: 4.060439014434815, val loss: 4.064931201934814, ETA in seconds: 12280153.241\n",
      "epoch: 653500, train loss: 4.053212451934814, val loss: 4.065517139434815, ETA in seconds: 12278406.188\n",
      "epoch: 653600, train loss: 4.053798389434815, val loss: 4.063954639434814, ETA in seconds: 12276716.748\n",
      "epoch: 653700, train loss: 4.050868701934815, val loss: 4.0696187019348145, ETA in seconds: 12275028.392\n",
      "epoch: 653800, train loss: 4.0500874519348145, val loss: 4.067860889434814, ETA in seconds: 12273531.419\n",
      "epoch: 653900, train loss: 4.051454639434814, val loss: 4.068056201934814, ETA in seconds: 12272053.315\n",
      "epoch: 654000, train loss: 4.056142139434814, val loss: 4.070009326934814, ETA in seconds: 12270437.749\n",
      "epoch: 654100, train loss: 4.046767139434815, val loss: 4.066103076934814, ETA in seconds: 12268702.087\n",
      "epoch: 654200, train loss: 4.052821826934815, val loss: 4.070009326934814, ETA in seconds: 12266969.936\n",
      "epoch: 654300, train loss: 4.047939014434815, val loss: 4.068251514434815, ETA in seconds: 12265267.675\n",
      "epoch: 654400, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 12263556.900\n",
      "epoch: 654500, train loss: 4.056142139434814, val loss: 4.068837451934814, ETA in seconds: 12261842.788\n",
      "epoch: 654600, train loss: 4.048524951934814, val loss: 4.067470264434815, ETA in seconds: 12260142.005\n",
      "epoch: 654700, train loss: 4.057118701934814, val loss: 4.066884326934814, ETA in seconds: 12258393.333\n",
      "epoch: 654800, train loss: 4.049306201934814, val loss: 4.068446826934815, ETA in seconds: 12256671.722\n",
      "epoch: 654900, train loss: 4.049501514434814, val loss: 4.065517139434815, ETA in seconds: 12254945.873\n",
      "epoch: 655000, train loss: 4.049306201934814, val loss: 4.068251514434815, ETA in seconds: 12253205.074\n",
      "epoch: 655100, train loss: 4.052235889434814, val loss: 4.062001514434814, ETA in seconds: 12251465.345\n",
      "epoch: 655200, train loss: 4.054384326934814, val loss: 4.0647358894348145, ETA in seconds: 12249740.232\n",
      "epoch: 655300, train loss: 4.0520405769348145, val loss: 4.0637593269348145, ETA in seconds: 12247977.332\n",
      "epoch: 655400, train loss: 4.051845264434815, val loss: 4.065321826934815, ETA in seconds: 12246256.978\n",
      "epoch: 655500, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 12244548.221\n",
      "epoch: 655600, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 12242924.730\n",
      "epoch: 655700, train loss: 4.048915576934815, val loss: 4.064345264434815, ETA in seconds: 12241248.799\n",
      "epoch: 655800, train loss: 4.055360889434814, val loss: 4.064540576934815, ETA in seconds: 12239491.208\n",
      "epoch: 655900, train loss: 4.053798389434815, val loss: 4.0705952644348145, ETA in seconds: 12237737.734\n",
      "epoch: 656000, train loss: 4.050478076934814, val loss: 4.071181201934815, ETA in seconds: 12235996.056\n",
      "epoch: 656100, train loss: 4.055165576934814, val loss: 4.068056201934814, ETA in seconds: 12234245.801\n",
      "epoch: 656200, train loss: 4.047548389434814, val loss: 4.0745015144348145, ETA in seconds: 12232509.272\n",
      "epoch: 656300, train loss: 4.045985889434815, val loss: 4.069032764434814, ETA in seconds: 12230972.518\n",
      "epoch: 656400, train loss: 4.050478076934814, val loss: 4.063173389434814, ETA in seconds: 12229364.116\n",
      "epoch: 656500, train loss: 4.047353076934814, val loss: 4.0637593269348145, ETA in seconds: 12227789.653\n",
      "epoch: 656600, train loss: 4.046571826934814, val loss: 4.0627827644348145, ETA in seconds: 12226184.817\n",
      "epoch: 656700, train loss: 4.053798389434815, val loss: 4.069423389434815, ETA in seconds: 12224589.019\n",
      "epoch: 656800, train loss: 4.054774951934815, val loss: 4.073720264434814, ETA in seconds: 12222952.379\n",
      "epoch: 656900, train loss: 4.053798389434815, val loss: 4.070790576934814, ETA in seconds: 12221217.470\n",
      "epoch: 657000, train loss: 4.0500874519348145, val loss: 4.073329639434815, ETA in seconds: 12219430.042\n",
      "epoch: 657100, train loss: 4.053407764434814, val loss: 4.0657124519348145, ETA in seconds: 12217689.030\n",
      "epoch: 657200, train loss: 4.063564014434815, val loss: 4.0647358894348145, ETA in seconds: 12215933.806\n",
      "epoch: 657300, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 12214215.221\n",
      "epoch: 657400, train loss: 4.056728076934815, val loss: 4.0666890144348145, ETA in seconds: 12212423.565\n",
      "epoch: 657500, train loss: 4.052821826934815, val loss: 4.061415576934815, ETA in seconds: 12210685.966\n",
      "epoch: 657600, train loss: 4.047743701934815, val loss: 4.0735249519348145, ETA in seconds: 12208931.504\n",
      "epoch: 657700, train loss: 4.0491108894348145, val loss: 4.070985889434814, ETA in seconds: 12207188.815\n",
      "epoch: 657800, train loss: 4.056142139434814, val loss: 4.062196826934814, ETA in seconds: 12205436.373\n",
      "epoch: 657900, train loss: 4.0510640144348145, val loss: 4.0637593269348145, ETA in seconds: 12203706.563\n",
      "epoch: 658000, train loss: 4.048524951934814, val loss: 4.070399951934815, ETA in seconds: 12201972.268\n",
      "epoch: 658100, train loss: 4.051845264434815, val loss: 4.067860889434814, ETA in seconds: 12200164.850\n",
      "epoch: 658200, train loss: 4.048720264434815, val loss: 4.070985889434814, ETA in seconds: 12198379.128\n",
      "epoch: 658300, train loss: 4.059657764434815, val loss: 4.069032764434814, ETA in seconds: 12196593.099\n",
      "epoch: 658400, train loss: 4.048720264434815, val loss: 4.070790576934814, ETA in seconds: 12194821.700\n",
      "epoch: 658500, train loss: 4.056337451934814, val loss: 4.069814014434814, ETA in seconds: 12193070.384\n",
      "epoch: 658600, train loss: 4.052626514434815, val loss: 4.064345264434815, ETA in seconds: 12191319.970\n",
      "epoch: 658700, train loss: 4.051845264434815, val loss: 4.061415576934815, ETA in seconds: 12189543.444\n",
      "epoch: 658800, train loss: 4.054189014434814, val loss: 4.0666890144348145, ETA in seconds: 12187792.762\n",
      "epoch: 658900, train loss: 4.054384326934814, val loss: 4.067079639434814, ETA in seconds: 12185999.622\n",
      "epoch: 659000, train loss: 4.0520405769348145, val loss: 4.071767139434814, ETA in seconds: 12184233.349\n",
      "epoch: 659100, train loss: 4.053603076934815, val loss: 4.066298389434815, ETA in seconds: 12182460.543\n",
      "epoch: 659200, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 12180676.990\n",
      "epoch: 659300, train loss: 4.049306201934814, val loss: 4.067079639434814, ETA in seconds: 12178907.427\n",
      "epoch: 659400, train loss: 4.054189014434814, val loss: 4.067274951934815, ETA in seconds: 12177135.353\n",
      "epoch: 659500, train loss: 4.0491108894348145, val loss: 4.064345264434815, ETA in seconds: 12175375.527\n",
      "epoch: 659600, train loss: 4.050673389434815, val loss: 4.0627827644348145, ETA in seconds: 12173577.638\n",
      "epoch: 659700, train loss: 4.057509326934815, val loss: 4.0666890144348145, ETA in seconds: 12171809.697\n",
      "epoch: 659800, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 12170040.874\n",
      "epoch: 659900, train loss: 4.049501514434814, val loss: 4.0657124519348145, ETA in seconds: 12168249.491\n",
      "epoch: 660000, train loss: 4.049696826934815, val loss: 4.063564014434815, ETA in seconds: 12166492.736\n",
      "epoch: 660100, train loss: 4.0500874519348145, val loss: 4.065907764434814, ETA in seconds: 12164726.565\n",
      "epoch: 660200, train loss: 4.050868701934815, val loss: 4.064931201934814, ETA in seconds: 12162902.896\n",
      "epoch: 660300, train loss: 4.054579639434815, val loss: 4.066493701934815, ETA in seconds: 12161141.681\n",
      "epoch: 660400, train loss: 4.053212451934814, val loss: 4.070790576934814, ETA in seconds: 12159360.280\n",
      "epoch: 660500, train loss: 4.0491108894348145, val loss: 4.066298389434815, ETA in seconds: 12157572.718\n",
      "epoch: 660600, train loss: 4.055556201934815, val loss: 4.065907764434814, ETA in seconds: 12155811.753\n",
      "epoch: 660700, train loss: 4.048915576934815, val loss: 4.064345264434815, ETA in seconds: 12153960.897\n",
      "epoch: 660800, train loss: 4.051649951934815, val loss: 4.074110889434815, ETA in seconds: 12152165.900\n",
      "epoch: 660900, train loss: 4.0530171394348145, val loss: 4.072157764434815, ETA in seconds: 12150323.321\n",
      "epoch: 661000, train loss: 4.047548389434814, val loss: 4.059267139434814, ETA in seconds: 12148535.929\n",
      "epoch: 661100, train loss: 4.047939014434815, val loss: 4.066103076934814, ETA in seconds: 12146723.193\n",
      "epoch: 661200, train loss: 4.051649951934815, val loss: 4.065517139434815, ETA in seconds: 12144919.328\n",
      "epoch: 661300, train loss: 4.053407764434814, val loss: 4.068056201934814, ETA in seconds: 12143152.386\n",
      "epoch: 661400, train loss: 4.046571826934814, val loss: 4.073720264434814, ETA in seconds: 12141344.622\n",
      "epoch: 661500, train loss: 4.046767139434815, val loss: 4.072157764434815, ETA in seconds: 12139542.391\n",
      "epoch: 661600, train loss: 4.057704639434815, val loss: 4.0666890144348145, ETA in seconds: 12137740.270\n",
      "epoch: 661700, train loss: 4.0530171394348145, val loss: 4.0686421394348145, ETA in seconds: 12135960.186\n",
      "epoch: 661800, train loss: 4.046767139434815, val loss: 4.0618062019348145, ETA in seconds: 12134145.177\n",
      "epoch: 661900, train loss: 4.0500874519348145, val loss: 4.064345264434815, ETA in seconds: 12132398.313\n",
      "epoch: 662000, train loss: 4.056532764434815, val loss: 4.066493701934815, ETA in seconds: 12130622.354\n",
      "epoch: 662100, train loss: 4.053407764434814, val loss: 4.071376514434815, ETA in seconds: 12128794.632\n",
      "epoch: 662200, train loss: 4.052626514434815, val loss: 4.072743701934814, ETA in seconds: 12126996.220\n",
      "epoch: 662300, train loss: 4.050282764434814, val loss: 4.071181201934815, ETA in seconds: 12125309.271\n",
      "epoch: 662400, train loss: 4.050673389434815, val loss: 4.067079639434814, ETA in seconds: 12123636.280\n",
      "epoch: 662500, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 12121835.689\n",
      "epoch: 662600, train loss: 4.050478076934814, val loss: 4.0686421394348145, ETA in seconds: 12120073.039\n",
      "epoch: 662700, train loss: 4.050478076934814, val loss: 4.0735249519348145, ETA in seconds: 12118322.170\n",
      "epoch: 662800, train loss: 4.0461812019348145, val loss: 4.067274951934815, ETA in seconds: 12116491.240\n",
      "epoch: 662900, train loss: 4.050282764434814, val loss: 4.065907764434814, ETA in seconds: 12114651.628\n",
      "epoch: 663000, train loss: 4.058290576934814, val loss: 4.0657124519348145, ETA in seconds: 12112820.972\n",
      "epoch: 663100, train loss: 4.054189014434814, val loss: 4.071181201934815, ETA in seconds: 12110981.337\n",
      "epoch: 663200, train loss: 4.043056201934815, val loss: 4.069032764434814, ETA in seconds: 12109131.825\n",
      "epoch: 663300, train loss: 4.048329639434814, val loss: 4.070790576934814, ETA in seconds: 12107310.708\n",
      "epoch: 663400, train loss: 4.053798389434815, val loss: 4.068056201934814, ETA in seconds: 12105458.314\n",
      "epoch: 663500, train loss: 4.056532764434815, val loss: 4.0676655769348145, ETA in seconds: 12103606.999\n",
      "epoch: 663600, train loss: 4.053407764434814, val loss: 4.0686421394348145, ETA in seconds: 12101734.996\n",
      "epoch: 663700, train loss: 4.0471577644348145, val loss: 4.069814014434814, ETA in seconds: 12099909.162\n",
      "epoch: 663800, train loss: 4.048720264434815, val loss: 4.0666890144348145, ETA in seconds: 12098101.688\n",
      "epoch: 663900, train loss: 4.052626514434815, val loss: 4.067079639434814, ETA in seconds: 12096259.073\n",
      "epoch: 664000, train loss: 4.056337451934814, val loss: 4.065907764434814, ETA in seconds: 12094439.440\n",
      "epoch: 664100, train loss: 4.060439014434815, val loss: 4.070399951934815, ETA in seconds: 12092621.795\n",
      "epoch: 664200, train loss: 4.050673389434815, val loss: 4.062392139434815, ETA in seconds: 12090823.096\n",
      "epoch: 664300, train loss: 4.0510640144348145, val loss: 4.0666890144348145, ETA in seconds: 12089119.799\n",
      "epoch: 664400, train loss: 4.047353076934814, val loss: 4.0657124519348145, ETA in seconds: 12087474.458\n",
      "epoch: 664500, train loss: 4.0520405769348145, val loss: 4.073915576934814, ETA in seconds: 12085738.465\n",
      "epoch: 664600, train loss: 4.052821826934815, val loss: 4.066884326934814, ETA in seconds: 12083905.267\n",
      "epoch: 664700, train loss: 4.051454639434814, val loss: 4.069423389434815, ETA in seconds: 12082090.439\n",
      "epoch: 664800, train loss: 4.058290576934814, val loss: 4.071376514434815, ETA in seconds: 12080314.923\n",
      "epoch: 664900, train loss: 4.0471577644348145, val loss: 4.066103076934814, ETA in seconds: 12078514.004\n",
      "epoch: 665000, train loss: 4.055360889434814, val loss: 4.068251514434815, ETA in seconds: 12076714.325\n",
      "epoch: 665100, train loss: 4.052235889434814, val loss: 4.059071826934814, ETA in seconds: 12074921.539\n",
      "epoch: 665200, train loss: 4.0520405769348145, val loss: 4.067860889434814, ETA in seconds: 12073104.858\n",
      "epoch: 665300, train loss: 4.049501514434814, val loss: 4.068056201934814, ETA in seconds: 12071273.975\n",
      "epoch: 665400, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 12069468.546\n",
      "epoch: 665500, train loss: 4.051649951934815, val loss: 4.065907764434814, ETA in seconds: 12067624.731\n",
      "epoch: 665600, train loss: 4.050868701934815, val loss: 4.072939014434814, ETA in seconds: 12065790.470\n",
      "epoch: 665700, train loss: 4.050478076934814, val loss: 4.064149951934814, ETA in seconds: 12063985.454\n",
      "epoch: 665800, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 12062166.121\n",
      "epoch: 665900, train loss: 4.0539937019348145, val loss: 4.0637593269348145, ETA in seconds: 12060309.203\n",
      "epoch: 666000, train loss: 4.047353076934814, val loss: 4.0666890144348145, ETA in seconds: 12058450.298\n",
      "epoch: 666100, train loss: 4.056142139434814, val loss: 4.063564014434815, ETA in seconds: 12056531.150\n",
      "epoch: 666200, train loss: 4.049501514434814, val loss: 4.068446826934815, ETA in seconds: 12054702.049\n",
      "epoch: 666300, train loss: 4.054579639434815, val loss: 4.0735249519348145, ETA in seconds: 12052964.387\n",
      "epoch: 666400, train loss: 4.054189014434814, val loss: 4.067274951934815, ETA in seconds: 12051161.001\n",
      "epoch: 666500, train loss: 4.046962451934815, val loss: 4.067274951934815, ETA in seconds: 12049333.673\n",
      "epoch: 666600, train loss: 4.0471577644348145, val loss: 4.0696187019348145, ETA in seconds: 12047451.216\n",
      "epoch: 666700, train loss: 4.051845264434815, val loss: 4.065907764434814, ETA in seconds: 12045614.538\n",
      "epoch: 666800, train loss: 4.052821826934815, val loss: 4.0637593269348145, ETA in seconds: 12043779.560\n",
      "epoch: 666900, train loss: 4.047743701934815, val loss: 4.0657124519348145, ETA in seconds: 12041903.086\n",
      "epoch: 667000, train loss: 4.045985889434815, val loss: 4.067079639434814, ETA in seconds: 12040014.284\n",
      "epoch: 667100, train loss: 4.049306201934814, val loss: 4.068446826934815, ETA in seconds: 12038157.340\n",
      "epoch: 667200, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 12036308.444\n",
      "epoch: 667300, train loss: 4.056142139434814, val loss: 4.061610889434815, ETA in seconds: 12034440.314\n",
      "epoch: 667400, train loss: 4.055165576934814, val loss: 4.067274951934815, ETA in seconds: 12032587.795\n",
      "epoch: 667500, train loss: 4.058290576934814, val loss: 4.0676655769348145, ETA in seconds: 12030730.471\n",
      "epoch: 667600, train loss: 4.047548389434814, val loss: 4.0696187019348145, ETA in seconds: 12028891.365\n",
      "epoch: 667700, train loss: 4.050282764434814, val loss: 4.0686421394348145, ETA in seconds: 12027120.141\n",
      "epoch: 667800, train loss: 4.051845264434815, val loss: 4.067274951934815, ETA in seconds: 12025376.582\n",
      "epoch: 667900, train loss: 4.054774951934815, val loss: 4.072353076934815, ETA in seconds: 12023535.106\n",
      "epoch: 668000, train loss: 4.0500874519348145, val loss: 4.0686421394348145, ETA in seconds: 12021707.208\n",
      "epoch: 668100, train loss: 4.050282764434814, val loss: 4.071962451934814, ETA in seconds: 12019903.019\n",
      "epoch: 668200, train loss: 4.0500874519348145, val loss: 4.065517139434815, ETA in seconds: 12018043.103\n",
      "epoch: 668300, train loss: 4.049696826934815, val loss: 4.061610889434815, ETA in seconds: 12016160.718\n",
      "epoch: 668400, train loss: 4.0491108894348145, val loss: 4.066298389434815, ETA in seconds: 12014293.780\n",
      "epoch: 668500, train loss: 4.054384326934814, val loss: 4.066298389434815, ETA in seconds: 12012400.785\n",
      "epoch: 668600, train loss: 4.0520405769348145, val loss: 4.062978076934814, ETA in seconds: 12010550.066\n",
      "epoch: 668700, train loss: 4.048329639434814, val loss: 4.0676655769348145, ETA in seconds: 12008704.114\n",
      "epoch: 668800, train loss: 4.046571826934814, val loss: 4.072743701934814, ETA in seconds: 12006839.821\n",
      "epoch: 668900, train loss: 4.050868701934815, val loss: 4.065321826934815, ETA in seconds: 12005021.168\n",
      "epoch: 669000, train loss: 4.048524951934814, val loss: 4.068837451934814, ETA in seconds: 12003162.939\n",
      "epoch: 669100, train loss: 4.050673389434815, val loss: 4.065126514434814, ETA in seconds: 12001250.959\n",
      "epoch: 669200, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 11999379.161\n",
      "epoch: 669300, train loss: 4.051259326934814, val loss: 4.0676655769348145, ETA in seconds: 11997498.463\n",
      "epoch: 669400, train loss: 4.055556201934815, val loss: 4.065126514434814, ETA in seconds: 11995630.856\n",
      "epoch: 669500, train loss: 4.049306201934814, val loss: 4.067470264434815, ETA in seconds: 11993797.289\n",
      "epoch: 669600, train loss: 4.0578999519348145, val loss: 4.0735249519348145, ETA in seconds: 11991939.395\n",
      "epoch: 669700, train loss: 4.053798389434815, val loss: 4.068056201934814, ETA in seconds: 11990079.323\n",
      "epoch: 669800, train loss: 4.049892139434815, val loss: 4.0618062019348145, ETA in seconds: 11988225.913\n",
      "epoch: 669900, train loss: 4.049892139434815, val loss: 4.069423389434815, ETA in seconds: 11986553.096\n",
      "epoch: 670000, train loss: 4.051649951934815, val loss: 4.064540576934815, ETA in seconds: 11984701.209\n",
      "epoch: 670100, train loss: 4.056337451934814, val loss: 4.0676655769348145, ETA in seconds: 11982811.792\n",
      "epoch: 670200, train loss: 4.0520405769348145, val loss: 4.073329639434815, ETA in seconds: 11980945.660\n",
      "epoch: 670300, train loss: 4.0491108894348145, val loss: 4.067079639434814, ETA in seconds: 11979032.510\n",
      "epoch: 670400, train loss: 4.054189014434814, val loss: 4.071962451934814, ETA in seconds: 11977157.992\n",
      "epoch: 670500, train loss: 4.056532764434815, val loss: 4.069032764434814, ETA in seconds: 11975268.303\n",
      "epoch: 670600, train loss: 4.046376514434814, val loss: 4.064931201934814, ETA in seconds: 11973341.971\n",
      "epoch: 670700, train loss: 4.055165576934814, val loss: 4.064540576934815, ETA in seconds: 11971478.688\n",
      "epoch: 670800, train loss: 4.0491108894348145, val loss: 4.0657124519348145, ETA in seconds: 11969619.132\n",
      "epoch: 670900, train loss: 4.0491108894348145, val loss: 4.061610889434815, ETA in seconds: 11967762.514\n",
      "epoch: 671000, train loss: 4.060439014434815, val loss: 4.0676655769348145, ETA in seconds: 11965880.696\n",
      "epoch: 671100, train loss: 4.050868701934815, val loss: 4.071962451934814, ETA in seconds: 11964027.741\n",
      "epoch: 671200, train loss: 4.049892139434815, val loss: 4.072157764434815, ETA in seconds: 11962148.954\n",
      "epoch: 671300, train loss: 4.052431201934814, val loss: 4.0696187019348145, ETA in seconds: 11960312.811\n",
      "epoch: 671400, train loss: 4.053212451934814, val loss: 4.070009326934814, ETA in seconds: 11958425.473\n",
      "epoch: 671500, train loss: 4.056337451934814, val loss: 4.063954639434814, ETA in seconds: 11956575.241\n",
      "epoch: 671600, train loss: 4.0559468269348145, val loss: 4.067860889434814, ETA in seconds: 11954699.898\n",
      "epoch: 671700, train loss: 4.050868701934815, val loss: 4.0637593269348145, ETA in seconds: 11952845.291\n",
      "epoch: 671800, train loss: 4.056142139434814, val loss: 4.062392139434815, ETA in seconds: 11950987.297\n",
      "epoch: 671900, train loss: 4.0578999519348145, val loss: 4.062978076934814, ETA in seconds: 11949122.617\n",
      "epoch: 672000, train loss: 4.057509326934815, val loss: 4.0618062019348145, ETA in seconds: 11947285.914\n",
      "epoch: 672100, train loss: 4.056142139434814, val loss: 4.068251514434815, ETA in seconds: 11945443.863\n",
      "epoch: 672200, train loss: 4.054774951934815, val loss: 4.062392139434815, ETA in seconds: 11943563.981\n",
      "epoch: 672300, train loss: 4.051454639434814, val loss: 4.067274951934815, ETA in seconds: 11941783.142\n",
      "epoch: 672400, train loss: 4.053603076934815, val loss: 4.068446826934815, ETA in seconds: 11939923.098\n",
      "epoch: 672500, train loss: 4.052235889434814, val loss: 4.069423389434815, ETA in seconds: 11937995.479\n",
      "epoch: 672600, train loss: 4.049501514434814, val loss: 4.068446826934815, ETA in seconds: 11936061.350\n",
      "epoch: 672700, train loss: 4.0471577644348145, val loss: 4.069423389434815, ETA in seconds: 11934235.648\n",
      "epoch: 672800, train loss: 4.0608296394348145, val loss: 4.0627827644348145, ETA in seconds: 11932384.955\n",
      "epoch: 672900, train loss: 4.043056201934815, val loss: 4.0637593269348145, ETA in seconds: 11930479.417\n",
      "epoch: 673000, train loss: 4.053407764434814, val loss: 4.064345264434815, ETA in seconds: 11928590.516\n",
      "epoch: 673100, train loss: 4.0510640144348145, val loss: 4.072743701934814, ETA in seconds: 11926636.051\n",
      "epoch: 673200, train loss: 4.051845264434815, val loss: 4.069228076934815, ETA in seconds: 11924780.911\n",
      "epoch: 673300, train loss: 4.051259326934814, val loss: 4.070399951934815, ETA in seconds: 11922824.335\n",
      "epoch: 673400, train loss: 4.048720264434815, val loss: 4.0676655769348145, ETA in seconds: 11920869.439\n",
      "epoch: 673500, train loss: 4.048329639434814, val loss: 4.067274951934815, ETA in seconds: 11918950.925\n",
      "epoch: 673600, train loss: 4.050673389434815, val loss: 4.067079639434814, ETA in seconds: 11917023.722\n",
      "epoch: 673700, train loss: 4.0549702644348145, val loss: 4.066493701934815, ETA in seconds: 11915104.395\n",
      "epoch: 673800, train loss: 4.052821826934815, val loss: 4.066493701934815, ETA in seconds: 11913166.431\n",
      "epoch: 673900, train loss: 4.053798389434815, val loss: 4.065907764434814, ETA in seconds: 11911211.052\n",
      "epoch: 674000, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 11909256.188\n",
      "epoch: 674100, train loss: 4.053407764434814, val loss: 4.067274951934815, ETA in seconds: 11907311.690\n",
      "epoch: 674200, train loss: 4.055165576934814, val loss: 4.062587451934815, ETA in seconds: 11905384.538\n",
      "epoch: 674300, train loss: 4.058681201934815, val loss: 4.0637593269348145, ETA in seconds: 11903443.057\n",
      "epoch: 674400, train loss: 4.049501514434814, val loss: 4.066493701934815, ETA in seconds: 11901503.424\n",
      "epoch: 674500, train loss: 4.047743701934815, val loss: 4.061415576934815, ETA in seconds: 11899558.014\n",
      "epoch: 674600, train loss: 4.051454639434814, val loss: 4.064931201934814, ETA in seconds: 11897619.804\n",
      "epoch: 674700, train loss: 4.047743701934815, val loss: 4.0666890144348145, ETA in seconds: 11895677.612\n",
      "epoch: 674800, train loss: 4.055360889434814, val loss: 4.067274951934815, ETA in seconds: 11893793.272\n",
      "epoch: 674900, train loss: 4.051845264434815, val loss: 4.066884326934814, ETA in seconds: 11891968.563\n",
      "epoch: 675000, train loss: 4.049501514434814, val loss: 4.066884326934814, ETA in seconds: 11890017.428\n",
      "epoch: 675100, train loss: 4.050673389434815, val loss: 4.064345264434815, ETA in seconds: 11888055.415\n",
      "epoch: 675200, train loss: 4.053212451934814, val loss: 4.0657124519348145, ETA in seconds: 11886083.766\n",
      "epoch: 675300, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 11884114.009\n",
      "epoch: 675400, train loss: 4.0461812019348145, val loss: 4.063954639434814, ETA in seconds: 11882151.349\n",
      "epoch: 675500, train loss: 4.049696826934815, val loss: 4.073134326934815, ETA in seconds: 11880265.686\n",
      "epoch: 675600, train loss: 4.057704639434815, val loss: 4.062587451934815, ETA in seconds: 11878336.144\n",
      "epoch: 675700, train loss: 4.043837451934815, val loss: 4.067470264434815, ETA in seconds: 11876441.583\n",
      "epoch: 675800, train loss: 4.049696826934815, val loss: 4.066298389434815, ETA in seconds: 11874510.755\n",
      "epoch: 675900, train loss: 4.0530171394348145, val loss: 4.068446826934815, ETA in seconds: 11872590.214\n",
      "epoch: 676000, train loss: 4.0461812019348145, val loss: 4.067079639434814, ETA in seconds: 11870709.902\n",
      "epoch: 676100, train loss: 4.055360889434814, val loss: 4.068251514434815, ETA in seconds: 11868720.969\n",
      "epoch: 676200, train loss: 4.049892139434815, val loss: 4.069032764434814, ETA in seconds: 11866802.925\n",
      "epoch: 676300, train loss: 4.0491108894348145, val loss: 4.068056201934814, ETA in seconds: 11864838.612\n",
      "epoch: 676400, train loss: 4.055751514434815, val loss: 4.0715718269348145, ETA in seconds: 11862858.502\n",
      "epoch: 676500, train loss: 4.0500874519348145, val loss: 4.070399951934815, ETA in seconds: 11860878.911\n",
      "epoch: 676600, train loss: 4.055556201934815, val loss: 4.0705952644348145, ETA in seconds: 11858953.571\n",
      "epoch: 676700, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 11857018.362\n",
      "epoch: 676800, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 11854987.957\n",
      "epoch: 676900, train loss: 4.049306201934814, val loss: 4.0686421394348145, ETA in seconds: 11852994.903\n",
      "epoch: 677000, train loss: 4.046571826934814, val loss: 4.0647358894348145, ETA in seconds: 11851003.123\n",
      "epoch: 677100, train loss: 4.054579639434815, val loss: 4.066298389434815, ETA in seconds: 11849037.239\n",
      "epoch: 677200, train loss: 4.049306201934814, val loss: 4.069228076934815, ETA in seconds: 11847084.607\n",
      "epoch: 677300, train loss: 4.049696826934815, val loss: 4.066103076934814, ETA in seconds: 11845092.562\n",
      "epoch: 677400, train loss: 4.052235889434814, val loss: 4.064149951934814, ETA in seconds: 11843097.779\n",
      "epoch: 677500, train loss: 4.059462451934815, val loss: 4.064149951934814, ETA in seconds: 11841112.413\n",
      "epoch: 677600, train loss: 4.045985889434815, val loss: 4.069032764434814, ETA in seconds: 11839174.673\n",
      "epoch: 677700, train loss: 4.044814014434815, val loss: 4.068446826934815, ETA in seconds: 11837213.109\n",
      "epoch: 677800, train loss: 4.060439014434815, val loss: 4.069228076934815, ETA in seconds: 11835253.189\n",
      "epoch: 677900, train loss: 4.050478076934814, val loss: 4.062001514434814, ETA in seconds: 11833260.747\n",
      "epoch: 678000, train loss: 4.048720264434815, val loss: 4.0696187019348145, ETA in seconds: 11831263.208\n",
      "epoch: 678100, train loss: 4.051259326934814, val loss: 4.0637593269348145, ETA in seconds: 11829336.780\n",
      "epoch: 678200, train loss: 4.0471577644348145, val loss: 4.073915576934814, ETA in seconds: 11827359.683\n",
      "epoch: 678300, train loss: 4.0530171394348145, val loss: 4.0686421394348145, ETA in seconds: 11825392.059\n",
      "epoch: 678400, train loss: 4.052626514434815, val loss: 4.062001514434814, ETA in seconds: 11823383.129\n",
      "epoch: 678500, train loss: 4.0491108894348145, val loss: 4.065321826934815, ETA in seconds: 11821444.267\n",
      "epoch: 678600, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 11819425.003\n",
      "epoch: 678700, train loss: 4.0520405769348145, val loss: 4.068056201934814, ETA in seconds: 11817453.045\n",
      "epoch: 678800, train loss: 4.047743701934815, val loss: 4.069814014434814, ETA in seconds: 11815461.637\n",
      "epoch: 678900, train loss: 4.049306201934814, val loss: 4.062392139434815, ETA in seconds: 11813468.760\n",
      "epoch: 679000, train loss: 4.053407764434814, val loss: 4.064149951934814, ETA in seconds: 11811494.778\n",
      "epoch: 679100, train loss: 4.052235889434814, val loss: 4.069423389434815, ETA in seconds: 11809505.369\n",
      "epoch: 679200, train loss: 4.052626514434815, val loss: 4.071767139434814, ETA in seconds: 11807517.917\n",
      "epoch: 679300, train loss: 4.0510640144348145, val loss: 4.069814014434814, ETA in seconds: 11805508.149\n",
      "epoch: 679400, train loss: 4.045595264434814, val loss: 4.063564014434815, ETA in seconds: 11803497.963\n",
      "epoch: 679500, train loss: 4.052431201934814, val loss: 4.075087451934815, ETA in seconds: 11801519.864\n",
      "epoch: 679600, train loss: 4.0578999519348145, val loss: 4.071962451934814, ETA in seconds: 11799520.429\n",
      "epoch: 679700, train loss: 4.049306201934814, val loss: 4.069423389434815, ETA in seconds: 11797535.129\n",
      "epoch: 679800, train loss: 4.049306201934814, val loss: 4.0686421394348145, ETA in seconds: 11795581.144\n",
      "epoch: 679900, train loss: 4.057314014434814, val loss: 4.069423389434815, ETA in seconds: 11793587.776\n",
      "epoch: 680000, train loss: 4.0539937019348145, val loss: 4.068837451934814, ETA in seconds: 11791599.100\n",
      "epoch: 680100, train loss: 4.054384326934814, val loss: 4.0686421394348145, ETA in seconds: 11789610.043\n",
      "epoch: 680200, train loss: 4.051845264434815, val loss: 4.067274951934815, ETA in seconds: 11787624.089\n",
      "epoch: 680300, train loss: 4.058681201934815, val loss: 4.063173389434814, ETA in seconds: 11785632.852\n",
      "epoch: 680400, train loss: 4.049501514434814, val loss: 4.068837451934814, ETA in seconds: 11783625.914\n",
      "epoch: 680500, train loss: 4.057118701934814, val loss: 4.0647358894348145, ETA in seconds: 11781700.647\n",
      "epoch: 680600, train loss: 4.054774951934815, val loss: 4.069032764434814, ETA in seconds: 11779688.986\n",
      "epoch: 680700, train loss: 4.0549702644348145, val loss: 4.0676655769348145, ETA in seconds: 11777699.070\n",
      "epoch: 680800, train loss: 4.047939014434815, val loss: 4.063368701934815, ETA in seconds: 11775740.445\n",
      "epoch: 680900, train loss: 4.0510640144348145, val loss: 4.070985889434814, ETA in seconds: 11773757.204\n",
      "epoch: 681000, train loss: 4.053212451934814, val loss: 4.069032764434814, ETA in seconds: 11771759.647\n",
      "epoch: 681100, train loss: 4.046376514434814, val loss: 4.067079639434814, ETA in seconds: 11769754.318\n",
      "epoch: 681200, train loss: 4.0481343269348145, val loss: 4.064540576934815, ETA in seconds: 11767753.395\n",
      "epoch: 681300, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 11765729.682\n",
      "epoch: 681400, train loss: 4.055165576934814, val loss: 4.0627827644348145, ETA in seconds: 11763704.485\n",
      "epoch: 681500, train loss: 4.052431201934814, val loss: 4.071767139434814, ETA in seconds: 11761696.940\n",
      "epoch: 681600, train loss: 4.053407764434814, val loss: 4.0696187019348145, ETA in seconds: 11759646.038\n",
      "epoch: 681700, train loss: 4.048524951934814, val loss: 4.0666890144348145, ETA in seconds: 11757702.672\n",
      "epoch: 681800, train loss: 4.044618701934814, val loss: 4.0647358894348145, ETA in seconds: 11755822.105\n",
      "epoch: 681900, train loss: 4.049892139434815, val loss: 4.069032764434814, ETA in seconds: 11753811.210\n",
      "epoch: 682000, train loss: 4.051454639434814, val loss: 4.0676655769348145, ETA in seconds: 11751791.024\n",
      "epoch: 682100, train loss: 4.060439014434815, val loss: 4.065517139434815, ETA in seconds: 11749750.406\n",
      "epoch: 682200, train loss: 4.055165576934814, val loss: 4.067470264434815, ETA in seconds: 11747736.059\n",
      "epoch: 682300, train loss: 4.050868701934815, val loss: 4.063954639434814, ETA in seconds: 11745709.802\n",
      "epoch: 682400, train loss: 4.056728076934815, val loss: 4.068446826934815, ETA in seconds: 11743726.346\n",
      "epoch: 682500, train loss: 4.057704639434815, val loss: 4.061610889434815, ETA in seconds: 11741698.339\n",
      "epoch: 682600, train loss: 4.053798389434815, val loss: 4.067860889434814, ETA in seconds: 11739663.701\n",
      "epoch: 682700, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 11737625.422\n",
      "epoch: 682800, train loss: 4.052821826934815, val loss: 4.063564014434815, ETA in seconds: 11735609.149\n",
      "epoch: 682900, train loss: 4.053798389434815, val loss: 4.061024951934814, ETA in seconds: 11733602.829\n",
      "epoch: 683000, train loss: 4.049696826934815, val loss: 4.063173389434814, ETA in seconds: 11731571.143\n",
      "epoch: 683100, train loss: 4.053603076934815, val loss: 4.068056201934814, ETA in seconds: 11729436.578\n",
      "epoch: 683200, train loss: 4.051845264434815, val loss: 4.069423389434815, ETA in seconds: 11727255.481\n",
      "epoch: 683300, train loss: 4.053407764434814, val loss: 4.0666890144348145, ETA in seconds: 11725211.664\n",
      "epoch: 683400, train loss: 4.045985889434815, val loss: 4.067860889434814, ETA in seconds: 11723130.042\n",
      "epoch: 683500, train loss: 4.056337451934814, val loss: 4.0647358894348145, ETA in seconds: 11721069.280\n",
      "epoch: 683600, train loss: 4.048915576934815, val loss: 4.062196826934814, ETA in seconds: 11719017.393\n",
      "epoch: 683700, train loss: 4.0618062019348145, val loss: 4.069423389434815, ETA in seconds: 11716971.072\n",
      "epoch: 683800, train loss: 4.0569233894348145, val loss: 4.0715718269348145, ETA in seconds: 11714953.284\n",
      "epoch: 683900, train loss: 4.0471577644348145, val loss: 4.0647358894348145, ETA in seconds: 11712898.072\n",
      "epoch: 684000, train loss: 4.050282764434814, val loss: 4.069228076934815, ETA in seconds: 11710834.065\n",
      "epoch: 684100, train loss: 4.051259326934814, val loss: 4.068251514434815, ETA in seconds: 11708810.666\n",
      "epoch: 684200, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 11706777.628\n",
      "epoch: 684300, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 11704721.477\n",
      "epoch: 684400, train loss: 4.047353076934814, val loss: 4.066493701934815, ETA in seconds: 11702664.908\n",
      "epoch: 684500, train loss: 4.058095264434814, val loss: 4.070399951934815, ETA in seconds: 11700629.815\n",
      "epoch: 684600, train loss: 4.051649951934815, val loss: 4.070009326934814, ETA in seconds: 11698601.021\n",
      "epoch: 684700, train loss: 4.054384326934814, val loss: 4.069423389434815, ETA in seconds: 11696530.088\n",
      "epoch: 684800, train loss: 4.057118701934814, val loss: 4.069423389434815, ETA in seconds: 11694462.783\n",
      "epoch: 684900, train loss: 4.045009326934815, val loss: 4.071767139434814, ETA in seconds: 11692496.967\n",
      "epoch: 685000, train loss: 4.0500874519348145, val loss: 4.069032764434814, ETA in seconds: 11690645.298\n",
      "epoch: 685100, train loss: 4.052626514434815, val loss: 4.0725483894348145, ETA in seconds: 11688655.573\n",
      "epoch: 685200, train loss: 4.053798389434815, val loss: 4.064540576934815, ETA in seconds: 11686599.863\n",
      "epoch: 685300, train loss: 4.0510640144348145, val loss: 4.063954639434814, ETA in seconds: 11684535.610\n",
      "epoch: 685400, train loss: 4.0500874519348145, val loss: 4.0696187019348145, ETA in seconds: 11682461.280\n",
      "epoch: 685500, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 11680422.617\n",
      "epoch: 685600, train loss: 4.051454639434814, val loss: 4.0735249519348145, ETA in seconds: 11678435.191\n",
      "epoch: 685700, train loss: 4.053212451934814, val loss: 4.0686421394348145, ETA in seconds: 11676385.251\n",
      "epoch: 685800, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 11674370.341\n",
      "epoch: 685900, train loss: 4.054579639434815, val loss: 4.063954639434814, ETA in seconds: 11672385.491\n",
      "epoch: 686000, train loss: 4.048720264434815, val loss: 4.0657124519348145, ETA in seconds: 11670385.745\n",
      "epoch: 686100, train loss: 4.045790576934815, val loss: 4.0647358894348145, ETA in seconds: 11668314.271\n",
      "epoch: 686200, train loss: 4.052626514434815, val loss: 4.062196826934814, ETA in seconds: 11666356.823\n",
      "epoch: 686300, train loss: 4.0549702644348145, val loss: 4.067860889434814, ETA in seconds: 11664514.009\n",
      "epoch: 686400, train loss: 4.0461812019348145, val loss: 4.067860889434814, ETA in seconds: 11662690.340\n",
      "epoch: 686500, train loss: 4.056142139434814, val loss: 4.063173389434814, ETA in seconds: 11660811.027\n",
      "epoch: 686600, train loss: 4.050478076934814, val loss: 4.068251514434815, ETA in seconds: 11658741.558\n",
      "epoch: 686700, train loss: 4.049892139434815, val loss: 4.066103076934814, ETA in seconds: 11656669.077\n",
      "epoch: 686800, train loss: 4.053212451934814, val loss: 4.0686421394348145, ETA in seconds: 11654646.190\n",
      "epoch: 686900, train loss: 4.052235889434814, val loss: 4.0657124519348145, ETA in seconds: 11652644.038\n",
      "epoch: 687000, train loss: 4.052431201934814, val loss: 4.0666890144348145, ETA in seconds: 11650626.719\n",
      "epoch: 687100, train loss: 4.0530171394348145, val loss: 4.068056201934814, ETA in seconds: 11648591.142\n",
      "epoch: 687200, train loss: 4.054384326934814, val loss: 4.0627827644348145, ETA in seconds: 11646555.455\n",
      "epoch: 687300, train loss: 4.047939014434815, val loss: 4.067860889434814, ETA in seconds: 11644452.055\n",
      "epoch: 687400, train loss: 4.054579639434815, val loss: 4.066493701934815, ETA in seconds: 11642386.891\n",
      "epoch: 687500, train loss: 4.054384326934814, val loss: 4.062392139434815, ETA in seconds: 11640327.854\n",
      "epoch: 687600, train loss: 4.055556201934815, val loss: 4.0666890144348145, ETA in seconds: 11638274.820\n",
      "epoch: 687700, train loss: 4.047939014434815, val loss: 4.066493701934815, ETA in seconds: 11636200.502\n",
      "epoch: 687800, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 11634131.460\n",
      "epoch: 687900, train loss: 4.0510640144348145, val loss: 4.066493701934815, ETA in seconds: 11632171.442\n",
      "epoch: 688000, train loss: 4.053603076934815, val loss: 4.065126514434814, ETA in seconds: 11630221.830\n",
      "epoch: 688100, train loss: 4.049696826934815, val loss: 4.068837451934814, ETA in seconds: 11628270.508\n",
      "epoch: 688200, train loss: 4.0500874519348145, val loss: 4.0657124519348145, ETA in seconds: 11626254.665\n",
      "epoch: 688300, train loss: 4.051649951934815, val loss: 4.069032764434814, ETA in seconds: 11624238.673\n",
      "epoch: 688400, train loss: 4.0530171394348145, val loss: 4.0657124519348145, ETA in seconds: 11622149.509\n",
      "epoch: 688500, train loss: 4.0569233894348145, val loss: 4.069814014434814, ETA in seconds: 11620053.607\n",
      "epoch: 688600, train loss: 4.053603076934815, val loss: 4.0676655769348145, ETA in seconds: 11618016.895\n",
      "epoch: 688700, train loss: 4.056532764434815, val loss: 4.071962451934814, ETA in seconds: 11615979.885\n",
      "epoch: 688800, train loss: 4.050282764434814, val loss: 4.067860889434814, ETA in seconds: 11613883.989\n",
      "epoch: 688900, train loss: 4.053798389434815, val loss: 4.065907764434814, ETA in seconds: 11611799.834\n",
      "epoch: 689000, train loss: 4.052235889434814, val loss: 4.070399951934815, ETA in seconds: 11609724.603\n",
      "epoch: 689100, train loss: 4.048329639434814, val loss: 4.065517139434815, ETA in seconds: 11607675.573\n",
      "epoch: 689200, train loss: 4.054579639434815, val loss: 4.064931201934814, ETA in seconds: 11605642.766\n",
      "epoch: 689300, train loss: 4.053798389434815, val loss: 4.0725483894348145, ETA in seconds: 11603576.807\n",
      "epoch: 689400, train loss: 4.051454639434814, val loss: 4.069814014434814, ETA in seconds: 11601496.095\n",
      "epoch: 689500, train loss: 4.062196826934814, val loss: 4.067470264434815, ETA in seconds: 11599452.140\n",
      "epoch: 689600, train loss: 4.051649951934815, val loss: 4.067860889434814, ETA in seconds: 11597374.901\n",
      "epoch: 689700, train loss: 4.053603076934815, val loss: 4.063173389434814, ETA in seconds: 11595294.793\n",
      "epoch: 689800, train loss: 4.050478076934814, val loss: 4.066298389434815, ETA in seconds: 11593233.326\n",
      "epoch: 689900, train loss: 4.045399951934814, val loss: 4.068056201934814, ETA in seconds: 11591143.745\n",
      "epoch: 690000, train loss: 4.052235889434814, val loss: 4.065517139434815, ETA in seconds: 11589090.464\n",
      "epoch: 690100, train loss: 4.059071826934814, val loss: 4.061610889434815, ETA in seconds: 11587021.006\n",
      "epoch: 690200, train loss: 4.051454639434814, val loss: 4.072157764434815, ETA in seconds: 11584900.702\n",
      "epoch: 690300, train loss: 4.056337451934814, val loss: 4.073915576934814, ETA in seconds: 11582817.744\n",
      "epoch: 690400, train loss: 4.0559468269348145, val loss: 4.074892139434814, ETA in seconds: 11580726.742\n",
      "epoch: 690500, train loss: 4.057704639434815, val loss: 4.061415576934815, ETA in seconds: 11578627.742\n",
      "epoch: 690600, train loss: 4.046962451934815, val loss: 4.065517139434815, ETA in seconds: 11576509.770\n",
      "epoch: 690700, train loss: 4.058290576934814, val loss: 4.0627827644348145, ETA in seconds: 11574369.302\n",
      "epoch: 690800, train loss: 4.051649951934815, val loss: 4.068056201934814, ETA in seconds: 11572264.574\n",
      "epoch: 690900, train loss: 4.057118701934814, val loss: 4.064345264434815, ETA in seconds: 11570143.680\n",
      "epoch: 691000, train loss: 4.051845264434815, val loss: 4.066298389434815, ETA in seconds: 11568013.060\n",
      "epoch: 691100, train loss: 4.045595264434814, val loss: 4.067860889434814, ETA in seconds: 11565909.548\n",
      "epoch: 691200, train loss: 4.054774951934815, val loss: 4.0745015144348145, ETA in seconds: 11563796.655\n",
      "epoch: 691300, train loss: 4.054579639434815, val loss: 4.071376514434815, ETA in seconds: 11561699.634\n",
      "epoch: 691400, train loss: 4.0500874519348145, val loss: 4.065321826934815, ETA in seconds: 11559592.182\n",
      "epoch: 691500, train loss: 4.051454639434814, val loss: 4.071181201934815, ETA in seconds: 11557476.626\n",
      "epoch: 691600, train loss: 4.047939014434815, val loss: 4.069228076934815, ETA in seconds: 11555334.765\n",
      "epoch: 691700, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 11553133.359\n",
      "epoch: 691800, train loss: 4.0549702644348145, val loss: 4.071376514434815, ETA in seconds: 11551026.404\n",
      "epoch: 691900, train loss: 4.049696826934815, val loss: 4.064931201934814, ETA in seconds: 11548885.761\n",
      "epoch: 692000, train loss: 4.0598530769348145, val loss: 4.067470264434815, ETA in seconds: 11546802.743\n",
      "epoch: 692100, train loss: 4.054384326934814, val loss: 4.064149951934814, ETA in seconds: 11544714.079\n",
      "epoch: 692200, train loss: 4.0500874519348145, val loss: 4.061024951934814, ETA in seconds: 11542623.444\n",
      "epoch: 692300, train loss: 4.048915576934815, val loss: 4.065517139434815, ETA in seconds: 11540510.891\n",
      "epoch: 692400, train loss: 4.057704639434815, val loss: 4.068837451934814, ETA in seconds: 11538424.053\n",
      "epoch: 692500, train loss: 4.048915576934815, val loss: 4.066103076934814, ETA in seconds: 11536340.519\n",
      "epoch: 692600, train loss: 4.0500874519348145, val loss: 4.070790576934814, ETA in seconds: 11534240.738\n",
      "epoch: 692700, train loss: 4.052431201934814, val loss: 4.0657124519348145, ETA in seconds: 11532103.656\n",
      "epoch: 692800, train loss: 4.049696826934815, val loss: 4.067079639434814, ETA in seconds: 11529999.478\n",
      "epoch: 692900, train loss: 4.056337451934814, val loss: 4.069423389434815, ETA in seconds: 11527906.124\n",
      "epoch: 693000, train loss: 4.0510640144348145, val loss: 4.067274951934815, ETA in seconds: 11525807.522\n",
      "epoch: 693100, train loss: 4.0510640144348145, val loss: 4.075282764434815, ETA in seconds: 11523677.136\n",
      "epoch: 693200, train loss: 4.061610889434815, val loss: 4.068446826934815, ETA in seconds: 11521569.462\n",
      "epoch: 693300, train loss: 4.054774951934815, val loss: 4.064931201934814, ETA in seconds: 11519490.282\n",
      "epoch: 693400, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 11517376.289\n",
      "epoch: 693500, train loss: 4.0578999519348145, val loss: 4.065126514434814, ETA in seconds: 11515399.597\n",
      "epoch: 693600, train loss: 4.0549702644348145, val loss: 4.0647358894348145, ETA in seconds: 11513270.828\n",
      "epoch: 693700, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 11511130.178\n",
      "epoch: 693800, train loss: 4.051845264434815, val loss: 4.067470264434815, ETA in seconds: 11509013.784\n",
      "epoch: 693900, train loss: 4.049501514434814, val loss: 4.0705952644348145, ETA in seconds: 11506867.693\n",
      "epoch: 694000, train loss: 4.0481343269348145, val loss: 4.068446826934815, ETA in seconds: 11504746.021\n",
      "epoch: 694100, train loss: 4.059071826934814, val loss: 4.0686421394348145, ETA in seconds: 11502625.490\n",
      "epoch: 694200, train loss: 4.045399951934814, val loss: 4.067470264434815, ETA in seconds: 11500457.314\n",
      "epoch: 694300, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 11498384.700\n",
      "epoch: 694400, train loss: 4.048720264434815, val loss: 4.061024951934814, ETA in seconds: 11496228.425\n",
      "epoch: 694500, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 11494118.339\n",
      "epoch: 694600, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 11491987.696\n",
      "epoch: 694700, train loss: 4.051845264434815, val loss: 4.057509326934815, ETA in seconds: 11489849.222\n",
      "epoch: 694800, train loss: 4.055165576934814, val loss: 4.066298389434815, ETA in seconds: 11487672.796\n",
      "epoch: 694900, train loss: 4.056337451934814, val loss: 4.065321826934815, ETA in seconds: 11485525.788\n",
      "epoch: 695000, train loss: 4.056142139434814, val loss: 4.069228076934815, ETA in seconds: 11483352.748\n",
      "epoch: 695100, train loss: 4.047743701934815, val loss: 4.070985889434814, ETA in seconds: 11481229.410\n",
      "epoch: 695200, train loss: 4.048329639434814, val loss: 4.063954639434814, ETA in seconds: 11479107.846\n",
      "epoch: 695300, train loss: 4.047548389434814, val loss: 4.067860889434814, ETA in seconds: 11476998.752\n",
      "epoch: 695400, train loss: 4.055165576934814, val loss: 4.067079639434814, ETA in seconds: 11474889.674\n",
      "epoch: 695500, train loss: 4.052821826934815, val loss: 4.067470264434815, ETA in seconds: 11472778.470\n",
      "epoch: 695600, train loss: 4.056142139434814, val loss: 4.070204639434815, ETA in seconds: 11470688.507\n",
      "epoch: 695700, train loss: 4.049696826934815, val loss: 4.068056201934814, ETA in seconds: 11468672.155\n",
      "epoch: 695800, train loss: 4.0491108894348145, val loss: 4.0618062019348145, ETA in seconds: 11466532.237\n",
      "epoch: 695900, train loss: 4.050868701934815, val loss: 4.0715718269348145, ETA in seconds: 11464495.072\n",
      "epoch: 696000, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 11462422.598\n",
      "epoch: 696100, train loss: 4.051454639434814, val loss: 4.074306201934815, ETA in seconds: 11460300.699\n",
      "epoch: 696200, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 11458122.902\n",
      "epoch: 696300, train loss: 4.059462451934815, val loss: 4.066103076934814, ETA in seconds: 11455957.421\n",
      "epoch: 696400, train loss: 4.050282764434814, val loss: 4.065126514434814, ETA in seconds: 11453784.438\n",
      "epoch: 696500, train loss: 4.056728076934815, val loss: 4.067079639434814, ETA in seconds: 11451641.828\n",
      "epoch: 696600, train loss: 4.053212451934814, val loss: 4.059462451934815, ETA in seconds: 11449489.003\n",
      "epoch: 696700, train loss: 4.048524951934814, val loss: 4.066298389434815, ETA in seconds: 11447296.072\n",
      "epoch: 696800, train loss: 4.057704639434815, val loss: 4.066493701934815, ETA in seconds: 11445130.433\n",
      "epoch: 696900, train loss: 4.054774951934815, val loss: 4.0666890144348145, ETA in seconds: 11442950.219\n",
      "epoch: 697000, train loss: 4.0471577644348145, val loss: 4.066493701934815, ETA in seconds: 11440785.610\n",
      "epoch: 697100, train loss: 4.046376514434814, val loss: 4.060634326934815, ETA in seconds: 11438620.086\n",
      "epoch: 697200, train loss: 4.055751514434815, val loss: 4.065517139434815, ETA in seconds: 11436423.475\n",
      "epoch: 697300, train loss: 4.051454639434814, val loss: 4.064931201934814, ETA in seconds: 11434247.461\n",
      "epoch: 697400, train loss: 4.057509326934815, val loss: 4.0686421394348145, ETA in seconds: 11432076.115\n",
      "epoch: 697500, train loss: 4.055360889434814, val loss: 4.063564014434815, ETA in seconds: 11429908.332\n",
      "epoch: 697600, train loss: 4.056532764434815, val loss: 4.069032764434814, ETA in seconds: 11427748.025\n",
      "epoch: 697700, train loss: 4.0510640144348145, val loss: 4.066103076934814, ETA in seconds: 11425521.269\n",
      "epoch: 697800, train loss: 4.0510640144348145, val loss: 4.066298389434815, ETA in seconds: 11423259.761\n",
      "epoch: 697900, train loss: 4.053212451934814, val loss: 4.061220264434814, ETA in seconds: 11421007.830\n",
      "epoch: 698000, train loss: 4.049501514434814, val loss: 4.069814014434814, ETA in seconds: 11418733.434\n",
      "epoch: 698100, train loss: 4.051259326934814, val loss: 4.068056201934814, ETA in seconds: 11416542.970\n",
      "epoch: 698200, train loss: 4.053603076934815, val loss: 4.061024951934814, ETA in seconds: 11414260.716\n",
      "epoch: 698300, train loss: 4.041689014434814, val loss: 4.0725483894348145, ETA in seconds: 11412088.181\n",
      "epoch: 698400, train loss: 4.0491108894348145, val loss: 4.0705952644348145, ETA in seconds: 11409862.360\n",
      "epoch: 698500, train loss: 4.051649951934815, val loss: 4.065907764434814, ETA in seconds: 11407623.776\n",
      "epoch: 698600, train loss: 4.050282764434814, val loss: 4.0686421394348145, ETA in seconds: 11405372.187\n",
      "epoch: 698700, train loss: 4.051454639434814, val loss: 4.064540576934815, ETA in seconds: 11403134.953\n",
      "epoch: 698800, train loss: 4.0549702644348145, val loss: 4.071376514434815, ETA in seconds: 11400853.676\n",
      "epoch: 698900, train loss: 4.049892139434815, val loss: 4.069814014434814, ETA in seconds: 11398583.910\n",
      "epoch: 699000, train loss: 4.054189014434814, val loss: 4.066493701934815, ETA in seconds: 11396349.032\n",
      "epoch: 699100, train loss: 4.052431201934814, val loss: 4.063564014434815, ETA in seconds: 11394105.386\n",
      "epoch: 699200, train loss: 4.0530171394348145, val loss: 4.074306201934815, ETA in seconds: 11391859.487\n",
      "epoch: 699300, train loss: 4.0471577644348145, val loss: 4.065517139434815, ETA in seconds: 11389590.902\n",
      "epoch: 699400, train loss: 4.049501514434814, val loss: 4.071767139434814, ETA in seconds: 11387306.254\n",
      "epoch: 699500, train loss: 4.055751514434815, val loss: 4.0647358894348145, ETA in seconds: 11385052.360\n",
      "epoch: 699600, train loss: 4.053212451934814, val loss: 4.070009326934814, ETA in seconds: 11382828.707\n",
      "epoch: 699700, train loss: 4.050478076934814, val loss: 4.064149951934814, ETA in seconds: 11380603.497\n",
      "epoch: 699800, train loss: 4.054384326934814, val loss: 4.068251514434815, ETA in seconds: 11378354.842\n",
      "epoch: 699900, train loss: 4.049306201934814, val loss: 4.069032764434814, ETA in seconds: 11376119.335\n",
      "epoch: 700000, train loss: 4.048720264434815, val loss: 4.067079639434814, ETA in seconds: 11373813.375\n",
      "epoch: 700100, train loss: 4.0520405769348145, val loss: 4.069032764434814, ETA in seconds: 11371546.076\n",
      "epoch: 700200, train loss: 4.049501514434814, val loss: 4.068251514434815, ETA in seconds: 11369232.424\n",
      "epoch: 700300, train loss: 4.048720264434815, val loss: 4.070399951934815, ETA in seconds: 11366994.463\n",
      "epoch: 700400, train loss: 4.0520405769348145, val loss: 4.068837451934814, ETA in seconds: 11364727.323\n",
      "epoch: 700500, train loss: 4.045595264434814, val loss: 4.070790576934814, ETA in seconds: 11362444.911\n",
      "epoch: 700600, train loss: 4.045985889434815, val loss: 4.062392139434815, ETA in seconds: 11360190.190\n",
      "epoch: 700700, train loss: 4.044423389434814, val loss: 4.069423389434815, ETA in seconds: 11357909.204\n",
      "epoch: 700800, train loss: 4.045985889434815, val loss: 4.064345264434815, ETA in seconds: 11355629.137\n",
      "epoch: 700900, train loss: 4.047743701934815, val loss: 4.063173389434814, ETA in seconds: 11353366.904\n",
      "epoch: 701000, train loss: 4.046571826934814, val loss: 4.064345264434815, ETA in seconds: 11351114.963\n",
      "epoch: 701100, train loss: 4.056142139434814, val loss: 4.066103076934814, ETA in seconds: 11348883.822\n",
      "epoch: 701200, train loss: 4.053407764434814, val loss: 4.064540576934815, ETA in seconds: 11346642.316\n",
      "epoch: 701300, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 11344427.377\n",
      "epoch: 701400, train loss: 4.0549702644348145, val loss: 4.063954639434814, ETA in seconds: 11342197.986\n",
      "epoch: 701500, train loss: 4.046376514434814, val loss: 4.063368701934815, ETA in seconds: 11339974.167\n",
      "epoch: 701600, train loss: 4.050478076934814, val loss: 4.068251514434815, ETA in seconds: 11337769.321\n",
      "epoch: 701700, train loss: 4.0461812019348145, val loss: 4.066493701934815, ETA in seconds: 11335581.626\n",
      "epoch: 701800, train loss: 4.052235889434814, val loss: 4.063954639434814, ETA in seconds: 11333359.269\n",
      "epoch: 701900, train loss: 4.051259326934814, val loss: 4.068056201934814, ETA in seconds: 11331154.201\n",
      "epoch: 702000, train loss: 4.048329639434814, val loss: 4.068837451934814, ETA in seconds: 11328946.661\n",
      "epoch: 702100, train loss: 4.048329639434814, val loss: 4.067274951934815, ETA in seconds: 11326739.342\n",
      "epoch: 702200, train loss: 4.054774951934815, val loss: 4.0608296394348145, ETA in seconds: 11324503.283\n",
      "epoch: 702300, train loss: 4.0481343269348145, val loss: 4.068446826934815, ETA in seconds: 11322286.031\n",
      "epoch: 702400, train loss: 4.055360889434814, val loss: 4.071181201934815, ETA in seconds: 11320052.689\n",
      "epoch: 702500, train loss: 4.054579639434815, val loss: 4.066884326934814, ETA in seconds: 11317804.228\n",
      "epoch: 702600, train loss: 4.049501514434814, val loss: 4.069228076934815, ETA in seconds: 11315560.358\n",
      "epoch: 702700, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 11313318.223\n",
      "epoch: 702800, train loss: 4.052821826934815, val loss: 4.0647358894348145, ETA in seconds: 11311080.545\n",
      "epoch: 702900, train loss: 4.0530171394348145, val loss: 4.066493701934815, ETA in seconds: 11308851.889\n",
      "epoch: 703000, train loss: 4.051454639434814, val loss: 4.072939014434814, ETA in seconds: 11306626.644\n",
      "epoch: 703100, train loss: 4.047548389434814, val loss: 4.0637593269348145, ETA in seconds: 11304386.025\n",
      "epoch: 703200, train loss: 4.057704639434815, val loss: 4.070204639434815, ETA in seconds: 11302131.364\n",
      "epoch: 703300, train loss: 4.059462451934815, val loss: 4.0647358894348145, ETA in seconds: 11299876.900\n",
      "epoch: 703400, train loss: 4.050673389434815, val loss: 4.0676655769348145, ETA in seconds: 11297670.261\n",
      "epoch: 703500, train loss: 4.0549702644348145, val loss: 4.0618062019348145, ETA in seconds: 11295470.259\n",
      "epoch: 703600, train loss: 4.048915576934815, val loss: 4.0657124519348145, ETA in seconds: 11293269.740\n",
      "epoch: 703700, train loss: 4.049892139434815, val loss: 4.0647358894348145, ETA in seconds: 11291045.894\n",
      "epoch: 703800, train loss: 4.056337451934814, val loss: 4.070009326934814, ETA in seconds: 11288831.109\n",
      "epoch: 703900, train loss: 4.062587451934815, val loss: 4.068446826934815, ETA in seconds: 11286593.182\n",
      "epoch: 704000, train loss: 4.045790576934815, val loss: 4.063564014434815, ETA in seconds: 11284374.844\n",
      "epoch: 704100, train loss: 4.052235889434814, val loss: 4.064149951934814, ETA in seconds: 11282167.214\n",
      "epoch: 704200, train loss: 4.058681201934815, val loss: 4.067274951934815, ETA in seconds: 11279953.410\n",
      "epoch: 704300, train loss: 4.053603076934815, val loss: 4.069814014434814, ETA in seconds: 11277762.342\n",
      "epoch: 704400, train loss: 4.059657764434815, val loss: 4.0666890144348145, ETA in seconds: 11275531.648\n",
      "epoch: 704500, train loss: 4.0559468269348145, val loss: 4.065321826934815, ETA in seconds: 11273316.073\n",
      "epoch: 704600, train loss: 4.067470264434815, val loss: 4.0676655769348145, ETA in seconds: 11271073.912\n",
      "epoch: 704700, train loss: 4.055751514434815, val loss: 4.065907764434814, ETA in seconds: 11268841.632\n",
      "epoch: 704800, train loss: 4.053212451934814, val loss: 4.068446826934815, ETA in seconds: 11266620.751\n",
      "epoch: 704900, train loss: 4.053212451934814, val loss: 4.069814014434814, ETA in seconds: 11264370.615\n",
      "epoch: 705000, train loss: 4.048915576934815, val loss: 4.069814014434814, ETA in seconds: 11262053.213\n",
      "epoch: 705100, train loss: 4.0549702644348145, val loss: 4.063954639434814, ETA in seconds: 11259758.516\n",
      "epoch: 705200, train loss: 4.052235889434814, val loss: 4.065126514434814, ETA in seconds: 11257573.667\n",
      "epoch: 705300, train loss: 4.055360889434814, val loss: 4.065321826934815, ETA in seconds: 11255323.823\n",
      "epoch: 705400, train loss: 4.053407764434814, val loss: 4.067860889434814, ETA in seconds: 11253058.570\n",
      "epoch: 705500, train loss: 4.054774951934815, val loss: 4.068446826934815, ETA in seconds: 11250860.255\n",
      "epoch: 705600, train loss: 4.055556201934815, val loss: 4.060243701934814, ETA in seconds: 11248630.847\n",
      "epoch: 705700, train loss: 4.059071826934814, val loss: 4.071962451934814, ETA in seconds: 11246387.510\n",
      "epoch: 705800, train loss: 4.050868701934815, val loss: 4.061610889434815, ETA in seconds: 11244136.898\n",
      "epoch: 705900, train loss: 4.056142139434814, val loss: 4.068056201934814, ETA in seconds: 11241899.537\n",
      "epoch: 706000, train loss: 4.053407764434814, val loss: 4.062978076934814, ETA in seconds: 11239658.056\n",
      "epoch: 706100, train loss: 4.049892139434815, val loss: 4.066493701934815, ETA in seconds: 11237442.531\n",
      "epoch: 706200, train loss: 4.049501514434814, val loss: 4.0696187019348145, ETA in seconds: 11235169.645\n",
      "epoch: 706300, train loss: 4.0491108894348145, val loss: 4.064345264434815, ETA in seconds: 11232921.477\n",
      "epoch: 706400, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 11230695.308\n",
      "epoch: 706500, train loss: 4.0452046394348145, val loss: 4.0666890144348145, ETA in seconds: 11228455.074\n",
      "epoch: 706600, train loss: 4.056142139434814, val loss: 4.074110889434815, ETA in seconds: 11226156.907\n",
      "epoch: 706700, train loss: 4.046571826934814, val loss: 4.063564014434815, ETA in seconds: 11223868.761\n",
      "epoch: 706800, train loss: 4.049696826934815, val loss: 4.066103076934814, ETA in seconds: 11221585.279\n",
      "epoch: 706900, train loss: 4.0461812019348145, val loss: 4.0676655769348145, ETA in seconds: 11219321.083\n",
      "epoch: 707000, train loss: 4.050673389434815, val loss: 4.066103076934814, ETA in seconds: 11217036.817\n",
      "epoch: 707100, train loss: 4.0549702644348145, val loss: 4.069423389434815, ETA in seconds: 11214741.628\n",
      "epoch: 707200, train loss: 4.054774951934815, val loss: 4.066493701934815, ETA in seconds: 11212414.529\n",
      "epoch: 707300, train loss: 4.051259326934814, val loss: 4.068056201934814, ETA in seconds: 11210143.413\n",
      "epoch: 707400, train loss: 4.052235889434814, val loss: 4.061220264434814, ETA in seconds: 11207857.976\n",
      "epoch: 707500, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 11205574.909\n",
      "epoch: 707600, train loss: 4.051454639434814, val loss: 4.0705952644348145, ETA in seconds: 11203355.017\n",
      "epoch: 707700, train loss: 4.053212451934814, val loss: 4.066298389434815, ETA in seconds: 11201104.770\n",
      "epoch: 707800, train loss: 4.049501514434814, val loss: 4.066103076934814, ETA in seconds: 11198942.697\n",
      "epoch: 707900, train loss: 4.046962451934815, val loss: 4.064931201934814, ETA in seconds: 11196790.284\n",
      "epoch: 708000, train loss: 4.054579639434815, val loss: 4.065321826934815, ETA in seconds: 11194539.596\n",
      "epoch: 708100, train loss: 4.048720264434815, val loss: 4.066884326934814, ETA in seconds: 11192253.206\n",
      "epoch: 708200, train loss: 4.059071826934814, val loss: 4.069228076934815, ETA in seconds: 11189986.594\n",
      "epoch: 708300, train loss: 4.054384326934814, val loss: 4.064931201934814, ETA in seconds: 11187675.211\n",
      "epoch: 708400, train loss: 4.054774951934815, val loss: 4.0676655769348145, ETA in seconds: 11185402.611\n",
      "epoch: 708500, train loss: 4.055165576934814, val loss: 4.067860889434814, ETA in seconds: 11183105.321\n",
      "epoch: 708600, train loss: 4.048524951934814, val loss: 4.070790576934814, ETA in seconds: 11180912.303\n",
      "epoch: 708700, train loss: 4.049306201934814, val loss: 4.068251514434815, ETA in seconds: 11178622.102\n",
      "epoch: 708800, train loss: 4.054579639434815, val loss: 4.069423389434815, ETA in seconds: 11176335.717\n",
      "epoch: 708900, train loss: 4.056337451934814, val loss: 4.075087451934815, ETA in seconds: 11174027.630\n",
      "epoch: 709000, train loss: 4.053407764434814, val loss: 4.071376514434815, ETA in seconds: 11171728.950\n",
      "epoch: 709100, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 11169420.414\n",
      "epoch: 709200, train loss: 4.045399951934814, val loss: 4.0647358894348145, ETA in seconds: 11167140.456\n",
      "epoch: 709300, train loss: 4.048329639434814, val loss: 4.074110889434815, ETA in seconds: 11164866.553\n",
      "epoch: 709400, train loss: 4.047548389434814, val loss: 4.070790576934814, ETA in seconds: 11162603.341\n",
      "epoch: 709500, train loss: 4.054189014434814, val loss: 4.064345264434815, ETA in seconds: 11160298.362\n",
      "epoch: 709600, train loss: 4.048329639434814, val loss: 4.067470264434815, ETA in seconds: 11157977.581\n",
      "epoch: 709700, train loss: 4.0539937019348145, val loss: 4.070790576934814, ETA in seconds: 11155649.607\n",
      "epoch: 709800, train loss: 4.049306201934814, val loss: 4.0627827644348145, ETA in seconds: 11153337.121\n",
      "epoch: 709900, train loss: 4.048720264434815, val loss: 4.066884326934814, ETA in seconds: 11151037.729\n",
      "epoch: 710000, train loss: 4.0500874519348145, val loss: 4.066298389434815, ETA in seconds: 11148730.899\n",
      "epoch: 710100, train loss: 4.051649951934815, val loss: 4.066884326934814, ETA in seconds: 11146437.551\n",
      "epoch: 710200, train loss: 4.0452046394348145, val loss: 4.066884326934814, ETA in seconds: 11144111.475\n",
      "epoch: 710300, train loss: 4.050868701934815, val loss: 4.065517139434815, ETA in seconds: 11141788.769\n",
      "epoch: 710400, train loss: 4.049696826934815, val loss: 4.067079639434814, ETA in seconds: 11139501.821\n",
      "epoch: 710500, train loss: 4.054579639434815, val loss: 4.071962451934814, ETA in seconds: 11137172.231\n",
      "epoch: 710600, train loss: 4.052821826934815, val loss: 4.067079639434814, ETA in seconds: 11134852.769\n",
      "epoch: 710700, train loss: 4.050868701934815, val loss: 4.0745015144348145, ETA in seconds: 11132521.091\n",
      "epoch: 710800, train loss: 4.051454639434814, val loss: 4.070790576934814, ETA in seconds: 11130233.125\n",
      "epoch: 710900, train loss: 4.048915576934815, val loss: 4.071767139434814, ETA in seconds: 11127911.860\n",
      "epoch: 711000, train loss: 4.0510640144348145, val loss: 4.066493701934815, ETA in seconds: 11125583.930\n",
      "epoch: 711100, train loss: 4.052431201934814, val loss: 4.071767139434814, ETA in seconds: 11123271.450\n",
      "epoch: 711200, train loss: 4.047548389434814, val loss: 4.063954639434814, ETA in seconds: 11120955.940\n",
      "epoch: 711300, train loss: 4.053407764434814, val loss: 4.065517139434815, ETA in seconds: 11118687.457\n",
      "epoch: 711400, train loss: 4.047353076934814, val loss: 4.067274951934815, ETA in seconds: 11116339.153\n",
      "epoch: 711500, train loss: 4.055165576934814, val loss: 4.061220264434814, ETA in seconds: 11114016.497\n",
      "epoch: 711600, train loss: 4.049892139434815, val loss: 4.070204639434815, ETA in seconds: 11111733.359\n",
      "epoch: 711700, train loss: 4.053407764434814, val loss: 4.065126514434814, ETA in seconds: 11109445.995\n",
      "epoch: 711800, train loss: 4.0510640144348145, val loss: 4.070399951934815, ETA in seconds: 11107113.869\n",
      "epoch: 711900, train loss: 4.051259326934814, val loss: 4.070790576934814, ETA in seconds: 11104776.864\n",
      "epoch: 712000, train loss: 4.046962451934815, val loss: 4.067860889434814, ETA in seconds: 11102426.778\n",
      "epoch: 712100, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 11100085.582\n",
      "epoch: 712200, train loss: 4.057314014434814, val loss: 4.068056201934814, ETA in seconds: 11097780.493\n",
      "epoch: 712300, train loss: 4.054774951934815, val loss: 4.069423389434815, ETA in seconds: 11095411.826\n",
      "epoch: 712400, train loss: 4.054579639434815, val loss: 4.0686421394348145, ETA in seconds: 11093078.470\n",
      "epoch: 712500, train loss: 4.047939014434815, val loss: 4.066103076934814, ETA in seconds: 11090742.772\n",
      "epoch: 712600, train loss: 4.047939014434815, val loss: 4.071767139434814, ETA in seconds: 11088410.790\n",
      "epoch: 712700, train loss: 4.051454639434814, val loss: 4.067274951934815, ETA in seconds: 11086074.095\n",
      "epoch: 712800, train loss: 4.0608296394348145, val loss: 4.071962451934814, ETA in seconds: 11083742.443\n",
      "epoch: 712900, train loss: 4.0539937019348145, val loss: 4.068056201934814, ETA in seconds: 11081409.084\n",
      "epoch: 713000, train loss: 4.0500874519348145, val loss: 4.061415576934815, ETA in seconds: 11079043.861\n",
      "epoch: 713100, train loss: 4.053212451934814, val loss: 4.074696826934814, ETA in seconds: 11076674.416\n",
      "epoch: 713200, train loss: 4.051259326934814, val loss: 4.066884326934814, ETA in seconds: 11074324.276\n",
      "epoch: 713300, train loss: 4.047353076934814, val loss: 4.072157764434815, ETA in seconds: 11071967.083\n",
      "epoch: 713400, train loss: 4.052821826934815, val loss: 4.068251514434815, ETA in seconds: 11069709.890\n",
      "epoch: 713500, train loss: 4.055360889434814, val loss: 4.069423389434815, ETA in seconds: 11067385.772\n",
      "epoch: 713600, train loss: 4.046376514434814, val loss: 4.070009326934814, ETA in seconds: 11065095.517\n",
      "epoch: 713700, train loss: 4.054774951934815, val loss: 4.072939014434814, ETA in seconds: 11062822.274\n",
      "epoch: 713800, train loss: 4.0549702644348145, val loss: 4.0666890144348145, ETA in seconds: 11060441.472\n",
      "epoch: 713900, train loss: 4.056142139434814, val loss: 4.068251514434815, ETA in seconds: 11058124.847\n",
      "epoch: 714000, train loss: 4.059462451934815, val loss: 4.0676655769348145, ETA in seconds: 11055788.441\n",
      "epoch: 714100, train loss: 4.050868701934815, val loss: 4.065321826934815, ETA in seconds: 11053431.146\n",
      "epoch: 714200, train loss: 4.054189014434814, val loss: 4.062978076934814, ETA in seconds: 11051076.232\n",
      "epoch: 714300, train loss: 4.057314014434814, val loss: 4.065907764434814, ETA in seconds: 11048749.210\n",
      "epoch: 714400, train loss: 4.050673389434815, val loss: 4.0608296394348145, ETA in seconds: 11046414.914\n",
      "epoch: 714500, train loss: 4.051845264434815, val loss: 4.070204639434815, ETA in seconds: 11044101.536\n",
      "epoch: 714600, train loss: 4.0530171394348145, val loss: 4.067274951934815, ETA in seconds: 11041766.564\n",
      "epoch: 714700, train loss: 4.059462451934815, val loss: 4.065517139434815, ETA in seconds: 11039403.662\n",
      "epoch: 714800, train loss: 4.053798389434815, val loss: 4.071962451934814, ETA in seconds: 11037048.805\n",
      "epoch: 714900, train loss: 4.050282764434814, val loss: 4.067470264434815, ETA in seconds: 11034701.858\n",
      "epoch: 715000, train loss: 4.050282764434814, val loss: 4.065907764434814, ETA in seconds: 11032319.600\n",
      "epoch: 715100, train loss: 4.045009326934815, val loss: 4.067470264434815, ETA in seconds: 11029987.854\n",
      "epoch: 715200, train loss: 4.049306201934814, val loss: 4.067079639434814, ETA in seconds: 11027639.738\n",
      "epoch: 715300, train loss: 4.0510640144348145, val loss: 4.066298389434815, ETA in seconds: 11025267.190\n",
      "epoch: 715400, train loss: 4.052626514434815, val loss: 4.074696826934814, ETA in seconds: 11022951.733\n",
      "epoch: 715500, train loss: 4.047548389434814, val loss: 4.068446826934815, ETA in seconds: 11020619.951\n",
      "epoch: 715600, train loss: 4.050673389434815, val loss: 4.062196826934814, ETA in seconds: 11018270.318\n",
      "epoch: 715700, train loss: 4.055360889434814, val loss: 4.0696187019348145, ETA in seconds: 11015891.136\n",
      "epoch: 715800, train loss: 4.053603076934815, val loss: 4.062392139434815, ETA in seconds: 11013532.441\n",
      "epoch: 715900, train loss: 4.046571826934814, val loss: 4.074306201934815, ETA in seconds: 11011158.123\n",
      "epoch: 716000, train loss: 4.055556201934815, val loss: 4.066493701934815, ETA in seconds: 11008760.287\n",
      "epoch: 716100, train loss: 4.0491108894348145, val loss: 4.071767139434814, ETA in seconds: 11006395.881\n",
      "epoch: 716200, train loss: 4.048524951934814, val loss: 4.065126514434814, ETA in seconds: 11003992.478\n",
      "epoch: 716300, train loss: 4.051454639434814, val loss: 4.0686421394348145, ETA in seconds: 11001593.109\n",
      "epoch: 716400, train loss: 4.0578999519348145, val loss: 4.066298389434815, ETA in seconds: 10999203.398\n",
      "epoch: 716500, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 10996833.482\n",
      "epoch: 716600, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 10994424.129\n",
      "epoch: 716700, train loss: 4.054579639434815, val loss: 4.061220264434814, ETA in seconds: 10992055.697\n",
      "epoch: 716800, train loss: 4.051454639434814, val loss: 4.067860889434814, ETA in seconds: 10989690.313\n",
      "epoch: 716900, train loss: 4.0549702644348145, val loss: 4.071962451934814, ETA in seconds: 10987380.426\n",
      "epoch: 717000, train loss: 4.051845264434815, val loss: 4.064149951934814, ETA in seconds: 10985005.506\n",
      "epoch: 717100, train loss: 4.054384326934814, val loss: 4.0715718269348145, ETA in seconds: 10982656.894\n",
      "epoch: 717200, train loss: 4.055360889434814, val loss: 4.066493701934815, ETA in seconds: 10980302.753\n",
      "epoch: 717300, train loss: 4.049696826934815, val loss: 4.0676655769348145, ETA in seconds: 10978038.536\n",
      "epoch: 717400, train loss: 4.049696826934815, val loss: 4.069228076934815, ETA in seconds: 10975757.392\n",
      "epoch: 717500, train loss: 4.050282764434814, val loss: 4.067274951934815, ETA in seconds: 10973423.274\n",
      "epoch: 717600, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 10971145.007\n",
      "epoch: 717700, train loss: 4.054579639434815, val loss: 4.075673389434814, ETA in seconds: 10968810.749\n",
      "epoch: 717800, train loss: 4.0569233894348145, val loss: 4.0686421394348145, ETA in seconds: 10966550.354\n",
      "epoch: 717900, train loss: 4.055165576934814, val loss: 4.0676655769348145, ETA in seconds: 10964303.797\n",
      "epoch: 718000, train loss: 4.051649951934815, val loss: 4.067274951934815, ETA in seconds: 10962006.911\n",
      "epoch: 718100, train loss: 4.048524951934814, val loss: 4.067470264434815, ETA in seconds: 10959757.464\n",
      "epoch: 718200, train loss: 4.058681201934815, val loss: 4.068837451934814, ETA in seconds: 10957516.727\n",
      "epoch: 718300, train loss: 4.054579639434815, val loss: 4.066884326934814, ETA in seconds: 10955259.805\n",
      "epoch: 718400, train loss: 4.045790576934815, val loss: 4.065126514434814, ETA in seconds: 10953034.812\n",
      "epoch: 718500, train loss: 4.047939014434815, val loss: 4.067274951934815, ETA in seconds: 10950823.074\n",
      "epoch: 718600, train loss: 4.0510640144348145, val loss: 4.0696187019348145, ETA in seconds: 10948563.013\n",
      "epoch: 718700, train loss: 4.051259326934814, val loss: 4.0666890144348145, ETA in seconds: 10946179.462\n",
      "epoch: 718800, train loss: 4.052821826934815, val loss: 4.0647358894348145, ETA in seconds: 10943796.303\n",
      "epoch: 718900, train loss: 4.0442280769348145, val loss: 4.064540576934815, ETA in seconds: 10941355.182\n",
      "epoch: 719000, train loss: 4.047353076934814, val loss: 4.070399951934815, ETA in seconds: 10938927.726\n",
      "epoch: 719100, train loss: 4.0539937019348145, val loss: 4.067274951934815, ETA in seconds: 10936495.555\n",
      "epoch: 719200, train loss: 4.052235889434814, val loss: 4.066103076934814, ETA in seconds: 10934069.660\n",
      "epoch: 719300, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 10931643.513\n",
      "epoch: 719400, train loss: 4.054579639434815, val loss: 4.0637593269348145, ETA in seconds: 10929257.420\n",
      "epoch: 719500, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 10926895.605\n",
      "epoch: 719600, train loss: 4.055556201934815, val loss: 4.068056201934814, ETA in seconds: 10924498.699\n",
      "epoch: 719700, train loss: 4.046376514434814, val loss: 4.062196826934814, ETA in seconds: 10922086.623\n",
      "epoch: 719800, train loss: 4.054579639434815, val loss: 4.0637593269348145, ETA in seconds: 10919709.878\n",
      "epoch: 719900, train loss: 4.055751514434815, val loss: 4.063173389434814, ETA in seconds: 10917280.379\n",
      "epoch: 720000, train loss: 4.047743701934815, val loss: 4.071376514434815, ETA in seconds: 10914911.709\n",
      "epoch: 720100, train loss: 4.048720264434815, val loss: 4.0657124519348145, ETA in seconds: 10912485.639\n",
      "epoch: 720200, train loss: 4.047353076934814, val loss: 4.070985889434814, ETA in seconds: 10910098.425\n",
      "epoch: 720300, train loss: 4.054189014434814, val loss: 4.060634326934815, ETA in seconds: 10907683.155\n",
      "epoch: 720400, train loss: 4.051259326934814, val loss: 4.067860889434814, ETA in seconds: 10905279.689\n",
      "epoch: 720500, train loss: 4.046767139434815, val loss: 4.067079639434814, ETA in seconds: 10902847.665\n",
      "epoch: 720600, train loss: 4.050868701934815, val loss: 4.066103076934814, ETA in seconds: 10900421.974\n",
      "epoch: 720700, train loss: 4.042860889434815, val loss: 4.064540576934815, ETA in seconds: 10897993.779\n",
      "epoch: 720800, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 10895592.074\n",
      "epoch: 720900, train loss: 4.053603076934815, val loss: 4.066103076934814, ETA in seconds: 10893138.302\n",
      "epoch: 721000, train loss: 4.051649951934815, val loss: 4.065126514434814, ETA in seconds: 10890694.514\n",
      "epoch: 721100, train loss: 4.054774951934815, val loss: 4.066298389434815, ETA in seconds: 10888243.930\n",
      "epoch: 721200, train loss: 4.054579639434815, val loss: 4.070399951934815, ETA in seconds: 10885809.513\n",
      "epoch: 721300, train loss: 4.049501514434814, val loss: 4.067274951934815, ETA in seconds: 10883398.441\n",
      "epoch: 721400, train loss: 4.052626514434815, val loss: 4.063564014434815, ETA in seconds: 10880977.073\n",
      "epoch: 721500, train loss: 4.0442280769348145, val loss: 4.0618062019348145, ETA in seconds: 10878544.807\n",
      "epoch: 721600, train loss: 4.055556201934815, val loss: 4.071376514434815, ETA in seconds: 10876117.993\n",
      "epoch: 721700, train loss: 4.054384326934814, val loss: 4.064345264434815, ETA in seconds: 10873709.069\n",
      "epoch: 721800, train loss: 4.0442280769348145, val loss: 4.065517139434815, ETA in seconds: 10871285.138\n",
      "epoch: 721900, train loss: 4.0471577644348145, val loss: 4.061220264434814, ETA in seconds: 10868867.185\n",
      "epoch: 722000, train loss: 4.050478076934814, val loss: 4.071767139434814, ETA in seconds: 10866409.185\n",
      "epoch: 722100, train loss: 4.059071826934814, val loss: 4.062392139434815, ETA in seconds: 10863969.452\n",
      "epoch: 722200, train loss: 4.051454639434814, val loss: 4.0647358894348145, ETA in seconds: 10861538.404\n",
      "epoch: 722300, train loss: 4.056728076934815, val loss: 4.067079639434814, ETA in seconds: 10859081.834\n",
      "epoch: 722400, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 10856637.312\n",
      "epoch: 722500, train loss: 4.055751514434815, val loss: 4.069814014434814, ETA in seconds: 10854207.035\n",
      "epoch: 722600, train loss: 4.048720264434815, val loss: 4.068251514434815, ETA in seconds: 10851754.370\n",
      "epoch: 722700, train loss: 4.047548389434814, val loss: 4.068446826934815, ETA in seconds: 10849342.550\n",
      "epoch: 722800, train loss: 4.055751514434815, val loss: 4.066884326934814, ETA in seconds: 10846935.134\n",
      "epoch: 722900, train loss: 4.045595264434814, val loss: 4.0608296394348145, ETA in seconds: 10844472.708\n",
      "epoch: 723000, train loss: 4.0491108894348145, val loss: 4.0725483894348145, ETA in seconds: 10842012.462\n",
      "epoch: 723100, train loss: 4.048915576934815, val loss: 4.064931201934814, ETA in seconds: 10839587.684\n",
      "epoch: 723200, train loss: 4.056142139434814, val loss: 4.066298389434815, ETA in seconds: 10837114.959\n",
      "epoch: 723300, train loss: 4.057314014434814, val loss: 4.069814014434814, ETA in seconds: 10834699.373\n",
      "epoch: 723400, train loss: 4.051454639434814, val loss: 4.065907764434814, ETA in seconds: 10832262.836\n",
      "epoch: 723500, train loss: 4.050478076934814, val loss: 4.067274951934815, ETA in seconds: 10829819.065\n",
      "epoch: 723600, train loss: 4.045595264434814, val loss: 4.067470264434815, ETA in seconds: 10827370.463\n",
      "epoch: 723700, train loss: 4.058485889434815, val loss: 4.0666890144348145, ETA in seconds: 10824935.018\n",
      "epoch: 723800, train loss: 4.057509326934815, val loss: 4.066298389434815, ETA in seconds: 10822497.294\n",
      "epoch: 723900, train loss: 4.056728076934815, val loss: 4.068837451934814, ETA in seconds: 10820097.621\n",
      "epoch: 724000, train loss: 4.0549702644348145, val loss: 4.0608296394348145, ETA in seconds: 10817659.307\n",
      "epoch: 724100, train loss: 4.0500874519348145, val loss: 4.067274951934815, ETA in seconds: 10815225.725\n",
      "epoch: 724200, train loss: 4.049306201934814, val loss: 4.061415576934815, ETA in seconds: 10812773.751\n",
      "epoch: 724300, train loss: 4.057314014434814, val loss: 4.066884326934814, ETA in seconds: 10810360.177\n",
      "epoch: 724400, train loss: 4.058095264434814, val loss: 4.0686421394348145, ETA in seconds: 10807930.396\n",
      "epoch: 724500, train loss: 4.059462451934815, val loss: 4.065907764434814, ETA in seconds: 10805489.405\n",
      "epoch: 724600, train loss: 4.055751514434815, val loss: 4.066493701934815, ETA in seconds: 10803066.404\n",
      "epoch: 724700, train loss: 4.054579639434815, val loss: 4.065907764434814, ETA in seconds: 10800623.834\n",
      "epoch: 724800, train loss: 4.059462451934815, val loss: 4.0666890144348145, ETA in seconds: 10798212.730\n",
      "epoch: 724900, train loss: 4.056142139434814, val loss: 4.067860889434814, ETA in seconds: 10795797.832\n",
      "epoch: 725000, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 10793324.372\n",
      "epoch: 725100, train loss: 4.048524951934814, val loss: 4.063368701934815, ETA in seconds: 10790882.090\n",
      "epoch: 725200, train loss: 4.046376514434814, val loss: 4.0676655769348145, ETA in seconds: 10788418.263\n",
      "epoch: 725300, train loss: 4.055360889434814, val loss: 4.067079639434814, ETA in seconds: 10785933.570\n",
      "epoch: 725400, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 10783458.092\n",
      "epoch: 725500, train loss: 4.0549702644348145, val loss: 4.0608296394348145, ETA in seconds: 10780996.982\n",
      "epoch: 725600, train loss: 4.0520405769348145, val loss: 4.069423389434815, ETA in seconds: 10778533.200\n",
      "epoch: 725700, train loss: 4.049696826934815, val loss: 4.0666890144348145, ETA in seconds: 10776045.003\n",
      "epoch: 725800, train loss: 4.056532764434815, val loss: 4.069423389434815, ETA in seconds: 10773511.166\n",
      "epoch: 725900, train loss: 4.056728076934815, val loss: 4.069423389434815, ETA in seconds: 10771029.615\n",
      "epoch: 726000, train loss: 4.053798389434815, val loss: 4.066298389434815, ETA in seconds: 10768567.016\n",
      "epoch: 726100, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 10766054.350\n",
      "epoch: 726200, train loss: 4.049306201934814, val loss: 4.0696187019348145, ETA in seconds: 10763567.924\n",
      "epoch: 726300, train loss: 4.0491108894348145, val loss: 4.070204639434815, ETA in seconds: 10761075.498\n",
      "epoch: 726400, train loss: 4.047743701934815, val loss: 4.066493701934815, ETA in seconds: 10758565.705\n",
      "epoch: 726500, train loss: 4.053212451934814, val loss: 4.062587451934815, ETA in seconds: 10756057.813\n",
      "epoch: 726600, train loss: 4.053798389434815, val loss: 4.064540576934815, ETA in seconds: 10753553.348\n",
      "epoch: 726700, train loss: 4.054384326934814, val loss: 4.062196826934814, ETA in seconds: 10751092.485\n",
      "epoch: 726800, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 10748749.875\n",
      "epoch: 726900, train loss: 4.048720264434815, val loss: 4.066884326934814, ETA in seconds: 10746442.516\n",
      "epoch: 727000, train loss: 4.059267139434814, val loss: 4.061024951934814, ETA in seconds: 10744072.661\n",
      "epoch: 727100, train loss: 4.047548389434814, val loss: 4.066884326934814, ETA in seconds: 10741697.815\n",
      "epoch: 727200, train loss: 4.0500874519348145, val loss: 4.070009326934814, ETA in seconds: 10739217.522\n",
      "epoch: 727300, train loss: 4.0530171394348145, val loss: 4.0705952644348145, ETA in seconds: 10736724.376\n",
      "epoch: 727400, train loss: 4.056728076934815, val loss: 4.072743701934814, ETA in seconds: 10734215.843\n",
      "epoch: 727500, train loss: 4.048329639434814, val loss: 4.068446826934815, ETA in seconds: 10731743.851\n",
      "epoch: 727600, train loss: 4.058485889434815, val loss: 4.063954639434814, ETA in seconds: 10729239.380\n",
      "epoch: 727700, train loss: 4.056142139434814, val loss: 4.069423389434815, ETA in seconds: 10726750.262\n",
      "epoch: 727800, train loss: 4.055751514434815, val loss: 4.064931201934814, ETA in seconds: 10724214.933\n",
      "epoch: 727900, train loss: 4.0491108894348145, val loss: 4.0686421394348145, ETA in seconds: 10721680.393\n",
      "epoch: 728000, train loss: 4.056142139434814, val loss: 4.0725483894348145, ETA in seconds: 10719138.019\n",
      "epoch: 728100, train loss: 4.047353076934814, val loss: 4.066298389434815, ETA in seconds: 10716623.540\n",
      "epoch: 728200, train loss: 4.057509326934815, val loss: 4.066884326934814, ETA in seconds: 10714096.252\n",
      "epoch: 728300, train loss: 4.052431201934814, val loss: 4.0715718269348145, ETA in seconds: 10711575.516\n",
      "epoch: 728400, train loss: 4.052431201934814, val loss: 4.068056201934814, ETA in seconds: 10709047.522\n",
      "epoch: 728500, train loss: 4.053798389434815, val loss: 4.072743701934814, ETA in seconds: 10706523.393\n",
      "epoch: 728600, train loss: 4.052821826934815, val loss: 4.071962451934814, ETA in seconds: 10704057.913\n",
      "epoch: 728700, train loss: 4.050673389434815, val loss: 4.067079639434814, ETA in seconds: 10701579.562\n",
      "epoch: 728800, train loss: 4.056532764434815, val loss: 4.067860889434814, ETA in seconds: 10699063.060\n",
      "epoch: 728900, train loss: 4.055556201934815, val loss: 4.0686421394348145, ETA in seconds: 10696572.022\n",
      "epoch: 729000, train loss: 4.055165576934814, val loss: 4.0676655769348145, ETA in seconds: 10694135.120\n",
      "epoch: 729100, train loss: 4.052821826934815, val loss: 4.064345264434815, ETA in seconds: 10691622.887\n",
      "epoch: 729200, train loss: 4.046962451934815, val loss: 4.0715718269348145, ETA in seconds: 10689107.853\n",
      "epoch: 729300, train loss: 4.057118701934814, val loss: 4.068056201934814, ETA in seconds: 10686614.259\n",
      "epoch: 729400, train loss: 4.050478076934814, val loss: 4.066493701934815, ETA in seconds: 10684160.240\n",
      "epoch: 729500, train loss: 4.056532764434815, val loss: 4.068837451934814, ETA in seconds: 10681675.310\n",
      "epoch: 729600, train loss: 4.048524951934814, val loss: 4.069423389434815, ETA in seconds: 10679142.473\n",
      "epoch: 729700, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 10676660.898\n",
      "epoch: 729800, train loss: 4.050868701934815, val loss: 4.069032764434814, ETA in seconds: 10674163.767\n",
      "epoch: 729900, train loss: 4.054774951934815, val loss: 4.068056201934814, ETA in seconds: 10671649.576\n",
      "epoch: 730000, train loss: 4.0578999519348145, val loss: 4.060634326934815, ETA in seconds: 10669096.094\n",
      "epoch: 730100, train loss: 4.052626514434815, val loss: 4.070399951934815, ETA in seconds: 10666565.211\n",
      "epoch: 730200, train loss: 4.054774951934815, val loss: 4.066103076934814, ETA in seconds: 10664036.646\n",
      "epoch: 730300, train loss: 4.0461812019348145, val loss: 4.070204639434815, ETA in seconds: 10661478.785\n",
      "epoch: 730400, train loss: 4.055556201934815, val loss: 4.067079639434814, ETA in seconds: 10658903.441\n",
      "epoch: 730500, train loss: 4.0520405769348145, val loss: 4.068837451934814, ETA in seconds: 10656376.651\n",
      "epoch: 730600, train loss: 4.0491108894348145, val loss: 4.062001514434814, ETA in seconds: 10653865.784\n",
      "epoch: 730700, train loss: 4.051649951934815, val loss: 4.063368701934815, ETA in seconds: 10651347.885\n",
      "epoch: 730800, train loss: 4.053212451934814, val loss: 4.064345264434815, ETA in seconds: 10648828.709\n",
      "epoch: 730900, train loss: 4.050478076934814, val loss: 4.066493701934815, ETA in seconds: 10646300.471\n",
      "epoch: 731000, train loss: 4.053603076934815, val loss: 4.063368701934815, ETA in seconds: 10643778.335\n",
      "epoch: 731100, train loss: 4.052626514434815, val loss: 4.063564014434815, ETA in seconds: 10641271.880\n",
      "epoch: 731200, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 10638747.701\n",
      "epoch: 731300, train loss: 4.054189014434814, val loss: 4.068056201934814, ETA in seconds: 10636277.821\n",
      "epoch: 731400, train loss: 4.052431201934814, val loss: 4.0715718269348145, ETA in seconds: 10633739.085\n",
      "epoch: 731500, train loss: 4.052235889434814, val loss: 4.064149951934814, ETA in seconds: 10631222.802\n",
      "epoch: 731600, train loss: 4.0510640144348145, val loss: 4.067860889434814, ETA in seconds: 10628675.454\n",
      "epoch: 731700, train loss: 4.057509326934815, val loss: 4.068446826934815, ETA in seconds: 10626172.823\n",
      "epoch: 731800, train loss: 4.0520405769348145, val loss: 4.067860889434814, ETA in seconds: 10623699.820\n",
      "epoch: 731900, train loss: 4.057118701934814, val loss: 4.067079639434814, ETA in seconds: 10621139.370\n",
      "epoch: 732000, train loss: 4.051454639434814, val loss: 4.070009326934814, ETA in seconds: 10618610.244\n",
      "epoch: 732100, train loss: 4.047743701934815, val loss: 4.071181201934815, ETA in seconds: 10616096.882\n",
      "epoch: 732200, train loss: 4.053407764434814, val loss: 4.072157764434815, ETA in seconds: 10613592.909\n",
      "epoch: 732300, train loss: 4.048329639434814, val loss: 4.072939014434814, ETA in seconds: 10611061.586\n",
      "epoch: 732400, train loss: 4.054579639434815, val loss: 4.071181201934815, ETA in seconds: 10608551.710\n",
      "epoch: 732500, train loss: 4.051845264434815, val loss: 4.070790576934814, ETA in seconds: 10606055.157\n",
      "epoch: 732600, train loss: 4.051259326934814, val loss: 4.0666890144348145, ETA in seconds: 10603518.918\n",
      "epoch: 732700, train loss: 4.055556201934815, val loss: 4.067860889434814, ETA in seconds: 10601008.867\n",
      "epoch: 732800, train loss: 4.050478076934814, val loss: 4.070399951934815, ETA in seconds: 10598463.808\n",
      "epoch: 732900, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 10595973.193\n",
      "epoch: 733000, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 10593483.991\n",
      "epoch: 733100, train loss: 4.050478076934814, val loss: 4.070204639434815, ETA in seconds: 10590958.852\n",
      "epoch: 733200, train loss: 4.056337451934814, val loss: 4.0676655769348145, ETA in seconds: 10588409.278\n",
      "epoch: 733300, train loss: 4.056142139434814, val loss: 4.0627827644348145, ETA in seconds: 10585892.703\n",
      "epoch: 733400, train loss: 4.0461812019348145, val loss: 4.068446826934815, ETA in seconds: 10583444.728\n",
      "epoch: 733500, train loss: 4.054189014434814, val loss: 4.065517139434815, ETA in seconds: 10580976.562\n",
      "epoch: 733600, train loss: 4.051845264434815, val loss: 4.0696187019348145, ETA in seconds: 10578469.521\n",
      "epoch: 733700, train loss: 4.044618701934814, val loss: 4.070985889434814, ETA in seconds: 10575834.717\n",
      "epoch: 733800, train loss: 4.053798389434815, val loss: 4.065907764434814, ETA in seconds: 10573230.303\n",
      "epoch: 733900, train loss: 4.056337451934814, val loss: 4.068251514434815, ETA in seconds: 10570685.680\n",
      "epoch: 734000, train loss: 4.0491108894348145, val loss: 4.063564014434815, ETA in seconds: 10568118.186\n",
      "epoch: 734100, train loss: 4.047353076934814, val loss: 4.068446826934815, ETA in seconds: 10565544.938\n",
      "epoch: 734200, train loss: 4.050868701934815, val loss: 4.0705952644348145, ETA in seconds: 10562984.172\n",
      "epoch: 734300, train loss: 4.0530171394348145, val loss: 4.064540576934815, ETA in seconds: 10560424.568\n",
      "epoch: 734400, train loss: 4.049306201934814, val loss: 4.072353076934815, ETA in seconds: 10557868.576\n",
      "epoch: 734500, train loss: 4.045399951934814, val loss: 4.066493701934815, ETA in seconds: 10555337.091\n",
      "epoch: 734600, train loss: 4.0598530769348145, val loss: 4.0657124519348145, ETA in seconds: 10552759.648\n",
      "epoch: 734700, train loss: 4.054384326934814, val loss: 4.0666890144348145, ETA in seconds: 10550232.792\n",
      "epoch: 734800, train loss: 4.047743701934815, val loss: 4.065517139434815, ETA in seconds: 10547777.136\n",
      "epoch: 734900, train loss: 4.047353076934814, val loss: 4.068446826934815, ETA in seconds: 10545201.755\n",
      "epoch: 735000, train loss: 4.047939014434815, val loss: 4.0637593269348145, ETA in seconds: 10542615.561\n",
      "epoch: 735100, train loss: 4.050478076934814, val loss: 4.071767139434814, ETA in seconds: 10540119.716\n",
      "epoch: 735200, train loss: 4.053407764434814, val loss: 4.070204639434815, ETA in seconds: 10537642.198\n",
      "epoch: 735300, train loss: 4.051259326934814, val loss: 4.0676655769348145, ETA in seconds: 10535136.271\n",
      "epoch: 735400, train loss: 4.055556201934815, val loss: 4.068251514434815, ETA in seconds: 10532545.954\n",
      "epoch: 735500, train loss: 4.051649951934815, val loss: 4.070399951934815, ETA in seconds: 10529955.795\n",
      "epoch: 735600, train loss: 4.061610889434815, val loss: 4.0696187019348145, ETA in seconds: 10527347.560\n",
      "epoch: 735700, train loss: 4.0481343269348145, val loss: 4.068837451934814, ETA in seconds: 10524769.340\n",
      "epoch: 735800, train loss: 4.050673389434815, val loss: 4.0696187019348145, ETA in seconds: 10522232.178\n",
      "epoch: 735900, train loss: 4.0530171394348145, val loss: 4.064540576934815, ETA in seconds: 10519646.393\n",
      "epoch: 736000, train loss: 4.049892139434815, val loss: 4.067079639434814, ETA in seconds: 10517059.310\n",
      "epoch: 736100, train loss: 4.054189014434814, val loss: 4.071962451934814, ETA in seconds: 10514503.117\n",
      "epoch: 736200, train loss: 4.055165576934814, val loss: 4.0627827644348145, ETA in seconds: 10511934.978\n",
      "epoch: 736300, train loss: 4.0471577644348145, val loss: 4.0666890144348145, ETA in seconds: 10509357.846\n",
      "epoch: 736400, train loss: 4.047743701934815, val loss: 4.068837451934814, ETA in seconds: 10506758.625\n",
      "epoch: 736500, train loss: 4.047548389434814, val loss: 4.068446826934815, ETA in seconds: 10504174.586\n",
      "epoch: 736600, train loss: 4.052821826934815, val loss: 4.073134326934815, ETA in seconds: 10501598.849\n",
      "epoch: 736700, train loss: 4.047548389434814, val loss: 4.066298389434815, ETA in seconds: 10499016.731\n",
      "epoch: 736800, train loss: 4.055556201934815, val loss: 4.0735249519348145, ETA in seconds: 10496426.074\n",
      "epoch: 736900, train loss: 4.051454639434814, val loss: 4.063564014434815, ETA in seconds: 10493806.142\n",
      "epoch: 737000, train loss: 4.049501514434814, val loss: 4.0676655769348145, ETA in seconds: 10491187.551\n",
      "epoch: 737100, train loss: 4.052431201934814, val loss: 4.070009326934814, ETA in seconds: 10488597.703\n",
      "epoch: 737200, train loss: 4.054774951934815, val loss: 4.072743701934814, ETA in seconds: 10485980.038\n",
      "epoch: 737300, train loss: 4.058095264434814, val loss: 4.068056201934814, ETA in seconds: 10483401.597\n",
      "epoch: 737400, train loss: 4.050868701934815, val loss: 4.064931201934814, ETA in seconds: 10480832.961\n",
      "epoch: 737500, train loss: 4.050673389434815, val loss: 4.0696187019348145, ETA in seconds: 10478284.690\n",
      "epoch: 737600, train loss: 4.051649951934815, val loss: 4.065321826934815, ETA in seconds: 10475701.654\n",
      "epoch: 737700, train loss: 4.048524951934814, val loss: 4.077040576934815, ETA in seconds: 10473109.079\n",
      "epoch: 737800, train loss: 4.053407764434814, val loss: 4.067274951934815, ETA in seconds: 10470559.114\n",
      "epoch: 737900, train loss: 4.049892139434815, val loss: 4.062392139434815, ETA in seconds: 10468014.107\n",
      "epoch: 738000, train loss: 4.052235889434814, val loss: 4.0637593269348145, ETA in seconds: 10465406.534\n",
      "epoch: 738100, train loss: 4.052431201934814, val loss: 4.0705952644348145, ETA in seconds: 10462803.313\n",
      "epoch: 738200, train loss: 4.0559468269348145, val loss: 4.065321826934815, ETA in seconds: 10460158.916\n",
      "epoch: 738300, train loss: 4.052626514434815, val loss: 4.064345264434815, ETA in seconds: 10457536.380\n",
      "epoch: 738400, train loss: 4.053212451934814, val loss: 4.070985889434814, ETA in seconds: 10454919.495\n",
      "epoch: 738500, train loss: 4.054774951934815, val loss: 4.064931201934814, ETA in seconds: 10452327.478\n",
      "epoch: 738600, train loss: 4.048524951934814, val loss: 4.066884326934814, ETA in seconds: 10449702.462\n",
      "epoch: 738700, train loss: 4.051649951934815, val loss: 4.0627827644348145, ETA in seconds: 10447166.680\n",
      "epoch: 738800, train loss: 4.049696826934815, val loss: 4.060048389434814, ETA in seconds: 10444649.460\n",
      "epoch: 738900, train loss: 4.057118701934814, val loss: 4.069423389434815, ETA in seconds: 10442108.945\n",
      "epoch: 739000, train loss: 4.046962451934815, val loss: 4.071767139434814, ETA in seconds: 10439585.723\n",
      "epoch: 739100, train loss: 4.059462451934815, val loss: 4.0647358894348145, ETA in seconds: 10437002.126\n",
      "epoch: 739200, train loss: 4.044618701934814, val loss: 4.064540576934815, ETA in seconds: 10434443.804\n",
      "epoch: 739300, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 10431842.085\n",
      "epoch: 739400, train loss: 4.051845264434815, val loss: 4.0686421394348145, ETA in seconds: 10429225.701\n",
      "epoch: 739500, train loss: 4.0520405769348145, val loss: 4.0627827644348145, ETA in seconds: 10426636.824\n",
      "epoch: 739600, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 10424031.728\n",
      "epoch: 739700, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 10421383.376\n",
      "epoch: 739800, train loss: 4.055165576934814, val loss: 4.067079639434814, ETA in seconds: 10418710.989\n",
      "epoch: 739900, train loss: 4.051845264434815, val loss: 4.064540576934815, ETA in seconds: 10416088.468\n",
      "epoch: 740000, train loss: 4.055360889434814, val loss: 4.063954639434814, ETA in seconds: 10413445.771\n",
      "epoch: 740100, train loss: 4.052235889434814, val loss: 4.0666890144348145, ETA in seconds: 10410818.343\n",
      "epoch: 740200, train loss: 4.053407764434814, val loss: 4.069423389434815, ETA in seconds: 10408189.222\n",
      "epoch: 740300, train loss: 4.045399951934814, val loss: 4.066884326934814, ETA in seconds: 10405558.810\n",
      "epoch: 740400, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 10402981.336\n",
      "epoch: 740500, train loss: 4.0481343269348145, val loss: 4.066884326934814, ETA in seconds: 10400369.553\n",
      "epoch: 740600, train loss: 4.049501514434814, val loss: 4.0627827644348145, ETA in seconds: 10397760.703\n",
      "epoch: 740700, train loss: 4.050478076934814, val loss: 4.068837451934814, ETA in seconds: 10395130.977\n",
      "epoch: 740800, train loss: 4.046962451934815, val loss: 4.069032764434814, ETA in seconds: 10392483.349\n",
      "epoch: 740900, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 10389833.103\n",
      "epoch: 741000, train loss: 4.045790576934815, val loss: 4.072353076934815, ETA in seconds: 10387173.326\n",
      "epoch: 741100, train loss: 4.059071826934814, val loss: 4.0666890144348145, ETA in seconds: 10384554.313\n",
      "epoch: 741200, train loss: 4.054579639434815, val loss: 4.067079639434814, ETA in seconds: 10381939.892\n",
      "epoch: 741300, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 10379382.932\n",
      "epoch: 741400, train loss: 4.054579639434815, val loss: 4.068056201934814, ETA in seconds: 10376766.403\n",
      "epoch: 741500, train loss: 4.050478076934814, val loss: 4.0686421394348145, ETA in seconds: 10374113.703\n",
      "epoch: 741600, train loss: 4.0481343269348145, val loss: 4.069814014434814, ETA in seconds: 10371469.248\n",
      "epoch: 741700, train loss: 4.055165576934814, val loss: 4.066493701934815, ETA in seconds: 10368837.064\n",
      "epoch: 741800, train loss: 4.052235889434814, val loss: 4.067274951934815, ETA in seconds: 10366173.323\n",
      "epoch: 741900, train loss: 4.051259326934814, val loss: 4.060048389434814, ETA in seconds: 10363538.721\n",
      "epoch: 742000, train loss: 4.058681201934815, val loss: 4.063954639434814, ETA in seconds: 10360886.583\n",
      "epoch: 742100, train loss: 4.0539937019348145, val loss: 4.072939014434814, ETA in seconds: 10358238.453\n",
      "epoch: 742200, train loss: 4.050478076934814, val loss: 4.0647358894348145, ETA in seconds: 10355611.052\n",
      "epoch: 742300, train loss: 4.049696826934815, val loss: 4.067860889434814, ETA in seconds: 10352978.878\n",
      "epoch: 742400, train loss: 4.055751514434815, val loss: 4.065907764434814, ETA in seconds: 10350373.316\n",
      "epoch: 742500, train loss: 4.051259326934814, val loss: 4.069032764434814, ETA in seconds: 10347716.239\n",
      "epoch: 742600, train loss: 4.055165576934814, val loss: 4.066298389434815, ETA in seconds: 10345027.372\n",
      "epoch: 742700, train loss: 4.052626514434815, val loss: 4.064540576934815, ETA in seconds: 10342390.291\n",
      "epoch: 742800, train loss: 4.059071826934814, val loss: 4.069228076934815, ETA in seconds: 10339841.248\n",
      "epoch: 742900, train loss: 4.057118701934814, val loss: 4.0647358894348145, ETA in seconds: 10337261.210\n",
      "epoch: 743000, train loss: 4.050868701934815, val loss: 4.0647358894348145, ETA in seconds: 10334625.980\n",
      "epoch: 743100, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 10331991.355\n",
      "epoch: 743200, train loss: 4.056142139434814, val loss: 4.069423389434815, ETA in seconds: 10329284.169\n",
      "epoch: 743300, train loss: 4.050868701934815, val loss: 4.065321826934815, ETA in seconds: 10326475.787\n",
      "epoch: 743400, train loss: 4.053798389434815, val loss: 4.066103076934814, ETA in seconds: 10323592.494\n",
      "epoch: 743500, train loss: 4.052626514434815, val loss: 4.065321826934815, ETA in seconds: 10320922.378\n",
      "epoch: 743600, train loss: 4.051649951934815, val loss: 4.067274951934815, ETA in seconds: 10318281.336\n",
      "epoch: 743700, train loss: 4.050868701934815, val loss: 4.064345264434815, ETA in seconds: 10315610.999\n",
      "epoch: 743800, train loss: 4.054774951934815, val loss: 4.072939014434814, ETA in seconds: 10312990.991\n",
      "epoch: 743900, train loss: 4.046767139434815, val loss: 4.067079639434814, ETA in seconds: 10310416.338\n",
      "epoch: 744000, train loss: 4.0491108894348145, val loss: 4.069228076934815, ETA in seconds: 10307723.063\n",
      "epoch: 744100, train loss: 4.048720264434815, val loss: 4.066298389434815, ETA in seconds: 10305040.942\n",
      "epoch: 744200, train loss: 4.050868701934815, val loss: 4.0696187019348145, ETA in seconds: 10302342.242\n",
      "epoch: 744300, train loss: 4.054579639434815, val loss: 4.064540576934815, ETA in seconds: 10299644.113\n",
      "epoch: 744400, train loss: 4.048524951934814, val loss: 4.069032764434814, ETA in seconds: 10296954.502\n",
      "epoch: 744500, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 10294278.763\n",
      "epoch: 744600, train loss: 4.0471577644348145, val loss: 4.066298389434815, ETA in seconds: 10291599.940\n",
      "epoch: 744700, train loss: 4.052626514434815, val loss: 4.063564014434815, ETA in seconds: 10288899.911\n",
      "epoch: 744800, train loss: 4.0588765144348145, val loss: 4.062587451934815, ETA in seconds: 10286260.230\n",
      "epoch: 744900, train loss: 4.050478076934814, val loss: 4.0676655769348145, ETA in seconds: 10283569.342\n",
      "epoch: 745000, train loss: 4.0481343269348145, val loss: 4.066298389434815, ETA in seconds: 10280872.124\n",
      "epoch: 745100, train loss: 4.049696826934815, val loss: 4.061415576934815, ETA in seconds: 10278197.507\n",
      "epoch: 745200, train loss: 4.058095264434814, val loss: 4.067860889434814, ETA in seconds: 10275506.632\n",
      "epoch: 745300, train loss: 4.0452046394348145, val loss: 4.063564014434815, ETA in seconds: 10272826.179\n",
      "epoch: 745400, train loss: 4.0549702644348145, val loss: 4.064540576934815, ETA in seconds: 10270146.464\n",
      "epoch: 745500, train loss: 4.051454639434814, val loss: 4.069032764434814, ETA in seconds: 10267471.109\n",
      "epoch: 745600, train loss: 4.059267139434814, val loss: 4.069032764434814, ETA in seconds: 10264798.372\n",
      "epoch: 745700, train loss: 4.048720264434815, val loss: 4.067860889434814, ETA in seconds: 10262105.176\n",
      "epoch: 745800, train loss: 4.050673389434815, val loss: 4.069228076934815, ETA in seconds: 10259390.976\n",
      "epoch: 745900, train loss: 4.0598530769348145, val loss: 4.070790576934814, ETA in seconds: 10256718.782\n",
      "epoch: 746000, train loss: 4.051259326934814, val loss: 4.065126514434814, ETA in seconds: 10254000.137\n",
      "epoch: 746100, train loss: 4.056532764434815, val loss: 4.073134326934815, ETA in seconds: 10251307.319\n",
      "epoch: 746200, train loss: 4.049306201934814, val loss: 4.069032764434814, ETA in seconds: 10248610.393\n",
      "epoch: 746300, train loss: 4.0578999519348145, val loss: 4.064540576934815, ETA in seconds: 10245909.170\n",
      "epoch: 746400, train loss: 4.046767139434815, val loss: 4.074110889434815, ETA in seconds: 10243251.812\n",
      "epoch: 746500, train loss: 4.0549702644348145, val loss: 4.066493701934815, ETA in seconds: 10240576.349\n",
      "epoch: 746600, train loss: 4.048720264434815, val loss: 4.070399951934815, ETA in seconds: 10237862.407\n",
      "epoch: 746700, train loss: 4.058485889434815, val loss: 4.062978076934814, ETA in seconds: 10235141.305\n",
      "epoch: 746800, train loss: 4.050868701934815, val loss: 4.070009326934814, ETA in seconds: 10232433.630\n",
      "epoch: 746900, train loss: 4.048524951934814, val loss: 4.070790576934814, ETA in seconds: 10229706.457\n",
      "epoch: 747000, train loss: 4.0549702644348145, val loss: 4.070790576934814, ETA in seconds: 10226991.651\n",
      "epoch: 747100, train loss: 4.0510640144348145, val loss: 4.067274951934815, ETA in seconds: 10224293.271\n",
      "epoch: 747200, train loss: 4.046571826934814, val loss: 4.0676655769348145, ETA in seconds: 10221684.666\n",
      "epoch: 747300, train loss: 4.0500874519348145, val loss: 4.0686421394348145, ETA in seconds: 10219070.349\n",
      "epoch: 747400, train loss: 4.061610889434815, val loss: 4.070399951934815, ETA in seconds: 10216442.319\n",
      "epoch: 747500, train loss: 4.044618701934814, val loss: 4.070985889434814, ETA in seconds: 10213769.441\n",
      "epoch: 747600, train loss: 4.043837451934815, val loss: 4.069228076934815, ETA in seconds: 10211050.009\n",
      "epoch: 747700, train loss: 4.048720264434815, val loss: 4.0735249519348145, ETA in seconds: 10208333.293\n",
      "epoch: 747800, train loss: 4.052626514434815, val loss: 4.0696187019348145, ETA in seconds: 10205609.041\n",
      "epoch: 747900, train loss: 4.054189014434814, val loss: 4.068446826934815, ETA in seconds: 10202864.939\n",
      "epoch: 748000, train loss: 4.056337451934814, val loss: 4.066298389434815, ETA in seconds: 10200032.784\n",
      "epoch: 748100, train loss: 4.046376514434814, val loss: 4.064149951934814, ETA in seconds: 10197294.406\n",
      "epoch: 748200, train loss: 4.046571826934814, val loss: 4.069228076934815, ETA in seconds: 10194609.160\n",
      "epoch: 748300, train loss: 4.048720264434815, val loss: 4.0725483894348145, ETA in seconds: 10191850.555\n",
      "epoch: 748400, train loss: 4.051454639434814, val loss: 4.0657124519348145, ETA in seconds: 10189108.946\n",
      "epoch: 748500, train loss: 4.049892139434815, val loss: 4.0627827644348145, ETA in seconds: 10186354.365\n",
      "epoch: 748600, train loss: 4.049306201934814, val loss: 4.063954639434814, ETA in seconds: 10183664.340\n",
      "epoch: 748700, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 10180918.760\n",
      "epoch: 748800, train loss: 4.052431201934814, val loss: 4.065321826934815, ETA in seconds: 10178224.727\n",
      "epoch: 748900, train loss: 4.061610889434815, val loss: 4.069228076934815, ETA in seconds: 10175513.220\n",
      "epoch: 749000, train loss: 4.0549702644348145, val loss: 4.068837451934814, ETA in seconds: 10172815.442\n",
      "epoch: 749100, train loss: 4.053798389434815, val loss: 4.065321826934815, ETA in seconds: 10170089.169\n",
      "epoch: 749200, train loss: 4.047548389434814, val loss: 4.0657124519348145, ETA in seconds: 10167353.359\n",
      "epoch: 749300, train loss: 4.050868701934815, val loss: 4.0676655769348145, ETA in seconds: 10164591.434\n",
      "epoch: 749400, train loss: 4.055360889434814, val loss: 4.063954639434814, ETA in seconds: 10161862.725\n",
      "epoch: 749500, train loss: 4.053407764434814, val loss: 4.062587451934815, ETA in seconds: 10159102.376\n",
      "epoch: 749600, train loss: 4.0491108894348145, val loss: 4.070790576934814, ETA in seconds: 10156345.911\n",
      "epoch: 749700, train loss: 4.055360889434814, val loss: 4.0686421394348145, ETA in seconds: 10153664.205\n",
      "epoch: 749800, train loss: 4.057118701934814, val loss: 4.0686421394348145, ETA in seconds: 10150980.703\n",
      "epoch: 749900, train loss: 4.052431201934814, val loss: 4.0647358894348145, ETA in seconds: 10148278.243\n",
      "epoch: 750000, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 10145605.010\n",
      "epoch: 750100, train loss: 4.0500874519348145, val loss: 4.069423389434815, ETA in seconds: 10142908.140\n",
      "epoch: 750200, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 10140232.961\n",
      "epoch: 750300, train loss: 4.056337451934814, val loss: 4.065517139434815, ETA in seconds: 10137541.236\n",
      "epoch: 750400, train loss: 4.050868701934815, val loss: 4.069032764434814, ETA in seconds: 10134873.051\n",
      "epoch: 750500, train loss: 4.051845264434815, val loss: 4.064931201934814, ETA in seconds: 10132138.672\n",
      "epoch: 750600, train loss: 4.055360889434814, val loss: 4.0657124519348145, ETA in seconds: 10129415.624\n",
      "epoch: 750700, train loss: 4.0559468269348145, val loss: 4.069423389434815, ETA in seconds: 10126726.072\n",
      "epoch: 750800, train loss: 4.048329639434814, val loss: 4.067860889434814, ETA in seconds: 10124005.588\n",
      "epoch: 750900, train loss: 4.051845264434815, val loss: 4.063564014434815, ETA in seconds: 10121282.728\n",
      "epoch: 751000, train loss: 4.052626514434815, val loss: 4.064149951934814, ETA in seconds: 10118538.294\n",
      "epoch: 751100, train loss: 4.050282764434814, val loss: 4.069228076934815, ETA in seconds: 10115755.608\n",
      "epoch: 751200, train loss: 4.049501514434814, val loss: 4.062587451934815, ETA in seconds: 10112985.767\n",
      "epoch: 751300, train loss: 4.048524951934814, val loss: 4.069228076934815, ETA in seconds: 10110223.183\n",
      "epoch: 751400, train loss: 4.0559468269348145, val loss: 4.068446826934815, ETA in seconds: 10107480.813\n",
      "epoch: 751500, train loss: 4.051649951934815, val loss: 4.070985889434814, ETA in seconds: 10104752.784\n",
      "epoch: 751600, train loss: 4.052626514434815, val loss: 4.065126514434814, ETA in seconds: 10102007.338\n",
      "epoch: 751700, train loss: 4.057314014434814, val loss: 4.068446826934815, ETA in seconds: 10099247.958\n",
      "epoch: 751800, train loss: 4.049696826934815, val loss: 4.071181201934815, ETA in seconds: 10096461.636\n",
      "epoch: 751900, train loss: 4.053798389434815, val loss: 4.063368701934815, ETA in seconds: 10093693.876\n",
      "epoch: 752000, train loss: 4.051454639434814, val loss: 4.0657124519348145, ETA in seconds: 10090903.363\n",
      "epoch: 752100, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 10088083.556\n",
      "epoch: 752200, train loss: 4.051845264434815, val loss: 4.068837451934814, ETA in seconds: 10085264.496\n",
      "epoch: 752300, train loss: 4.050478076934814, val loss: 4.065321826934815, ETA in seconds: 10082444.172\n",
      "epoch: 752400, train loss: 4.056142139434814, val loss: 4.066884326934814, ETA in seconds: 10079638.895\n",
      "epoch: 752500, train loss: 4.0539937019348145, val loss: 4.0696187019348145, ETA in seconds: 10076815.898\n",
      "epoch: 752600, train loss: 4.053603076934815, val loss: 4.069814014434814, ETA in seconds: 10073977.075\n",
      "epoch: 752700, train loss: 4.050868701934815, val loss: 4.068446826934815, ETA in seconds: 10071155.522\n",
      "epoch: 752800, train loss: 4.0471577644348145, val loss: 4.067079639434814, ETA in seconds: 10068333.647\n",
      "epoch: 752900, train loss: 4.056532764434815, val loss: 4.0696187019348145, ETA in seconds: 10065491.158\n",
      "epoch: 753000, train loss: 4.054384326934814, val loss: 4.069032764434814, ETA in seconds: 10062665.292\n",
      "epoch: 753100, train loss: 4.060243701934814, val loss: 4.069814014434814, ETA in seconds: 10059850.974\n",
      "epoch: 753200, train loss: 4.049501514434814, val loss: 4.0735249519348145, ETA in seconds: 10057001.397\n",
      "epoch: 753300, train loss: 4.045985889434815, val loss: 4.064345264434815, ETA in seconds: 10054189.867\n",
      "epoch: 753400, train loss: 4.0618062019348145, val loss: 4.066298389434815, ETA in seconds: 10051323.355\n",
      "epoch: 753500, train loss: 4.0510640144348145, val loss: 4.069228076934815, ETA in seconds: 10048470.077\n",
      "epoch: 753600, train loss: 4.052235889434814, val loss: 4.067274951934815, ETA in seconds: 10045635.032\n",
      "epoch: 753700, train loss: 4.049306201934814, val loss: 4.066884326934814, ETA in seconds: 10042782.948\n",
      "epoch: 753800, train loss: 4.050868701934815, val loss: 4.0705952644348145, ETA in seconds: 10039945.358\n",
      "epoch: 753900, train loss: 4.055556201934815, val loss: 4.066298389434815, ETA in seconds: 10037130.626\n",
      "epoch: 754000, train loss: 4.048329639434814, val loss: 4.070204639434815, ETA in seconds: 10034284.494\n",
      "epoch: 754100, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 10031464.538\n",
      "epoch: 754200, train loss: 4.048524951934814, val loss: 4.065907764434814, ETA in seconds: 10028568.527\n",
      "epoch: 754300, train loss: 4.049892139434815, val loss: 4.065321826934815, ETA in seconds: 10025716.085\n",
      "epoch: 754400, train loss: 4.053798389434815, val loss: 4.0696187019348145, ETA in seconds: 10022856.949\n",
      "epoch: 754500, train loss: 4.0530171394348145, val loss: 4.060243701934814, ETA in seconds: 10020023.528\n",
      "epoch: 754600, train loss: 4.050673389434815, val loss: 4.0637593269348145, ETA in seconds: 10017159.750\n",
      "epoch: 754700, train loss: 4.051259326934814, val loss: 4.064149951934814, ETA in seconds: 10014320.482\n",
      "epoch: 754800, train loss: 4.055556201934815, val loss: 4.0735249519348145, ETA in seconds: 10011494.666\n",
      "epoch: 754900, train loss: 4.052821826934815, val loss: 4.068446826934815, ETA in seconds: 10008694.537\n",
      "epoch: 755000, train loss: 4.0530171394348145, val loss: 4.0686421394348145, ETA in seconds: 10005856.236\n",
      "epoch: 755100, train loss: 4.050868701934815, val loss: 4.063564014434815, ETA in seconds: 10002976.852\n",
      "epoch: 755200, train loss: 4.053407764434814, val loss: 4.061610889434815, ETA in seconds: 10000210.573\n",
      "epoch: 755300, train loss: 4.050282764434814, val loss: 4.063954639434814, ETA in seconds: 9997470.574\n",
      "epoch: 755400, train loss: 4.056532764434815, val loss: 4.069228076934815, ETA in seconds: 9994684.889\n",
      "epoch: 755500, train loss: 4.054384326934814, val loss: 4.0637593269348145, ETA in seconds: 9991885.911\n",
      "epoch: 755600, train loss: 4.050868701934815, val loss: 4.076064014434815, ETA in seconds: 9989061.370\n",
      "epoch: 755700, train loss: 4.053798389434815, val loss: 4.067079639434814, ETA in seconds: 9986257.385\n",
      "epoch: 755800, train loss: 4.046767139434815, val loss: 4.0657124519348145, ETA in seconds: 9983482.920\n",
      "epoch: 755900, train loss: 4.050478076934814, val loss: 4.065517139434815, ETA in seconds: 9980681.762\n",
      "epoch: 756000, train loss: 4.054579639434815, val loss: 4.070204639434815, ETA in seconds: 9977907.461\n",
      "epoch: 756100, train loss: 4.0549702644348145, val loss: 4.069228076934815, ETA in seconds: 9975100.269\n",
      "epoch: 756200, train loss: 4.047939014434815, val loss: 4.066298389434815, ETA in seconds: 9972279.607\n",
      "epoch: 756300, train loss: 4.0481343269348145, val loss: 4.0657124519348145, ETA in seconds: 9969472.575\n",
      "epoch: 756400, train loss: 4.059267139434814, val loss: 4.070009326934814, ETA in seconds: 9966663.880\n",
      "epoch: 756500, train loss: 4.0539937019348145, val loss: 4.069032764434814, ETA in seconds: 9963854.525\n",
      "epoch: 756600, train loss: 4.052235889434814, val loss: 4.0657124519348145, ETA in seconds: 9961067.766\n",
      "epoch: 756700, train loss: 4.051845264434815, val loss: 4.070204639434815, ETA in seconds: 9958280.284\n",
      "epoch: 756800, train loss: 4.050673389434815, val loss: 4.068837451934814, ETA in seconds: 9955464.112\n",
      "epoch: 756900, train loss: 4.055165576934814, val loss: 4.065126514434814, ETA in seconds: 9952663.536\n",
      "epoch: 757000, train loss: 4.0471577644348145, val loss: 4.066493701934815, ETA in seconds: 9949858.722\n",
      "epoch: 757100, train loss: 4.053603076934815, val loss: 4.067079639434814, ETA in seconds: 9947174.523\n",
      "epoch: 757200, train loss: 4.057509326934815, val loss: 4.068056201934814, ETA in seconds: 9944358.624\n",
      "epoch: 757300, train loss: 4.041689014434814, val loss: 4.063173389434814, ETA in seconds: 9941574.808\n",
      "epoch: 757400, train loss: 4.052235889434814, val loss: 4.062587451934815, ETA in seconds: 9938757.391\n",
      "epoch: 757500, train loss: 4.046571826934814, val loss: 4.0676655769348145, ETA in seconds: 9935980.218\n",
      "epoch: 757600, train loss: 4.057314014434814, val loss: 4.064345264434815, ETA in seconds: 9933175.719\n",
      "epoch: 757700, train loss: 4.057704639434815, val loss: 4.066103076934814, ETA in seconds: 9930337.195\n",
      "epoch: 757800, train loss: 4.047548389434814, val loss: 4.064345264434815, ETA in seconds: 9927546.139\n",
      "epoch: 757900, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 9924738.614\n",
      "epoch: 758000, train loss: 4.0549702644348145, val loss: 4.069423389434815, ETA in seconds: 9921925.890\n",
      "epoch: 758100, train loss: 4.050868701934815, val loss: 4.0696187019348145, ETA in seconds: 9919120.967\n",
      "epoch: 758200, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 9916341.808\n",
      "epoch: 758300, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 9913531.551\n",
      "epoch: 758400, train loss: 4.060243701934814, val loss: 4.0686421394348145, ETA in seconds: 9910729.807\n",
      "epoch: 758500, train loss: 4.050282764434814, val loss: 4.066298389434815, ETA in seconds: 9907881.870\n",
      "epoch: 758600, train loss: 4.055556201934815, val loss: 4.063954639434814, ETA in seconds: 9905069.104\n",
      "epoch: 758700, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 9902240.505\n",
      "epoch: 758800, train loss: 4.051649951934815, val loss: 4.0676655769348145, ETA in seconds: 9899425.571\n",
      "epoch: 758900, train loss: 4.046571826934814, val loss: 4.070985889434814, ETA in seconds: 9896594.162\n",
      "epoch: 759000, train loss: 4.049892139434815, val loss: 4.0637593269348145, ETA in seconds: 9893785.805\n",
      "epoch: 759100, train loss: 4.049501514434814, val loss: 4.069423389434815, ETA in seconds: 9890960.808\n",
      "epoch: 759200, train loss: 4.046376514434814, val loss: 4.066493701934815, ETA in seconds: 9888138.618\n",
      "epoch: 759300, train loss: 4.047548389434814, val loss: 4.070204639434815, ETA in seconds: 9885270.457\n",
      "epoch: 759400, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 9882474.938\n",
      "epoch: 759500, train loss: 4.0500874519348145, val loss: 4.064149951934814, ETA in seconds: 9879666.954\n",
      "epoch: 759600, train loss: 4.051454639434814, val loss: 4.065321826934815, ETA in seconds: 9876796.625\n",
      "epoch: 759700, train loss: 4.057314014434814, val loss: 4.069423389434815, ETA in seconds: 9873980.382\n",
      "epoch: 759800, train loss: 4.049306201934814, val loss: 4.0696187019348145, ETA in seconds: 9871141.014\n",
      "epoch: 759900, train loss: 4.050478076934814, val loss: 4.064540576934815, ETA in seconds: 9868289.903\n",
      "epoch: 760000, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 9865461.752\n",
      "epoch: 760100, train loss: 4.054774951934815, val loss: 4.066493701934815, ETA in seconds: 9862608.438\n",
      "epoch: 760200, train loss: 4.051649951934815, val loss: 4.0725483894348145, ETA in seconds: 9859760.588\n",
      "epoch: 760300, train loss: 4.050868701934815, val loss: 4.065517139434815, ETA in seconds: 9856912.893\n",
      "epoch: 760400, train loss: 4.0481343269348145, val loss: 4.069228076934815, ETA in seconds: 9854041.391\n",
      "epoch: 760500, train loss: 4.058681201934815, val loss: 4.069032764434814, ETA in seconds: 9851193.443\n",
      "epoch: 760600, train loss: 4.0559468269348145, val loss: 4.069228076934815, ETA in seconds: 9848380.932\n",
      "epoch: 760700, train loss: 4.045595264434814, val loss: 4.068251514434815, ETA in seconds: 9845524.874\n",
      "epoch: 760800, train loss: 4.055751514434815, val loss: 4.0676655769348145, ETA in seconds: 9842663.992\n",
      "epoch: 760900, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 9839788.586\n",
      "epoch: 761000, train loss: 4.058095264434814, val loss: 4.0676655769348145, ETA in seconds: 9836995.613\n",
      "epoch: 761100, train loss: 4.055556201934815, val loss: 4.070985889434814, ETA in seconds: 9834146.898\n",
      "epoch: 761200, train loss: 4.042470264434814, val loss: 4.063368701934815, ETA in seconds: 9831316.101\n",
      "epoch: 761300, train loss: 4.0510640144348145, val loss: 4.062978076934814, ETA in seconds: 9828462.660\n",
      "epoch: 761400, train loss: 4.049501514434814, val loss: 4.071767139434814, ETA in seconds: 9825607.557\n",
      "epoch: 761500, train loss: 4.052235889434814, val loss: 4.065517139434815, ETA in seconds: 9822738.436\n",
      "epoch: 761600, train loss: 4.048329639434814, val loss: 4.064931201934814, ETA in seconds: 9819849.620\n",
      "epoch: 761700, train loss: 4.050868701934815, val loss: 4.0686421394348145, ETA in seconds: 9817096.001\n",
      "epoch: 761800, train loss: 4.056532764434815, val loss: 4.0686421394348145, ETA in seconds: 9814253.371\n",
      "epoch: 761900, train loss: 4.057118701934814, val loss: 4.067470264434815, ETA in seconds: 9811402.709\n",
      "epoch: 762000, train loss: 4.055165576934814, val loss: 4.070009326934814, ETA in seconds: 9808535.832\n",
      "epoch: 762100, train loss: 4.0627827644348145, val loss: 4.070399951934815, ETA in seconds: 9805690.240\n",
      "epoch: 762200, train loss: 4.0452046394348145, val loss: 4.067274951934815, ETA in seconds: 9802851.197\n",
      "epoch: 762300, train loss: 4.0491108894348145, val loss: 4.065517139434815, ETA in seconds: 9800000.806\n",
      "epoch: 762400, train loss: 4.056142139434814, val loss: 4.071767139434814, ETA in seconds: 9797125.491\n",
      "epoch: 762500, train loss: 4.047548389434814, val loss: 4.070985889434814, ETA in seconds: 9794259.642\n",
      "epoch: 762600, train loss: 4.047743701934815, val loss: 4.066298389434815, ETA in seconds: 9791393.103\n",
      "epoch: 762700, train loss: 4.0500874519348145, val loss: 4.069814014434814, ETA in seconds: 9788523.623\n",
      "epoch: 762800, train loss: 4.0491108894348145, val loss: 4.066103076934814, ETA in seconds: 9785641.715\n",
      "epoch: 762900, train loss: 4.048524951934814, val loss: 4.069228076934815, ETA in seconds: 9782758.355\n",
      "epoch: 763000, train loss: 4.052821826934815, val loss: 4.068056201934814, ETA in seconds: 9779897.348\n",
      "epoch: 763100, train loss: 4.0520405769348145, val loss: 4.070204639434815, ETA in seconds: 9777023.208\n",
      "epoch: 763200, train loss: 4.051259326934814, val loss: 4.059071826934814, ETA in seconds: 9774148.207\n",
      "epoch: 763300, train loss: 4.061610889434815, val loss: 4.065517139434815, ETA in seconds: 9771263.977\n",
      "epoch: 763400, train loss: 4.055165576934814, val loss: 4.0637593269348145, ETA in seconds: 9768365.823\n",
      "epoch: 763500, train loss: 4.0549702644348145, val loss: 4.068837451934814, ETA in seconds: 9765526.754\n",
      "epoch: 763600, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 9762602.987\n",
      "epoch: 763700, train loss: 4.049501514434814, val loss: 4.070790576934814, ETA in seconds: 9759735.044\n",
      "epoch: 763800, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 9756875.086\n",
      "epoch: 763900, train loss: 4.0500874519348145, val loss: 4.0686421394348145, ETA in seconds: 9753988.283\n",
      "epoch: 764000, train loss: 4.053798389434815, val loss: 4.063368701934815, ETA in seconds: 9751117.000\n",
      "epoch: 764100, train loss: 4.0549702644348145, val loss: 4.074306201934815, ETA in seconds: 9748225.701\n",
      "epoch: 764200, train loss: 4.049306201934814, val loss: 4.070204639434815, ETA in seconds: 9745347.003\n",
      "epoch: 764300, train loss: 4.049501514434814, val loss: 4.069814014434814, ETA in seconds: 9742476.400\n",
      "epoch: 764400, train loss: 4.054774951934815, val loss: 4.0657124519348145, ETA in seconds: 9739554.088\n",
      "epoch: 764500, train loss: 4.048720264434815, val loss: 4.067470264434815, ETA in seconds: 9736662.000\n",
      "epoch: 764600, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 9733786.460\n",
      "epoch: 764700, train loss: 4.051259326934814, val loss: 4.067470264434815, ETA in seconds: 9730886.094\n",
      "epoch: 764800, train loss: 4.050478076934814, val loss: 4.073134326934815, ETA in seconds: 9727993.036\n",
      "epoch: 764900, train loss: 4.048915576934815, val loss: 4.071962451934814, ETA in seconds: 9725064.162\n",
      "epoch: 765000, train loss: 4.051259326934814, val loss: 4.065907764434814, ETA in seconds: 9722146.832\n",
      "epoch: 765100, train loss: 4.053407764434814, val loss: 4.065517139434815, ETA in seconds: 9719237.436\n",
      "epoch: 765200, train loss: 4.052626514434815, val loss: 4.0686421394348145, ETA in seconds: 9716363.789\n",
      "epoch: 765300, train loss: 4.0569233894348145, val loss: 4.064540576934815, ETA in seconds: 9713466.729\n",
      "epoch: 765400, train loss: 4.0539937019348145, val loss: 4.073329639434815, ETA in seconds: 9710590.841\n",
      "epoch: 765500, train loss: 4.051649951934815, val loss: 4.063368701934815, ETA in seconds: 9707727.964\n",
      "epoch: 765600, train loss: 4.047548389434814, val loss: 4.0696187019348145, ETA in seconds: 9704814.728\n",
      "epoch: 765700, train loss: 4.049696826934815, val loss: 4.068446826934815, ETA in seconds: 9701922.593\n",
      "epoch: 765800, train loss: 4.049306201934814, val loss: 4.068446826934815, ETA in seconds: 9698998.215\n",
      "epoch: 765900, train loss: 4.050868701934815, val loss: 4.069423389434815, ETA in seconds: 9696078.090\n",
      "epoch: 766000, train loss: 4.056728076934815, val loss: 4.070204639434815, ETA in seconds: 9693169.840\n",
      "epoch: 766100, train loss: 4.052626514434815, val loss: 4.069814014434814, ETA in seconds: 9690226.432\n",
      "epoch: 766200, train loss: 4.0520405769348145, val loss: 4.0696187019348145, ETA in seconds: 9687316.635\n",
      "epoch: 766300, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 9684374.464\n",
      "epoch: 766400, train loss: 4.048915576934815, val loss: 4.068251514434815, ETA in seconds: 9681434.248\n",
      "epoch: 766500, train loss: 4.0588765144348145, val loss: 4.068056201934814, ETA in seconds: 9678520.264\n",
      "epoch: 766600, train loss: 4.051845264434815, val loss: 4.072939014434814, ETA in seconds: 9675606.096\n",
      "epoch: 766700, train loss: 4.050282764434814, val loss: 4.061415576934815, ETA in seconds: 9672687.379\n",
      "epoch: 766800, train loss: 4.059071826934814, val loss: 4.069423389434815, ETA in seconds: 9669769.443\n",
      "epoch: 766900, train loss: 4.049501514434814, val loss: 4.0637593269348145, ETA in seconds: 9666846.210\n",
      "epoch: 767000, train loss: 4.047743701934815, val loss: 4.069423389434815, ETA in seconds: 9663867.482\n",
      "epoch: 767100, train loss: 4.052235889434814, val loss: 4.062196826934814, ETA in seconds: 9660943.192\n",
      "epoch: 767200, train loss: 4.055360889434814, val loss: 4.063368701934815, ETA in seconds: 9658034.250\n",
      "epoch: 767300, train loss: 4.051845264434815, val loss: 4.065907764434814, ETA in seconds: 9655112.366\n",
      "epoch: 767400, train loss: 4.052626514434815, val loss: 4.070204639434815, ETA in seconds: 9652187.464\n",
      "epoch: 767500, train loss: 4.053603076934815, val loss: 4.0637593269348145, ETA in seconds: 9649259.882\n",
      "epoch: 767600, train loss: 4.047548389434814, val loss: 4.068837451934814, ETA in seconds: 9646335.792\n",
      "epoch: 767700, train loss: 4.055556201934815, val loss: 4.0637593269348145, ETA in seconds: 9643428.217\n",
      "epoch: 767800, train loss: 4.0510640144348145, val loss: 4.066298389434815, ETA in seconds: 9640534.909\n",
      "epoch: 767900, train loss: 4.056728076934815, val loss: 4.065126514434814, ETA in seconds: 9637630.068\n",
      "epoch: 768000, train loss: 4.052235889434814, val loss: 4.067860889434814, ETA in seconds: 9634732.068\n",
      "epoch: 768100, train loss: 4.0471577644348145, val loss: 4.069423389434815, ETA in seconds: 9631836.943\n",
      "epoch: 768200, train loss: 4.0539937019348145, val loss: 4.059267139434814, ETA in seconds: 9628920.796\n",
      "epoch: 768300, train loss: 4.050478076934814, val loss: 4.0666890144348145, ETA in seconds: 9625996.896\n",
      "epoch: 768400, train loss: 4.057704639434815, val loss: 4.069228076934815, ETA in seconds: 9623060.427\n",
      "epoch: 768500, train loss: 4.056532764434815, val loss: 4.0715718269348145, ETA in seconds: 9620107.920\n",
      "epoch: 768600, train loss: 4.048915576934815, val loss: 4.068056201934814, ETA in seconds: 9617158.772\n",
      "epoch: 768700, train loss: 4.0520405769348145, val loss: 4.0666890144348145, ETA in seconds: 9614203.079\n",
      "epoch: 768800, train loss: 4.062001514434814, val loss: 4.062978076934814, ETA in seconds: 9611283.579\n",
      "epoch: 768900, train loss: 4.050673389434815, val loss: 4.074306201934815, ETA in seconds: 9608340.870\n",
      "epoch: 769000, train loss: 4.055751514434815, val loss: 4.065321826934815, ETA in seconds: 9605402.239\n",
      "epoch: 769100, train loss: 4.052821826934815, val loss: 4.063564014434815, ETA in seconds: 9602466.523\n",
      "epoch: 769200, train loss: 4.0559468269348145, val loss: 4.0637593269348145, ETA in seconds: 9599535.215\n",
      "epoch: 769300, train loss: 4.0520405769348145, val loss: 4.062392139434815, ETA in seconds: 9596553.620\n",
      "epoch: 769400, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 9593604.818\n",
      "epoch: 769500, train loss: 4.048329639434814, val loss: 4.0647358894348145, ETA in seconds: 9590659.359\n",
      "epoch: 769600, train loss: 4.050868701934815, val loss: 4.073134326934815, ETA in seconds: 9587767.412\n",
      "epoch: 769700, train loss: 4.057704639434815, val loss: 4.068446826934815, ETA in seconds: 9584833.087\n",
      "epoch: 769800, train loss: 4.049892139434815, val loss: 4.0705952644348145, ETA in seconds: 9581929.441\n",
      "epoch: 769900, train loss: 4.052431201934814, val loss: 4.061220264434814, ETA in seconds: 9578992.103\n",
      "epoch: 770000, train loss: 4.0500874519348145, val loss: 4.063173389434814, ETA in seconds: 9576072.034\n",
      "epoch: 770100, train loss: 4.0481343269348145, val loss: 4.066493701934815, ETA in seconds: 9573203.339\n",
      "epoch: 770200, train loss: 4.046376514434814, val loss: 4.067470264434815, ETA in seconds: 9570260.194\n",
      "epoch: 770300, train loss: 4.047743701934815, val loss: 4.065321826934815, ETA in seconds: 9567323.263\n",
      "epoch: 770400, train loss: 4.0520405769348145, val loss: 4.063954639434814, ETA in seconds: 9564386.027\n",
      "epoch: 770500, train loss: 4.056142139434814, val loss: 4.071376514434815, ETA in seconds: 9561441.727\n",
      "epoch: 770600, train loss: 4.051649951934815, val loss: 4.072157764434815, ETA in seconds: 9558504.113\n",
      "epoch: 770700, train loss: 4.046571826934814, val loss: 4.066884326934814, ETA in seconds: 9555444.017\n",
      "epoch: 770800, train loss: 4.054189014434814, val loss: 4.064931201934814, ETA in seconds: 9552482.680\n",
      "epoch: 770900, train loss: 4.055360889434814, val loss: 4.065517139434815, ETA in seconds: 9549558.000\n",
      "epoch: 771000, train loss: 4.047353076934814, val loss: 4.067274951934815, ETA in seconds: 9546638.815\n",
      "epoch: 771100, train loss: 4.048915576934815, val loss: 4.064345264434815, ETA in seconds: 9543824.426\n",
      "epoch: 771200, train loss: 4.0559468269348145, val loss: 4.071767139434814, ETA in seconds: 9540996.749\n",
      "epoch: 771300, train loss: 4.051454639434814, val loss: 4.071767139434814, ETA in seconds: 9538131.850\n",
      "epoch: 771400, train loss: 4.047548389434814, val loss: 4.069032764434814, ETA in seconds: 9535211.578\n",
      "epoch: 771500, train loss: 4.053407764434814, val loss: 4.070399951934815, ETA in seconds: 9532267.216\n",
      "epoch: 771600, train loss: 4.049306201934814, val loss: 4.0696187019348145, ETA in seconds: 9529313.570\n",
      "epoch: 771700, train loss: 4.055556201934815, val loss: 4.068056201934814, ETA in seconds: 9526332.585\n",
      "epoch: 771800, train loss: 4.0578999519348145, val loss: 4.064345264434815, ETA in seconds: 9523345.188\n",
      "epoch: 771900, train loss: 4.0510640144348145, val loss: 4.061610889434815, ETA in seconds: 9520372.006\n",
      "epoch: 772000, train loss: 4.056337451934814, val loss: 4.0676655769348145, ETA in seconds: 9517427.528\n",
      "epoch: 772100, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 9514493.616\n",
      "epoch: 772200, train loss: 4.047939014434815, val loss: 4.070985889434814, ETA in seconds: 9511533.530\n",
      "epoch: 772300, train loss: 4.053798389434815, val loss: 4.073134326934815, ETA in seconds: 9508571.447\n",
      "epoch: 772400, train loss: 4.050868701934815, val loss: 4.0715718269348145, ETA in seconds: 9505594.918\n",
      "epoch: 772500, train loss: 4.0569233894348145, val loss: 4.066103076934814, ETA in seconds: 9502663.742\n",
      "epoch: 772600, train loss: 4.0530171394348145, val loss: 4.0686421394348145, ETA in seconds: 9499716.423\n",
      "epoch: 772700, train loss: 4.045009326934815, val loss: 4.070204639434815, ETA in seconds: 9496782.274\n",
      "epoch: 772800, train loss: 4.058095264434814, val loss: 4.066103076934814, ETA in seconds: 9493785.024\n",
      "epoch: 772900, train loss: 4.0500874519348145, val loss: 4.067860889434814, ETA in seconds: 9490831.595\n",
      "epoch: 773000, train loss: 4.052626514434815, val loss: 4.062196826934814, ETA in seconds: 9487845.814\n",
      "epoch: 773100, train loss: 4.045009326934815, val loss: 4.0705952644348145, ETA in seconds: 9484847.217\n",
      "epoch: 773200, train loss: 4.054774951934815, val loss: 4.066884326934814, ETA in seconds: 9481862.896\n",
      "epoch: 773300, train loss: 4.053798389434815, val loss: 4.067079639434814, ETA in seconds: 9478908.162\n",
      "epoch: 773400, train loss: 4.049306201934814, val loss: 4.065907764434814, ETA in seconds: 9475936.151\n",
      "epoch: 773500, train loss: 4.047743701934815, val loss: 4.070204639434815, ETA in seconds: 9472954.539\n",
      "epoch: 773600, train loss: 4.053798389434815, val loss: 4.063368701934815, ETA in seconds: 9469990.697\n",
      "epoch: 773700, train loss: 4.0539937019348145, val loss: 4.071767139434814, ETA in seconds: 9467013.988\n",
      "epoch: 773800, train loss: 4.049501514434814, val loss: 4.062587451934815, ETA in seconds: 9464131.687\n",
      "epoch: 773900, train loss: 4.058681201934815, val loss: 4.067274951934815, ETA in seconds: 9461208.480\n",
      "epoch: 774000, train loss: 4.050282764434814, val loss: 4.067079639434814, ETA in seconds: 9458219.696\n",
      "epoch: 774100, train loss: 4.049696826934815, val loss: 4.061220264434814, ETA in seconds: 9455270.907\n",
      "epoch: 774200, train loss: 4.047548389434814, val loss: 4.069032764434814, ETA in seconds: 9452312.276\n",
      "epoch: 774300, train loss: 4.057704639434815, val loss: 4.065517139434815, ETA in seconds: 9449353.966\n",
      "epoch: 774400, train loss: 4.048329639434814, val loss: 4.064931201934814, ETA in seconds: 9446297.409\n",
      "epoch: 774500, train loss: 4.057704639434815, val loss: 4.064931201934814, ETA in seconds: 9443154.466\n",
      "epoch: 774600, train loss: 4.049892139434815, val loss: 4.061415576934815, ETA in seconds: 9440170.068\n",
      "epoch: 774700, train loss: 4.049501514434814, val loss: 4.064540576934815, ETA in seconds: 9437205.701\n",
      "epoch: 774800, train loss: 4.050282764434814, val loss: 4.067079639434814, ETA in seconds: 9434223.502\n",
      "epoch: 774900, train loss: 4.043056201934815, val loss: 4.0705952644348145, ETA in seconds: 9431214.090\n",
      "epoch: 775000, train loss: 4.051259326934814, val loss: 4.063954639434814, ETA in seconds: 9428209.873\n",
      "epoch: 775100, train loss: 4.054774951934815, val loss: 4.0696187019348145, ETA in seconds: 9425219.034\n",
      "epoch: 775200, train loss: 4.049892139434815, val loss: 4.0686421394348145, ETA in seconds: 9422210.996\n",
      "epoch: 775300, train loss: 4.051649951934815, val loss: 4.0657124519348145, ETA in seconds: 9419213.462\n",
      "epoch: 775400, train loss: 4.048720264434815, val loss: 4.062001514434814, ETA in seconds: 9416200.177\n",
      "epoch: 775500, train loss: 4.054579639434815, val loss: 4.073329639434815, ETA in seconds: 9413205.314\n",
      "epoch: 775600, train loss: 4.054774951934815, val loss: 4.0637593269348145, ETA in seconds: 9410205.310\n",
      "epoch: 775700, train loss: 4.048720264434815, val loss: 4.069032764434814, ETA in seconds: 9407209.964\n",
      "epoch: 775800, train loss: 4.058095264434814, val loss: 4.070009326934814, ETA in seconds: 9404197.242\n",
      "epoch: 775900, train loss: 4.0598530769348145, val loss: 4.069423389434815, ETA in seconds: 9401185.585\n",
      "epoch: 776000, train loss: 4.050868701934815, val loss: 4.0657124519348145, ETA in seconds: 9398182.803\n",
      "epoch: 776100, train loss: 4.047743701934815, val loss: 4.065907764434814, ETA in seconds: 9395154.799\n",
      "epoch: 776200, train loss: 4.0549702644348145, val loss: 4.0657124519348145, ETA in seconds: 9392104.469\n",
      "epoch: 776300, train loss: 4.058681201934815, val loss: 4.069228076934815, ETA in seconds: 9389055.913\n",
      "epoch: 776400, train loss: 4.052821826934815, val loss: 4.0725483894348145, ETA in seconds: 9386039.626\n",
      "epoch: 776500, train loss: 4.0471577644348145, val loss: 4.0666890144348145, ETA in seconds: 9383015.598\n",
      "epoch: 776600, train loss: 4.054579639434815, val loss: 4.066103076934814, ETA in seconds: 9379971.984\n",
      "epoch: 776700, train loss: 4.050282764434814, val loss: 4.065907764434814, ETA in seconds: 9376950.236\n",
      "epoch: 776800, train loss: 4.053798389434815, val loss: 4.068837451934814, ETA in seconds: 9373938.076\n",
      "epoch: 776900, train loss: 4.047548389434814, val loss: 4.065517139434815, ETA in seconds: 9370896.770\n",
      "epoch: 777000, train loss: 4.056142139434814, val loss: 4.063173389434814, ETA in seconds: 9367870.958\n",
      "epoch: 777100, train loss: 4.054189014434814, val loss: 4.070790576934814, ETA in seconds: 9364800.259\n",
      "epoch: 777200, train loss: 4.047939014434815, val loss: 4.0666890144348145, ETA in seconds: 9361749.636\n",
      "epoch: 777300, train loss: 4.054774951934815, val loss: 4.074892139434814, ETA in seconds: 9358725.045\n",
      "epoch: 777400, train loss: 4.049696826934815, val loss: 4.070399951934815, ETA in seconds: 9355701.498\n",
      "epoch: 777500, train loss: 4.052821826934815, val loss: 4.068837451934814, ETA in seconds: 9352672.250\n",
      "epoch: 777600, train loss: 4.045985889434815, val loss: 4.071181201934815, ETA in seconds: 9349669.580\n",
      "epoch: 777700, train loss: 4.052235889434814, val loss: 4.067079639434814, ETA in seconds: 9346669.375\n",
      "epoch: 777800, train loss: 4.050868701934815, val loss: 4.069814014434814, ETA in seconds: 9343667.154\n",
      "epoch: 777900, train loss: 4.052235889434814, val loss: 4.070009326934814, ETA in seconds: 9340627.457\n",
      "epoch: 778000, train loss: 4.056728076934815, val loss: 4.068446826934815, ETA in seconds: 9337597.108\n",
      "epoch: 778100, train loss: 4.0510640144348145, val loss: 4.070204639434815, ETA in seconds: 9334543.185\n",
      "epoch: 778200, train loss: 4.049501514434814, val loss: 4.074306201934815, ETA in seconds: 9331506.943\n",
      "epoch: 778300, train loss: 4.050478076934814, val loss: 4.071181201934815, ETA in seconds: 9328482.111\n",
      "epoch: 778400, train loss: 4.053603076934815, val loss: 4.0735249519348145, ETA in seconds: 9325452.876\n",
      "epoch: 778500, train loss: 4.0530171394348145, val loss: 4.0705952644348145, ETA in seconds: 9322404.018\n",
      "epoch: 778600, train loss: 4.050282764434814, val loss: 4.063173389434814, ETA in seconds: 9319359.581\n",
      "epoch: 778700, train loss: 4.057118701934814, val loss: 4.067860889434814, ETA in seconds: 9316301.852\n",
      "epoch: 778800, train loss: 4.049696826934815, val loss: 4.066884326934814, ETA in seconds: 9313254.511\n",
      "epoch: 778900, train loss: 4.049696826934815, val loss: 4.0657124519348145, ETA in seconds: 9310202.479\n",
      "epoch: 779000, train loss: 4.051649951934815, val loss: 4.069032764434814, ETA in seconds: 9307158.258\n",
      "epoch: 779100, train loss: 4.0500874519348145, val loss: 4.067860889434814, ETA in seconds: 9304138.744\n",
      "epoch: 779200, train loss: 4.052235889434814, val loss: 4.073134326934815, ETA in seconds: 9301129.657\n",
      "epoch: 779300, train loss: 4.054384326934814, val loss: 4.0686421394348145, ETA in seconds: 9298130.750\n",
      "epoch: 779400, train loss: 4.057118701934814, val loss: 4.061220264434814, ETA in seconds: 9295125.113\n",
      "epoch: 779500, train loss: 4.0500874519348145, val loss: 4.0578999519348145, ETA in seconds: 9292096.943\n",
      "epoch: 779600, train loss: 4.053212451934814, val loss: 4.067079639434814, ETA in seconds: 9289044.670\n",
      "epoch: 779700, train loss: 4.049501514434814, val loss: 4.0696187019348145, ETA in seconds: 9286043.887\n",
      "epoch: 779800, train loss: 4.051649951934815, val loss: 4.069228076934815, ETA in seconds: 9283028.615\n",
      "epoch: 779900, train loss: 4.054579639434815, val loss: 4.0657124519348145, ETA in seconds: 9280008.278\n",
      "epoch: 780000, train loss: 4.046962451934815, val loss: 4.071376514434815, ETA in seconds: 9276965.399\n",
      "epoch: 780100, train loss: 4.050868701934815, val loss: 4.0657124519348145, ETA in seconds: 9273949.872\n",
      "epoch: 780200, train loss: 4.0559468269348145, val loss: 4.072939014434814, ETA in seconds: 9270887.551\n",
      "epoch: 780300, train loss: 4.0471577644348145, val loss: 4.070204639434815, ETA in seconds: 9267914.318\n",
      "epoch: 780400, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 9264884.790\n",
      "epoch: 780500, train loss: 4.053407764434814, val loss: 4.0686421394348145, ETA in seconds: 9261900.861\n",
      "epoch: 780600, train loss: 4.0578999519348145, val loss: 4.063564014434815, ETA in seconds: 9258885.113\n",
      "epoch: 780700, train loss: 4.055165576934814, val loss: 4.074696826934814, ETA in seconds: 9255844.116\n",
      "epoch: 780800, train loss: 4.054774951934815, val loss: 4.070790576934814, ETA in seconds: 9252767.546\n",
      "epoch: 780900, train loss: 4.0500874519348145, val loss: 4.065907764434814, ETA in seconds: 9249702.694\n",
      "epoch: 781000, train loss: 4.058681201934815, val loss: 4.0618062019348145, ETA in seconds: 9246639.020\n",
      "epoch: 781100, train loss: 4.0569233894348145, val loss: 4.064931201934814, ETA in seconds: 9243584.943\n",
      "epoch: 781200, train loss: 4.0510640144348145, val loss: 4.0686421394348145, ETA in seconds: 9240512.966\n",
      "epoch: 781300, train loss: 4.046571826934814, val loss: 4.067860889434814, ETA in seconds: 9237446.381\n",
      "epoch: 781400, train loss: 4.051259326934814, val loss: 4.0647358894348145, ETA in seconds: 9234380.602\n",
      "epoch: 781500, train loss: 4.048915576934815, val loss: 4.062978076934814, ETA in seconds: 9231318.951\n",
      "epoch: 781600, train loss: 4.049892139434815, val loss: 4.071962451934814, ETA in seconds: 9228275.666\n",
      "epoch: 781700, train loss: 4.047939014434815, val loss: 4.064540576934815, ETA in seconds: 9225206.088\n",
      "epoch: 781800, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 9222107.543\n",
      "epoch: 781900, train loss: 4.0491108894348145, val loss: 4.063564014434815, ETA in seconds: 9219030.064\n",
      "epoch: 782000, train loss: 4.0471577644348145, val loss: 4.0608296394348145, ETA in seconds: 9215958.952\n",
      "epoch: 782100, train loss: 4.044814014434815, val loss: 4.066103076934814, ETA in seconds: 9212866.232\n",
      "epoch: 782200, train loss: 4.051845264434815, val loss: 4.067079639434814, ETA in seconds: 9209794.305\n",
      "epoch: 782300, train loss: 4.060048389434814, val loss: 4.064931201934814, ETA in seconds: 9206734.959\n",
      "epoch: 782400, train loss: 4.056337451934814, val loss: 4.0657124519348145, ETA in seconds: 9203667.619\n",
      "epoch: 782500, train loss: 4.051259326934814, val loss: 4.072353076934815, ETA in seconds: 9200598.117\n",
      "epoch: 782600, train loss: 4.050673389434815, val loss: 4.064149951934814, ETA in seconds: 9197530.963\n",
      "epoch: 782700, train loss: 4.0530171394348145, val loss: 4.066298389434815, ETA in seconds: 9194464.536\n",
      "epoch: 782800, train loss: 4.054579639434815, val loss: 4.073915576934814, ETA in seconds: 9191413.260\n",
      "epoch: 782900, train loss: 4.051454639434814, val loss: 4.068446826934815, ETA in seconds: 9188410.030\n",
      "epoch: 783000, train loss: 4.055165576934814, val loss: 4.070399951934815, ETA in seconds: 9185413.400\n",
      "epoch: 783100, train loss: 4.048329639434814, val loss: 4.0608296394348145, ETA in seconds: 9182405.654\n",
      "epoch: 783200, train loss: 4.055751514434815, val loss: 4.070009326934814, ETA in seconds: 9179350.523\n",
      "epoch: 783300, train loss: 4.058095264434814, val loss: 4.065907764434814, ETA in seconds: 9176345.797\n",
      "epoch: 783400, train loss: 4.0452046394348145, val loss: 4.062978076934814, ETA in seconds: 9173315.635\n",
      "epoch: 783500, train loss: 4.056728076934815, val loss: 4.070009326934814, ETA in seconds: 9170259.806\n",
      "epoch: 783600, train loss: 4.055751514434815, val loss: 4.063954639434814, ETA in seconds: 9167157.730\n",
      "epoch: 783700, train loss: 4.052626514434815, val loss: 4.062392139434815, ETA in seconds: 9164147.091\n",
      "epoch: 783800, train loss: 4.045985889434815, val loss: 4.068837451934814, ETA in seconds: 9161105.759\n",
      "epoch: 783900, train loss: 4.050868701934815, val loss: 4.064931201934814, ETA in seconds: 9158087.083\n",
      "epoch: 784000, train loss: 4.0549702644348145, val loss: 4.066493701934815, ETA in seconds: 9155016.366\n",
      "epoch: 784100, train loss: 4.057314014434814, val loss: 4.068251514434815, ETA in seconds: 9151982.989\n",
      "epoch: 784200, train loss: 4.0432515144348145, val loss: 4.067274951934815, ETA in seconds: 9148913.225\n",
      "epoch: 784300, train loss: 4.053212451934814, val loss: 4.0608296394348145, ETA in seconds: 9145852.449\n",
      "epoch: 784400, train loss: 4.057509326934815, val loss: 4.063368701934815, ETA in seconds: 9142737.842\n",
      "epoch: 784500, train loss: 4.055165576934814, val loss: 4.072743701934814, ETA in seconds: 9139641.742\n",
      "epoch: 784600, train loss: 4.050282764434814, val loss: 4.066298389434815, ETA in seconds: 9136561.962\n",
      "epoch: 784700, train loss: 4.0539937019348145, val loss: 4.068837451934814, ETA in seconds: 9133457.484\n",
      "epoch: 784800, train loss: 4.0530171394348145, val loss: 4.065517139434815, ETA in seconds: 9130370.967\n",
      "epoch: 784900, train loss: 4.0539937019348145, val loss: 4.077040576934815, ETA in seconds: 9127262.404\n",
      "epoch: 785000, train loss: 4.048720264434815, val loss: 4.074892139434814, ETA in seconds: 9124163.006\n",
      "epoch: 785100, train loss: 4.050282764434814, val loss: 4.067079639434814, ETA in seconds: 9121055.144\n",
      "epoch: 785200, train loss: 4.060243701934814, val loss: 4.062978076934814, ETA in seconds: 9117968.837\n",
      "epoch: 785300, train loss: 4.049696826934815, val loss: 4.069814014434814, ETA in seconds: 9114841.198\n",
      "epoch: 785400, train loss: 4.055556201934815, val loss: 4.067860889434814, ETA in seconds: 9111750.470\n",
      "epoch: 785500, train loss: 4.0520405769348145, val loss: 4.063564014434815, ETA in seconds: 9108651.373\n",
      "epoch: 785600, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 9105536.509\n",
      "epoch: 785700, train loss: 4.0500874519348145, val loss: 4.065126514434814, ETA in seconds: 9102441.861\n",
      "epoch: 785800, train loss: 4.049306201934814, val loss: 4.065321826934815, ETA in seconds: 9099360.763\n",
      "epoch: 785900, train loss: 4.0500874519348145, val loss: 4.066103076934814, ETA in seconds: 9096284.507\n",
      "epoch: 786000, train loss: 4.0491108894348145, val loss: 4.065321826934815, ETA in seconds: 9093184.154\n",
      "epoch: 786100, train loss: 4.051649951934815, val loss: 4.0696187019348145, ETA in seconds: 9090065.380\n",
      "epoch: 786200, train loss: 4.0520405769348145, val loss: 4.069032764434814, ETA in seconds: 9086933.918\n",
      "epoch: 786300, train loss: 4.0549702644348145, val loss: 4.064540576934815, ETA in seconds: 9083802.649\n",
      "epoch: 786400, train loss: 4.046571826934814, val loss: 4.064345264434815, ETA in seconds: 9080769.392\n",
      "epoch: 786500, train loss: 4.0569233894348145, val loss: 4.066884326934814, ETA in seconds: 9077727.074\n",
      "epoch: 786600, train loss: 4.051649951934815, val loss: 4.0657124519348145, ETA in seconds: 9074603.801\n",
      "epoch: 786700, train loss: 4.052626514434815, val loss: 4.070790576934814, ETA in seconds: 9071470.739\n",
      "epoch: 786800, train loss: 4.0569233894348145, val loss: 4.065907764434814, ETA in seconds: 9068344.036\n",
      "epoch: 786900, train loss: 4.044814014434815, val loss: 4.066298389434815, ETA in seconds: 9065188.138\n",
      "epoch: 787000, train loss: 4.046376514434814, val loss: 4.068056201934814, ETA in seconds: 9062060.622\n",
      "epoch: 787100, train loss: 4.051454639434814, val loss: 4.064540576934815, ETA in seconds: 9058951.235\n",
      "epoch: 787200, train loss: 4.050282764434814, val loss: 4.0666890144348145, ETA in seconds: 9055819.400\n",
      "epoch: 787300, train loss: 4.047548389434814, val loss: 4.067274951934815, ETA in seconds: 9052676.356\n",
      "epoch: 787400, train loss: 4.058485889434815, val loss: 4.064345264434815, ETA in seconds: 9049544.064\n",
      "epoch: 787500, train loss: 4.051454639434814, val loss: 4.064345264434815, ETA in seconds: 9046408.073\n",
      "epoch: 787600, train loss: 4.048720264434815, val loss: 4.067470264434815, ETA in seconds: 9043286.584\n",
      "epoch: 787700, train loss: 4.049892139434815, val loss: 4.066884326934814, ETA in seconds: 9040155.999\n",
      "epoch: 787800, train loss: 4.0578999519348145, val loss: 4.068251514434815, ETA in seconds: 9036984.681\n",
      "epoch: 787900, train loss: 4.053212451934814, val loss: 4.062196826934814, ETA in seconds: 9033831.914\n",
      "epoch: 788000, train loss: 4.0491108894348145, val loss: 4.062978076934814, ETA in seconds: 9030670.274\n",
      "epoch: 788100, train loss: 4.057704639434815, val loss: 4.065517139434815, ETA in seconds: 9027527.363\n",
      "epoch: 788200, train loss: 4.049501514434814, val loss: 4.066493701934815, ETA in seconds: 9024401.347\n",
      "epoch: 788300, train loss: 4.054774951934815, val loss: 4.067079639434814, ETA in seconds: 9021298.194\n",
      "epoch: 788400, train loss: 4.048720264434815, val loss: 4.060634326934815, ETA in seconds: 9018173.932\n",
      "epoch: 788500, train loss: 4.047743701934815, val loss: 4.070985889434814, ETA in seconds: 9015070.414\n",
      "epoch: 788600, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 9011938.834\n",
      "epoch: 788700, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 9008772.699\n",
      "epoch: 788800, train loss: 4.062196826934814, val loss: 4.065321826934815, ETA in seconds: 9005606.889\n",
      "epoch: 788900, train loss: 4.050868701934815, val loss: 4.070204639434815, ETA in seconds: 9002484.937\n",
      "epoch: 789000, train loss: 4.0510640144348145, val loss: 4.068251514434815, ETA in seconds: 8999335.096\n",
      "epoch: 789100, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 8996188.525\n",
      "epoch: 789200, train loss: 4.051259326934814, val loss: 4.066493701934815, ETA in seconds: 8993041.029\n",
      "epoch: 789300, train loss: 4.045399951934814, val loss: 4.068837451934814, ETA in seconds: 8989873.119\n",
      "epoch: 789400, train loss: 4.055165576934814, val loss: 4.0676655769348145, ETA in seconds: 8986713.749\n",
      "epoch: 789500, train loss: 4.055556201934815, val loss: 4.0705952644348145, ETA in seconds: 8983339.564\n",
      "epoch: 789600, train loss: 4.049696826934815, val loss: 4.068837451934814, ETA in seconds: 8980090.509\n",
      "epoch: 789700, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 8976910.202\n",
      "epoch: 789800, train loss: 4.051845264434815, val loss: 4.069228076934815, ETA in seconds: 8973741.434\n",
      "epoch: 789900, train loss: 4.0549702644348145, val loss: 4.069032764434814, ETA in seconds: 8970590.584\n",
      "epoch: 790000, train loss: 4.053798389434815, val loss: 4.070399951934815, ETA in seconds: 8967425.345\n",
      "epoch: 790100, train loss: 4.058290576934814, val loss: 4.067274951934815, ETA in seconds: 8964250.001\n",
      "epoch: 790200, train loss: 4.047939014434815, val loss: 4.072939014434814, ETA in seconds: 8961054.499\n",
      "epoch: 790300, train loss: 4.046767139434815, val loss: 4.070985889434814, ETA in seconds: 8957906.381\n",
      "epoch: 790400, train loss: 4.0598530769348145, val loss: 4.073329639434815, ETA in seconds: 8954760.115\n",
      "epoch: 790500, train loss: 4.049501514434814, val loss: 4.0676655769348145, ETA in seconds: 8951580.128\n",
      "epoch: 790600, train loss: 4.046376514434814, val loss: 4.061024951934814, ETA in seconds: 8948442.490\n",
      "epoch: 790700, train loss: 4.046962451934815, val loss: 4.069228076934815, ETA in seconds: 8945271.038\n",
      "epoch: 790800, train loss: 4.048524951934814, val loss: 4.067860889434814, ETA in seconds: 8942108.823\n",
      "epoch: 790900, train loss: 4.048915576934815, val loss: 4.070009326934814, ETA in seconds: 8938927.483\n",
      "epoch: 791000, train loss: 4.054189014434814, val loss: 4.068837451934814, ETA in seconds: 8935744.956\n",
      "epoch: 791100, train loss: 4.0481343269348145, val loss: 4.064931201934814, ETA in seconds: 8932635.889\n",
      "epoch: 791200, train loss: 4.047353076934814, val loss: 4.067274951934815, ETA in seconds: 8929482.724\n",
      "epoch: 791300, train loss: 4.049696826934815, val loss: 4.063954639434814, ETA in seconds: 8926307.554\n",
      "epoch: 791400, train loss: 4.048524951934814, val loss: 4.064149951934814, ETA in seconds: 8923118.399\n",
      "epoch: 791500, train loss: 4.049306201934814, val loss: 4.063368701934815, ETA in seconds: 8919945.895\n",
      "epoch: 791600, train loss: 4.051649951934815, val loss: 4.069814014434814, ETA in seconds: 8916754.252\n",
      "epoch: 791700, train loss: 4.055165576934814, val loss: 4.072157764434815, ETA in seconds: 8913581.528\n",
      "epoch: 791800, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 8910397.487\n",
      "epoch: 791900, train loss: 4.054384326934814, val loss: 4.066884326934814, ETA in seconds: 8907224.166\n",
      "epoch: 792000, train loss: 4.054189014434814, val loss: 4.064931201934814, ETA in seconds: 8904059.205\n",
      "epoch: 792100, train loss: 4.049696826934815, val loss: 4.068837451934814, ETA in seconds: 8900883.657\n",
      "epoch: 792200, train loss: 4.053603076934815, val loss: 4.069032764434814, ETA in seconds: 8897715.123\n",
      "epoch: 792300, train loss: 4.050478076934814, val loss: 4.070985889434814, ETA in seconds: 8894535.267\n",
      "epoch: 792400, train loss: 4.0559468269348145, val loss: 4.068837451934814, ETA in seconds: 8891350.048\n",
      "epoch: 792500, train loss: 4.046376514434814, val loss: 4.069423389434815, ETA in seconds: 8888186.552\n",
      "epoch: 792600, train loss: 4.047353076934814, val loss: 4.061415576934815, ETA in seconds: 8885015.467\n",
      "epoch: 792700, train loss: 4.0559468269348145, val loss: 4.073915576934814, ETA in seconds: 8881817.984\n",
      "epoch: 792800, train loss: 4.054579639434815, val loss: 4.064540576934815, ETA in seconds: 8878630.412\n",
      "epoch: 792900, train loss: 4.0510640144348145, val loss: 4.066493701934815, ETA in seconds: 8875446.696\n",
      "epoch: 793000, train loss: 4.048720264434815, val loss: 4.067860889434814, ETA in seconds: 8872272.580\n",
      "epoch: 793100, train loss: 4.054189014434814, val loss: 4.0666890144348145, ETA in seconds: 8869082.024\n",
      "epoch: 793200, train loss: 4.054579639434815, val loss: 4.0686421394348145, ETA in seconds: 8865870.553\n",
      "epoch: 793300, train loss: 4.047743701934815, val loss: 4.0657124519348145, ETA in seconds: 8862662.066\n",
      "epoch: 793400, train loss: 4.046767139434815, val loss: 4.066884326934814, ETA in seconds: 8859475.999\n",
      "epoch: 793500, train loss: 4.0578999519348145, val loss: 4.071181201934815, ETA in seconds: 8856291.346\n",
      "epoch: 793600, train loss: 4.056337451934814, val loss: 4.065517139434815, ETA in seconds: 8853114.536\n",
      "epoch: 793700, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 8849922.691\n",
      "epoch: 793800, train loss: 4.0491108894348145, val loss: 4.065126514434814, ETA in seconds: 8846716.150\n",
      "epoch: 793900, train loss: 4.050478076934814, val loss: 4.063954639434814, ETA in seconds: 8843499.603\n",
      "epoch: 794000, train loss: 4.054579639434815, val loss: 4.064931201934814, ETA in seconds: 8840303.033\n",
      "epoch: 794100, train loss: 4.048720264434815, val loss: 4.066103076934814, ETA in seconds: 8837107.374\n",
      "epoch: 794200, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 8833908.264\n",
      "epoch: 794300, train loss: 4.048524951934814, val loss: 4.0657124519348145, ETA in seconds: 8830700.055\n",
      "epoch: 794400, train loss: 4.0500874519348145, val loss: 4.071962451934814, ETA in seconds: 8827499.126\n",
      "epoch: 794500, train loss: 4.0471577644348145, val loss: 4.067079639434814, ETA in seconds: 8824407.223\n",
      "epoch: 794600, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 8821241.378\n",
      "epoch: 794700, train loss: 4.0500874519348145, val loss: 4.061415576934815, ETA in seconds: 8818040.780\n",
      "epoch: 794800, train loss: 4.0500874519348145, val loss: 4.069228076934815, ETA in seconds: 8814851.613\n",
      "epoch: 794900, train loss: 4.049696826934815, val loss: 4.069032764434814, ETA in seconds: 8811653.626\n",
      "epoch: 795000, train loss: 4.048720264434815, val loss: 4.070009326934814, ETA in seconds: 8808462.456\n",
      "epoch: 795100, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 8805247.500\n",
      "epoch: 795200, train loss: 4.058290576934814, val loss: 4.0696187019348145, ETA in seconds: 8802061.114\n",
      "epoch: 795300, train loss: 4.052235889434814, val loss: 4.0666890144348145, ETA in seconds: 8798877.623\n",
      "epoch: 795400, train loss: 4.052431201934814, val loss: 4.065907764434814, ETA in seconds: 8795686.773\n",
      "epoch: 795500, train loss: 4.055556201934815, val loss: 4.0666890144348145, ETA in seconds: 8792500.657\n",
      "epoch: 795600, train loss: 4.047939014434815, val loss: 4.061610889434815, ETA in seconds: 8789296.136\n",
      "epoch: 795700, train loss: 4.050868701934815, val loss: 4.067470264434815, ETA in seconds: 8786117.637\n",
      "epoch: 795800, train loss: 4.051845264434815, val loss: 4.071962451934814, ETA in seconds: 8782885.483\n",
      "epoch: 795900, train loss: 4.0559468269348145, val loss: 4.0696187019348145, ETA in seconds: 8779657.433\n",
      "epoch: 796000, train loss: 4.0520405769348145, val loss: 4.070009326934814, ETA in seconds: 8776430.243\n",
      "epoch: 796100, train loss: 4.051845264434815, val loss: 4.064540576934815, ETA in seconds: 8773222.118\n",
      "epoch: 796200, train loss: 4.050673389434815, val loss: 4.067079639434814, ETA in seconds: 8769984.208\n",
      "epoch: 796300, train loss: 4.052431201934814, val loss: 4.065321826934815, ETA in seconds: 8766742.939\n",
      "epoch: 796400, train loss: 4.055360889434814, val loss: 4.0647358894348145, ETA in seconds: 8763551.411\n",
      "epoch: 796500, train loss: 4.057118701934814, val loss: 4.0627827644348145, ETA in seconds: 8760326.570\n",
      "epoch: 796600, train loss: 4.048915576934815, val loss: 4.065517139434815, ETA in seconds: 8757075.140\n",
      "epoch: 796700, train loss: 4.056142139434814, val loss: 4.066103076934814, ETA in seconds: 8753871.276\n",
      "epoch: 796800, train loss: 4.055165576934814, val loss: 4.069228076934815, ETA in seconds: 8750639.660\n",
      "epoch: 796900, train loss: 4.055751514434815, val loss: 4.062978076934814, ETA in seconds: 8747452.421\n",
      "epoch: 797000, train loss: 4.043446826934814, val loss: 4.067079639434814, ETA in seconds: 8744270.871\n",
      "epoch: 797100, train loss: 4.045009326934815, val loss: 4.069228076934815, ETA in seconds: 8741085.801\n",
      "epoch: 797200, train loss: 4.050868701934815, val loss: 4.0647358894348145, ETA in seconds: 8737869.395\n",
      "epoch: 797300, train loss: 4.0500874519348145, val loss: 4.064931201934814, ETA in seconds: 8734649.252\n",
      "epoch: 797400, train loss: 4.055360889434814, val loss: 4.065321826934815, ETA in seconds: 8731490.066\n",
      "epoch: 797500, train loss: 4.0530171394348145, val loss: 4.068446826934815, ETA in seconds: 8728279.569\n",
      "epoch: 797600, train loss: 4.053603076934815, val loss: 4.068056201934814, ETA in seconds: 8725066.452\n",
      "epoch: 797700, train loss: 4.050868701934815, val loss: 4.0676655769348145, ETA in seconds: 8721861.339\n",
      "epoch: 797800, train loss: 4.0461812019348145, val loss: 4.067860889434814, ETA in seconds: 8718646.592\n",
      "epoch: 797900, train loss: 4.046376514434814, val loss: 4.070985889434814, ETA in seconds: 8715403.956\n",
      "epoch: 798000, train loss: 4.0569233894348145, val loss: 4.0686421394348145, ETA in seconds: 8712149.487\n",
      "epoch: 798100, train loss: 4.053407764434814, val loss: 4.063173389434814, ETA in seconds: 8708886.752\n",
      "epoch: 798200, train loss: 4.046571826934814, val loss: 4.069423389434815, ETA in seconds: 8705639.635\n",
      "epoch: 798300, train loss: 4.0481343269348145, val loss: 4.065126514434814, ETA in seconds: 8702388.211\n",
      "epoch: 798400, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 8699140.108\n",
      "epoch: 798500, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 8695875.335\n",
      "epoch: 798600, train loss: 4.051259326934814, val loss: 4.058095264434814, ETA in seconds: 8692620.914\n",
      "epoch: 798700, train loss: 4.042079639434815, val loss: 4.061220264434814, ETA in seconds: 8689330.744\n",
      "epoch: 798800, train loss: 4.0471577644348145, val loss: 4.066493701934815, ETA in seconds: 8686087.217\n",
      "epoch: 798900, train loss: 4.054579639434815, val loss: 4.065907764434814, ETA in seconds: 8682828.633\n",
      "epoch: 799000, train loss: 4.048329639434814, val loss: 4.063954639434814, ETA in seconds: 8679566.141\n",
      "epoch: 799100, train loss: 4.054774951934815, val loss: 4.0696187019348145, ETA in seconds: 8676299.762\n",
      "epoch: 799200, train loss: 4.0491108894348145, val loss: 4.062392139434815, ETA in seconds: 8673040.745\n",
      "epoch: 799300, train loss: 4.0491108894348145, val loss: 4.073134326934815, ETA in seconds: 8669795.514\n",
      "epoch: 799400, train loss: 4.051454639434814, val loss: 4.069423389434815, ETA in seconds: 8666558.645\n",
      "epoch: 799500, train loss: 4.051259326934814, val loss: 4.064149951934814, ETA in seconds: 8663374.385\n",
      "epoch: 799600, train loss: 4.051454639434814, val loss: 4.062196826934814, ETA in seconds: 8660126.020\n",
      "epoch: 799700, train loss: 4.056532764434815, val loss: 4.067274951934815, ETA in seconds: 8656872.346\n",
      "epoch: 799800, train loss: 4.0471577644348145, val loss: 4.068837451934814, ETA in seconds: 8653709.140\n",
      "epoch: 799900, train loss: 4.052821826934815, val loss: 4.070009326934814, ETA in seconds: 8650469.505\n",
      "epoch: 800000, train loss: 4.0520405769348145, val loss: 4.063173389434814, ETA in seconds: 8647208.592\n",
      "epoch: 800100, train loss: 4.0549702644348145, val loss: 4.0637593269348145, ETA in seconds: 8643966.076\n",
      "epoch: 800200, train loss: 4.052821826934815, val loss: 4.069814014434814, ETA in seconds: 8640707.489\n",
      "epoch: 800300, train loss: 4.053407764434814, val loss: 4.067470264434815, ETA in seconds: 8637475.706\n",
      "epoch: 800400, train loss: 4.047548389434814, val loss: 4.070009326934814, ETA in seconds: 8634238.797\n",
      "epoch: 800500, train loss: 4.047939014434815, val loss: 4.068446826934815, ETA in seconds: 8630996.427\n",
      "epoch: 800600, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 8627748.185\n",
      "epoch: 800700, train loss: 4.051845264434815, val loss: 4.0618062019348145, ETA in seconds: 8624489.408\n",
      "epoch: 800800, train loss: 4.059657764434815, val loss: 4.0696187019348145, ETA in seconds: 8621193.092\n",
      "epoch: 800900, train loss: 4.0510640144348145, val loss: 4.058095264434814, ETA in seconds: 8617907.383\n",
      "epoch: 801000, train loss: 4.053603076934815, val loss: 4.0618062019348145, ETA in seconds: 8614617.147\n",
      "epoch: 801100, train loss: 4.056337451934814, val loss: 4.065126514434814, ETA in seconds: 8611331.595\n",
      "epoch: 801200, train loss: 4.055751514434815, val loss: 4.067274951934815, ETA in seconds: 8608039.435\n",
      "epoch: 801300, train loss: 4.057314014434814, val loss: 4.069423389434815, ETA in seconds: 8604735.053\n",
      "epoch: 801400, train loss: 4.049892139434815, val loss: 4.067079639434814, ETA in seconds: 8601442.886\n",
      "epoch: 801500, train loss: 4.057314014434814, val loss: 4.062978076934814, ETA in seconds: 8598142.270\n",
      "epoch: 801600, train loss: 4.0539937019348145, val loss: 4.068446826934815, ETA in seconds: 8594842.337\n",
      "epoch: 801700, train loss: 4.049892139434815, val loss: 4.060048389434814, ETA in seconds: 8591561.238\n",
      "epoch: 801800, train loss: 4.0539937019348145, val loss: 4.065321826934815, ETA in seconds: 8588245.212\n",
      "epoch: 801900, train loss: 4.055360889434814, val loss: 4.0627827644348145, ETA in seconds: 8584958.586\n",
      "epoch: 802000, train loss: 4.052626514434815, val loss: 4.068056201934814, ETA in seconds: 8581673.344\n",
      "epoch: 802100, train loss: 4.055751514434815, val loss: 4.072157764434815, ETA in seconds: 8578375.021\n",
      "epoch: 802200, train loss: 4.045790576934815, val loss: 4.065517139434815, ETA in seconds: 8575091.347\n",
      "epoch: 802300, train loss: 4.054579639434815, val loss: 4.060048389434814, ETA in seconds: 8571794.753\n",
      "epoch: 802400, train loss: 4.050868701934815, val loss: 4.067274951934815, ETA in seconds: 8568510.661\n",
      "epoch: 802500, train loss: 4.050673389434815, val loss: 4.067274951934815, ETA in seconds: 8565201.049\n",
      "epoch: 802600, train loss: 4.0539937019348145, val loss: 4.064345264434815, ETA in seconds: 8561894.300\n",
      "epoch: 802700, train loss: 4.048524951934814, val loss: 4.067860889434814, ETA in seconds: 8558607.184\n",
      "epoch: 802800, train loss: 4.043056201934815, val loss: 4.0647358894348145, ETA in seconds: 8555342.749\n",
      "epoch: 802900, train loss: 4.062587451934815, val loss: 4.067860889434814, ETA in seconds: 8552076.678\n",
      "epoch: 803000, train loss: 4.047743701934815, val loss: 4.066103076934814, ETA in seconds: 8548745.552\n",
      "epoch: 803100, train loss: 4.049501514434814, val loss: 4.069423389434815, ETA in seconds: 8545434.246\n",
      "epoch: 803200, train loss: 4.0510640144348145, val loss: 4.063368701934815, ETA in seconds: 8542156.606\n",
      "epoch: 803300, train loss: 4.049696826934815, val loss: 4.067274951934815, ETA in seconds: 8538858.805\n",
      "epoch: 803400, train loss: 4.052431201934814, val loss: 4.0725483894348145, ETA in seconds: 8535580.089\n",
      "epoch: 803500, train loss: 4.058290576934814, val loss: 4.064345264434815, ETA in seconds: 8532283.751\n",
      "epoch: 803600, train loss: 4.0569233894348145, val loss: 4.070985889434814, ETA in seconds: 8528995.081\n",
      "epoch: 803700, train loss: 4.053407764434814, val loss: 4.067079639434814, ETA in seconds: 8525699.931\n",
      "epoch: 803800, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 8522397.742\n",
      "epoch: 803900, train loss: 4.058095264434814, val loss: 4.076259326934815, ETA in seconds: 8519106.282\n",
      "epoch: 804000, train loss: 4.049696826934815, val loss: 4.063368701934815, ETA in seconds: 8515814.411\n",
      "epoch: 804100, train loss: 4.055556201934815, val loss: 4.064931201934814, ETA in seconds: 8512518.329\n",
      "epoch: 804200, train loss: 4.051454639434814, val loss: 4.065517139434815, ETA in seconds: 8509234.077\n",
      "epoch: 804300, train loss: 4.046376514434814, val loss: 4.070985889434814, ETA in seconds: 8505933.821\n",
      "epoch: 804400, train loss: 4.048915576934815, val loss: 4.069228076934815, ETA in seconds: 8502630.015\n",
      "epoch: 804500, train loss: 4.0569233894348145, val loss: 4.066884326934814, ETA in seconds: 8499326.235\n",
      "epoch: 804600, train loss: 4.0403218269348145, val loss: 4.068837451934814, ETA in seconds: 8496020.269\n",
      "epoch: 804700, train loss: 4.049501514434814, val loss: 4.060243701934814, ETA in seconds: 8492748.591\n",
      "epoch: 804800, train loss: 4.051845264434815, val loss: 4.064540576934815, ETA in seconds: 8489448.698\n",
      "epoch: 804900, train loss: 4.0549702644348145, val loss: 4.067860889434814, ETA in seconds: 8486149.292\n",
      "epoch: 805000, train loss: 4.049696826934815, val loss: 4.067860889434814, ETA in seconds: 8482845.163\n",
      "epoch: 805100, train loss: 4.047353076934814, val loss: 4.064149951934814, ETA in seconds: 8479542.381\n",
      "epoch: 805200, train loss: 4.049892139434815, val loss: 4.068446826934815, ETA in seconds: 8476298.023\n",
      "epoch: 805300, train loss: 4.047353076934814, val loss: 4.063564014434815, ETA in seconds: 8473104.473\n",
      "epoch: 805400, train loss: 4.0510640144348145, val loss: 4.0705952644348145, ETA in seconds: 8469814.494\n",
      "epoch: 805500, train loss: 4.0491108894348145, val loss: 4.070790576934814, ETA in seconds: 8466515.355\n",
      "epoch: 805600, train loss: 4.046571826934814, val loss: 4.068056201934814, ETA in seconds: 8463206.522\n",
      "epoch: 805700, train loss: 4.051259326934814, val loss: 4.0608296394348145, ETA in seconds: 8459909.096\n",
      "epoch: 805800, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 8456616.589\n",
      "epoch: 805900, train loss: 4.046767139434815, val loss: 4.064540576934815, ETA in seconds: 8453297.471\n",
      "epoch: 806000, train loss: 4.049501514434814, val loss: 4.0725483894348145, ETA in seconds: 8449984.382\n",
      "epoch: 806100, train loss: 4.047939014434815, val loss: 4.076064014434815, ETA in seconds: 8446688.172\n",
      "epoch: 806200, train loss: 4.050282764434814, val loss: 4.063564014434815, ETA in seconds: 8443372.828\n",
      "epoch: 806300, train loss: 4.0549702644348145, val loss: 4.0686421394348145, ETA in seconds: 8440053.674\n",
      "epoch: 806400, train loss: 4.049696826934815, val loss: 4.0715718269348145, ETA in seconds: 8436750.799\n",
      "epoch: 806500, train loss: 4.047743701934815, val loss: 4.070009326934814, ETA in seconds: 8433434.333\n",
      "epoch: 806600, train loss: 4.051259326934814, val loss: 4.069814014434814, ETA in seconds: 8430122.729\n",
      "epoch: 806700, train loss: 4.048915576934815, val loss: 4.067079639434814, ETA in seconds: 8426804.725\n",
      "epoch: 806800, train loss: 4.052821826934815, val loss: 4.066884326934814, ETA in seconds: 8423497.340\n",
      "epoch: 806900, train loss: 4.0500874519348145, val loss: 4.065321826934815, ETA in seconds: 8420196.598\n",
      "epoch: 807000, train loss: 4.050868701934815, val loss: 4.063954639434814, ETA in seconds: 8416925.357\n",
      "epoch: 807100, train loss: 4.0461812019348145, val loss: 4.063368701934815, ETA in seconds: 8413583.097\n",
      "epoch: 807200, train loss: 4.055360889434814, val loss: 4.0637593269348145, ETA in seconds: 8410292.629\n",
      "epoch: 807300, train loss: 4.053798389434815, val loss: 4.071376514434815, ETA in seconds: 8407064.552\n",
      "epoch: 807400, train loss: 4.0559468269348145, val loss: 4.067860889434814, ETA in seconds: 8403812.126\n",
      "epoch: 807500, train loss: 4.042860889434815, val loss: 4.065126514434814, ETA in seconds: 8400459.897\n",
      "epoch: 807600, train loss: 4.049501514434814, val loss: 4.068446826934815, ETA in seconds: 8397114.432\n",
      "epoch: 807700, train loss: 4.054579639434815, val loss: 4.071376514434815, ETA in seconds: 8393784.244\n",
      "epoch: 807800, train loss: 4.054189014434814, val loss: 4.064345264434815, ETA in seconds: 8390416.040\n",
      "epoch: 807900, train loss: 4.047939014434815, val loss: 4.065907764434814, ETA in seconds: 8387058.040\n",
      "epoch: 808000, train loss: 4.054189014434814, val loss: 4.0657124519348145, ETA in seconds: 8383683.700\n",
      "epoch: 808100, train loss: 4.0539937019348145, val loss: 4.065321826934815, ETA in seconds: 8380328.138\n",
      "epoch: 808200, train loss: 4.053798389434815, val loss: 4.066103076934814, ETA in seconds: 8376981.511\n",
      "epoch: 808300, train loss: 4.0520405769348145, val loss: 4.0686421394348145, ETA in seconds: 8373624.397\n",
      "epoch: 808400, train loss: 4.056142139434814, val loss: 4.067860889434814, ETA in seconds: 8370258.803\n",
      "epoch: 808500, train loss: 4.053212451934814, val loss: 4.068837451934814, ETA in seconds: 8366919.405\n",
      "epoch: 808600, train loss: 4.049501514434814, val loss: 4.067274951934815, ETA in seconds: 8363578.309\n",
      "epoch: 808700, train loss: 4.050478076934814, val loss: 4.065321826934815, ETA in seconds: 8360232.688\n",
      "epoch: 808800, train loss: 4.052431201934814, val loss: 4.066103076934814, ETA in seconds: 8356880.129\n",
      "epoch: 808900, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 8353515.402\n",
      "epoch: 809000, train loss: 4.053798389434815, val loss: 4.0627827644348145, ETA in seconds: 8350140.744\n",
      "epoch: 809100, train loss: 4.0569233894348145, val loss: 4.064345264434815, ETA in seconds: 8346771.475\n",
      "epoch: 809200, train loss: 4.0539937019348145, val loss: 4.0735249519348145, ETA in seconds: 8343396.734\n",
      "epoch: 809300, train loss: 4.0608296394348145, val loss: 4.0647358894348145, ETA in seconds: 8340008.782\n",
      "epoch: 809400, train loss: 4.045790576934815, val loss: 4.0618062019348145, ETA in seconds: 8336631.157\n",
      "epoch: 809500, train loss: 4.043642139434814, val loss: 4.063368701934815, ETA in seconds: 8333277.442\n",
      "epoch: 809600, train loss: 4.0481343269348145, val loss: 4.069423389434815, ETA in seconds: 8330002.050\n",
      "epoch: 809700, train loss: 4.050478076934814, val loss: 4.067079639434814, ETA in seconds: 8326684.440\n",
      "epoch: 809800, train loss: 4.052431201934814, val loss: 4.066884326934814, ETA in seconds: 8323371.134\n",
      "epoch: 809900, train loss: 4.050673389434815, val loss: 4.067079639434814, ETA in seconds: 8320015.586\n",
      "epoch: 810000, train loss: 4.048720264434815, val loss: 4.068837451934814, ETA in seconds: 8316688.622\n",
      "epoch: 810100, train loss: 4.053212451934814, val loss: 4.065517139434815, ETA in seconds: 8313323.762\n",
      "epoch: 810200, train loss: 4.053798389434815, val loss: 4.0715718269348145, ETA in seconds: 8309948.445\n",
      "epoch: 810300, train loss: 4.056728076934815, val loss: 4.064540576934815, ETA in seconds: 8306567.993\n",
      "epoch: 810400, train loss: 4.049892139434815, val loss: 4.064931201934814, ETA in seconds: 8303237.329\n",
      "epoch: 810500, train loss: 4.047548389434814, val loss: 4.060634326934815, ETA in seconds: 8299913.898\n",
      "epoch: 810600, train loss: 4.0500874519348145, val loss: 4.069032764434814, ETA in seconds: 8296587.562\n",
      "epoch: 810700, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 8293250.171\n",
      "epoch: 810800, train loss: 4.049306201934814, val loss: 4.063954639434814, ETA in seconds: 8289913.889\n",
      "epoch: 810900, train loss: 4.053603076934815, val loss: 4.072157764434815, ETA in seconds: 8286531.132\n",
      "epoch: 811000, train loss: 4.051649951934815, val loss: 4.062392139434815, ETA in seconds: 8283135.382\n",
      "epoch: 811100, train loss: 4.058681201934815, val loss: 4.070009326934814, ETA in seconds: 8279755.453\n",
      "epoch: 811200, train loss: 4.050478076934814, val loss: 4.069228076934815, ETA in seconds: 8276383.988\n",
      "epoch: 811300, train loss: 4.059462451934815, val loss: 4.067079639434814, ETA in seconds: 8273004.210\n",
      "epoch: 811400, train loss: 4.048915576934815, val loss: 4.0676655769348145, ETA in seconds: 8269632.874\n",
      "epoch: 811500, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 8266264.159\n",
      "epoch: 811600, train loss: 4.055751514434815, val loss: 4.0666890144348145, ETA in seconds: 8262945.408\n",
      "epoch: 811700, train loss: 4.055556201934815, val loss: 4.0725483894348145, ETA in seconds: 8259609.568\n",
      "epoch: 811800, train loss: 4.0510640144348145, val loss: 4.061024951934814, ETA in seconds: 8256247.967\n",
      "epoch: 811900, train loss: 4.045399951934814, val loss: 4.067274951934815, ETA in seconds: 8252880.793\n",
      "epoch: 812000, train loss: 4.051454639434814, val loss: 4.067470264434815, ETA in seconds: 8249501.367\n",
      "epoch: 812100, train loss: 4.045009326934815, val loss: 4.060634326934815, ETA in seconds: 8246168.485\n",
      "epoch: 812200, train loss: 4.041493701934814, val loss: 4.068446826934815, ETA in seconds: 8242879.996\n",
      "epoch: 812300, train loss: 4.047743701934815, val loss: 4.068837451934814, ETA in seconds: 8239582.336\n",
      "epoch: 812400, train loss: 4.050868701934815, val loss: 4.0686421394348145, ETA in seconds: 8236256.236\n",
      "epoch: 812500, train loss: 4.049696826934815, val loss: 4.064345264434815, ETA in seconds: 8232885.925\n",
      "epoch: 812600, train loss: 4.049306201934814, val loss: 4.068251514434815, ETA in seconds: 8229516.282\n",
      "epoch: 812700, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 8226130.450\n",
      "epoch: 812800, train loss: 4.053798389434815, val loss: 4.066103076934814, ETA in seconds: 8222811.681\n",
      "epoch: 812900, train loss: 4.052626514434815, val loss: 4.062392139434815, ETA in seconds: 8219427.761\n",
      "epoch: 813000, train loss: 4.053212451934814, val loss: 4.066884326934814, ETA in seconds: 8216019.255\n",
      "epoch: 813100, train loss: 4.058290576934814, val loss: 4.0676655769348145, ETA in seconds: 8212615.718\n",
      "epoch: 813200, train loss: 4.045399951934814, val loss: 4.067079639434814, ETA in seconds: 8209200.349\n",
      "epoch: 813300, train loss: 4.056337451934814, val loss: 4.069228076934815, ETA in seconds: 8205806.760\n",
      "epoch: 813400, train loss: 4.055556201934815, val loss: 4.067079639434814, ETA in seconds: 8202424.835\n",
      "epoch: 813500, train loss: 4.045595264434814, val loss: 4.068837451934814, ETA in seconds: 8199037.272\n",
      "epoch: 813600, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 8195653.661\n",
      "epoch: 813700, train loss: 4.055556201934815, val loss: 4.075087451934815, ETA in seconds: 8192259.396\n",
      "epoch: 813800, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 8188891.853\n",
      "epoch: 813900, train loss: 4.055165576934814, val loss: 4.068446826934815, ETA in seconds: 8185507.776\n",
      "epoch: 814000, train loss: 4.051845264434815, val loss: 4.069032764434814, ETA in seconds: 8182130.594\n",
      "epoch: 814100, train loss: 4.046376514434814, val loss: 4.068446826934815, ETA in seconds: 8178729.170\n",
      "epoch: 814200, train loss: 4.050282764434814, val loss: 4.067860889434814, ETA in seconds: 8175299.493\n",
      "epoch: 814300, train loss: 4.049306201934814, val loss: 4.0715718269348145, ETA in seconds: 8171912.447\n",
      "epoch: 814400, train loss: 4.051845264434815, val loss: 4.067274951934815, ETA in seconds: 8168539.365\n",
      "epoch: 814500, train loss: 4.0520405769348145, val loss: 4.067860889434814, ETA in seconds: 8165110.283\n",
      "epoch: 814600, train loss: 4.0481343269348145, val loss: 4.068446826934815, ETA in seconds: 8161718.783\n",
      "epoch: 814700, train loss: 4.054579639434815, val loss: 4.071181201934815, ETA in seconds: 8158333.170\n",
      "epoch: 814800, train loss: 4.046962451934815, val loss: 4.063368701934815, ETA in seconds: 8154908.904\n",
      "epoch: 814900, train loss: 4.054189014434814, val loss: 4.067470264434815, ETA in seconds: 8151485.811\n",
      "epoch: 815000, train loss: 4.060439014434815, val loss: 4.068056201934814, ETA in seconds: 8148090.683\n",
      "epoch: 815100, train loss: 4.048915576934815, val loss: 4.066298389434815, ETA in seconds: 8144669.095\n",
      "epoch: 815200, train loss: 4.046571826934814, val loss: 4.070399951934815, ETA in seconds: 8141240.987\n",
      "epoch: 815300, train loss: 4.049696826934815, val loss: 4.069423389434815, ETA in seconds: 8137827.971\n",
      "epoch: 815400, train loss: 4.049501514434814, val loss: 4.064931201934814, ETA in seconds: 8134395.559\n",
      "epoch: 815500, train loss: 4.051845264434815, val loss: 4.065517139434815, ETA in seconds: 8130965.407\n",
      "epoch: 815600, train loss: 4.053212451934814, val loss: 4.064540576934815, ETA in seconds: 8127538.061\n",
      "epoch: 815700, train loss: 4.055360889434814, val loss: 4.0705952644348145, ETA in seconds: 8124191.235\n",
      "epoch: 815800, train loss: 4.057509326934815, val loss: 4.068251514434815, ETA in seconds: 8120847.113\n",
      "epoch: 815900, train loss: 4.052431201934814, val loss: 4.070009326934814, ETA in seconds: 8117424.312\n",
      "epoch: 816000, train loss: 4.051649951934815, val loss: 4.064931201934814, ETA in seconds: 8114010.407\n",
      "epoch: 816100, train loss: 4.052235889434814, val loss: 4.069423389434815, ETA in seconds: 8110605.942\n",
      "epoch: 816200, train loss: 4.052821826934815, val loss: 4.073134326934815, ETA in seconds: 8107163.365\n",
      "epoch: 816300, train loss: 4.046962451934815, val loss: 4.066493701934815, ETA in seconds: 8103735.401\n",
      "epoch: 816400, train loss: 4.055751514434815, val loss: 4.0676655769348145, ETA in seconds: 8100268.203\n",
      "epoch: 816500, train loss: 4.0500874519348145, val loss: 4.074892139434814, ETA in seconds: 8096845.241\n",
      "epoch: 816600, train loss: 4.047743701934815, val loss: 4.064931201934814, ETA in seconds: 8093406.443\n",
      "epoch: 816700, train loss: 4.0491108894348145, val loss: 4.072353076934815, ETA in seconds: 8089981.518\n",
      "epoch: 816800, train loss: 4.060243701934814, val loss: 4.070204639434815, ETA in seconds: 8086530.714\n",
      "epoch: 816900, train loss: 4.052626514434815, val loss: 4.064540576934815, ETA in seconds: 8083097.019\n",
      "epoch: 817000, train loss: 4.051649951934815, val loss: 4.0676655769348145, ETA in seconds: 8079661.241\n",
      "epoch: 817100, train loss: 4.050673389434815, val loss: 4.071962451934814, ETA in seconds: 8076241.186\n",
      "epoch: 817200, train loss: 4.053212451934814, val loss: 4.066298389434815, ETA in seconds: 8072834.815\n",
      "epoch: 817300, train loss: 4.055165576934814, val loss: 4.068837451934814, ETA in seconds: 8069412.623\n",
      "epoch: 817400, train loss: 4.057704639434815, val loss: 4.068251514434815, ETA in seconds: 8065969.107\n",
      "epoch: 817500, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 8062541.452\n",
      "epoch: 817600, train loss: 4.053798389434815, val loss: 4.070204639434815, ETA in seconds: 8059088.558\n",
      "epoch: 817700, train loss: 4.052235889434814, val loss: 4.067274951934815, ETA in seconds: 8055668.744\n",
      "epoch: 817800, train loss: 4.050673389434815, val loss: 4.064931201934814, ETA in seconds: 8052340.288\n",
      "epoch: 817900, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 8048963.158\n",
      "epoch: 818000, train loss: 4.0500874519348145, val loss: 4.062392139434815, ETA in seconds: 8045583.454\n",
      "epoch: 818100, train loss: 4.048524951934814, val loss: 4.066298389434815, ETA in seconds: 8042219.901\n",
      "epoch: 818200, train loss: 4.044814014434815, val loss: 4.065126514434814, ETA in seconds: 8038832.453\n",
      "epoch: 818300, train loss: 4.055751514434815, val loss: 4.0666890144348145, ETA in seconds: 8035421.136\n",
      "epoch: 818400, train loss: 4.053603076934815, val loss: 4.066103076934814, ETA in seconds: 8031956.315\n",
      "epoch: 818500, train loss: 4.056532764434815, val loss: 4.069814014434814, ETA in seconds: 8028508.600\n",
      "epoch: 818600, train loss: 4.048915576934815, val loss: 4.070790576934814, ETA in seconds: 8025053.005\n",
      "epoch: 818700, train loss: 4.0491108894348145, val loss: 4.061610889434815, ETA in seconds: 8021593.148\n",
      "epoch: 818800, train loss: 4.042860889434815, val loss: 4.070009326934814, ETA in seconds: 8018131.805\n",
      "epoch: 818900, train loss: 4.049501514434814, val loss: 4.0647358894348145, ETA in seconds: 8014689.023\n",
      "epoch: 819000, train loss: 4.055751514434815, val loss: 4.0657124519348145, ETA in seconds: 8011242.769\n",
      "epoch: 819100, train loss: 4.045595264434814, val loss: 4.066884326934814, ETA in seconds: 8007776.217\n",
      "epoch: 819200, train loss: 4.056728076934815, val loss: 4.065321826934815, ETA in seconds: 8004326.119\n",
      "epoch: 819300, train loss: 4.0539937019348145, val loss: 4.070009326934814, ETA in seconds: 8000886.099\n",
      "epoch: 819400, train loss: 4.051259326934814, val loss: 4.066493701934815, ETA in seconds: 7997430.053\n",
      "epoch: 819500, train loss: 4.046571826934814, val loss: 4.067860889434814, ETA in seconds: 7993980.231\n",
      "epoch: 819600, train loss: 4.051649951934815, val loss: 4.063368701934815, ETA in seconds: 7990519.694\n",
      "epoch: 819700, train loss: 4.053798389434815, val loss: 4.0657124519348145, ETA in seconds: 7987056.632\n",
      "epoch: 819800, train loss: 4.055556201934815, val loss: 4.071962451934814, ETA in seconds: 7983613.104\n",
      "epoch: 819900, train loss: 4.0510640144348145, val loss: 4.067274951934815, ETA in seconds: 7980153.232\n",
      "epoch: 820000, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 7976665.689\n",
      "epoch: 820100, train loss: 4.051845264434815, val loss: 4.068837451934814, ETA in seconds: 7973194.609\n",
      "epoch: 820200, train loss: 4.048720264434815, val loss: 4.0686421394348145, ETA in seconds: 7969748.510\n",
      "epoch: 820300, train loss: 4.053407764434814, val loss: 4.062196826934814, ETA in seconds: 7966280.811\n",
      "epoch: 820400, train loss: 4.0520405769348145, val loss: 4.064540576934815, ETA in seconds: 7962809.455\n",
      "epoch: 820500, train loss: 4.049892139434815, val loss: 4.063954639434814, ETA in seconds: 7959374.569\n",
      "epoch: 820600, train loss: 4.054189014434814, val loss: 4.072353076934815, ETA in seconds: 7955961.900\n",
      "epoch: 820700, train loss: 4.051649951934815, val loss: 4.063954639434814, ETA in seconds: 7952527.191\n",
      "epoch: 820800, train loss: 4.0500874519348145, val loss: 4.065907764434814, ETA in seconds: 7949073.263\n",
      "epoch: 820900, train loss: 4.051845264434815, val loss: 4.0666890144348145, ETA in seconds: 7945562.132\n",
      "epoch: 821000, train loss: 4.048524951934814, val loss: 4.063564014434815, ETA in seconds: 7942055.616\n",
      "epoch: 821100, train loss: 4.0530171394348145, val loss: 4.065321826934815, ETA in seconds: 7938558.348\n",
      "epoch: 821200, train loss: 4.058095264434814, val loss: 4.066103076934814, ETA in seconds: 7935063.211\n",
      "epoch: 821300, train loss: 4.051454639434814, val loss: 4.0686421394348145, ETA in seconds: 7931540.738\n",
      "epoch: 821400, train loss: 4.053212451934814, val loss: 4.070790576934814, ETA in seconds: 7928032.403\n",
      "epoch: 821500, train loss: 4.050478076934814, val loss: 4.067470264434815, ETA in seconds: 7924522.941\n",
      "epoch: 821600, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 7921007.299\n",
      "epoch: 821700, train loss: 4.048915576934815, val loss: 4.065907764434814, ETA in seconds: 7917489.434\n",
      "epoch: 821800, train loss: 4.0491108894348145, val loss: 4.067470264434815, ETA in seconds: 7913991.661\n",
      "epoch: 821900, train loss: 4.0559468269348145, val loss: 4.0715718269348145, ETA in seconds: 7910502.440\n",
      "epoch: 822000, train loss: 4.046571826934814, val loss: 4.068056201934814, ETA in seconds: 7907018.834\n",
      "epoch: 822100, train loss: 4.042860889434815, val loss: 4.068446826934815, ETA in seconds: 7903550.237\n",
      "epoch: 822200, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 7900060.107\n",
      "epoch: 822300, train loss: 4.053603076934815, val loss: 4.073134326934815, ETA in seconds: 7896572.121\n",
      "epoch: 822400, train loss: 4.054579639434815, val loss: 4.066493701934815, ETA in seconds: 7893083.723\n",
      "epoch: 822500, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 7889637.356\n",
      "epoch: 822600, train loss: 4.045399951934814, val loss: 4.067860889434814, ETA in seconds: 7886139.074\n",
      "epoch: 822700, train loss: 4.048329639434814, val loss: 4.0666890144348145, ETA in seconds: 7882632.032\n",
      "epoch: 822800, train loss: 4.056532764434815, val loss: 4.0647358894348145, ETA in seconds: 7879190.316\n",
      "epoch: 822900, train loss: 4.041689014434814, val loss: 4.0686421394348145, ETA in seconds: 7875775.642\n",
      "epoch: 823000, train loss: 4.044423389434814, val loss: 4.066103076934814, ETA in seconds: 7872369.341\n",
      "epoch: 823100, train loss: 4.050673389434815, val loss: 4.065321826934815, ETA in seconds: 7868980.199\n",
      "epoch: 823200, train loss: 4.053212451934814, val loss: 4.0637593269348145, ETA in seconds: 7865471.780\n",
      "epoch: 823300, train loss: 4.055751514434815, val loss: 4.0666890144348145, ETA in seconds: 7861995.241\n",
      "epoch: 823400, train loss: 4.052431201934814, val loss: 4.067079639434814, ETA in seconds: 7858523.385\n",
      "epoch: 823500, train loss: 4.047548389434814, val loss: 4.0725483894348145, ETA in seconds: 7855125.504\n",
      "epoch: 823600, train loss: 4.049696826934815, val loss: 4.060634326934815, ETA in seconds: 7851678.621\n",
      "epoch: 823700, train loss: 4.055165576934814, val loss: 4.070204639434815, ETA in seconds: 7848275.710\n",
      "epoch: 823800, train loss: 4.054774951934815, val loss: 4.070985889434814, ETA in seconds: 7844831.484\n",
      "epoch: 823900, train loss: 4.057509326934815, val loss: 4.065126514434814, ETA in seconds: 7841387.428\n",
      "epoch: 824000, train loss: 4.0491108894348145, val loss: 4.0627827644348145, ETA in seconds: 7837936.706\n",
      "epoch: 824100, train loss: 4.054189014434814, val loss: 4.068056201934814, ETA in seconds: 7834464.493\n",
      "epoch: 824200, train loss: 4.0500874519348145, val loss: 4.065517139434815, ETA in seconds: 7830935.199\n",
      "epoch: 824300, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 7827410.020\n",
      "epoch: 824400, train loss: 4.0539937019348145, val loss: 4.0715718269348145, ETA in seconds: 7823895.745\n",
      "epoch: 824500, train loss: 4.055556201934815, val loss: 4.070985889434814, ETA in seconds: 7820363.378\n",
      "epoch: 824600, train loss: 4.053212451934814, val loss: 4.070204639434815, ETA in seconds: 7816835.417\n",
      "epoch: 824700, train loss: 4.048915576934815, val loss: 4.0705952644348145, ETA in seconds: 7813298.881\n",
      "epoch: 824800, train loss: 4.0481343269348145, val loss: 4.067079639434814, ETA in seconds: 7809755.718\n",
      "epoch: 824900, train loss: 4.0539937019348145, val loss: 4.064345264434815, ETA in seconds: 7806212.558\n",
      "epoch: 825000, train loss: 4.045985889434815, val loss: 4.065907764434814, ETA in seconds: 7802671.811\n",
      "epoch: 825100, train loss: 4.058681201934815, val loss: 4.071767139434814, ETA in seconds: 7799207.309\n",
      "epoch: 825200, train loss: 4.054384326934814, val loss: 4.063564014434815, ETA in seconds: 7795761.584\n",
      "epoch: 825300, train loss: 4.053212451934814, val loss: 4.062978076934814, ETA in seconds: 7792337.071\n",
      "epoch: 825400, train loss: 4.0530171394348145, val loss: 4.0618062019348145, ETA in seconds: 7788872.912\n",
      "epoch: 825500, train loss: 4.057118701934814, val loss: 4.0676655769348145, ETA in seconds: 7785342.791\n",
      "epoch: 825600, train loss: 4.055360889434814, val loss: 4.069423389434815, ETA in seconds: 7781791.144\n",
      "epoch: 825700, train loss: 4.054189014434814, val loss: 4.0618062019348145, ETA in seconds: 7778261.892\n",
      "epoch: 825800, train loss: 4.051649951934815, val loss: 4.068056201934814, ETA in seconds: 7774725.516\n",
      "epoch: 825900, train loss: 4.053212451934814, val loss: 4.0637593269348145, ETA in seconds: 7771196.817\n",
      "epoch: 826000, train loss: 4.051649951934815, val loss: 4.064540576934815, ETA in seconds: 7767642.360\n",
      "epoch: 826100, train loss: 4.044618701934814, val loss: 4.0686421394348145, ETA in seconds: 7764095.026\n",
      "epoch: 826200, train loss: 4.056532764434815, val loss: 4.068837451934814, ETA in seconds: 7760547.727\n",
      "epoch: 826300, train loss: 4.048915576934815, val loss: 4.066103076934814, ETA in seconds: 7757007.225\n",
      "epoch: 826400, train loss: 4.054384326934814, val loss: 4.065517139434815, ETA in seconds: 7753440.692\n",
      "epoch: 826500, train loss: 4.050868701934815, val loss: 4.0696187019348145, ETA in seconds: 7749902.220\n",
      "epoch: 826600, train loss: 4.056142139434814, val loss: 4.070009326934814, ETA in seconds: 7746370.302\n",
      "epoch: 826700, train loss: 4.047548389434814, val loss: 4.0735249519348145, ETA in seconds: 7742816.373\n",
      "epoch: 826800, train loss: 4.055165576934814, val loss: 4.0686421394348145, ETA in seconds: 7739249.068\n",
      "epoch: 826900, train loss: 4.0491108894348145, val loss: 4.062001514434814, ETA in seconds: 7735694.699\n",
      "epoch: 827000, train loss: 4.053212451934814, val loss: 4.067860889434814, ETA in seconds: 7732125.353\n",
      "epoch: 827100, train loss: 4.048329639434814, val loss: 4.065126514434814, ETA in seconds: 7728570.090\n",
      "epoch: 827200, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 7725014.432\n",
      "epoch: 827300, train loss: 4.055165576934814, val loss: 4.067079639434814, ETA in seconds: 7721460.359\n",
      "epoch: 827400, train loss: 4.0510640144348145, val loss: 4.066298389434815, ETA in seconds: 7717914.470\n",
      "epoch: 827500, train loss: 4.0530171394348145, val loss: 4.074306201934815, ETA in seconds: 7714359.266\n",
      "epoch: 827600, train loss: 4.057314014434814, val loss: 4.070399951934815, ETA in seconds: 7710797.075\n",
      "epoch: 827700, train loss: 4.0559468269348145, val loss: 4.0657124519348145, ETA in seconds: 7707233.629\n",
      "epoch: 827800, train loss: 4.051845264434815, val loss: 4.067274951934815, ETA in seconds: 7703661.067\n",
      "epoch: 827900, train loss: 4.050282764434814, val loss: 4.0696187019348145, ETA in seconds: 7700103.447\n",
      "epoch: 828000, train loss: 4.053212451934814, val loss: 4.069423389434815, ETA in seconds: 7696530.504\n",
      "epoch: 828100, train loss: 4.053212451934814, val loss: 4.073134326934815, ETA in seconds: 7692980.868\n",
      "epoch: 828200, train loss: 4.053212451934814, val loss: 4.067860889434814, ETA in seconds: 7689411.175\n",
      "epoch: 828300, train loss: 4.054774951934815, val loss: 4.0637593269348145, ETA in seconds: 7685852.829\n",
      "epoch: 828400, train loss: 4.050478076934814, val loss: 4.067274951934815, ETA in seconds: 7682301.063\n",
      "epoch: 828500, train loss: 4.046962451934815, val loss: 4.068837451934814, ETA in seconds: 7678742.358\n",
      "epoch: 828600, train loss: 4.0539937019348145, val loss: 4.061220264434814, ETA in seconds: 7675192.029\n",
      "epoch: 828700, train loss: 4.051259326934814, val loss: 4.0705952644348145, ETA in seconds: 7671644.391\n",
      "epoch: 828800, train loss: 4.050478076934814, val loss: 4.069032764434814, ETA in seconds: 7668078.735\n",
      "epoch: 828900, train loss: 4.0520405769348145, val loss: 4.066103076934814, ETA in seconds: 7664502.055\n",
      "epoch: 829000, train loss: 4.053603076934815, val loss: 4.066493701934815, ETA in seconds: 7660928.476\n",
      "epoch: 829100, train loss: 4.052431201934814, val loss: 4.071767139434814, ETA in seconds: 7657342.220\n",
      "epoch: 829200, train loss: 4.0500874519348145, val loss: 4.062001514434814, ETA in seconds: 7653758.075\n",
      "epoch: 829300, train loss: 4.049306201934814, val loss: 4.069228076934815, ETA in seconds: 7650172.341\n",
      "epoch: 829400, train loss: 4.059267139434814, val loss: 4.070204639434815, ETA in seconds: 7646587.494\n",
      "epoch: 829500, train loss: 4.053798389434815, val loss: 4.065517139434815, ETA in seconds: 7642994.966\n",
      "epoch: 829600, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 7639408.121\n",
      "epoch: 829700, train loss: 4.051845264434815, val loss: 4.072939014434814, ETA in seconds: 7635845.390\n",
      "epoch: 829800, train loss: 4.054189014434814, val loss: 4.063368701934815, ETA in seconds: 7632259.875\n",
      "epoch: 829900, train loss: 4.049501514434814, val loss: 4.066298389434815, ETA in seconds: 7628701.375\n",
      "epoch: 830000, train loss: 4.054579639434815, val loss: 4.0705952644348145, ETA in seconds: 7625214.359\n",
      "epoch: 830100, train loss: 4.057118701934814, val loss: 4.0705952644348145, ETA in seconds: 7621744.446\n",
      "epoch: 830200, train loss: 4.052431201934814, val loss: 4.066493701934815, ETA in seconds: 7618260.163\n",
      "epoch: 830300, train loss: 4.0559468269348145, val loss: 4.066298389434815, ETA in seconds: 7614745.286\n",
      "epoch: 830400, train loss: 4.058681201934815, val loss: 4.065126514434814, ETA in seconds: 7611221.086\n",
      "epoch: 830500, train loss: 4.048329639434814, val loss: 4.066103076934814, ETA in seconds: 7607633.445\n",
      "epoch: 830600, train loss: 4.049501514434814, val loss: 4.067470264434815, ETA in seconds: 7604031.941\n",
      "epoch: 830700, train loss: 4.052626514434815, val loss: 4.066884326934814, ETA in seconds: 7600448.032\n",
      "epoch: 830800, train loss: 4.054189014434814, val loss: 4.068446826934815, ETA in seconds: 7596880.712\n",
      "epoch: 830900, train loss: 4.045985889434815, val loss: 4.067470264434815, ETA in seconds: 7593280.939\n",
      "epoch: 831000, train loss: 4.054579639434815, val loss: 4.067860889434814, ETA in seconds: 7589688.114\n",
      "epoch: 831100, train loss: 4.0471577644348145, val loss: 4.066884326934814, ETA in seconds: 7586082.427\n",
      "epoch: 831200, train loss: 4.055751514434815, val loss: 4.070985889434814, ETA in seconds: 7582490.142\n",
      "epoch: 831300, train loss: 4.054774951934815, val loss: 4.066884326934814, ETA in seconds: 7578879.365\n",
      "epoch: 831400, train loss: 4.0491108894348145, val loss: 4.067470264434815, ETA in seconds: 7575287.776\n",
      "epoch: 831500, train loss: 4.048524951934814, val loss: 4.059071826934814, ETA in seconds: 7571700.776\n",
      "epoch: 831600, train loss: 4.0520405769348145, val loss: 4.0647358894348145, ETA in seconds: 7568109.522\n",
      "epoch: 831700, train loss: 4.054774951934815, val loss: 4.066884326934814, ETA in seconds: 7564535.831\n",
      "epoch: 831800, train loss: 4.046767139434815, val loss: 4.063173389434814, ETA in seconds: 7560954.308\n",
      "epoch: 831900, train loss: 4.051259326934814, val loss: 4.068446826934815, ETA in seconds: 7557354.474\n",
      "epoch: 832000, train loss: 4.052821826934815, val loss: 4.065517139434815, ETA in seconds: 7553781.064\n",
      "epoch: 832100, train loss: 4.045009326934815, val loss: 4.067274951934815, ETA in seconds: 7550224.606\n",
      "epoch: 832200, train loss: 4.051259326934814, val loss: 4.068251514434815, ETA in seconds: 7546620.106\n",
      "epoch: 832300, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 7543018.141\n",
      "epoch: 832400, train loss: 4.0530171394348145, val loss: 4.066103076934814, ETA in seconds: 7539403.464\n",
      "epoch: 832500, train loss: 4.054384326934814, val loss: 4.064540576934815, ETA in seconds: 7535790.536\n",
      "epoch: 832600, train loss: 4.058095264434814, val loss: 4.067470264434815, ETA in seconds: 7532176.998\n",
      "epoch: 832700, train loss: 4.048329639434814, val loss: 4.071376514434815, ETA in seconds: 7528561.248\n",
      "epoch: 832800, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 7524966.981\n",
      "epoch: 832900, train loss: 4.052235889434814, val loss: 4.069814014434814, ETA in seconds: 7521361.350\n",
      "epoch: 833000, train loss: 4.048524951934814, val loss: 4.0647358894348145, ETA in seconds: 7517761.504\n",
      "epoch: 833100, train loss: 4.059462451934815, val loss: 4.0657124519348145, ETA in seconds: 7514162.624\n",
      "epoch: 833200, train loss: 4.050868701934815, val loss: 4.067079639434814, ETA in seconds: 7510536.883\n",
      "epoch: 833300, train loss: 4.0520405769348145, val loss: 4.067470264434815, ETA in seconds: 7506911.520\n",
      "epoch: 833400, train loss: 4.051454639434814, val loss: 4.0705952644348145, ETA in seconds: 7503296.142\n",
      "epoch: 833500, train loss: 4.056142139434814, val loss: 4.063954639434814, ETA in seconds: 7499668.837\n",
      "epoch: 833600, train loss: 4.047548389434814, val loss: 4.069423389434815, ETA in seconds: 7496016.017\n",
      "epoch: 833700, train loss: 4.052431201934814, val loss: 4.0676655769348145, ETA in seconds: 7492419.584\n",
      "epoch: 833800, train loss: 4.0569233894348145, val loss: 4.062392139434815, ETA in seconds: 7488834.777\n",
      "epoch: 833900, train loss: 4.047548389434814, val loss: 4.0666890144348145, ETA in seconds: 7485224.169\n",
      "epoch: 834000, train loss: 4.056728076934815, val loss: 4.064540576934815, ETA in seconds: 7481579.074\n",
      "epoch: 834100, train loss: 4.054579639434815, val loss: 4.067274951934815, ETA in seconds: 7477954.730\n",
      "epoch: 834200, train loss: 4.054774951934815, val loss: 4.068251514434815, ETA in seconds: 7474307.565\n",
      "epoch: 834300, train loss: 4.052626514434815, val loss: 4.0627827644348145, ETA in seconds: 7470672.879\n",
      "epoch: 834400, train loss: 4.0510640144348145, val loss: 4.065517139434815, ETA in seconds: 7467048.545\n",
      "epoch: 834500, train loss: 4.052431201934814, val loss: 4.066493701934815, ETA in seconds: 7463418.756\n",
      "epoch: 834600, train loss: 4.052235889434814, val loss: 4.069814014434814, ETA in seconds: 7459784.042\n",
      "epoch: 834700, train loss: 4.054189014434814, val loss: 4.061024951934814, ETA in seconds: 7456142.820\n",
      "epoch: 834800, train loss: 4.0510640144348145, val loss: 4.067274951934815, ETA in seconds: 7452488.886\n",
      "epoch: 834900, train loss: 4.046571826934814, val loss: 4.067079639434814, ETA in seconds: 7448849.130\n",
      "epoch: 835000, train loss: 4.055556201934815, val loss: 4.066298389434815, ETA in seconds: 7445193.698\n",
      "epoch: 835100, train loss: 4.052235889434814, val loss: 4.063564014434815, ETA in seconds: 7441557.009\n",
      "epoch: 835200, train loss: 4.050478076934814, val loss: 4.068446826934815, ETA in seconds: 7437924.477\n",
      "epoch: 835300, train loss: 4.052821826934815, val loss: 4.068837451934814, ETA in seconds: 7434289.809\n",
      "epoch: 835400, train loss: 4.056532764434815, val loss: 4.067470264434815, ETA in seconds: 7430706.627\n",
      "epoch: 835500, train loss: 4.052235889434814, val loss: 4.066493701934815, ETA in seconds: 7427106.799\n",
      "epoch: 835600, train loss: 4.0452046394348145, val loss: 4.065517139434815, ETA in seconds: 7423456.806\n",
      "epoch: 835700, train loss: 4.053212451934814, val loss: 4.0676655769348145, ETA in seconds: 7419797.294\n",
      "epoch: 835800, train loss: 4.0520405769348145, val loss: 4.064149951934814, ETA in seconds: 7416145.142\n",
      "epoch: 835900, train loss: 4.054579639434815, val loss: 4.068056201934814, ETA in seconds: 7412511.164\n",
      "epoch: 836000, train loss: 4.0539937019348145, val loss: 4.068251514434815, ETA in seconds: 7408849.318\n",
      "epoch: 836100, train loss: 4.0520405769348145, val loss: 4.067860889434814, ETA in seconds: 7405199.192\n",
      "epoch: 836200, train loss: 4.050282764434814, val loss: 4.067470264434815, ETA in seconds: 7401545.644\n",
      "epoch: 836300, train loss: 4.052431201934814, val loss: 4.062978076934814, ETA in seconds: 7397899.539\n",
      "epoch: 836400, train loss: 4.055556201934815, val loss: 4.0637593269348145, ETA in seconds: 7394246.898\n",
      "epoch: 836500, train loss: 4.0500874519348145, val loss: 4.062978076934814, ETA in seconds: 7390597.116\n",
      "epoch: 836600, train loss: 4.062001514434814, val loss: 4.065126514434814, ETA in seconds: 7386941.669\n",
      "epoch: 836700, train loss: 4.052431201934814, val loss: 4.065321826934815, ETA in seconds: 7383286.742\n",
      "epoch: 836800, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 7379642.300\n",
      "epoch: 836900, train loss: 4.048329639434814, val loss: 4.069032764434814, ETA in seconds: 7375970.997\n",
      "epoch: 837000, train loss: 4.053798389434815, val loss: 4.0578999519348145, ETA in seconds: 7372321.395\n",
      "epoch: 837100, train loss: 4.0549702644348145, val loss: 4.067470264434815, ETA in seconds: 7368662.804\n",
      "epoch: 837200, train loss: 4.0569233894348145, val loss: 4.067274951934815, ETA in seconds: 7365015.009\n",
      "epoch: 837300, train loss: 4.0549702644348145, val loss: 4.065126514434814, ETA in seconds: 7361350.032\n",
      "epoch: 837400, train loss: 4.048915576934815, val loss: 4.069423389434815, ETA in seconds: 7357670.232\n",
      "epoch: 837500, train loss: 4.048720264434815, val loss: 4.070204639434815, ETA in seconds: 7353984.299\n",
      "epoch: 837600, train loss: 4.0539937019348145, val loss: 4.063954639434814, ETA in seconds: 7350324.021\n",
      "epoch: 837700, train loss: 4.0559468269348145, val loss: 4.0705952644348145, ETA in seconds: 7346649.680\n",
      "epoch: 837800, train loss: 4.047353076934814, val loss: 4.070790576934814, ETA in seconds: 7342989.243\n",
      "epoch: 837900, train loss: 4.049501514434814, val loss: 4.066298389434815, ETA in seconds: 7339319.572\n",
      "epoch: 838000, train loss: 4.051649951934815, val loss: 4.067274951934815, ETA in seconds: 7335646.245\n",
      "epoch: 838100, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 7331985.107\n",
      "epoch: 838200, train loss: 4.055165576934814, val loss: 4.067274951934815, ETA in seconds: 7328344.091\n",
      "epoch: 838300, train loss: 4.0530171394348145, val loss: 4.069814014434814, ETA in seconds: 7324691.146\n",
      "epoch: 838400, train loss: 4.054579639434815, val loss: 4.068446826934815, ETA in seconds: 7321019.354\n",
      "epoch: 838500, train loss: 4.051845264434815, val loss: 4.067079639434814, ETA in seconds: 7317333.754\n",
      "epoch: 838600, train loss: 4.055556201934815, val loss: 4.0686421394348145, ETA in seconds: 7313667.900\n",
      "epoch: 838700, train loss: 4.053603076934815, val loss: 4.073915576934814, ETA in seconds: 7310008.228\n",
      "epoch: 838800, train loss: 4.044032764434815, val loss: 4.070399951934815, ETA in seconds: 7306330.008\n",
      "epoch: 838900, train loss: 4.047939014434815, val loss: 4.064931201934814, ETA in seconds: 7302647.528\n",
      "epoch: 839000, train loss: 4.059267139434814, val loss: 4.0637593269348145, ETA in seconds: 7298982.502\n",
      "epoch: 839100, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 7295301.732\n",
      "epoch: 839200, train loss: 4.050868701934815, val loss: 4.062196826934814, ETA in seconds: 7291614.709\n",
      "epoch: 839300, train loss: 4.046571826934814, val loss: 4.064345264434815, ETA in seconds: 7287931.344\n",
      "epoch: 839400, train loss: 4.0452046394348145, val loss: 4.070009326934814, ETA in seconds: 7284253.264\n",
      "epoch: 839500, train loss: 4.053798389434815, val loss: 4.065907764434814, ETA in seconds: 7280562.884\n",
      "epoch: 839600, train loss: 4.053798389434815, val loss: 4.065517139434815, ETA in seconds: 7276875.400\n",
      "epoch: 839700, train loss: 4.054774951934815, val loss: 4.070790576934814, ETA in seconds: 7273190.929\n",
      "epoch: 839800, train loss: 4.046962451934815, val loss: 4.065907764434814, ETA in seconds: 7269499.210\n",
      "epoch: 839900, train loss: 4.051649951934815, val loss: 4.0686421394348145, ETA in seconds: 7265824.332\n",
      "epoch: 840000, train loss: 4.0578999519348145, val loss: 4.069423389434815, ETA in seconds: 7262118.378\n",
      "epoch: 840100, train loss: 4.049501514434814, val loss: 4.067470264434815, ETA in seconds: 7258425.914\n",
      "epoch: 840200, train loss: 4.055751514434815, val loss: 4.071767139434814, ETA in seconds: 7254735.660\n",
      "epoch: 840300, train loss: 4.052626514434815, val loss: 4.063954639434814, ETA in seconds: 7251030.730\n",
      "epoch: 840400, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 7247328.401\n",
      "epoch: 840500, train loss: 4.052431201934814, val loss: 4.066103076934814, ETA in seconds: 7243623.901\n",
      "epoch: 840600, train loss: 4.051259326934814, val loss: 4.068056201934814, ETA in seconds: 7239917.502\n",
      "epoch: 840700, train loss: 4.055360889434814, val loss: 4.065321826934815, ETA in seconds: 7236221.396\n",
      "epoch: 840800, train loss: 4.047743701934815, val loss: 4.068056201934814, ETA in seconds: 7232531.756\n",
      "epoch: 840900, train loss: 4.051649951934815, val loss: 4.0657124519348145, ETA in seconds: 7228831.211\n",
      "epoch: 841000, train loss: 4.052431201934814, val loss: 4.063564014434815, ETA in seconds: 7225143.959\n",
      "epoch: 841100, train loss: 4.056337451934814, val loss: 4.064149951934814, ETA in seconds: 7221444.689\n",
      "epoch: 841200, train loss: 4.056728076934815, val loss: 4.065321826934815, ETA in seconds: 7217727.999\n",
      "epoch: 841300, train loss: 4.058095264434814, val loss: 4.062587451934815, ETA in seconds: 7214009.096\n",
      "epoch: 841400, train loss: 4.0442280769348145, val loss: 4.0637593269348145, ETA in seconds: 7210307.152\n",
      "epoch: 841500, train loss: 4.057704639434815, val loss: 4.067079639434814, ETA in seconds: 7206596.432\n",
      "epoch: 841600, train loss: 4.049696826934815, val loss: 4.065321826934815, ETA in seconds: 7202888.883\n",
      "epoch: 841700, train loss: 4.049501514434814, val loss: 4.0666890144348145, ETA in seconds: 7199175.592\n",
      "epoch: 841800, train loss: 4.054189014434814, val loss: 4.065126514434814, ETA in seconds: 7195449.937\n",
      "epoch: 841900, train loss: 4.055165576934814, val loss: 4.066884326934814, ETA in seconds: 7191669.261\n",
      "epoch: 842000, train loss: 4.052235889434814, val loss: 4.065517139434815, ETA in seconds: 7187910.112\n",
      "epoch: 842100, train loss: 4.045985889434815, val loss: 4.071767139434814, ETA in seconds: 7184154.491\n",
      "epoch: 842200, train loss: 4.052431201934814, val loss: 4.067274951934815, ETA in seconds: 7180382.981\n",
      "epoch: 842300, train loss: 4.0539937019348145, val loss: 4.066298389434815, ETA in seconds: 7176653.931\n",
      "epoch: 842400, train loss: 4.056728076934815, val loss: 4.069032764434814, ETA in seconds: 7172901.666\n",
      "epoch: 842500, train loss: 4.053603076934815, val loss: 4.066298389434815, ETA in seconds: 7169127.490\n",
      "epoch: 842600, train loss: 4.0559468269348145, val loss: 4.067860889434814, ETA in seconds: 7165385.893\n",
      "epoch: 842700, train loss: 4.051454639434814, val loss: 4.063954639434814, ETA in seconds: 7161621.764\n",
      "epoch: 842800, train loss: 4.0578999519348145, val loss: 4.069228076934815, ETA in seconds: 7157855.864\n",
      "epoch: 842900, train loss: 4.0500874519348145, val loss: 4.066884326934814, ETA in seconds: 7154089.275\n",
      "epoch: 843000, train loss: 4.0569233894348145, val loss: 4.070009326934814, ETA in seconds: 7150347.802\n",
      "epoch: 843100, train loss: 4.053798389434815, val loss: 4.067470264434815, ETA in seconds: 7146580.173\n",
      "epoch: 843200, train loss: 4.044814014434815, val loss: 4.065907764434814, ETA in seconds: 7142823.590\n",
      "epoch: 843300, train loss: 4.053212451934814, val loss: 4.0666890144348145, ETA in seconds: 7139043.235\n",
      "epoch: 843400, train loss: 4.050673389434815, val loss: 4.065321826934815, ETA in seconds: 7135238.761\n",
      "epoch: 843500, train loss: 4.046571826934814, val loss: 4.069228076934815, ETA in seconds: 7131455.688\n",
      "epoch: 843600, train loss: 4.048524951934814, val loss: 4.0657124519348145, ETA in seconds: 7127730.943\n",
      "epoch: 843700, train loss: 4.0461812019348145, val loss: 4.070009326934814, ETA in seconds: 7123984.815\n",
      "epoch: 843800, train loss: 4.046767139434815, val loss: 4.066103076934814, ETA in seconds: 7120253.881\n",
      "epoch: 843900, train loss: 4.051845264434815, val loss: 4.0676655769348145, ETA in seconds: 7116510.881\n",
      "epoch: 844000, train loss: 4.054189014434814, val loss: 4.069228076934815, ETA in seconds: 7112771.761\n",
      "epoch: 844100, train loss: 4.0539937019348145, val loss: 4.073329639434815, ETA in seconds: 7109019.348\n",
      "epoch: 844200, train loss: 4.053212451934814, val loss: 4.072353076934815, ETA in seconds: 7105291.352\n",
      "epoch: 844300, train loss: 4.0559468269348145, val loss: 4.0696187019348145, ETA in seconds: 7101523.292\n",
      "epoch: 844400, train loss: 4.055165576934814, val loss: 4.0686421394348145, ETA in seconds: 7097778.071\n",
      "epoch: 844500, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 7094010.606\n",
      "epoch: 844600, train loss: 4.0588765144348145, val loss: 4.060439014434815, ETA in seconds: 7090261.364\n",
      "epoch: 844700, train loss: 4.051845264434815, val loss: 4.064931201934814, ETA in seconds: 7086540.644\n",
      "epoch: 844800, train loss: 4.052821826934815, val loss: 4.068837451934814, ETA in seconds: 7082822.013\n",
      "epoch: 844900, train loss: 4.047939014434815, val loss: 4.064540576934815, ETA in seconds: 7079079.268\n",
      "epoch: 845000, train loss: 4.0549702644348145, val loss: 4.0705952644348145, ETA in seconds: 7075323.571\n",
      "epoch: 845100, train loss: 4.054189014434814, val loss: 4.0598530769348145, ETA in seconds: 7071576.445\n",
      "epoch: 845200, train loss: 4.0530171394348145, val loss: 4.069814014434814, ETA in seconds: 7067820.658\n",
      "epoch: 845300, train loss: 4.053212451934814, val loss: 4.069423389434815, ETA in seconds: 7064068.866\n",
      "epoch: 845400, train loss: 4.049696826934815, val loss: 4.065907764434814, ETA in seconds: 7060304.129\n",
      "epoch: 845500, train loss: 4.049306201934814, val loss: 4.0715718269348145, ETA in seconds: 7056551.255\n",
      "epoch: 845600, train loss: 4.047743701934815, val loss: 4.063368701934815, ETA in seconds: 7052804.510\n",
      "epoch: 845700, train loss: 4.053407764434814, val loss: 4.063564014434815, ETA in seconds: 7049052.374\n",
      "epoch: 845800, train loss: 4.053407764434814, val loss: 4.067470264434815, ETA in seconds: 7045309.595\n",
      "epoch: 845900, train loss: 4.0549702644348145, val loss: 4.070985889434814, ETA in seconds: 7041645.330\n",
      "epoch: 846000, train loss: 4.0500874519348145, val loss: 4.0705952644348145, ETA in seconds: 7037895.271\n",
      "epoch: 846100, train loss: 4.054189014434814, val loss: 4.067079639434814, ETA in seconds: 7034148.127\n",
      "epoch: 846200, train loss: 4.046962451934815, val loss: 4.069032764434814, ETA in seconds: 7030387.101\n",
      "epoch: 846300, train loss: 4.049306201934814, val loss: 4.067274951934815, ETA in seconds: 7026634.733\n",
      "epoch: 846400, train loss: 4.055751514434815, val loss: 4.065321826934815, ETA in seconds: 7022883.227\n",
      "epoch: 846500, train loss: 4.057118701934814, val loss: 4.069032764434814, ETA in seconds: 7019137.447\n",
      "epoch: 846600, train loss: 4.050868701934815, val loss: 4.0686421394348145, ETA in seconds: 7015377.264\n",
      "epoch: 846700, train loss: 4.057118701934814, val loss: 4.066103076934814, ETA in seconds: 7011606.765\n",
      "epoch: 846800, train loss: 4.051454639434814, val loss: 4.071376514434815, ETA in seconds: 7007836.132\n",
      "epoch: 846900, train loss: 4.047353076934814, val loss: 4.065126514434814, ETA in seconds: 7004069.459\n",
      "epoch: 847000, train loss: 4.049501514434814, val loss: 4.066884326934814, ETA in seconds: 7000310.084\n",
      "epoch: 847100, train loss: 4.056337451934814, val loss: 4.070399951934815, ETA in seconds: 6996555.400\n",
      "epoch: 847200, train loss: 4.0481343269348145, val loss: 4.067470264434815, ETA in seconds: 6992811.333\n",
      "epoch: 847300, train loss: 4.050478076934814, val loss: 4.073915576934814, ETA in seconds: 6989049.694\n",
      "epoch: 847400, train loss: 4.056142139434814, val loss: 4.066103076934814, ETA in seconds: 6985268.294\n",
      "epoch: 847500, train loss: 4.051845264434815, val loss: 4.073134326934815, ETA in seconds: 6981513.956\n",
      "epoch: 847600, train loss: 4.0559468269348145, val loss: 4.065517139434815, ETA in seconds: 6977760.924\n",
      "epoch: 847700, train loss: 4.052235889434814, val loss: 4.062587451934815, ETA in seconds: 6973994.175\n",
      "epoch: 847800, train loss: 4.0569233894348145, val loss: 4.070399951934815, ETA in seconds: 6970228.921\n",
      "epoch: 847900, train loss: 4.055360889434814, val loss: 4.067860889434814, ETA in seconds: 6966482.566\n",
      "epoch: 848000, train loss: 4.050868701934815, val loss: 4.070790576934814, ETA in seconds: 6962721.131\n",
      "epoch: 848100, train loss: 4.048915576934815, val loss: 4.065126514434814, ETA in seconds: 6958942.192\n",
      "epoch: 848200, train loss: 4.049892139434815, val loss: 4.0637593269348145, ETA in seconds: 6955166.276\n",
      "epoch: 848300, train loss: 4.050282764434814, val loss: 4.058681201934815, ETA in seconds: 6951404.003\n",
      "epoch: 848400, train loss: 4.0500874519348145, val loss: 4.067860889434814, ETA in seconds: 6947625.891\n",
      "epoch: 848500, train loss: 4.049696826934815, val loss: 4.0754780769348145, ETA in seconds: 6943857.703\n",
      "epoch: 848600, train loss: 4.048524951934814, val loss: 4.071376514434815, ETA in seconds: 6940172.361\n",
      "epoch: 848700, train loss: 4.045790576934815, val loss: 4.069032764434814, ETA in seconds: 6936516.348\n",
      "epoch: 848800, train loss: 4.057118701934814, val loss: 4.0637593269348145, ETA in seconds: 6932802.582\n",
      "epoch: 848900, train loss: 4.055165576934814, val loss: 4.076649951934814, ETA in seconds: 6929045.453\n",
      "epoch: 849000, train loss: 4.053212451934814, val loss: 4.066298389434815, ETA in seconds: 6925259.554\n",
      "epoch: 849100, train loss: 4.052235889434814, val loss: 4.0666890144348145, ETA in seconds: 6921457.138\n",
      "epoch: 849200, train loss: 4.052626514434815, val loss: 4.069814014434814, ETA in seconds: 6917694.092\n",
      "epoch: 849300, train loss: 4.050478076934814, val loss: 4.069423389434815, ETA in seconds: 6913943.752\n",
      "epoch: 849400, train loss: 4.049892139434815, val loss: 4.070790576934814, ETA in seconds: 6910166.549\n",
      "epoch: 849500, train loss: 4.048329639434814, val loss: 4.067274951934815, ETA in seconds: 6906393.659\n",
      "epoch: 849600, train loss: 4.053212451934814, val loss: 4.068251514434815, ETA in seconds: 6902631.045\n",
      "epoch: 849700, train loss: 4.050673389434815, val loss: 4.0745015144348145, ETA in seconds: 6898849.737\n",
      "epoch: 849800, train loss: 4.045985889434815, val loss: 4.0666890144348145, ETA in seconds: 6895052.600\n",
      "epoch: 849900, train loss: 4.055556201934815, val loss: 4.065126514434814, ETA in seconds: 6891278.489\n",
      "epoch: 850000, train loss: 4.045790576934815, val loss: 4.069814014434814, ETA in seconds: 6887488.314\n",
      "epoch: 850100, train loss: 4.047548389434814, val loss: 4.0647358894348145, ETA in seconds: 6883710.947\n",
      "epoch: 850200, train loss: 4.054579639434815, val loss: 4.068446826934815, ETA in seconds: 6879920.803\n",
      "epoch: 850300, train loss: 4.049306201934814, val loss: 4.065907764434814, ETA in seconds: 6876140.644\n",
      "epoch: 850400, train loss: 4.048915576934815, val loss: 4.064931201934814, ETA in seconds: 6872373.727\n",
      "epoch: 850500, train loss: 4.0471577644348145, val loss: 4.065517139434815, ETA in seconds: 6868555.666\n",
      "epoch: 850600, train loss: 4.049306201934814, val loss: 4.064149951934814, ETA in seconds: 6864771.184\n",
      "epoch: 850700, train loss: 4.053798389434815, val loss: 4.0705952644348145, ETA in seconds: 6860964.737\n",
      "epoch: 850800, train loss: 4.048329639434814, val loss: 4.064345264434815, ETA in seconds: 6857149.058\n",
      "epoch: 850900, train loss: 4.047939014434815, val loss: 4.070790576934814, ETA in seconds: 6853344.692\n",
      "epoch: 851000, train loss: 4.056142139434814, val loss: 4.070790576934814, ETA in seconds: 6849525.810\n",
      "epoch: 851100, train loss: 4.0530171394348145, val loss: 4.065907764434814, ETA in seconds: 6845708.983\n",
      "epoch: 851200, train loss: 4.056532764434815, val loss: 4.070790576934814, ETA in seconds: 6841899.483\n",
      "epoch: 851300, train loss: 4.046962451934815, val loss: 4.067274951934815, ETA in seconds: 6838088.574\n",
      "epoch: 851400, train loss: 4.054774951934815, val loss: 4.067470264434815, ETA in seconds: 6834262.082\n",
      "epoch: 851500, train loss: 4.056532764434815, val loss: 4.072157764434815, ETA in seconds: 6830438.426\n",
      "epoch: 851600, train loss: 4.049306201934814, val loss: 4.068446826934815, ETA in seconds: 6826626.703\n",
      "epoch: 851700, train loss: 4.052821826934815, val loss: 4.068251514434815, ETA in seconds: 6822818.006\n",
      "epoch: 851800, train loss: 4.042079639434815, val loss: 4.067470264434815, ETA in seconds: 6819003.234\n",
      "epoch: 851900, train loss: 4.047548389434814, val loss: 4.069423389434815, ETA in seconds: 6815189.649\n",
      "epoch: 852000, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 6811362.981\n",
      "epoch: 852100, train loss: 4.0549702644348145, val loss: 4.063368701934815, ETA in seconds: 6807577.063\n",
      "epoch: 852200, train loss: 4.059462451934815, val loss: 4.065321826934815, ETA in seconds: 6803854.975\n",
      "epoch: 852300, train loss: 4.048915576934815, val loss: 4.0666890144348145, ETA in seconds: 6800149.958\n",
      "epoch: 852400, train loss: 4.048915576934815, val loss: 4.070009326934814, ETA in seconds: 6796440.181\n",
      "epoch: 852500, train loss: 4.052626514434815, val loss: 4.064149951934814, ETA in seconds: 6792719.056\n",
      "epoch: 852600, train loss: 4.053407764434814, val loss: 4.068251514434815, ETA in seconds: 6789013.186\n",
      "epoch: 852700, train loss: 4.048720264434815, val loss: 4.067860889434814, ETA in seconds: 6785307.170\n",
      "epoch: 852800, train loss: 4.051454639434814, val loss: 4.068251514434815, ETA in seconds: 6781545.530\n",
      "epoch: 852900, train loss: 4.0549702644348145, val loss: 4.065907764434814, ETA in seconds: 6777734.229\n",
      "epoch: 853000, train loss: 4.0578999519348145, val loss: 4.067274951934815, ETA in seconds: 6773939.429\n",
      "epoch: 853100, train loss: 4.057118701934814, val loss: 4.064149951934814, ETA in seconds: 6770117.095\n",
      "epoch: 853200, train loss: 4.050478076934814, val loss: 4.073329639434815, ETA in seconds: 6766322.618\n",
      "epoch: 853300, train loss: 4.046571826934814, val loss: 4.0666890144348145, ETA in seconds: 6762495.084\n",
      "epoch: 853400, train loss: 4.0510640144348145, val loss: 4.066884326934814, ETA in seconds: 6758678.877\n",
      "epoch: 853500, train loss: 4.050282764434814, val loss: 4.0676655769348145, ETA in seconds: 6754862.995\n",
      "epoch: 853600, train loss: 4.048329639434814, val loss: 4.0696187019348145, ETA in seconds: 6751034.724\n",
      "epoch: 853700, train loss: 4.048329639434814, val loss: 4.071376514434815, ETA in seconds: 6747211.408\n",
      "epoch: 853800, train loss: 4.054774951934815, val loss: 4.066493701934815, ETA in seconds: 6743376.947\n",
      "epoch: 853900, train loss: 4.0471577644348145, val loss: 4.066103076934814, ETA in seconds: 6739549.399\n",
      "epoch: 854000, train loss: 4.0500874519348145, val loss: 4.072157764434815, ETA in seconds: 6735718.987\n",
      "epoch: 854100, train loss: 4.0530171394348145, val loss: 4.067470264434815, ETA in seconds: 6731903.949\n",
      "epoch: 854200, train loss: 4.0530171394348145, val loss: 4.0705952644348145, ETA in seconds: 6728073.673\n",
      "epoch: 854300, train loss: 4.057118701934814, val loss: 4.069032764434814, ETA in seconds: 6724256.600\n",
      "epoch: 854400, train loss: 4.0530171394348145, val loss: 4.066884326934814, ETA in seconds: 6720413.679\n",
      "epoch: 854500, train loss: 4.053407764434814, val loss: 4.067470264434815, ETA in seconds: 6716621.083\n",
      "epoch: 854600, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 6712806.538\n",
      "epoch: 854700, train loss: 4.049306201934814, val loss: 4.066493701934815, ETA in seconds: 6708976.233\n",
      "epoch: 854800, train loss: 4.053407764434814, val loss: 4.0745015144348145, ETA in seconds: 6705117.626\n",
      "epoch: 854900, train loss: 4.060634326934815, val loss: 4.071376514434815, ETA in seconds: 6701276.555\n",
      "epoch: 855000, train loss: 4.054384326934814, val loss: 4.0696187019348145, ETA in seconds: 6697437.589\n",
      "epoch: 855100, train loss: 4.048915576934815, val loss: 4.067860889434814, ETA in seconds: 6693593.105\n",
      "epoch: 855200, train loss: 4.048720264434815, val loss: 4.066103076934814, ETA in seconds: 6689750.554\n",
      "epoch: 855300, train loss: 4.050282764434814, val loss: 4.0686421394348145, ETA in seconds: 6685894.668\n",
      "epoch: 855400, train loss: 4.047743701934815, val loss: 4.071376514434815, ETA in seconds: 6682048.250\n",
      "epoch: 855500, train loss: 4.0471577644348145, val loss: 4.067274951934815, ETA in seconds: 6678180.855\n",
      "epoch: 855600, train loss: 4.050282764434814, val loss: 4.064931201934814, ETA in seconds: 6674325.802\n",
      "epoch: 855700, train loss: 4.052626514434815, val loss: 4.063564014434815, ETA in seconds: 6670476.348\n",
      "epoch: 855800, train loss: 4.045009326934815, val loss: 4.0676655769348145, ETA in seconds: 6666712.175\n",
      "epoch: 855900, train loss: 4.0598530769348145, val loss: 4.066493701934815, ETA in seconds: 6662937.698\n",
      "epoch: 856000, train loss: 4.047939014434815, val loss: 4.073720264434814, ETA in seconds: 6659057.426\n",
      "epoch: 856100, train loss: 4.058290576934814, val loss: 4.067470264434815, ETA in seconds: 6655203.908\n",
      "epoch: 856200, train loss: 4.0549702644348145, val loss: 4.0657124519348145, ETA in seconds: 6651340.491\n",
      "epoch: 856300, train loss: 4.054579639434815, val loss: 4.063954639434814, ETA in seconds: 6647478.960\n",
      "epoch: 856400, train loss: 4.056142139434814, val loss: 4.0627827644348145, ETA in seconds: 6643608.968\n",
      "epoch: 856500, train loss: 4.045790576934815, val loss: 4.064931201934814, ETA in seconds: 6639722.784\n",
      "epoch: 856600, train loss: 4.054579639434815, val loss: 4.067274951934815, ETA in seconds: 6635864.336\n",
      "epoch: 856700, train loss: 4.051845264434815, val loss: 4.063564014434815, ETA in seconds: 6631983.283\n",
      "epoch: 856800, train loss: 4.056142139434814, val loss: 4.0686421394348145, ETA in seconds: 6628091.103\n",
      "epoch: 856900, train loss: 4.0530171394348145, val loss: 4.069814014434814, ETA in seconds: 6624231.730\n",
      "epoch: 857000, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 6620385.295\n",
      "epoch: 857100, train loss: 4.049501514434814, val loss: 4.063173389434814, ETA in seconds: 6616498.252\n",
      "epoch: 857200, train loss: 4.044423389434814, val loss: 4.0666890144348145, ETA in seconds: 6612637.393\n",
      "epoch: 857300, train loss: 4.045595264434814, val loss: 4.0608296394348145, ETA in seconds: 6608766.513\n",
      "epoch: 857400, train loss: 4.049892139434815, val loss: 4.061610889434815, ETA in seconds: 6604891.874\n",
      "epoch: 857500, train loss: 4.055360889434814, val loss: 4.069423389434815, ETA in seconds: 6601006.427\n",
      "epoch: 857600, train loss: 4.053798389434815, val loss: 4.0637593269348145, ETA in seconds: 6597137.698\n",
      "epoch: 857700, train loss: 4.053212451934814, val loss: 4.068056201934814, ETA in seconds: 6593261.519\n",
      "epoch: 857800, train loss: 4.0500874519348145, val loss: 4.070204639434815, ETA in seconds: 6589387.488\n",
      "epoch: 857900, train loss: 4.055360889434814, val loss: 4.066884326934814, ETA in seconds: 6585507.113\n",
      "epoch: 858000, train loss: 4.047743701934815, val loss: 4.0705952644348145, ETA in seconds: 6581627.624\n",
      "epoch: 858100, train loss: 4.054189014434814, val loss: 4.061415576934815, ETA in seconds: 6577733.468\n",
      "epoch: 858200, train loss: 4.052821826934815, val loss: 4.0657124519348145, ETA in seconds: 6573837.359\n",
      "epoch: 858300, train loss: 4.042860889434815, val loss: 4.067274951934815, ETA in seconds: 6569964.808\n",
      "epoch: 858400, train loss: 4.050478076934814, val loss: 4.0637593269348145, ETA in seconds: 6566087.207\n",
      "epoch: 858500, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 6562294.485\n",
      "epoch: 858600, train loss: 4.058681201934815, val loss: 4.067860889434814, ETA in seconds: 6558489.217\n",
      "epoch: 858700, train loss: 4.055751514434815, val loss: 4.067274951934815, ETA in seconds: 6554598.707\n",
      "epoch: 858800, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 6550706.108\n",
      "epoch: 858900, train loss: 4.049306201934814, val loss: 4.065321826934815, ETA in seconds: 6546832.618\n",
      "epoch: 859000, train loss: 4.046767139434815, val loss: 4.067079639434814, ETA in seconds: 6542947.917\n",
      "epoch: 859100, train loss: 4.0510640144348145, val loss: 4.064540576934815, ETA in seconds: 6539076.377\n",
      "epoch: 859200, train loss: 4.052431201934814, val loss: 4.070009326934814, ETA in seconds: 6535188.299\n",
      "epoch: 859300, train loss: 4.050478076934814, val loss: 4.069814014434814, ETA in seconds: 6531304.379\n",
      "epoch: 859400, train loss: 4.052431201934814, val loss: 4.0666890144348145, ETA in seconds: 6527416.826\n",
      "epoch: 859500, train loss: 4.053603076934815, val loss: 4.065907764434814, ETA in seconds: 6523523.771\n",
      "epoch: 859600, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 6519638.345\n",
      "epoch: 859700, train loss: 4.046962451934815, val loss: 4.065321826934815, ETA in seconds: 6515753.472\n",
      "epoch: 859800, train loss: 4.053212451934814, val loss: 4.066493701934815, ETA in seconds: 6511838.227\n",
      "epoch: 859900, train loss: 4.051259326934814, val loss: 4.068837451934814, ETA in seconds: 6507939.097\n",
      "epoch: 860000, train loss: 4.0549702644348145, val loss: 4.0666890144348145, ETA in seconds: 6504049.483\n",
      "epoch: 860100, train loss: 4.052626514434815, val loss: 4.0657124519348145, ETA in seconds: 6500166.088\n",
      "epoch: 860200, train loss: 4.041689014434814, val loss: 4.065907764434814, ETA in seconds: 6496356.023\n",
      "epoch: 860300, train loss: 4.056142139434814, val loss: 4.066884326934814, ETA in seconds: 6492559.906\n",
      "epoch: 860400, train loss: 4.052235889434814, val loss: 4.069423389434815, ETA in seconds: 6488648.811\n",
      "epoch: 860500, train loss: 4.055165576934814, val loss: 4.074110889434815, ETA in seconds: 6484725.978\n",
      "epoch: 860600, train loss: 4.053603076934815, val loss: 4.0676655769348145, ETA in seconds: 6480810.124\n",
      "epoch: 860700, train loss: 4.046571826934814, val loss: 4.060634326934815, ETA in seconds: 6476888.246\n",
      "epoch: 860800, train loss: 4.060439014434815, val loss: 4.064149951934814, ETA in seconds: 6472974.781\n",
      "epoch: 860900, train loss: 4.0510640144348145, val loss: 4.062196826934814, ETA in seconds: 6469081.469\n",
      "epoch: 861000, train loss: 4.0461812019348145, val loss: 4.068251514434815, ETA in seconds: 6465202.684\n",
      "epoch: 861100, train loss: 4.051259326934814, val loss: 4.0666890144348145, ETA in seconds: 6461313.911\n",
      "epoch: 861200, train loss: 4.054579639434815, val loss: 4.071962451934814, ETA in seconds: 6457380.342\n",
      "epoch: 861300, train loss: 4.048329639434814, val loss: 4.067079639434814, ETA in seconds: 6453452.145\n",
      "epoch: 861400, train loss: 4.055556201934815, val loss: 4.071767139434814, ETA in seconds: 6449519.688\n",
      "epoch: 861500, train loss: 4.048329639434814, val loss: 4.072157764434815, ETA in seconds: 6445607.297\n",
      "epoch: 861600, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 6441713.750\n",
      "epoch: 861700, train loss: 4.0500874519348145, val loss: 4.073720264434814, ETA in seconds: 6437855.703\n",
      "epoch: 861800, train loss: 4.047743701934815, val loss: 4.065907764434814, ETA in seconds: 6433987.486\n",
      "epoch: 861900, train loss: 4.0578999519348145, val loss: 4.0657124519348145, ETA in seconds: 6430113.905\n",
      "epoch: 862000, train loss: 4.054579639434815, val loss: 4.066493701934815, ETA in seconds: 6426229.829\n",
      "epoch: 862100, train loss: 4.046767139434815, val loss: 4.071767139434814, ETA in seconds: 6422297.619\n",
      "epoch: 862200, train loss: 4.0500874519348145, val loss: 4.066493701934815, ETA in seconds: 6418367.880\n",
      "epoch: 862300, train loss: 4.049892139434815, val loss: 4.0725483894348145, ETA in seconds: 6414461.464\n",
      "epoch: 862400, train loss: 4.050673389434815, val loss: 4.068837451934814, ETA in seconds: 6410536.481\n",
      "epoch: 862500, train loss: 4.057118701934814, val loss: 4.070204639434815, ETA in seconds: 6406607.502\n",
      "epoch: 862600, train loss: 4.049892139434815, val loss: 4.069814014434814, ETA in seconds: 6402691.382\n",
      "epoch: 862700, train loss: 4.051454639434814, val loss: 4.067860889434814, ETA in seconds: 6398774.319\n",
      "epoch: 862800, train loss: 4.049892139434815, val loss: 4.064345264434815, ETA in seconds: 6394842.811\n",
      "epoch: 862900, train loss: 4.051845264434815, val loss: 4.064931201934814, ETA in seconds: 6390932.299\n",
      "epoch: 863000, train loss: 4.046962451934815, val loss: 4.064540576934815, ETA in seconds: 6386995.696\n",
      "epoch: 863100, train loss: 4.048524951934814, val loss: 4.071376514434815, ETA in seconds: 6383043.320\n",
      "epoch: 863200, train loss: 4.0549702644348145, val loss: 4.063368701934815, ETA in seconds: 6379113.492\n",
      "epoch: 863300, train loss: 4.051454639434814, val loss: 4.066103076934814, ETA in seconds: 6375170.834\n",
      "epoch: 863400, train loss: 4.050478076934814, val loss: 4.064345264434815, ETA in seconds: 6371227.650\n",
      "epoch: 863500, train loss: 4.051259326934814, val loss: 4.069228076934815, ETA in seconds: 6367293.687\n",
      "epoch: 863600, train loss: 4.045790576934815, val loss: 4.0676655769348145, ETA in seconds: 6363343.258\n",
      "epoch: 863700, train loss: 4.0539937019348145, val loss: 4.0705952644348145, ETA in seconds: 6359401.534\n",
      "epoch: 863800, train loss: 4.0520405769348145, val loss: 4.066884326934814, ETA in seconds: 6355447.872\n",
      "epoch: 863900, train loss: 4.057314014434814, val loss: 4.065126514434814, ETA in seconds: 6351486.241\n",
      "epoch: 864000, train loss: 4.047939014434815, val loss: 4.065126514434814, ETA in seconds: 6347549.572\n",
      "epoch: 864100, train loss: 4.057118701934814, val loss: 4.0686421394348145, ETA in seconds: 6343654.714\n",
      "epoch: 864200, train loss: 4.058485889434815, val loss: 4.066884326934814, ETA in seconds: 6339712.075\n",
      "epoch: 864300, train loss: 4.054189014434814, val loss: 4.069423389434815, ETA in seconds: 6335768.044\n",
      "epoch: 864400, train loss: 4.053212451934814, val loss: 4.063173389434814, ETA in seconds: 6331816.108\n",
      "epoch: 864500, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 6327871.606\n",
      "epoch: 864600, train loss: 4.055360889434814, val loss: 4.0696187019348145, ETA in seconds: 6323913.699\n",
      "epoch: 864700, train loss: 4.046767139434815, val loss: 4.071376514434815, ETA in seconds: 6319952.276\n",
      "epoch: 864800, train loss: 4.0549702644348145, val loss: 4.062001514434814, ETA in seconds: 6316004.512\n",
      "epoch: 864900, train loss: 4.051649951934815, val loss: 4.0666890144348145, ETA in seconds: 6312048.634\n",
      "epoch: 865000, train loss: 4.050868701934815, val loss: 4.0657124519348145, ETA in seconds: 6308092.813\n",
      "epoch: 865100, train loss: 4.047548389434814, val loss: 4.069423389434815, ETA in seconds: 6304134.562\n",
      "epoch: 865200, train loss: 4.047353076934814, val loss: 4.066493701934815, ETA in seconds: 6300181.946\n",
      "epoch: 865300, train loss: 4.051454639434814, val loss: 4.0676655769348145, ETA in seconds: 6296211.834\n",
      "epoch: 865400, train loss: 4.0491108894348145, val loss: 4.066493701934815, ETA in seconds: 6292254.760\n",
      "epoch: 865500, train loss: 4.052821826934815, val loss: 4.064931201934814, ETA in seconds: 6288288.349\n",
      "epoch: 865600, train loss: 4.052235889434814, val loss: 4.070399951934815, ETA in seconds: 6284319.203\n",
      "epoch: 865700, train loss: 4.057704639434815, val loss: 4.069814014434814, ETA in seconds: 6280364.126\n",
      "epoch: 865800, train loss: 4.057314014434814, val loss: 4.0627827644348145, ETA in seconds: 6276402.809\n",
      "epoch: 865900, train loss: 4.049306201934814, val loss: 4.071962451934814, ETA in seconds: 6272423.320\n",
      "epoch: 866000, train loss: 4.049306201934814, val loss: 4.070009326934814, ETA in seconds: 6268465.376\n",
      "epoch: 866100, train loss: 4.048329639434814, val loss: 4.070009326934814, ETA in seconds: 6264496.879\n",
      "epoch: 866200, train loss: 4.055165576934814, val loss: 4.070790576934814, ETA in seconds: 6260506.117\n",
      "epoch: 866300, train loss: 4.051259326934814, val loss: 4.067470264434815, ETA in seconds: 6256528.589\n",
      "epoch: 866400, train loss: 4.048720264434815, val loss: 4.0696187019348145, ETA in seconds: 6252545.931\n",
      "epoch: 866500, train loss: 4.046767139434815, val loss: 4.068837451934814, ETA in seconds: 6248577.766\n",
      "epoch: 866600, train loss: 4.049306201934814, val loss: 4.069032764434814, ETA in seconds: 6244601.190\n",
      "epoch: 866700, train loss: 4.045595264434814, val loss: 4.065517139434815, ETA in seconds: 6240625.384\n",
      "epoch: 866800, train loss: 4.053212451934814, val loss: 4.067079639434814, ETA in seconds: 6236646.731\n",
      "epoch: 866900, train loss: 4.038954639434815, val loss: 4.065126514434814, ETA in seconds: 6232656.494\n",
      "epoch: 867000, train loss: 4.059071826934814, val loss: 4.073329639434815, ETA in seconds: 6228677.049\n",
      "epoch: 867100, train loss: 4.0481343269348145, val loss: 4.069228076934815, ETA in seconds: 6224711.808\n",
      "epoch: 867200, train loss: 4.050478076934814, val loss: 4.070399951934815, ETA in seconds: 6220751.942\n",
      "epoch: 867300, train loss: 4.048329639434814, val loss: 4.072939014434814, ETA in seconds: 6216791.579\n",
      "epoch: 867400, train loss: 4.057509326934815, val loss: 4.069228076934815, ETA in seconds: 6212811.077\n",
      "epoch: 867500, train loss: 4.047939014434815, val loss: 4.067860889434814, ETA in seconds: 6208825.574\n",
      "epoch: 867600, train loss: 4.056337451934814, val loss: 4.068837451934814, ETA in seconds: 6204843.227\n",
      "epoch: 867700, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 6200857.149\n",
      "epoch: 867800, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 6196877.279\n",
      "epoch: 867900, train loss: 4.050478076934814, val loss: 4.065126514434814, ETA in seconds: 6192885.828\n",
      "epoch: 868000, train loss: 4.0520405769348145, val loss: 4.063368701934815, ETA in seconds: 6188893.955\n",
      "epoch: 868100, train loss: 4.0452046394348145, val loss: 4.070399951934815, ETA in seconds: 6184897.697\n",
      "epoch: 868200, train loss: 4.0549702644348145, val loss: 4.071181201934815, ETA in seconds: 6180921.384\n",
      "epoch: 868300, train loss: 4.051649951934815, val loss: 4.062001514434814, ETA in seconds: 6176933.058\n",
      "epoch: 868400, train loss: 4.049892139434815, val loss: 4.066493701934815, ETA in seconds: 6172960.746\n",
      "epoch: 868500, train loss: 4.0530171394348145, val loss: 4.064345264434815, ETA in seconds: 6169044.059\n",
      "epoch: 868600, train loss: 4.048524951934814, val loss: 4.069228076934815, ETA in seconds: 6165066.399\n",
      "epoch: 868700, train loss: 4.051845264434815, val loss: 4.070985889434814, ETA in seconds: 6161152.799\n",
      "epoch: 868800, train loss: 4.051649951934815, val loss: 4.067470264434815, ETA in seconds: 6157147.343\n",
      "epoch: 868900, train loss: 4.051454639434814, val loss: 4.064931201934814, ETA in seconds: 6153142.910\n",
      "epoch: 869000, train loss: 4.055751514434815, val loss: 4.066493701934815, ETA in seconds: 6149177.632\n",
      "epoch: 869100, train loss: 4.0569233894348145, val loss: 4.067079639434814, ETA in seconds: 6145186.313\n",
      "epoch: 869200, train loss: 4.055751514434815, val loss: 4.067860889434814, ETA in seconds: 6141220.226\n",
      "epoch: 869300, train loss: 4.047939014434815, val loss: 4.064149951934814, ETA in seconds: 6137221.557\n",
      "epoch: 869400, train loss: 4.051454639434814, val loss: 4.071962451934814, ETA in seconds: 6133238.475\n",
      "epoch: 869500, train loss: 4.055751514434815, val loss: 4.0657124519348145, ETA in seconds: 6129232.308\n",
      "epoch: 869600, train loss: 4.060634326934815, val loss: 4.065907764434814, ETA in seconds: 6125225.830\n",
      "epoch: 869700, train loss: 4.050673389434815, val loss: 4.069032764434814, ETA in seconds: 6121209.667\n",
      "epoch: 869800, train loss: 4.053407764434814, val loss: 4.070790576934814, ETA in seconds: 6117198.607\n",
      "epoch: 869900, train loss: 4.045595264434814, val loss: 4.066298389434815, ETA in seconds: 6113176.990\n",
      "epoch: 870000, train loss: 4.058095264434814, val loss: 4.063564014434815, ETA in seconds: 6109175.062\n",
      "epoch: 870100, train loss: 4.0500874519348145, val loss: 4.0715718269348145, ETA in seconds: 6105171.946\n",
      "epoch: 870200, train loss: 4.048524951934814, val loss: 4.068056201934814, ETA in seconds: 6101146.299\n",
      "epoch: 870300, train loss: 4.0520405769348145, val loss: 4.069032764434814, ETA in seconds: 6097136.482\n",
      "epoch: 870400, train loss: 4.0481343269348145, val loss: 4.0657124519348145, ETA in seconds: 6093112.557\n",
      "epoch: 870500, train loss: 4.047548389434814, val loss: 4.062587451934815, ETA in seconds: 6089096.993\n",
      "epoch: 870600, train loss: 4.049306201934814, val loss: 4.060048389434814, ETA in seconds: 6085080.253\n",
      "epoch: 870700, train loss: 4.056337451934814, val loss: 4.064540576934815, ETA in seconds: 6081059.186\n",
      "epoch: 870800, train loss: 4.052235889434814, val loss: 4.071181201934815, ETA in seconds: 6077037.052\n",
      "epoch: 870900, train loss: 4.058485889434815, val loss: 4.072743701934814, ETA in seconds: 6073011.884\n",
      "epoch: 871000, train loss: 4.046767139434815, val loss: 4.070399951934815, ETA in seconds: 6068989.982\n",
      "epoch: 871100, train loss: 4.051259326934814, val loss: 4.066884326934814, ETA in seconds: 6064960.208\n",
      "epoch: 871200, train loss: 4.056532764434815, val loss: 4.068056201934814, ETA in seconds: 6060933.195\n",
      "epoch: 871300, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 6056904.614\n",
      "epoch: 871400, train loss: 4.048915576934815, val loss: 4.063564014434815, ETA in seconds: 6052880.921\n",
      "epoch: 871500, train loss: 4.0500874519348145, val loss: 4.075282764434815, ETA in seconds: 6048862.008\n",
      "epoch: 871600, train loss: 4.047353076934814, val loss: 4.067860889434814, ETA in seconds: 6044837.552\n",
      "epoch: 871700, train loss: 4.047743701934815, val loss: 4.066103076934814, ETA in seconds: 6040813.942\n",
      "epoch: 871800, train loss: 4.050673389434815, val loss: 4.066103076934814, ETA in seconds: 6036803.225\n",
      "epoch: 871900, train loss: 4.055556201934815, val loss: 4.065517139434815, ETA in seconds: 6032759.516\n",
      "epoch: 872000, train loss: 4.061415576934815, val loss: 4.0676655769348145, ETA in seconds: 6028719.049\n",
      "epoch: 872100, train loss: 4.052821826934815, val loss: 4.069423389434815, ETA in seconds: 6024684.576\n",
      "epoch: 872200, train loss: 4.0559468269348145, val loss: 4.064931201934814, ETA in seconds: 6020660.118\n",
      "epoch: 872300, train loss: 4.051845264434815, val loss: 4.0608296394348145, ETA in seconds: 6016625.822\n",
      "epoch: 872400, train loss: 4.058095264434814, val loss: 4.067470264434815, ETA in seconds: 6012607.848\n",
      "epoch: 872500, train loss: 4.055360889434814, val loss: 4.071181201934815, ETA in seconds: 6008570.144\n",
      "epoch: 872600, train loss: 4.054774951934815, val loss: 4.071376514434815, ETA in seconds: 6004496.827\n",
      "epoch: 872700, train loss: 4.052235889434814, val loss: 4.066298389434815, ETA in seconds: 6000470.157\n",
      "epoch: 872800, train loss: 4.053603076934815, val loss: 4.063954639434814, ETA in seconds: 5996426.295\n",
      "epoch: 872900, train loss: 4.048524951934814, val loss: 4.062978076934814, ETA in seconds: 5992382.829\n",
      "epoch: 873000, train loss: 4.0500874519348145, val loss: 4.064345264434815, ETA in seconds: 5988348.390\n",
      "epoch: 873100, train loss: 4.046376514434814, val loss: 4.068251514434815, ETA in seconds: 5984306.543\n",
      "epoch: 873200, train loss: 4.054774951934815, val loss: 4.068056201934814, ETA in seconds: 5980263.304\n",
      "epoch: 873300, train loss: 4.050478076934814, val loss: 4.064540576934815, ETA in seconds: 5976216.926\n",
      "epoch: 873400, train loss: 4.051845264434815, val loss: 4.063564014434815, ETA in seconds: 5972173.912\n",
      "epoch: 873500, train loss: 4.058681201934815, val loss: 4.060634326934815, ETA in seconds: 5968132.280\n",
      "epoch: 873600, train loss: 4.048329639434814, val loss: 4.066884326934814, ETA in seconds: 5964089.787\n",
      "epoch: 873700, train loss: 4.0539937019348145, val loss: 4.065907764434814, ETA in seconds: 5960041.315\n",
      "epoch: 873800, train loss: 4.050478076934814, val loss: 4.067274951934815, ETA in seconds: 5955987.068\n",
      "epoch: 873900, train loss: 4.052235889434814, val loss: 4.065907764434814, ETA in seconds: 5951950.016\n",
      "epoch: 874000, train loss: 4.0539937019348145, val loss: 4.060634326934815, ETA in seconds: 5947883.940\n",
      "epoch: 874100, train loss: 4.0569233894348145, val loss: 4.071181201934815, ETA in seconds: 5943838.773\n",
      "epoch: 874200, train loss: 4.048915576934815, val loss: 4.062001514434814, ETA in seconds: 5939785.741\n",
      "epoch: 874300, train loss: 4.053798389434815, val loss: 4.070985889434814, ETA in seconds: 5935736.459\n",
      "epoch: 874400, train loss: 4.048329639434814, val loss: 4.063173389434814, ETA in seconds: 5931677.156\n",
      "epoch: 874500, train loss: 4.0471577644348145, val loss: 4.068837451934814, ETA in seconds: 5927607.560\n",
      "epoch: 874600, train loss: 4.056337451934814, val loss: 4.065126514434814, ETA in seconds: 5923547.425\n",
      "epoch: 874700, train loss: 4.0559468269348145, val loss: 4.067274951934815, ETA in seconds: 5919483.498\n",
      "epoch: 874800, train loss: 4.049892139434815, val loss: 4.068837451934814, ETA in seconds: 5915425.070\n",
      "epoch: 874900, train loss: 4.054579639434815, val loss: 4.070204639434815, ETA in seconds: 5911377.004\n",
      "epoch: 875000, train loss: 4.052431201934814, val loss: 4.063173389434814, ETA in seconds: 5907318.992\n",
      "epoch: 875100, train loss: 4.0471577644348145, val loss: 4.065126514434814, ETA in seconds: 5903275.240\n",
      "epoch: 875200, train loss: 4.048329639434814, val loss: 4.058095264434814, ETA in seconds: 5899215.779\n",
      "epoch: 875300, train loss: 4.0588765144348145, val loss: 4.0666890144348145, ETA in seconds: 5895147.978\n",
      "epoch: 875400, train loss: 4.056337451934814, val loss: 4.063954639434814, ETA in seconds: 5891070.620\n",
      "epoch: 875500, train loss: 4.051845264434815, val loss: 4.062587451934815, ETA in seconds: 5887003.881\n",
      "epoch: 875600, train loss: 4.052431201934814, val loss: 4.063564014434815, ETA in seconds: 5882931.247\n",
      "epoch: 875700, train loss: 4.049696826934815, val loss: 4.067860889434814, ETA in seconds: 5878862.940\n",
      "epoch: 875800, train loss: 4.060634326934815, val loss: 4.066103076934814, ETA in seconds: 5874791.802\n",
      "epoch: 875900, train loss: 4.055751514434815, val loss: 4.066884326934814, ETA in seconds: 5870713.725\n",
      "epoch: 876000, train loss: 4.050868701934815, val loss: 4.0647358894348145, ETA in seconds: 5866637.929\n",
      "epoch: 876100, train loss: 4.0432515144348145, val loss: 4.064540576934815, ETA in seconds: 5862660.441\n",
      "epoch: 876200, train loss: 4.0549702644348145, val loss: 4.0676655769348145, ETA in seconds: 5858583.501\n",
      "epoch: 876300, train loss: 4.048915576934815, val loss: 4.065517139434815, ETA in seconds: 5854505.807\n",
      "epoch: 876400, train loss: 4.056337451934814, val loss: 4.067274951934815, ETA in seconds: 5850414.260\n",
      "epoch: 876500, train loss: 4.058095264434814, val loss: 4.066884326934814, ETA in seconds: 5846326.346\n",
      "epoch: 876600, train loss: 4.051845264434815, val loss: 4.066298389434815, ETA in seconds: 5842237.145\n",
      "epoch: 876700, train loss: 4.053603076934815, val loss: 4.074110889434815, ETA in seconds: 5838152.259\n",
      "epoch: 876800, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 5834066.438\n",
      "epoch: 876900, train loss: 4.047743701934815, val loss: 4.0676655769348145, ETA in seconds: 5829987.058\n",
      "epoch: 877000, train loss: 4.0559468269348145, val loss: 4.068251514434815, ETA in seconds: 5825910.311\n",
      "epoch: 877100, train loss: 4.047548389434814, val loss: 4.065907764434814, ETA in seconds: 5821838.471\n",
      "epoch: 877200, train loss: 4.055360889434814, val loss: 4.071181201934815, ETA in seconds: 5817750.397\n",
      "epoch: 877300, train loss: 4.049696826934815, val loss: 4.0705952644348145, ETA in seconds: 5813671.510\n",
      "epoch: 877400, train loss: 4.052235889434814, val loss: 4.0657124519348145, ETA in seconds: 5809571.978\n",
      "epoch: 877500, train loss: 4.049306201934814, val loss: 4.068056201934814, ETA in seconds: 5805472.392\n",
      "epoch: 877600, train loss: 4.059267139434814, val loss: 4.0686421394348145, ETA in seconds: 5801374.136\n",
      "epoch: 877700, train loss: 4.0510640144348145, val loss: 4.071181201934815, ETA in seconds: 5797270.324\n",
      "epoch: 877800, train loss: 4.051845264434815, val loss: 4.071181201934815, ETA in seconds: 5793181.369\n",
      "epoch: 877900, train loss: 4.0549702644348145, val loss: 4.0705952644348145, ETA in seconds: 5789112.060\n",
      "epoch: 878000, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 5785009.931\n",
      "epoch: 878100, train loss: 4.0569233894348145, val loss: 4.068446826934815, ETA in seconds: 5780994.258\n",
      "epoch: 878200, train loss: 4.0549702644348145, val loss: 4.0647358894348145, ETA in seconds: 5776946.553\n",
      "epoch: 878300, train loss: 4.055556201934815, val loss: 4.063173389434814, ETA in seconds: 5772835.976\n",
      "epoch: 878400, train loss: 4.059267139434814, val loss: 4.067079639434814, ETA in seconds: 5768739.156\n",
      "epoch: 878500, train loss: 4.057314014434814, val loss: 4.072743701934814, ETA in seconds: 5764632.853\n",
      "epoch: 878600, train loss: 4.054774951934815, val loss: 4.064931201934814, ETA in seconds: 5760528.000\n",
      "epoch: 878700, train loss: 4.054384326934814, val loss: 4.070399951934815, ETA in seconds: 5756440.780\n",
      "epoch: 878800, train loss: 4.055165576934814, val loss: 4.066493701934815, ETA in seconds: 5752343.436\n",
      "epoch: 878900, train loss: 4.046767139434815, val loss: 4.068446826934815, ETA in seconds: 5748252.144\n",
      "epoch: 879000, train loss: 4.055360889434814, val loss: 4.068837451934814, ETA in seconds: 5744162.867\n",
      "epoch: 879100, train loss: 4.0578999519348145, val loss: 4.065126514434814, ETA in seconds: 5740062.137\n",
      "epoch: 879200, train loss: 4.054384326934814, val loss: 4.066884326934814, ETA in seconds: 5735952.442\n",
      "epoch: 879300, train loss: 4.052431201934814, val loss: 4.068837451934814, ETA in seconds: 5731853.436\n",
      "epoch: 879400, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 5727763.259\n",
      "epoch: 879500, train loss: 4.047548389434814, val loss: 4.061610889434815, ETA in seconds: 5723658.578\n",
      "epoch: 879600, train loss: 4.0510640144348145, val loss: 4.072157764434815, ETA in seconds: 5719525.702\n",
      "epoch: 879700, train loss: 4.056142139434814, val loss: 4.0696187019348145, ETA in seconds: 5715489.791\n",
      "epoch: 879800, train loss: 4.0520405769348145, val loss: 4.069032764434814, ETA in seconds: 5711469.337\n",
      "epoch: 879900, train loss: 4.046767139434815, val loss: 4.0647358894348145, ETA in seconds: 5707452.789\n",
      "epoch: 880000, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 5703430.983\n",
      "epoch: 880100, train loss: 4.052821826934815, val loss: 4.0637593269348145, ETA in seconds: 5699356.131\n",
      "epoch: 880200, train loss: 4.048915576934815, val loss: 4.064345264434815, ETA in seconds: 5695303.252\n",
      "epoch: 880300, train loss: 4.057314014434814, val loss: 4.063173389434814, ETA in seconds: 5691192.806\n",
      "epoch: 880400, train loss: 4.049892139434815, val loss: 4.0715718269348145, ETA in seconds: 5687064.812\n",
      "epoch: 880500, train loss: 4.054579639434815, val loss: 4.073329639434815, ETA in seconds: 5682938.342\n",
      "epoch: 880600, train loss: 4.0491108894348145, val loss: 4.073915576934814, ETA in seconds: 5678819.480\n",
      "epoch: 880700, train loss: 4.052235889434814, val loss: 4.0715718269348145, ETA in seconds: 5674694.044\n",
      "epoch: 880800, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 5670567.980\n",
      "epoch: 880900, train loss: 4.051259326934814, val loss: 4.066493701934815, ETA in seconds: 5666440.055\n",
      "epoch: 881000, train loss: 4.0481343269348145, val loss: 4.070204639434815, ETA in seconds: 5662312.661\n",
      "epoch: 881100, train loss: 4.055165576934814, val loss: 4.0696187019348145, ETA in seconds: 5658195.821\n",
      "epoch: 881200, train loss: 4.053407764434814, val loss: 4.070009326934814, ETA in seconds: 5654067.114\n",
      "epoch: 881300, train loss: 4.046962451934815, val loss: 4.0705952644348145, ETA in seconds: 5649923.652\n",
      "epoch: 881400, train loss: 4.0530171394348145, val loss: 4.074110889434815, ETA in seconds: 5645778.043\n",
      "epoch: 881500, train loss: 4.060439014434815, val loss: 4.060243701934814, ETA in seconds: 5641641.413\n",
      "epoch: 881600, train loss: 4.0530171394348145, val loss: 4.070399951934815, ETA in seconds: 5637603.414\n",
      "epoch: 881700, train loss: 4.049306201934814, val loss: 4.0686421394348145, ETA in seconds: 5633530.979\n",
      "epoch: 881800, train loss: 4.053603076934815, val loss: 4.065907764434814, ETA in seconds: 5629428.205\n",
      "epoch: 881900, train loss: 4.0530171394348145, val loss: 4.067470264434815, ETA in seconds: 5625280.996\n",
      "epoch: 882000, train loss: 4.0500874519348145, val loss: 4.070399951934815, ETA in seconds: 5621136.077\n",
      "epoch: 882100, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 5616992.852\n",
      "epoch: 882200, train loss: 4.046376514434814, val loss: 4.067079639434814, ETA in seconds: 5612836.218\n",
      "epoch: 882300, train loss: 4.0569233894348145, val loss: 4.0657124519348145, ETA in seconds: 5608689.388\n",
      "epoch: 882400, train loss: 4.046767139434815, val loss: 4.0696187019348145, ETA in seconds: 5604529.864\n",
      "epoch: 882500, train loss: 4.0500874519348145, val loss: 4.0705952644348145, ETA in seconds: 5600372.606\n",
      "epoch: 882600, train loss: 4.054774951934815, val loss: 4.061024951934814, ETA in seconds: 5596225.352\n",
      "epoch: 882700, train loss: 4.048915576934815, val loss: 4.070204639434815, ETA in seconds: 5592056.349\n",
      "epoch: 882800, train loss: 4.057509326934815, val loss: 4.0637593269348145, ETA in seconds: 5587914.438\n",
      "epoch: 882900, train loss: 4.049696826934815, val loss: 4.0725483894348145, ETA in seconds: 5583769.177\n",
      "epoch: 883000, train loss: 4.053212451934814, val loss: 4.075087451934815, ETA in seconds: 5579622.992\n",
      "epoch: 883100, train loss: 4.049501514434814, val loss: 4.0676655769348145, ETA in seconds: 5575469.283\n",
      "epoch: 883200, train loss: 4.048524951934814, val loss: 4.0608296394348145, ETA in seconds: 5571303.355\n",
      "epoch: 883300, train loss: 4.0471577644348145, val loss: 4.064345264434815, ETA in seconds: 5567152.952\n",
      "epoch: 883400, train loss: 4.052431201934814, val loss: 4.0686421394348145, ETA in seconds: 5563009.254\n",
      "epoch: 883500, train loss: 4.056142139434814, val loss: 4.070009326934814, ETA in seconds: 5558866.981\n",
      "epoch: 883600, train loss: 4.054579639434815, val loss: 4.069032764434814, ETA in seconds: 5554704.404\n",
      "epoch: 883700, train loss: 4.047939014434815, val loss: 4.068251514434815, ETA in seconds: 5550533.234\n",
      "epoch: 883800, train loss: 4.0481343269348145, val loss: 4.063564014434815, ETA in seconds: 5546371.612\n",
      "epoch: 883900, train loss: 4.054189014434814, val loss: 4.071181201934815, ETA in seconds: 5542207.063\n",
      "epoch: 884000, train loss: 4.0549702644348145, val loss: 4.069814014434814, ETA in seconds: 5538048.200\n",
      "epoch: 884100, train loss: 4.060048389434814, val loss: 4.062978076934814, ETA in seconds: 5533888.338\n",
      "epoch: 884200, train loss: 4.0559468269348145, val loss: 4.066298389434815, ETA in seconds: 5529724.807\n",
      "epoch: 884300, train loss: 4.0471577644348145, val loss: 4.071181201934815, ETA in seconds: 5525555.643\n",
      "epoch: 884400, train loss: 4.051454639434814, val loss: 4.070790576934814, ETA in seconds: 5521385.622\n",
      "epoch: 884500, train loss: 4.0549702644348145, val loss: 4.0705952644348145, ETA in seconds: 5517224.527\n",
      "epoch: 884600, train loss: 4.048329639434814, val loss: 4.066298389434815, ETA in seconds: 5513051.320\n",
      "epoch: 884700, train loss: 4.049696826934815, val loss: 4.068251514434815, ETA in seconds: 5508880.574\n",
      "epoch: 884800, train loss: 4.049306201934814, val loss: 4.072353076934815, ETA in seconds: 5504702.285\n",
      "epoch: 884900, train loss: 4.066884326934814, val loss: 4.065126514434814, ETA in seconds: 5500534.763\n",
      "epoch: 885000, train loss: 4.048720264434815, val loss: 4.069228076934815, ETA in seconds: 5496452.012\n",
      "epoch: 885100, train loss: 4.050673389434815, val loss: 4.067470264434815, ETA in seconds: 5492366.599\n",
      "epoch: 885200, train loss: 4.0500874519348145, val loss: 4.0666890144348145, ETA in seconds: 5488257.681\n",
      "epoch: 885300, train loss: 4.0520405769348145, val loss: 4.067470264434815, ETA in seconds: 5484101.453\n",
      "epoch: 885400, train loss: 4.0530171394348145, val loss: 4.065126514434814, ETA in seconds: 5479947.932\n",
      "epoch: 885500, train loss: 4.051259326934814, val loss: 4.0696187019348145, ETA in seconds: 5475772.740\n",
      "epoch: 885600, train loss: 4.051259326934814, val loss: 4.067860889434814, ETA in seconds: 5471647.743\n",
      "epoch: 885700, train loss: 4.050868701934815, val loss: 4.069228076934815, ETA in seconds: 5467471.330\n",
      "epoch: 885800, train loss: 4.061415576934815, val loss: 4.066493701934815, ETA in seconds: 5463294.275\n",
      "epoch: 885900, train loss: 4.048720264434815, val loss: 4.0676655769348145, ETA in seconds: 5459118.437\n",
      "epoch: 886000, train loss: 4.052626514434815, val loss: 4.068837451934814, ETA in seconds: 5454938.015\n",
      "epoch: 886100, train loss: 4.046767139434815, val loss: 4.065517139434815, ETA in seconds: 5450765.263\n",
      "epoch: 886200, train loss: 4.046962451934815, val loss: 4.064345264434815, ETA in seconds: 5446584.168\n",
      "epoch: 886300, train loss: 4.046767139434815, val loss: 4.070009326934814, ETA in seconds: 5442403.845\n",
      "epoch: 886400, train loss: 4.049501514434814, val loss: 4.0735249519348145, ETA in seconds: 5438221.269\n",
      "epoch: 886500, train loss: 4.052431201934814, val loss: 4.071767139434814, ETA in seconds: 5434033.569\n",
      "epoch: 886600, train loss: 4.054384326934814, val loss: 4.057118701934814, ETA in seconds: 5429831.460\n",
      "epoch: 886700, train loss: 4.052821826934815, val loss: 4.061610889434815, ETA in seconds: 5425638.794\n",
      "epoch: 886800, train loss: 4.054774951934815, val loss: 4.070790576934814, ETA in seconds: 5421446.155\n",
      "epoch: 886900, train loss: 4.049696826934815, val loss: 4.067079639434814, ETA in seconds: 5417261.389\n",
      "epoch: 887000, train loss: 4.0461812019348145, val loss: 4.060439014434815, ETA in seconds: 5413053.956\n",
      "epoch: 887100, train loss: 4.046767139434815, val loss: 4.068251514434815, ETA in seconds: 5408858.986\n",
      "epoch: 887200, train loss: 4.048524951934814, val loss: 4.072353076934815, ETA in seconds: 5404663.546\n",
      "epoch: 887300, train loss: 4.050868701934815, val loss: 4.065907764434814, ETA in seconds: 5400458.573\n",
      "epoch: 887400, train loss: 4.053407764434814, val loss: 4.072157764434815, ETA in seconds: 5396305.496\n",
      "epoch: 887500, train loss: 4.058095264434814, val loss: 4.070985889434814, ETA in seconds: 5392116.992\n",
      "epoch: 887600, train loss: 4.0432515144348145, val loss: 4.067274951934815, ETA in seconds: 5387903.669\n",
      "epoch: 887700, train loss: 4.053212451934814, val loss: 4.067860889434814, ETA in seconds: 5383694.113\n",
      "epoch: 887800, train loss: 4.058485889434815, val loss: 4.073134326934815, ETA in seconds: 5379482.975\n",
      "epoch: 887900, train loss: 4.051454639434814, val loss: 4.0725483894348145, ETA in seconds: 5375278.800\n",
      "epoch: 888000, train loss: 4.0539937019348145, val loss: 4.071181201934815, ETA in seconds: 5371055.605\n",
      "epoch: 888100, train loss: 4.052235889434814, val loss: 4.066493701934815, ETA in seconds: 5366840.078\n",
      "epoch: 888200, train loss: 4.053407764434814, val loss: 4.066103076934814, ETA in seconds: 5362632.894\n",
      "epoch: 888300, train loss: 4.044423389434814, val loss: 4.065321826934815, ETA in seconds: 5358418.789\n",
      "epoch: 888400, train loss: 4.048524951934814, val loss: 4.065321826934815, ETA in seconds: 5354198.840\n",
      "epoch: 888500, train loss: 4.051454639434814, val loss: 4.063564014434815, ETA in seconds: 5349978.001\n",
      "epoch: 888600, train loss: 4.0569233894348145, val loss: 4.0637593269348145, ETA in seconds: 5345773.982\n",
      "epoch: 888700, train loss: 4.056337451934814, val loss: 4.058485889434815, ETA in seconds: 5341562.113\n",
      "epoch: 888800, train loss: 4.0510640144348145, val loss: 4.0715718269348145, ETA in seconds: 5337344.220\n",
      "epoch: 888900, train loss: 4.052626514434815, val loss: 4.065907764434814, ETA in seconds: 5333134.705\n",
      "epoch: 889000, train loss: 4.058485889434815, val loss: 4.063564014434815, ETA in seconds: 5328915.412\n",
      "epoch: 889100, train loss: 4.047743701934815, val loss: 4.068251514434815, ETA in seconds: 5324694.242\n",
      "epoch: 889200, train loss: 4.053603076934815, val loss: 4.067860889434814, ETA in seconds: 5320462.385\n",
      "epoch: 889300, train loss: 4.047743701934815, val loss: 4.063173389434814, ETA in seconds: 5316225.713\n",
      "epoch: 889400, train loss: 4.055165576934814, val loss: 4.0686421394348145, ETA in seconds: 5311998.993\n",
      "epoch: 889500, train loss: 4.054774951934815, val loss: 4.062978076934814, ETA in seconds: 5307767.750\n",
      "epoch: 889600, train loss: 4.0520405769348145, val loss: 4.064345264434815, ETA in seconds: 5303549.164\n",
      "epoch: 889700, train loss: 4.053798389434815, val loss: 4.067470264434815, ETA in seconds: 5299325.869\n",
      "epoch: 889800, train loss: 4.053603076934815, val loss: 4.0627827644348145, ETA in seconds: 5295096.016\n",
      "epoch: 889900, train loss: 4.054774951934815, val loss: 4.060634326934815, ETA in seconds: 5290874.096\n",
      "epoch: 890000, train loss: 4.052431201934814, val loss: 4.066884326934814, ETA in seconds: 5286654.094\n",
      "epoch: 890100, train loss: 4.056532764434815, val loss: 4.0705952644348145, ETA in seconds: 5282422.561\n",
      "epoch: 890200, train loss: 4.054189014434814, val loss: 4.064540576934815, ETA in seconds: 5278220.192\n",
      "epoch: 890300, train loss: 4.052821826934815, val loss: 4.067470264434815, ETA in seconds: 5273984.139\n",
      "epoch: 890400, train loss: 4.0491108894348145, val loss: 4.064149951934814, ETA in seconds: 5269752.358\n",
      "epoch: 890500, train loss: 4.054384326934814, val loss: 4.069814014434814, ETA in seconds: 5265510.860\n",
      "epoch: 890600, train loss: 4.055165576934814, val loss: 4.062978076934814, ETA in seconds: 5261280.586\n",
      "epoch: 890700, train loss: 4.0627827644348145, val loss: 4.070204639434815, ETA in seconds: 5257044.930\n",
      "epoch: 890800, train loss: 4.057314014434814, val loss: 4.070790576934814, ETA in seconds: 5252812.824\n",
      "epoch: 890900, train loss: 4.047743701934815, val loss: 4.065517139434815, ETA in seconds: 5248570.318\n",
      "epoch: 891000, train loss: 4.050673389434815, val loss: 4.0676655769348145, ETA in seconds: 5244334.194\n",
      "epoch: 891100, train loss: 4.0491108894348145, val loss: 4.070204639434815, ETA in seconds: 5240090.424\n",
      "epoch: 891200, train loss: 4.050868701934815, val loss: 4.0725483894348145, ETA in seconds: 5235836.096\n",
      "epoch: 891300, train loss: 4.0520405769348145, val loss: 4.068056201934814, ETA in seconds: 5231598.737\n",
      "epoch: 891400, train loss: 4.050478076934814, val loss: 4.0657124519348145, ETA in seconds: 5227360.446\n",
      "epoch: 891500, train loss: 4.057509326934815, val loss: 4.068056201934814, ETA in seconds: 5223142.195\n",
      "epoch: 891600, train loss: 4.053603076934815, val loss: 4.073329639434815, ETA in seconds: 5218950.049\n",
      "epoch: 891700, train loss: 4.049501514434814, val loss: 4.0666890144348145, ETA in seconds: 5214761.277\n",
      "epoch: 891800, train loss: 4.055360889434814, val loss: 4.0686421394348145, ETA in seconds: 5210557.710\n",
      "epoch: 891900, train loss: 4.051259326934814, val loss: 4.064345264434815, ETA in seconds: 5206324.293\n",
      "epoch: 892000, train loss: 4.0481343269348145, val loss: 4.068056201934814, ETA in seconds: 5202071.276\n",
      "epoch: 892100, train loss: 4.054579639434815, val loss: 4.067079639434814, ETA in seconds: 5197827.461\n",
      "epoch: 892200, train loss: 4.053407764434814, val loss: 4.064345264434815, ETA in seconds: 5193587.516\n",
      "epoch: 892300, train loss: 4.045985889434815, val loss: 4.068837451934814, ETA in seconds: 5189334.767\n",
      "epoch: 892400, train loss: 4.054579639434815, val loss: 4.068837451934814, ETA in seconds: 5185092.258\n",
      "epoch: 892500, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 5180837.589\n",
      "epoch: 892600, train loss: 4.053212451934814, val loss: 4.066493701934815, ETA in seconds: 5176581.225\n",
      "epoch: 892700, train loss: 4.045985889434815, val loss: 4.068837451934814, ETA in seconds: 5172323.592\n",
      "epoch: 892800, train loss: 4.0481343269348145, val loss: 4.065126514434814, ETA in seconds: 5168064.222\n",
      "epoch: 892900, train loss: 4.056728076934815, val loss: 4.069423389434815, ETA in seconds: 5163801.400\n",
      "epoch: 893000, train loss: 4.055556201934815, val loss: 4.066103076934814, ETA in seconds: 5159547.026\n",
      "epoch: 893100, train loss: 4.049696826934815, val loss: 4.068056201934814, ETA in seconds: 5155293.556\n",
      "epoch: 893200, train loss: 4.054774951934815, val loss: 4.0686421394348145, ETA in seconds: 5151031.878\n",
      "epoch: 893300, train loss: 4.051845264434815, val loss: 4.063173389434814, ETA in seconds: 5146778.281\n",
      "epoch: 893400, train loss: 4.052821826934815, val loss: 4.063564014434815, ETA in seconds: 5142543.823\n",
      "epoch: 893500, train loss: 4.049501514434814, val loss: 4.068446826934815, ETA in seconds: 5138304.635\n",
      "epoch: 893600, train loss: 4.054579639434815, val loss: 4.065517139434815, ETA in seconds: 5134059.040\n",
      "epoch: 893700, train loss: 4.0539937019348145, val loss: 4.069423389434815, ETA in seconds: 5129813.379\n",
      "epoch: 893800, train loss: 4.057704639434815, val loss: 4.065126514434814, ETA in seconds: 5125567.594\n",
      "epoch: 893900, train loss: 4.055751514434815, val loss: 4.070009326934814, ETA in seconds: 5121304.188\n",
      "epoch: 894000, train loss: 4.047353076934814, val loss: 4.065517139434815, ETA in seconds: 5117060.171\n",
      "epoch: 894100, train loss: 4.051454639434814, val loss: 4.072743701934814, ETA in seconds: 5112802.452\n",
      "epoch: 894200, train loss: 4.051454639434814, val loss: 4.0676655769348145, ETA in seconds: 5108530.649\n",
      "epoch: 894300, train loss: 4.050478076934814, val loss: 4.064345264434815, ETA in seconds: 5104268.467\n",
      "epoch: 894400, train loss: 4.057314014434814, val loss: 4.066298389434815, ETA in seconds: 5100012.339\n",
      "epoch: 894500, train loss: 4.054189014434814, val loss: 4.0657124519348145, ETA in seconds: 5095755.497\n",
      "epoch: 894600, train loss: 4.055556201934815, val loss: 4.069228076934815, ETA in seconds: 5091497.341\n",
      "epoch: 894700, train loss: 4.0461812019348145, val loss: 4.067079639434814, ETA in seconds: 5087250.098\n",
      "epoch: 894800, train loss: 4.047939014434815, val loss: 4.070985889434814, ETA in seconds: 5082970.378\n",
      "epoch: 894900, train loss: 4.057118701934814, val loss: 4.071181201934815, ETA in seconds: 5078701.940\n",
      "epoch: 895000, train loss: 4.048524951934814, val loss: 4.069423389434815, ETA in seconds: 5074426.439\n",
      "epoch: 895100, train loss: 4.0461812019348145, val loss: 4.067079639434814, ETA in seconds: 5070151.210\n",
      "epoch: 895200, train loss: 4.054384326934814, val loss: 4.065517139434815, ETA in seconds: 5065865.514\n",
      "epoch: 895300, train loss: 4.055751514434815, val loss: 4.073329639434815, ETA in seconds: 5061583.893\n",
      "epoch: 895400, train loss: 4.049501514434814, val loss: 4.066103076934814, ETA in seconds: 5057295.372\n",
      "epoch: 895500, train loss: 4.053407764434814, val loss: 4.070204639434815, ETA in seconds: 5053020.944\n",
      "epoch: 895600, train loss: 4.061024951934814, val loss: 4.067274951934815, ETA in seconds: 5048737.544\n",
      "epoch: 895700, train loss: 4.045595264434814, val loss: 4.068056201934814, ETA in seconds: 5044451.621\n",
      "epoch: 895800, train loss: 4.0471577644348145, val loss: 4.067470264434815, ETA in seconds: 5040171.836\n",
      "epoch: 895900, train loss: 4.053407764434814, val loss: 4.070985889434814, ETA in seconds: 5035892.774\n",
      "epoch: 896000, train loss: 4.050282764434814, val loss: 4.061415576934815, ETA in seconds: 5031615.474\n",
      "epoch: 896100, train loss: 4.059462451934815, val loss: 4.069423389434815, ETA in seconds: 5027331.214\n",
      "epoch: 896200, train loss: 4.053212451934814, val loss: 4.0725483894348145, ETA in seconds: 5023040.883\n",
      "epoch: 896300, train loss: 4.0569233894348145, val loss: 4.0686421394348145, ETA in seconds: 5018765.075\n",
      "epoch: 896400, train loss: 4.043446826934814, val loss: 4.071962451934814, ETA in seconds: 5014481.293\n",
      "epoch: 896500, train loss: 4.056532764434815, val loss: 4.073915576934814, ETA in seconds: 5010196.438\n",
      "epoch: 896600, train loss: 4.055165576934814, val loss: 4.068056201934814, ETA in seconds: 5005919.981\n",
      "epoch: 896700, train loss: 4.0539937019348145, val loss: 4.0657124519348145, ETA in seconds: 5001621.069\n",
      "epoch: 896800, train loss: 4.050478076934814, val loss: 4.0705952644348145, ETA in seconds: 4997318.743\n",
      "epoch: 896900, train loss: 4.057704639434815, val loss: 4.067274951934815, ETA in seconds: 4993031.816\n",
      "epoch: 897000, train loss: 4.054579639434815, val loss: 4.066884326934814, ETA in seconds: 4988724.138\n",
      "epoch: 897100, train loss: 4.0559468269348145, val loss: 4.066103076934814, ETA in seconds: 4984438.827\n",
      "epoch: 897200, train loss: 4.052821826934815, val loss: 4.065907764434814, ETA in seconds: 4980143.297\n",
      "epoch: 897300, train loss: 4.054189014434814, val loss: 4.070204639434815, ETA in seconds: 4975846.476\n",
      "epoch: 897400, train loss: 4.0559468269348145, val loss: 4.068056201934814, ETA in seconds: 4971540.323\n",
      "epoch: 897500, train loss: 4.050478076934814, val loss: 4.0637593269348145, ETA in seconds: 4967242.552\n",
      "epoch: 897600, train loss: 4.049306201934814, val loss: 4.068056201934814, ETA in seconds: 4962945.622\n",
      "epoch: 897700, train loss: 4.050282764434814, val loss: 4.064149951934814, ETA in seconds: 4958643.766\n",
      "epoch: 897800, train loss: 4.055360889434814, val loss: 4.066493701934815, ETA in seconds: 4954347.619\n",
      "epoch: 897900, train loss: 4.051259326934814, val loss: 4.070790576934814, ETA in seconds: 4950042.441\n",
      "epoch: 898000, train loss: 4.054384326934814, val loss: 4.0676655769348145, ETA in seconds: 4945725.308\n",
      "epoch: 898100, train loss: 4.058485889434815, val loss: 4.068056201934814, ETA in seconds: 4941409.261\n",
      "epoch: 898200, train loss: 4.047743701934815, val loss: 4.069032764434814, ETA in seconds: 4937097.488\n",
      "epoch: 898300, train loss: 4.0432515144348145, val loss: 4.063564014434815, ETA in seconds: 4932784.241\n",
      "epoch: 898400, train loss: 4.043642139434814, val loss: 4.067470264434815, ETA in seconds: 4928472.301\n",
      "epoch: 898500, train loss: 4.050478076934814, val loss: 4.066884326934814, ETA in seconds: 4924160.691\n",
      "epoch: 898600, train loss: 4.052821826934815, val loss: 4.064540576934815, ETA in seconds: 4919847.225\n",
      "epoch: 898700, train loss: 4.057314014434814, val loss: 4.063368701934815, ETA in seconds: 4915520.834\n",
      "epoch: 898800, train loss: 4.052626514434815, val loss: 4.066103076934814, ETA in seconds: 4911193.061\n",
      "epoch: 898900, train loss: 4.045985889434815, val loss: 4.069814014434814, ETA in seconds: 4906914.402\n",
      "epoch: 899000, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 4902650.281\n",
      "epoch: 899100, train loss: 4.052626514434815, val loss: 4.066884326934814, ETA in seconds: 4898322.870\n",
      "epoch: 899200, train loss: 4.0471577644348145, val loss: 4.065517139434815, ETA in seconds: 4894008.335\n",
      "epoch: 899300, train loss: 4.056532764434815, val loss: 4.0676655769348145, ETA in seconds: 4889694.703\n",
      "epoch: 899400, train loss: 4.054579639434815, val loss: 4.0657124519348145, ETA in seconds: 4885370.641\n",
      "epoch: 899500, train loss: 4.047548389434814, val loss: 4.059462451934815, ETA in seconds: 4881045.339\n",
      "epoch: 899600, train loss: 4.047353076934814, val loss: 4.0725483894348145, ETA in seconds: 4876713.607\n",
      "epoch: 899700, train loss: 4.0530171394348145, val loss: 4.069032764434814, ETA in seconds: 4872378.858\n",
      "epoch: 899800, train loss: 4.0510640144348145, val loss: 4.0686421394348145, ETA in seconds: 4868041.944\n",
      "epoch: 899900, train loss: 4.051259326934814, val loss: 4.071962451934814, ETA in seconds: 4863703.904\n",
      "epoch: 900000, train loss: 4.046962451934815, val loss: 4.065126514434814, ETA in seconds: 4859370.401\n",
      "epoch: 900100, train loss: 4.050868701934815, val loss: 4.066493701934815, ETA in seconds: 4855033.096\n",
      "epoch: 900200, train loss: 4.045790576934815, val loss: 4.066493701934815, ETA in seconds: 4850690.027\n",
      "epoch: 900300, train loss: 4.050868701934815, val loss: 4.070399951934815, ETA in seconds: 4846358.570\n",
      "epoch: 900400, train loss: 4.058681201934815, val loss: 4.0764546394348145, ETA in seconds: 4842033.402\n",
      "epoch: 900500, train loss: 4.0510640144348145, val loss: 4.0676655769348145, ETA in seconds: 4837698.092\n",
      "epoch: 900600, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 4833353.989\n",
      "epoch: 900700, train loss: 4.055360889434814, val loss: 4.065907764434814, ETA in seconds: 4829005.734\n",
      "epoch: 900800, train loss: 4.053212451934814, val loss: 4.066884326934814, ETA in seconds: 4824665.661\n",
      "epoch: 900900, train loss: 4.050282764434814, val loss: 4.069228076934815, ETA in seconds: 4820316.400\n",
      "epoch: 901000, train loss: 4.056337451934814, val loss: 4.0676655769348145, ETA in seconds: 4815974.750\n",
      "epoch: 901100, train loss: 4.051259326934814, val loss: 4.066103076934814, ETA in seconds: 4811634.471\n",
      "epoch: 901200, train loss: 4.0549702644348145, val loss: 4.066298389434815, ETA in seconds: 4807288.133\n",
      "epoch: 901300, train loss: 4.052821826934815, val loss: 4.072353076934815, ETA in seconds: 4802939.597\n",
      "epoch: 901400, train loss: 4.052235889434814, val loss: 4.0705952644348145, ETA in seconds: 4798619.339\n",
      "epoch: 901500, train loss: 4.060439014434815, val loss: 4.072157764434815, ETA in seconds: 4794260.159\n",
      "epoch: 901600, train loss: 4.046571826934814, val loss: 4.066103076934814, ETA in seconds: 4789952.254\n",
      "epoch: 901700, train loss: 4.050868701934815, val loss: 4.0627827644348145, ETA in seconds: 4785622.222\n",
      "epoch: 901800, train loss: 4.054189014434814, val loss: 4.070790576934814, ETA in seconds: 4781265.418\n",
      "epoch: 901900, train loss: 4.050673389434815, val loss: 4.065907764434814, ETA in seconds: 4776898.199\n",
      "epoch: 902000, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 4772549.239\n",
      "epoch: 902100, train loss: 4.057509326934815, val loss: 4.064540576934815, ETA in seconds: 4768185.534\n",
      "epoch: 902200, train loss: 4.048720264434815, val loss: 4.071181201934815, ETA in seconds: 4763832.513\n",
      "epoch: 902300, train loss: 4.047743701934815, val loss: 4.0647358894348145, ETA in seconds: 4759483.548\n",
      "epoch: 902400, train loss: 4.050673389434815, val loss: 4.0705952644348145, ETA in seconds: 4755138.978\n",
      "epoch: 902500, train loss: 4.058290576934814, val loss: 4.067470264434815, ETA in seconds: 4750788.186\n",
      "epoch: 902600, train loss: 4.048329639434814, val loss: 4.060243701934814, ETA in seconds: 4746433.977\n",
      "epoch: 902700, train loss: 4.049696826934815, val loss: 4.068056201934814, ETA in seconds: 4742078.618\n",
      "epoch: 902800, train loss: 4.0520405769348145, val loss: 4.068056201934814, ETA in seconds: 4737719.398\n",
      "epoch: 902900, train loss: 4.0530171394348145, val loss: 4.071181201934815, ETA in seconds: 4733356.567\n",
      "epoch: 903000, train loss: 4.056728076934815, val loss: 4.066493701934815, ETA in seconds: 4728986.654\n",
      "epoch: 903100, train loss: 4.049696826934815, val loss: 4.067470264434815, ETA in seconds: 4724619.503\n",
      "epoch: 903200, train loss: 4.050673389434815, val loss: 4.067079639434814, ETA in seconds: 4720250.494\n",
      "epoch: 903300, train loss: 4.054774951934815, val loss: 4.0715718269348145, ETA in seconds: 4715877.710\n",
      "epoch: 903400, train loss: 4.0491108894348145, val loss: 4.065517139434815, ETA in seconds: 4711511.243\n",
      "epoch: 903500, train loss: 4.052235889434814, val loss: 4.067470264434815, ETA in seconds: 4707150.054\n",
      "epoch: 903600, train loss: 4.046376514434814, val loss: 4.073134326934815, ETA in seconds: 4702792.747\n",
      "epoch: 903700, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 4698435.056\n",
      "epoch: 903800, train loss: 4.045790576934815, val loss: 4.074696826934814, ETA in seconds: 4694065.134\n",
      "epoch: 903900, train loss: 4.050673389434815, val loss: 4.066103076934814, ETA in seconds: 4689696.260\n",
      "epoch: 904000, train loss: 4.0549702644348145, val loss: 4.060048389434814, ETA in seconds: 4685327.748\n",
      "epoch: 904100, train loss: 4.0481343269348145, val loss: 4.071181201934815, ETA in seconds: 4680991.268\n",
      "epoch: 904200, train loss: 4.055556201934815, val loss: 4.063173389434814, ETA in seconds: 4676654.936\n",
      "epoch: 904300, train loss: 4.050673389434815, val loss: 4.068837451934814, ETA in seconds: 4672300.647\n",
      "epoch: 904400, train loss: 4.052431201934814, val loss: 4.065517139434815, ETA in seconds: 4667933.140\n",
      "epoch: 904500, train loss: 4.044423389434814, val loss: 4.068446826934815, ETA in seconds: 4663551.041\n",
      "epoch: 904600, train loss: 4.048915576934815, val loss: 4.066884326934814, ETA in seconds: 4659172.749\n",
      "epoch: 904700, train loss: 4.048720264434815, val loss: 4.068446826934815, ETA in seconds: 4654798.789\n",
      "epoch: 904800, train loss: 4.051454639434814, val loss: 4.069032764434814, ETA in seconds: 4650412.966\n",
      "epoch: 904900, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 4646031.992\n",
      "epoch: 905000, train loss: 4.051845264434815, val loss: 4.064540576934815, ETA in seconds: 4641641.270\n",
      "epoch: 905100, train loss: 4.045399951934814, val loss: 4.065517139434815, ETA in seconds: 4637251.580\n",
      "epoch: 905200, train loss: 4.053212451934814, val loss: 4.071962451934814, ETA in seconds: 4632872.989\n",
      "epoch: 905300, train loss: 4.055556201934815, val loss: 4.0696187019348145, ETA in seconds: 4628496.542\n",
      "epoch: 905400, train loss: 4.054384326934814, val loss: 4.064149951934814, ETA in seconds: 4624115.853\n",
      "epoch: 905500, train loss: 4.0569233894348145, val loss: 4.066103076934814, ETA in seconds: 4619717.378\n",
      "epoch: 905600, train loss: 4.0530171394348145, val loss: 4.072939014434814, ETA in seconds: 4615324.158\n",
      "epoch: 905700, train loss: 4.043446826934814, val loss: 4.0676655769348145, ETA in seconds: 4610937.654\n",
      "epoch: 905800, train loss: 4.055751514434815, val loss: 4.065126514434814, ETA in seconds: 4606539.869\n",
      "epoch: 905900, train loss: 4.053407764434814, val loss: 4.069423389434815, ETA in seconds: 4602142.990\n",
      "epoch: 906000, train loss: 4.057118701934814, val loss: 4.070985889434814, ETA in seconds: 4597749.756\n",
      "epoch: 906100, train loss: 4.0549702644348145, val loss: 4.063564014434815, ETA in seconds: 4593359.330\n",
      "epoch: 906200, train loss: 4.051845264434815, val loss: 4.067274951934815, ETA in seconds: 4588955.278\n",
      "epoch: 906300, train loss: 4.045985889434815, val loss: 4.074696826934814, ETA in seconds: 4584562.347\n",
      "epoch: 906400, train loss: 4.055165576934814, val loss: 4.068837451934814, ETA in seconds: 4580167.247\n",
      "epoch: 906500, train loss: 4.052235889434814, val loss: 4.068056201934814, ETA in seconds: 4575772.432\n",
      "epoch: 906600, train loss: 4.053798389434815, val loss: 4.067470264434815, ETA in seconds: 4571370.013\n",
      "epoch: 906700, train loss: 4.047939014434815, val loss: 4.065517139434815, ETA in seconds: 4566972.277\n",
      "epoch: 906800, train loss: 4.053212451934814, val loss: 4.066103076934814, ETA in seconds: 4562560.861\n",
      "epoch: 906900, train loss: 4.047548389434814, val loss: 4.065907764434814, ETA in seconds: 4558147.341\n",
      "epoch: 907000, train loss: 4.057509326934815, val loss: 4.0657124519348145, ETA in seconds: 4553758.594\n",
      "epoch: 907100, train loss: 4.049696826934815, val loss: 4.0657124519348145, ETA in seconds: 4549349.136\n",
      "epoch: 907200, train loss: 4.050673389434815, val loss: 4.069032764434814, ETA in seconds: 4544941.328\n",
      "epoch: 907300, train loss: 4.0539937019348145, val loss: 4.069814014434814, ETA in seconds: 4540529.649\n",
      "epoch: 907400, train loss: 4.0461812019348145, val loss: 4.065517139434815, ETA in seconds: 4536126.830\n",
      "epoch: 907500, train loss: 4.048524951934814, val loss: 4.0696187019348145, ETA in seconds: 4531715.062\n",
      "epoch: 907600, train loss: 4.047939014434815, val loss: 4.066493701934815, ETA in seconds: 4527297.969\n",
      "epoch: 907700, train loss: 4.059071826934814, val loss: 4.068056201934814, ETA in seconds: 4522879.486\n",
      "epoch: 907800, train loss: 4.0481343269348145, val loss: 4.065907764434814, ETA in seconds: 4518465.791\n",
      "epoch: 907900, train loss: 4.060243701934814, val loss: 4.0696187019348145, ETA in seconds: 4514035.577\n",
      "epoch: 908000, train loss: 4.0481343269348145, val loss: 4.066884326934814, ETA in seconds: 4509625.677\n",
      "epoch: 908100, train loss: 4.0569233894348145, val loss: 4.071376514434815, ETA in seconds: 4505213.905\n",
      "epoch: 908200, train loss: 4.049501514434814, val loss: 4.0657124519348145, ETA in seconds: 4500799.864\n",
      "epoch: 908300, train loss: 4.048524951934814, val loss: 4.067860889434814, ETA in seconds: 4496377.689\n",
      "epoch: 908400, train loss: 4.049892139434815, val loss: 4.066298389434815, ETA in seconds: 4491956.177\n",
      "epoch: 908500, train loss: 4.054579639434815, val loss: 4.062392139434815, ETA in seconds: 4487541.493\n",
      "epoch: 908600, train loss: 4.054189014434814, val loss: 4.065907764434814, ETA in seconds: 4483116.759\n",
      "epoch: 908700, train loss: 4.054189014434814, val loss: 4.070399951934815, ETA in seconds: 4478705.307\n",
      "epoch: 908800, train loss: 4.053212451934814, val loss: 4.062587451934815, ETA in seconds: 4474283.208\n",
      "epoch: 908900, train loss: 4.053212451934814, val loss: 4.0705952644348145, ETA in seconds: 4469862.075\n",
      "epoch: 909000, train loss: 4.048915576934815, val loss: 4.065126514434814, ETA in seconds: 4465444.369\n",
      "epoch: 909100, train loss: 4.049696826934815, val loss: 4.070009326934814, ETA in seconds: 4461020.749\n",
      "epoch: 909200, train loss: 4.050673389434815, val loss: 4.072939014434814, ETA in seconds: 4456600.323\n",
      "epoch: 909300, train loss: 4.056728076934815, val loss: 4.066884326934814, ETA in seconds: 4452183.555\n",
      "epoch: 909400, train loss: 4.057314014434814, val loss: 4.063173389434814, ETA in seconds: 4447756.416\n",
      "epoch: 909500, train loss: 4.053212451934814, val loss: 4.065907764434814, ETA in seconds: 4443326.247\n",
      "epoch: 909600, train loss: 4.052431201934814, val loss: 4.066298389434815, ETA in seconds: 4438916.113\n",
      "epoch: 909700, train loss: 4.047939014434815, val loss: 4.070204639434815, ETA in seconds: 4434515.112\n",
      "epoch: 909800, train loss: 4.052235889434814, val loss: 4.066298389434815, ETA in seconds: 4430082.009\n",
      "epoch: 909900, train loss: 4.048524951934814, val loss: 4.0637593269348145, ETA in seconds: 4425642.647\n",
      "epoch: 910000, train loss: 4.048329639434814, val loss: 4.065907764434814, ETA in seconds: 4421207.064\n",
      "epoch: 910100, train loss: 4.054189014434814, val loss: 4.064931201934814, ETA in seconds: 4416785.337\n",
      "epoch: 910200, train loss: 4.052821826934815, val loss: 4.063564014434815, ETA in seconds: 4412340.438\n",
      "epoch: 910300, train loss: 4.0530171394348145, val loss: 4.066493701934815, ETA in seconds: 4407902.067\n",
      "epoch: 910400, train loss: 4.052626514434815, val loss: 4.065907764434814, ETA in seconds: 4403457.284\n",
      "epoch: 910500, train loss: 4.0520405769348145, val loss: 4.064149951934814, ETA in seconds: 4399011.754\n",
      "epoch: 910600, train loss: 4.053798389434815, val loss: 4.070009326934814, ETA in seconds: 4394565.292\n",
      "epoch: 910700, train loss: 4.057118701934814, val loss: 4.070204639434815, ETA in seconds: 4390103.477\n",
      "epoch: 910800, train loss: 4.053407764434814, val loss: 4.0705952644348145, ETA in seconds: 4385650.693\n",
      "epoch: 910900, train loss: 4.047548389434814, val loss: 4.064540576934815, ETA in seconds: 4381206.097\n",
      "epoch: 911000, train loss: 4.049501514434814, val loss: 4.062978076934814, ETA in seconds: 4376752.372\n",
      "epoch: 911100, train loss: 4.050868701934815, val loss: 4.066103076934814, ETA in seconds: 4372303.698\n",
      "epoch: 911200, train loss: 4.048720264434815, val loss: 4.069032764434814, ETA in seconds: 4367863.481\n",
      "epoch: 911300, train loss: 4.046962451934815, val loss: 4.063173389434814, ETA in seconds: 4363406.290\n",
      "epoch: 911400, train loss: 4.0539937019348145, val loss: 4.0676655769348145, ETA in seconds: 4358992.990\n",
      "epoch: 911500, train loss: 4.054579639434815, val loss: 4.0705952644348145, ETA in seconds: 4354554.878\n",
      "epoch: 911600, train loss: 4.057704639434815, val loss: 4.070399951934815, ETA in seconds: 4350107.090\n",
      "epoch: 911700, train loss: 4.052821826934815, val loss: 4.065321826934815, ETA in seconds: 4345658.820\n",
      "epoch: 911800, train loss: 4.055360889434814, val loss: 4.0676655769348145, ETA in seconds: 4341208.646\n",
      "epoch: 911900, train loss: 4.054774951934815, val loss: 4.0696187019348145, ETA in seconds: 4336755.934\n",
      "epoch: 912000, train loss: 4.046962451934815, val loss: 4.064345264434815, ETA in seconds: 4332309.821\n",
      "epoch: 912100, train loss: 4.0539937019348145, val loss: 4.066493701934815, ETA in seconds: 4327860.129\n",
      "epoch: 912200, train loss: 4.0520405769348145, val loss: 4.069228076934815, ETA in seconds: 4323414.092\n",
      "epoch: 912300, train loss: 4.0559468269348145, val loss: 4.066298389434815, ETA in seconds: 4318955.310\n",
      "epoch: 912400, train loss: 4.051845264434815, val loss: 4.065321826934815, ETA in seconds: 4314492.686\n",
      "epoch: 912500, train loss: 4.0491108894348145, val loss: 4.0686421394348145, ETA in seconds: 4310031.062\n",
      "epoch: 912600, train loss: 4.051259326934814, val loss: 4.070790576934814, ETA in seconds: 4305562.614\n",
      "epoch: 912700, train loss: 4.049306201934814, val loss: 4.066298389434815, ETA in seconds: 4301096.599\n",
      "epoch: 912800, train loss: 4.050868701934815, val loss: 4.064540576934815, ETA in seconds: 4296627.871\n",
      "epoch: 912900, train loss: 4.0530171394348145, val loss: 4.070009326934814, ETA in seconds: 4292168.036\n",
      "epoch: 913000, train loss: 4.048720264434815, val loss: 4.068837451934814, ETA in seconds: 4287720.449\n",
      "epoch: 913100, train loss: 4.055751514434815, val loss: 4.062587451934815, ETA in seconds: 4283252.553\n",
      "epoch: 913200, train loss: 4.052626514434815, val loss: 4.066298389434815, ETA in seconds: 4278778.913\n",
      "epoch: 913300, train loss: 4.046571826934814, val loss: 4.068837451934814, ETA in seconds: 4274308.059\n",
      "epoch: 913400, train loss: 4.051259326934814, val loss: 4.066103076934814, ETA in seconds: 4269837.733\n",
      "epoch: 913500, train loss: 4.047353076934814, val loss: 4.070790576934814, ETA in seconds: 4265359.522\n",
      "epoch: 913600, train loss: 4.047353076934814, val loss: 4.070204639434815, ETA in seconds: 4260886.448\n",
      "epoch: 913700, train loss: 4.0627827644348145, val loss: 4.0676655769348145, ETA in seconds: 4256400.106\n",
      "epoch: 913800, train loss: 4.053212451934814, val loss: 4.071962451934814, ETA in seconds: 4251924.792\n",
      "epoch: 913900, train loss: 4.052431201934814, val loss: 4.063954639434814, ETA in seconds: 4247445.185\n",
      "epoch: 914000, train loss: 4.0559468269348145, val loss: 4.069423389434815, ETA in seconds: 4242961.568\n",
      "epoch: 914100, train loss: 4.050868701934815, val loss: 4.061415576934815, ETA in seconds: 4238475.445\n",
      "epoch: 914200, train loss: 4.0569233894348145, val loss: 4.069423389434815, ETA in seconds: 4233993.734\n",
      "epoch: 914300, train loss: 4.0481343269348145, val loss: 4.066493701934815, ETA in seconds: 4229514.992\n",
      "epoch: 914400, train loss: 4.054579639434815, val loss: 4.069423389434815, ETA in seconds: 4225034.232\n",
      "epoch: 914500, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 4220545.925\n",
      "epoch: 914600, train loss: 4.0530171394348145, val loss: 4.0657124519348145, ETA in seconds: 4216071.056\n",
      "epoch: 914700, train loss: 4.055556201934815, val loss: 4.068446826934815, ETA in seconds: 4211597.060\n",
      "epoch: 914800, train loss: 4.0520405769348145, val loss: 4.070399951934815, ETA in seconds: 4207113.017\n",
      "epoch: 914900, train loss: 4.0559468269348145, val loss: 4.065126514434814, ETA in seconds: 4202625.993\n",
      "epoch: 915000, train loss: 4.053603076934815, val loss: 4.0637593269348145, ETA in seconds: 4198135.984\n",
      "epoch: 915100, train loss: 4.046376514434814, val loss: 4.068251514434815, ETA in seconds: 4193641.108\n",
      "epoch: 915200, train loss: 4.050478076934814, val loss: 4.070790576934814, ETA in seconds: 4189155.278\n",
      "epoch: 915300, train loss: 4.054189014434814, val loss: 4.0647358894348145, ETA in seconds: 4184650.146\n",
      "epoch: 915400, train loss: 4.053212451934814, val loss: 4.070399951934815, ETA in seconds: 4180157.599\n",
      "epoch: 915500, train loss: 4.053798389434815, val loss: 4.067274951934815, ETA in seconds: 4175658.995\n",
      "epoch: 915600, train loss: 4.048915576934815, val loss: 4.068056201934814, ETA in seconds: 4171163.110\n",
      "epoch: 915700, train loss: 4.052235889434814, val loss: 4.067274951934815, ETA in seconds: 4166665.854\n",
      "epoch: 915800, train loss: 4.0539937019348145, val loss: 4.070985889434814, ETA in seconds: 4162167.160\n",
      "epoch: 915900, train loss: 4.049892139434815, val loss: 4.061415576934815, ETA in seconds: 4157658.799\n",
      "epoch: 916000, train loss: 4.044618701934814, val loss: 4.072157764434815, ETA in seconds: 4153154.274\n",
      "epoch: 916100, train loss: 4.048524951934814, val loss: 4.061220264434814, ETA in seconds: 4148648.911\n",
      "epoch: 916200, train loss: 4.057118701934814, val loss: 4.067274951934815, ETA in seconds: 4144152.602\n",
      "epoch: 916300, train loss: 4.054774951934815, val loss: 4.070399951934815, ETA in seconds: 4139647.342\n",
      "epoch: 916400, train loss: 4.048915576934815, val loss: 4.061220264434814, ETA in seconds: 4135149.873\n",
      "epoch: 916500, train loss: 4.0510640144348145, val loss: 4.066884326934814, ETA in seconds: 4130653.490\n",
      "epoch: 916600, train loss: 4.050868701934815, val loss: 4.066884326934814, ETA in seconds: 4126157.968\n",
      "epoch: 916700, train loss: 4.053798389434815, val loss: 4.069228076934815, ETA in seconds: 4121652.758\n",
      "epoch: 916800, train loss: 4.049696826934815, val loss: 4.064345264434815, ETA in seconds: 4117147.165\n",
      "epoch: 916900, train loss: 4.049696826934815, val loss: 4.064540576934815, ETA in seconds: 4112627.972\n",
      "epoch: 917000, train loss: 4.053407764434814, val loss: 4.062978076934814, ETA in seconds: 4108116.651\n",
      "epoch: 917100, train loss: 4.049306201934814, val loss: 4.069228076934815, ETA in seconds: 4103607.422\n",
      "epoch: 917200, train loss: 4.053407764434814, val loss: 4.070204639434815, ETA in seconds: 4099097.471\n",
      "epoch: 917300, train loss: 4.0578999519348145, val loss: 4.064345264434815, ETA in seconds: 4094594.673\n",
      "epoch: 917400, train loss: 4.056142139434814, val loss: 4.069423389434815, ETA in seconds: 4090092.114\n",
      "epoch: 917500, train loss: 4.050282764434814, val loss: 4.071962451934814, ETA in seconds: 4085581.180\n",
      "epoch: 917600, train loss: 4.053212451934814, val loss: 4.064149951934814, ETA in seconds: 4081068.472\n",
      "epoch: 917700, train loss: 4.058681201934815, val loss: 4.067470264434815, ETA in seconds: 4076553.497\n",
      "epoch: 917800, train loss: 4.043446826934814, val loss: 4.0666890144348145, ETA in seconds: 4072041.908\n",
      "epoch: 917900, train loss: 4.049696826934815, val loss: 4.069423389434815, ETA in seconds: 4067524.387\n",
      "epoch: 918000, train loss: 4.051259326934814, val loss: 4.061415576934815, ETA in seconds: 4063001.029\n",
      "epoch: 918100, train loss: 4.056532764434815, val loss: 4.069032764434814, ETA in seconds: 4058480.372\n",
      "epoch: 918200, train loss: 4.050673389434815, val loss: 4.065321826934815, ETA in seconds: 4053961.934\n",
      "epoch: 918300, train loss: 4.048720264434815, val loss: 4.067470264434815, ETA in seconds: 4049444.736\n",
      "epoch: 918400, train loss: 4.055556201934815, val loss: 4.073720264434814, ETA in seconds: 4044926.086\n",
      "epoch: 918500, train loss: 4.045985889434815, val loss: 4.070399951934815, ETA in seconds: 4040402.077\n",
      "epoch: 918600, train loss: 4.059657764434815, val loss: 4.064149951934814, ETA in seconds: 4035876.505\n",
      "epoch: 918700, train loss: 4.055556201934815, val loss: 4.072157764434815, ETA in seconds: 4031360.372\n",
      "epoch: 918800, train loss: 4.051649951934815, val loss: 4.061220264434814, ETA in seconds: 4026834.293\n",
      "epoch: 918900, train loss: 4.055751514434815, val loss: 4.0676655769348145, ETA in seconds: 4022304.243\n",
      "epoch: 919000, train loss: 4.054189014434814, val loss: 4.065126514434814, ETA in seconds: 4017776.356\n",
      "epoch: 919100, train loss: 4.057118701934814, val loss: 4.067274951934815, ETA in seconds: 4013243.980\n",
      "epoch: 919200, train loss: 4.045985889434815, val loss: 4.065126514434814, ETA in seconds: 4008710.890\n",
      "epoch: 919300, train loss: 4.0500874519348145, val loss: 4.0666890144348145, ETA in seconds: 4004174.265\n",
      "epoch: 919400, train loss: 4.053212451934814, val loss: 4.067470264434815, ETA in seconds: 3999633.302\n",
      "epoch: 919500, train loss: 4.054189014434814, val loss: 4.069423389434815, ETA in seconds: 3995097.059\n",
      "epoch: 919600, train loss: 4.053212451934814, val loss: 4.067860889434814, ETA in seconds: 3990564.382\n",
      "epoch: 919700, train loss: 4.051845264434815, val loss: 4.063173389434814, ETA in seconds: 3986017.512\n",
      "epoch: 919800, train loss: 4.054189014434814, val loss: 4.067274951934815, ETA in seconds: 3981480.429\n",
      "epoch: 919900, train loss: 4.057118701934814, val loss: 4.069814014434814, ETA in seconds: 3976943.514\n",
      "epoch: 920000, train loss: 4.0461812019348145, val loss: 4.0676655769348145, ETA in seconds: 3972401.873\n",
      "epoch: 920100, train loss: 4.045790576934815, val loss: 4.065907764434814, ETA in seconds: 3967871.437\n",
      "epoch: 920200, train loss: 4.049306201934814, val loss: 4.068837451934814, ETA in seconds: 3963324.582\n",
      "epoch: 920300, train loss: 4.048524951934814, val loss: 4.067470264434815, ETA in seconds: 3958776.746\n",
      "epoch: 920400, train loss: 4.055556201934815, val loss: 4.067079639434814, ETA in seconds: 3954229.444\n",
      "epoch: 920500, train loss: 4.0471577644348145, val loss: 4.070399951934815, ETA in seconds: 3949677.723\n",
      "epoch: 920600, train loss: 4.050673389434815, val loss: 4.062978076934814, ETA in seconds: 3945137.208\n",
      "epoch: 920700, train loss: 4.054384326934814, val loss: 4.064149951934814, ETA in seconds: 3940585.145\n",
      "epoch: 920800, train loss: 4.0510640144348145, val loss: 4.067860889434814, ETA in seconds: 3936032.110\n",
      "epoch: 920900, train loss: 4.0471577644348145, val loss: 4.067860889434814, ETA in seconds: 3931452.573\n",
      "epoch: 921000, train loss: 4.057314014434814, val loss: 4.0676655769348145, ETA in seconds: 3926908.702\n",
      "epoch: 921100, train loss: 4.052235889434814, val loss: 4.070399951934815, ETA in seconds: 3922351.601\n",
      "epoch: 921200, train loss: 4.0471577644348145, val loss: 4.070790576934814, ETA in seconds: 3917810.899\n",
      "epoch: 921300, train loss: 4.055165576934814, val loss: 4.063564014434815, ETA in seconds: 3913248.168\n",
      "epoch: 921400, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 3908687.786\n",
      "epoch: 921500, train loss: 4.0539937019348145, val loss: 4.063368701934815, ETA in seconds: 3904126.105\n",
      "epoch: 921600, train loss: 4.058095264434814, val loss: 4.0686421394348145, ETA in seconds: 3899571.986\n",
      "epoch: 921700, train loss: 4.052235889434814, val loss: 4.068446826934815, ETA in seconds: 3895014.170\n",
      "epoch: 921800, train loss: 4.047743701934815, val loss: 4.070204639434815, ETA in seconds: 3890454.197\n",
      "epoch: 921900, train loss: 4.056532764434815, val loss: 4.068837451934814, ETA in seconds: 3885901.919\n",
      "epoch: 922000, train loss: 4.048915576934815, val loss: 4.069423389434815, ETA in seconds: 3881356.863\n",
      "epoch: 922100, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 3876786.716\n",
      "epoch: 922200, train loss: 4.052626514434815, val loss: 4.074892139434814, ETA in seconds: 3872221.599\n",
      "epoch: 922300, train loss: 4.048720264434815, val loss: 4.066884326934814, ETA in seconds: 3867648.732\n",
      "epoch: 922400, train loss: 4.054189014434814, val loss: 4.066884326934814, ETA in seconds: 3863086.493\n",
      "epoch: 922500, train loss: 4.0452046394348145, val loss: 4.065517139434815, ETA in seconds: 3858516.584\n",
      "epoch: 922600, train loss: 4.052431201934814, val loss: 4.064345264434815, ETA in seconds: 3853942.388\n",
      "epoch: 922700, train loss: 4.046962451934815, val loss: 4.0696187019348145, ETA in seconds: 3849368.072\n",
      "epoch: 922800, train loss: 4.055165576934814, val loss: 4.064149951934814, ETA in seconds: 3844813.021\n",
      "epoch: 922900, train loss: 4.0539937019348145, val loss: 4.074892139434814, ETA in seconds: 3840240.642\n",
      "epoch: 923000, train loss: 4.046571826934814, val loss: 4.070204639434815, ETA in seconds: 3835674.986\n",
      "epoch: 923100, train loss: 4.050478076934814, val loss: 4.067860889434814, ETA in seconds: 3831115.149\n",
      "epoch: 923200, train loss: 4.0461812019348145, val loss: 4.067274951934815, ETA in seconds: 3826560.559\n",
      "epoch: 923300, train loss: 4.047939014434815, val loss: 4.065907764434814, ETA in seconds: 3821983.303\n",
      "epoch: 923400, train loss: 4.045595264434814, val loss: 4.067470264434815, ETA in seconds: 3817393.937\n",
      "epoch: 923500, train loss: 4.052626514434815, val loss: 4.0696187019348145, ETA in seconds: 3812812.391\n",
      "epoch: 923600, train loss: 4.056337451934814, val loss: 4.0686421394348145, ETA in seconds: 3808233.156\n",
      "epoch: 923700, train loss: 4.051259326934814, val loss: 4.067470264434815, ETA in seconds: 3803660.317\n",
      "epoch: 923800, train loss: 4.052431201934814, val loss: 4.067470264434815, ETA in seconds: 3799080.844\n",
      "epoch: 923900, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 3794507.993\n",
      "epoch: 924000, train loss: 4.048720264434815, val loss: 4.072353076934815, ETA in seconds: 3789927.419\n",
      "epoch: 924100, train loss: 4.050868701934815, val loss: 4.067470264434815, ETA in seconds: 3785346.614\n",
      "epoch: 924200, train loss: 4.046767139434815, val loss: 4.066103076934814, ETA in seconds: 3780759.845\n",
      "epoch: 924300, train loss: 4.044618701934814, val loss: 4.064345264434815, ETA in seconds: 3776172.541\n",
      "epoch: 924400, train loss: 4.056728076934815, val loss: 4.068837451934814, ETA in seconds: 3771590.612\n",
      "epoch: 924500, train loss: 4.056142139434814, val loss: 4.064540576934815, ETA in seconds: 3767008.968\n",
      "epoch: 924600, train loss: 4.053798389434815, val loss: 4.0676655769348145, ETA in seconds: 3762417.704\n",
      "epoch: 924700, train loss: 4.0452046394348145, val loss: 4.067470264434815, ETA in seconds: 3757826.081\n",
      "epoch: 924800, train loss: 4.055360889434814, val loss: 4.068056201934814, ETA in seconds: 3753230.244\n",
      "epoch: 924900, train loss: 4.059462451934815, val loss: 4.067274951934815, ETA in seconds: 3748632.666\n",
      "epoch: 925000, train loss: 4.049306201934814, val loss: 4.068837451934814, ETA in seconds: 3744035.546\n",
      "epoch: 925100, train loss: 4.054774951934815, val loss: 4.068251514434815, ETA in seconds: 3739448.799\n",
      "epoch: 925200, train loss: 4.058290576934814, val loss: 4.072157764434815, ETA in seconds: 3734852.923\n",
      "epoch: 925300, train loss: 4.046571826934814, val loss: 4.0618062019348145, ETA in seconds: 3730257.948\n",
      "epoch: 925400, train loss: 4.0559468269348145, val loss: 4.064345264434815, ETA in seconds: 3725658.223\n",
      "epoch: 925500, train loss: 4.045790576934815, val loss: 4.069032764434814, ETA in seconds: 3721063.172\n",
      "epoch: 925600, train loss: 4.057704639434815, val loss: 4.067274951934815, ETA in seconds: 3716470.865\n",
      "epoch: 925700, train loss: 4.050282764434814, val loss: 4.0657124519348145, ETA in seconds: 3711875.905\n",
      "epoch: 925800, train loss: 4.055360889434814, val loss: 4.070399951934815, ETA in seconds: 3707291.073\n",
      "epoch: 925900, train loss: 4.045399951934814, val loss: 4.073915576934814, ETA in seconds: 3702690.035\n",
      "epoch: 926000, train loss: 4.050673389434815, val loss: 4.066493701934815, ETA in seconds: 3698086.536\n",
      "epoch: 926100, train loss: 4.0559468269348145, val loss: 4.068837451934814, ETA in seconds: 3693487.196\n",
      "epoch: 926200, train loss: 4.054774951934815, val loss: 4.0637593269348145, ETA in seconds: 3688873.710\n",
      "epoch: 926300, train loss: 4.050478076934814, val loss: 4.060439014434815, ETA in seconds: 3684264.513\n",
      "epoch: 926400, train loss: 4.050282764434814, val loss: 4.066493701934815, ETA in seconds: 3679648.781\n",
      "epoch: 926500, train loss: 4.047353076934814, val loss: 4.068251514434815, ETA in seconds: 3675042.321\n",
      "epoch: 926600, train loss: 4.054189014434814, val loss: 4.0657124519348145, ETA in seconds: 3670431.098\n",
      "epoch: 926700, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 3665810.424\n",
      "epoch: 926800, train loss: 4.048915576934815, val loss: 4.0754780769348145, ETA in seconds: 3661200.485\n",
      "epoch: 926900, train loss: 4.054579639434815, val loss: 4.068056201934814, ETA in seconds: 3656585.830\n",
      "epoch: 927000, train loss: 4.048915576934815, val loss: 4.062196826934814, ETA in seconds: 3651970.811\n",
      "epoch: 927100, train loss: 4.0520405769348145, val loss: 4.067079639434814, ETA in seconds: 3647358.757\n",
      "epoch: 927200, train loss: 4.045595264434814, val loss: 4.0715718269348145, ETA in seconds: 3642742.990\n",
      "epoch: 927300, train loss: 4.045790576934815, val loss: 4.069228076934815, ETA in seconds: 3638115.497\n",
      "epoch: 927400, train loss: 4.0530171394348145, val loss: 4.069228076934815, ETA in seconds: 3633503.631\n",
      "epoch: 927500, train loss: 4.052626514434815, val loss: 4.068056201934814, ETA in seconds: 3628896.060\n",
      "epoch: 927600, train loss: 4.048720264434815, val loss: 4.066493701934815, ETA in seconds: 3624273.632\n",
      "epoch: 927700, train loss: 4.0510640144348145, val loss: 4.066493701934815, ETA in seconds: 3619651.024\n",
      "epoch: 927800, train loss: 4.059462451934815, val loss: 4.067470264434815, ETA in seconds: 3615024.743\n",
      "epoch: 927900, train loss: 4.052626514434815, val loss: 4.064540576934815, ETA in seconds: 3610407.309\n",
      "epoch: 928000, train loss: 4.048329639434814, val loss: 4.070009326934814, ETA in seconds: 3605780.960\n",
      "epoch: 928100, train loss: 4.054774951934815, val loss: 4.070790576934814, ETA in seconds: 3601154.915\n",
      "epoch: 928200, train loss: 4.0549702644348145, val loss: 4.067079639434814, ETA in seconds: 3596521.927\n",
      "epoch: 928300, train loss: 4.050868701934815, val loss: 4.065321826934815, ETA in seconds: 3591884.483\n",
      "epoch: 928400, train loss: 4.056728076934815, val loss: 4.062196826934814, ETA in seconds: 3587257.483\n",
      "epoch: 928500, train loss: 4.052431201934814, val loss: 4.071376514434815, ETA in seconds: 3582623.953\n",
      "epoch: 928600, train loss: 4.047939014434815, val loss: 4.068837451934814, ETA in seconds: 3577992.029\n",
      "epoch: 928700, train loss: 4.052431201934814, val loss: 4.0725483894348145, ETA in seconds: 3573348.293\n",
      "epoch: 928800, train loss: 4.052626514434815, val loss: 4.066298389434815, ETA in seconds: 3568719.834\n",
      "epoch: 928900, train loss: 4.050282764434814, val loss: 4.062001514434814, ETA in seconds: 3564069.945\n",
      "epoch: 929000, train loss: 4.051454639434814, val loss: 4.0676655769348145, ETA in seconds: 3559425.907\n",
      "epoch: 929100, train loss: 4.044423389434814, val loss: 4.068446826934815, ETA in seconds: 3554792.356\n",
      "epoch: 929200, train loss: 4.0539937019348145, val loss: 4.071376514434815, ETA in seconds: 3550153.441\n",
      "epoch: 929300, train loss: 4.052431201934814, val loss: 4.068251514434815, ETA in seconds: 3545524.059\n",
      "epoch: 929400, train loss: 4.053407764434814, val loss: 4.0657124519348145, ETA in seconds: 3540903.417\n",
      "epoch: 929500, train loss: 4.055360889434814, val loss: 4.070009326934814, ETA in seconds: 3536278.960\n",
      "epoch: 929600, train loss: 4.0500874519348145, val loss: 4.067274951934815, ETA in seconds: 3531640.900\n",
      "epoch: 929700, train loss: 4.049306201934814, val loss: 4.063368701934815, ETA in seconds: 3527004.838\n",
      "epoch: 929800, train loss: 4.053603076934815, val loss: 4.0657124519348145, ETA in seconds: 3522354.323\n",
      "epoch: 929900, train loss: 4.051649951934815, val loss: 4.070399951934815, ETA in seconds: 3517707.898\n",
      "epoch: 930000, train loss: 4.055360889434814, val loss: 4.069814014434814, ETA in seconds: 3513058.518\n",
      "epoch: 930100, train loss: 4.0442280769348145, val loss: 4.065126514434814, ETA in seconds: 3508420.423\n",
      "epoch: 930200, train loss: 4.046571826934814, val loss: 4.066103076934814, ETA in seconds: 3503782.947\n",
      "epoch: 930300, train loss: 4.051845264434815, val loss: 4.070985889434814, ETA in seconds: 3499147.838\n",
      "epoch: 930400, train loss: 4.052235889434814, val loss: 4.069032764434814, ETA in seconds: 3494518.000\n",
      "epoch: 930500, train loss: 4.052821826934815, val loss: 4.0618062019348145, ETA in seconds: 3489868.721\n",
      "epoch: 930600, train loss: 4.0559468269348145, val loss: 4.071767139434814, ETA in seconds: 3485239.970\n",
      "epoch: 930700, train loss: 4.047939014434815, val loss: 4.0725483894348145, ETA in seconds: 3480598.785\n",
      "epoch: 930800, train loss: 4.053798389434815, val loss: 4.070009326934814, ETA in seconds: 3475957.760\n",
      "epoch: 930900, train loss: 4.0520405769348145, val loss: 4.070985889434814, ETA in seconds: 3471308.574\n",
      "epoch: 931000, train loss: 4.045790576934815, val loss: 4.069228076934815, ETA in seconds: 3466665.276\n",
      "epoch: 931100, train loss: 4.0520405769348145, val loss: 4.0686421394348145, ETA in seconds: 3462015.085\n",
      "epoch: 931200, train loss: 4.051649951934815, val loss: 4.068837451934814, ETA in seconds: 3457358.303\n",
      "epoch: 931300, train loss: 4.047353076934814, val loss: 4.0715718269348145, ETA in seconds: 3452700.989\n",
      "epoch: 931400, train loss: 4.057314014434814, val loss: 4.067470264434815, ETA in seconds: 3448038.027\n",
      "epoch: 931500, train loss: 4.053798389434815, val loss: 4.077626514434814, ETA in seconds: 3443383.214\n",
      "epoch: 931600, train loss: 4.0471577644348145, val loss: 4.066103076934814, ETA in seconds: 3438714.559\n",
      "epoch: 931700, train loss: 4.0578999519348145, val loss: 4.067470264434815, ETA in seconds: 3434052.508\n",
      "epoch: 931800, train loss: 4.062196826934814, val loss: 4.064931201934814, ETA in seconds: 3429386.130\n",
      "epoch: 931900, train loss: 4.060243701934814, val loss: 4.069032764434814, ETA in seconds: 3424718.762\n",
      "epoch: 932000, train loss: 4.048720264434815, val loss: 4.071181201934815, ETA in seconds: 3420049.509\n",
      "epoch: 932100, train loss: 4.048915576934815, val loss: 4.069814014434814, ETA in seconds: 3415378.040\n",
      "epoch: 932200, train loss: 4.049696826934815, val loss: 4.063368701934815, ETA in seconds: 3410703.841\n",
      "epoch: 932300, train loss: 4.051649951934815, val loss: 4.065126514434814, ETA in seconds: 3406025.424\n",
      "epoch: 932400, train loss: 4.0539937019348145, val loss: 4.070985889434814, ETA in seconds: 3401351.204\n",
      "epoch: 932500, train loss: 4.050673389434815, val loss: 4.065517139434815, ETA in seconds: 3396673.499\n",
      "epoch: 932600, train loss: 4.054384326934814, val loss: 4.063368701934815, ETA in seconds: 3391990.413\n",
      "epoch: 932700, train loss: 4.055360889434814, val loss: 4.065321826934815, ETA in seconds: 3387317.638\n",
      "epoch: 932800, train loss: 4.053212451934814, val loss: 4.075087451934815, ETA in seconds: 3382639.989\n",
      "epoch: 932900, train loss: 4.049892139434815, val loss: 4.070009326934814, ETA in seconds: 3377962.085\n",
      "epoch: 933000, train loss: 4.047353076934814, val loss: 4.065321826934815, ETA in seconds: 3373281.896\n",
      "epoch: 933100, train loss: 4.050282764434814, val loss: 4.069814014434814, ETA in seconds: 3368602.919\n",
      "epoch: 933200, train loss: 4.046376514434814, val loss: 4.068446826934815, ETA in seconds: 3363922.513\n",
      "epoch: 933300, train loss: 4.051845264434815, val loss: 4.067470264434815, ETA in seconds: 3359233.908\n",
      "epoch: 933400, train loss: 4.0461812019348145, val loss: 4.068837451934814, ETA in seconds: 3354554.501\n",
      "epoch: 933500, train loss: 4.0569233894348145, val loss: 4.064540576934815, ETA in seconds: 3349860.530\n",
      "epoch: 933600, train loss: 4.055165576934814, val loss: 4.068446826934815, ETA in seconds: 3345180.696\n",
      "epoch: 933700, train loss: 4.050478076934814, val loss: 4.067470264434815, ETA in seconds: 3340491.706\n",
      "epoch: 933800, train loss: 4.057118701934814, val loss: 4.063173389434814, ETA in seconds: 3335804.592\n",
      "epoch: 933900, train loss: 4.0569233894348145, val loss: 4.064540576934815, ETA in seconds: 3331125.035\n",
      "epoch: 934000, train loss: 4.056142139434814, val loss: 4.069032764434814, ETA in seconds: 3326442.455\n",
      "epoch: 934100, train loss: 4.047548389434814, val loss: 4.074696826934814, ETA in seconds: 3321748.119\n",
      "epoch: 934200, train loss: 4.052821826934815, val loss: 4.070009326934814, ETA in seconds: 3317061.217\n",
      "epoch: 934300, train loss: 4.052821826934815, val loss: 4.066103076934814, ETA in seconds: 3312362.899\n",
      "epoch: 934400, train loss: 4.051845264434815, val loss: 4.071376514434815, ETA in seconds: 3307669.623\n",
      "epoch: 934500, train loss: 4.050673389434815, val loss: 4.066884326934814, ETA in seconds: 3302978.321\n",
      "epoch: 934600, train loss: 4.046376514434814, val loss: 4.068446826934815, ETA in seconds: 3298288.315\n",
      "epoch: 934700, train loss: 4.0549702644348145, val loss: 4.070009326934814, ETA in seconds: 3293591.476\n",
      "epoch: 934800, train loss: 4.051454639434814, val loss: 4.071962451934814, ETA in seconds: 3288896.540\n",
      "epoch: 934900, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 3284191.660\n",
      "epoch: 935000, train loss: 4.057509326934815, val loss: 4.070204639434815, ETA in seconds: 3279484.458\n",
      "epoch: 935100, train loss: 4.048720264434815, val loss: 4.065321826934815, ETA in seconds: 3274793.797\n",
      "epoch: 935200, train loss: 4.054384326934814, val loss: 4.069032764434814, ETA in seconds: 3270088.620\n",
      "epoch: 935300, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 3265399.917\n",
      "epoch: 935400, train loss: 4.052626514434815, val loss: 4.0676655769348145, ETA in seconds: 3260706.632\n",
      "epoch: 935500, train loss: 4.056532764434815, val loss: 4.065126514434814, ETA in seconds: 3256002.050\n",
      "epoch: 935600, train loss: 4.044618701934814, val loss: 4.068056201934814, ETA in seconds: 3251312.015\n",
      "epoch: 935700, train loss: 4.044618701934814, val loss: 4.069032764434814, ETA in seconds: 3246607.613\n",
      "epoch: 935800, train loss: 4.059071826934814, val loss: 4.063368701934815, ETA in seconds: 3241918.363\n",
      "epoch: 935900, train loss: 4.056337451934814, val loss: 4.070009326934814, ETA in seconds: 3237215.208\n",
      "epoch: 936000, train loss: 4.052431201934814, val loss: 4.068837451934814, ETA in seconds: 3232510.878\n",
      "epoch: 936100, train loss: 4.057118701934814, val loss: 4.067470264434815, ETA in seconds: 3227798.864\n",
      "epoch: 936200, train loss: 4.055751514434815, val loss: 4.062392139434815, ETA in seconds: 3223084.773\n",
      "epoch: 936300, train loss: 4.046767139434815, val loss: 4.063368701934815, ETA in seconds: 3218378.289\n",
      "epoch: 936400, train loss: 4.0559468269348145, val loss: 4.0735249519348145, ETA in seconds: 3213663.588\n",
      "epoch: 936500, train loss: 4.056532764434815, val loss: 4.071181201934815, ETA in seconds: 3208945.919\n",
      "epoch: 936600, train loss: 4.058095264434814, val loss: 4.0637593269348145, ETA in seconds: 3204237.194\n",
      "epoch: 936700, train loss: 4.052235889434814, val loss: 4.064149951934814, ETA in seconds: 3199519.910\n",
      "epoch: 936800, train loss: 4.047353076934814, val loss: 4.070790576934814, ETA in seconds: 3194798.214\n",
      "epoch: 936900, train loss: 4.052821826934815, val loss: 4.066298389434815, ETA in seconds: 3190092.195\n",
      "epoch: 937000, train loss: 4.0520405769348145, val loss: 4.068446826934815, ETA in seconds: 3185377.889\n",
      "epoch: 937100, train loss: 4.0491108894348145, val loss: 4.069032764434814, ETA in seconds: 3180658.941\n",
      "epoch: 937200, train loss: 4.054189014434814, val loss: 4.067079639434814, ETA in seconds: 3175941.767\n",
      "epoch: 937300, train loss: 4.0549702644348145, val loss: 4.062196826934814, ETA in seconds: 3171213.925\n",
      "epoch: 937400, train loss: 4.056142139434814, val loss: 4.068251514434815, ETA in seconds: 3166483.859\n",
      "epoch: 937500, train loss: 4.0549702644348145, val loss: 4.065907764434814, ETA in seconds: 3161778.818\n",
      "epoch: 937600, train loss: 4.050868701934815, val loss: 4.068056201934814, ETA in seconds: 3157053.218\n",
      "epoch: 937700, train loss: 4.048915576934815, val loss: 4.065321826934815, ETA in seconds: 3152349.561\n",
      "epoch: 937800, train loss: 4.052431201934814, val loss: 4.071376514434815, ETA in seconds: 3147614.290\n",
      "epoch: 937900, train loss: 4.051454639434814, val loss: 4.066103076934814, ETA in seconds: 3142878.930\n",
      "epoch: 938000, train loss: 4.057704639434815, val loss: 4.0696187019348145, ETA in seconds: 3138150.391\n",
      "epoch: 938100, train loss: 4.0491108894348145, val loss: 4.070985889434814, ETA in seconds: 3133434.750\n",
      "epoch: 938200, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 3128719.704\n",
      "epoch: 938300, train loss: 4.053212451934814, val loss: 4.069814014434814, ETA in seconds: 3123987.773\n",
      "epoch: 938400, train loss: 4.048720264434815, val loss: 4.066103076934814, ETA in seconds: 3119252.876\n",
      "epoch: 938500, train loss: 4.045399951934814, val loss: 4.070985889434814, ETA in seconds: 3114523.318\n",
      "epoch: 938600, train loss: 4.062587451934815, val loss: 4.068446826934815, ETA in seconds: 3109781.345\n",
      "epoch: 938700, train loss: 4.055751514434815, val loss: 4.065907764434814, ETA in seconds: 3105037.199\n",
      "epoch: 938800, train loss: 4.0549702644348145, val loss: 4.0705952644348145, ETA in seconds: 3100303.402\n",
      "epoch: 938900, train loss: 4.050673389434815, val loss: 4.071767139434814, ETA in seconds: 3095562.933\n",
      "epoch: 939000, train loss: 4.0559468269348145, val loss: 4.0657124519348145, ETA in seconds: 3090815.371\n",
      "epoch: 939100, train loss: 4.053798389434815, val loss: 4.062392139434815, ETA in seconds: 3086080.061\n",
      "epoch: 939200, train loss: 4.047939014434815, val loss: 4.0676655769348145, ETA in seconds: 3081340.005\n",
      "epoch: 939300, train loss: 4.052626514434815, val loss: 4.067470264434815, ETA in seconds: 3076601.584\n",
      "epoch: 939400, train loss: 4.060439014434815, val loss: 4.069423389434815, ETA in seconds: 3071858.308\n",
      "epoch: 939500, train loss: 4.056728076934815, val loss: 4.066103076934814, ETA in seconds: 3067117.641\n",
      "epoch: 939600, train loss: 4.054384326934814, val loss: 4.065907764434814, ETA in seconds: 3062375.616\n",
      "epoch: 939700, train loss: 4.055360889434814, val loss: 4.069032764434814, ETA in seconds: 3057632.362\n",
      "epoch: 939800, train loss: 4.0559468269348145, val loss: 4.064931201934814, ETA in seconds: 3052880.126\n",
      "epoch: 939900, train loss: 4.056337451934814, val loss: 4.062001514434814, ETA in seconds: 3048131.960\n",
      "epoch: 940000, train loss: 4.057509326934815, val loss: 4.069228076934815, ETA in seconds: 3043384.432\n",
      "epoch: 940100, train loss: 4.053407764434814, val loss: 4.065321826934815, ETA in seconds: 3038629.697\n",
      "epoch: 940200, train loss: 4.056337451934814, val loss: 4.068251514434815, ETA in seconds: 3033877.182\n",
      "epoch: 940300, train loss: 4.052431201934814, val loss: 4.072939014434814, ETA in seconds: 3029130.660\n",
      "epoch: 940400, train loss: 4.0539937019348145, val loss: 4.071767139434814, ETA in seconds: 3024369.447\n",
      "epoch: 940500, train loss: 4.053603076934815, val loss: 4.066103076934814, ETA in seconds: 3019612.404\n",
      "epoch: 940600, train loss: 4.0481343269348145, val loss: 4.064345264434815, ETA in seconds: 3014852.342\n",
      "epoch: 940700, train loss: 4.049501514434814, val loss: 4.065907764434814, ETA in seconds: 3010088.691\n",
      "epoch: 940800, train loss: 4.052626514434815, val loss: 4.067079639434814, ETA in seconds: 3005323.862\n",
      "epoch: 940900, train loss: 4.051649951934815, val loss: 4.075282764434815, ETA in seconds: 3000555.576\n",
      "epoch: 941000, train loss: 4.053798389434815, val loss: 4.071962451934814, ETA in seconds: 2995791.997\n",
      "epoch: 941100, train loss: 4.0520405769348145, val loss: 4.069032764434814, ETA in seconds: 2991025.880\n",
      "epoch: 941200, train loss: 4.054579639434815, val loss: 4.072157764434815, ETA in seconds: 2986271.372\n",
      "epoch: 941300, train loss: 4.052626514434815, val loss: 4.069228076934815, ETA in seconds: 2981532.658\n",
      "epoch: 941400, train loss: 4.062392139434815, val loss: 4.064149951934814, ETA in seconds: 2976797.613\n",
      "epoch: 941500, train loss: 4.0549702644348145, val loss: 4.069228076934815, ETA in seconds: 2972066.479\n",
      "epoch: 941600, train loss: 4.052626514434815, val loss: 4.067470264434815, ETA in seconds: 2967321.361\n",
      "epoch: 941700, train loss: 4.0510640144348145, val loss: 4.065907764434814, ETA in seconds: 2962582.909\n",
      "epoch: 941800, train loss: 4.0588765144348145, val loss: 4.068837451934814, ETA in seconds: 2957819.099\n",
      "epoch: 941900, train loss: 4.052431201934814, val loss: 4.067860889434814, ETA in seconds: 2953037.864\n",
      "epoch: 942000, train loss: 4.053603076934815, val loss: 4.0686421394348145, ETA in seconds: 2948212.560\n",
      "epoch: 942100, train loss: 4.050478076934814, val loss: 4.070790576934814, ETA in seconds: 2943432.940\n",
      "epoch: 942200, train loss: 4.048720264434815, val loss: 4.069228076934815, ETA in seconds: 2938657.987\n",
      "epoch: 942300, train loss: 4.047939014434815, val loss: 4.067470264434815, ETA in seconds: 2933878.490\n",
      "epoch: 942400, train loss: 4.049892139434815, val loss: 4.064540576934815, ETA in seconds: 2929098.574\n",
      "epoch: 942500, train loss: 4.0500874519348145, val loss: 4.066884326934814, ETA in seconds: 2924311.546\n",
      "epoch: 942600, train loss: 4.055165576934814, val loss: 4.068251514434815, ETA in seconds: 2919535.978\n",
      "epoch: 942700, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 2914759.456\n",
      "epoch: 942800, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 2909975.627\n",
      "epoch: 942900, train loss: 4.0510640144348145, val loss: 4.0637593269348145, ETA in seconds: 2905191.952\n",
      "epoch: 943000, train loss: 4.059657764434815, val loss: 4.071767139434814, ETA in seconds: 2900402.130\n",
      "epoch: 943100, train loss: 4.051845264434815, val loss: 4.063954639434814, ETA in seconds: 2895617.312\n",
      "epoch: 943200, train loss: 4.048524951934814, val loss: 4.0657124519348145, ETA in seconds: 2890824.502\n",
      "epoch: 943300, train loss: 4.048524951934814, val loss: 4.069032764434814, ETA in seconds: 2886038.763\n",
      "epoch: 943400, train loss: 4.051259326934814, val loss: 4.0676655769348145, ETA in seconds: 2881253.664\n",
      "epoch: 943500, train loss: 4.049306201934814, val loss: 4.062392139434815, ETA in seconds: 2876458.164\n",
      "epoch: 943600, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 2871679.605\n",
      "epoch: 943700, train loss: 4.051845264434815, val loss: 4.061220264434814, ETA in seconds: 2866890.249\n",
      "epoch: 943800, train loss: 4.051649951934815, val loss: 4.065907764434814, ETA in seconds: 2862102.697\n",
      "epoch: 943900, train loss: 4.053603076934815, val loss: 4.0657124519348145, ETA in seconds: 2857310.438\n",
      "epoch: 944000, train loss: 4.052431201934814, val loss: 4.0666890144348145, ETA in seconds: 2852507.995\n",
      "epoch: 944100, train loss: 4.045790576934815, val loss: 4.069228076934815, ETA in seconds: 2847714.541\n",
      "epoch: 944200, train loss: 4.047939014434815, val loss: 4.070790576934814, ETA in seconds: 2842911.005\n",
      "epoch: 944300, train loss: 4.0510640144348145, val loss: 4.072157764434815, ETA in seconds: 2838113.832\n",
      "epoch: 944400, train loss: 4.053212451934814, val loss: 4.065126514434814, ETA in seconds: 2833313.199\n",
      "epoch: 944500, train loss: 4.055556201934815, val loss: 4.066493701934815, ETA in seconds: 2828504.131\n",
      "epoch: 944600, train loss: 4.051845264434815, val loss: 4.065321826934815, ETA in seconds: 2823711.809\n",
      "epoch: 944700, train loss: 4.054774951934815, val loss: 4.063368701934815, ETA in seconds: 2818910.422\n",
      "epoch: 944800, train loss: 4.050673389434815, val loss: 4.070399951934815, ETA in seconds: 2814103.322\n",
      "epoch: 944900, train loss: 4.0559468269348145, val loss: 4.068251514434815, ETA in seconds: 2809302.172\n",
      "epoch: 945000, train loss: 4.052821826934815, val loss: 4.070204639434815, ETA in seconds: 2804494.453\n",
      "epoch: 945100, train loss: 4.047548389434814, val loss: 4.066103076934814, ETA in seconds: 2799687.443\n",
      "epoch: 945200, train loss: 4.048524951934814, val loss: 4.067860889434814, ETA in seconds: 2794880.393\n",
      "epoch: 945300, train loss: 4.054384326934814, val loss: 4.0657124519348145, ETA in seconds: 2790070.432\n",
      "epoch: 945400, train loss: 4.054384326934814, val loss: 4.063954639434814, ETA in seconds: 2785260.258\n",
      "epoch: 945500, train loss: 4.0500874519348145, val loss: 4.063368701934815, ETA in seconds: 2780451.049\n",
      "epoch: 945600, train loss: 4.0442280769348145, val loss: 4.0676655769348145, ETA in seconds: 2775645.043\n",
      "epoch: 945700, train loss: 4.051845264434815, val loss: 4.0696187019348145, ETA in seconds: 2770837.408\n",
      "epoch: 945800, train loss: 4.050673389434815, val loss: 4.067860889434814, ETA in seconds: 2766022.573\n",
      "epoch: 945900, train loss: 4.061024951934814, val loss: 4.068251514434815, ETA in seconds: 2761209.836\n",
      "epoch: 946000, train loss: 4.055751514434815, val loss: 4.069032764434814, ETA in seconds: 2756390.905\n",
      "epoch: 946100, train loss: 4.054774951934815, val loss: 4.066884326934814, ETA in seconds: 2751590.487\n",
      "epoch: 946200, train loss: 4.045399951934814, val loss: 4.066298389434815, ETA in seconds: 2746775.138\n",
      "epoch: 946300, train loss: 4.049306201934814, val loss: 4.0676655769348145, ETA in seconds: 2741962.529\n",
      "epoch: 946400, train loss: 4.049501514434814, val loss: 4.067860889434814, ETA in seconds: 2737140.024\n",
      "epoch: 946500, train loss: 4.050478076934814, val loss: 4.066493701934815, ETA in seconds: 2732325.920\n",
      "epoch: 946600, train loss: 4.0530171394348145, val loss: 4.067470264434815, ETA in seconds: 2727502.779\n",
      "epoch: 946700, train loss: 4.046376514434814, val loss: 4.069032764434814, ETA in seconds: 2722675.029\n",
      "epoch: 946800, train loss: 4.050868701934815, val loss: 4.067274951934815, ETA in seconds: 2717850.496\n",
      "epoch: 946900, train loss: 4.054384326934814, val loss: 4.066103076934814, ETA in seconds: 2713022.328\n",
      "epoch: 947000, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 2708197.408\n",
      "epoch: 947100, train loss: 4.060243701934814, val loss: 4.063368701934815, ETA in seconds: 2703376.790\n",
      "epoch: 947200, train loss: 4.056532764434815, val loss: 4.065126514434814, ETA in seconds: 2698545.417\n",
      "epoch: 947300, train loss: 4.0520405769348145, val loss: 4.069814014434814, ETA in seconds: 2693712.557\n",
      "epoch: 947400, train loss: 4.060634326934815, val loss: 4.071767139434814, ETA in seconds: 2688881.517\n",
      "epoch: 947500, train loss: 4.0491108894348145, val loss: 4.0637593269348145, ETA in seconds: 2684052.479\n",
      "epoch: 947600, train loss: 4.0530171394348145, val loss: 4.068251514434815, ETA in seconds: 2679229.185\n",
      "epoch: 947700, train loss: 4.056728076934815, val loss: 4.061220264434814, ETA in seconds: 2674397.050\n",
      "epoch: 947800, train loss: 4.055556201934815, val loss: 4.061024951934814, ETA in seconds: 2669554.743\n",
      "epoch: 947900, train loss: 4.054774951934815, val loss: 4.068251514434815, ETA in seconds: 2664707.854\n",
      "epoch: 948000, train loss: 4.055360889434814, val loss: 4.066298389434815, ETA in seconds: 2659872.850\n",
      "epoch: 948100, train loss: 4.0520405769348145, val loss: 4.0676655769348145, ETA in seconds: 2655027.943\n",
      "epoch: 948200, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 2650192.394\n",
      "epoch: 948300, train loss: 4.046962451934815, val loss: 4.063173389434814, ETA in seconds: 2645349.316\n",
      "epoch: 948400, train loss: 4.060634326934815, val loss: 4.069814014434814, ETA in seconds: 2640522.990\n",
      "epoch: 948500, train loss: 4.052626514434815, val loss: 4.070009326934814, ETA in seconds: 2635686.220\n",
      "epoch: 948600, train loss: 4.050673389434815, val loss: 4.0657124519348145, ETA in seconds: 2630857.549\n",
      "epoch: 948700, train loss: 4.0500874519348145, val loss: 4.069423389434815, ETA in seconds: 2626016.486\n",
      "epoch: 948800, train loss: 4.057509326934815, val loss: 4.067860889434814, ETA in seconds: 2621166.287\n",
      "epoch: 948900, train loss: 4.056532764434815, val loss: 4.066298389434815, ETA in seconds: 2616312.307\n",
      "epoch: 949000, train loss: 4.046767139434815, val loss: 4.0676655769348145, ETA in seconds: 2611455.439\n",
      "epoch: 949100, train loss: 4.042665576934814, val loss: 4.067860889434814, ETA in seconds: 2606603.438\n",
      "epoch: 949200, train loss: 4.055751514434815, val loss: 4.068446826934815, ETA in seconds: 2601753.625\n",
      "epoch: 949300, train loss: 4.053212451934814, val loss: 4.0647358894348145, ETA in seconds: 2596912.486\n",
      "epoch: 949400, train loss: 4.046376514434814, val loss: 4.070790576934814, ETA in seconds: 2592070.205\n",
      "epoch: 949500, train loss: 4.054774951934815, val loss: 4.065517139434815, ETA in seconds: 2587226.139\n",
      "epoch: 949600, train loss: 4.0520405769348145, val loss: 4.070790576934814, ETA in seconds: 2582394.124\n",
      "epoch: 949700, train loss: 4.050673389434815, val loss: 4.0686421394348145, ETA in seconds: 2577542.455\n",
      "epoch: 949800, train loss: 4.051845264434815, val loss: 4.063173389434814, ETA in seconds: 2572686.268\n",
      "epoch: 949900, train loss: 4.050673389434815, val loss: 4.063173389434814, ETA in seconds: 2567856.782\n",
      "epoch: 950000, train loss: 4.0569233894348145, val loss: 4.064540576934815, ETA in seconds: 2563029.195\n",
      "epoch: 950100, train loss: 4.049501514434814, val loss: 4.065907764434814, ETA in seconds: 2558199.496\n",
      "epoch: 950200, train loss: 4.0539937019348145, val loss: 4.0666890144348145, ETA in seconds: 2553368.328\n",
      "epoch: 950300, train loss: 4.046962451934815, val loss: 4.065517139434815, ETA in seconds: 2548533.513\n",
      "epoch: 950400, train loss: 4.052235889434814, val loss: 4.0686421394348145, ETA in seconds: 2543684.822\n",
      "epoch: 950500, train loss: 4.049306201934814, val loss: 4.067470264434815, ETA in seconds: 2538829.160\n",
      "epoch: 950600, train loss: 4.050868701934815, val loss: 4.072157764434815, ETA in seconds: 2533968.531\n",
      "epoch: 950700, train loss: 4.052821826934815, val loss: 4.069814014434814, ETA in seconds: 2529105.520\n",
      "epoch: 950800, train loss: 4.053603076934815, val loss: 4.065517139434815, ETA in seconds: 2524240.567\n",
      "epoch: 950900, train loss: 4.053603076934815, val loss: 4.065907764434814, ETA in seconds: 2519373.934\n",
      "epoch: 951000, train loss: 4.048720264434815, val loss: 4.067860889434814, ETA in seconds: 2514502.407\n",
      "epoch: 951100, train loss: 4.048720264434815, val loss: 4.069814014434814, ETA in seconds: 2509635.750\n",
      "epoch: 951200, train loss: 4.0530171394348145, val loss: 4.0686421394348145, ETA in seconds: 2504761.769\n",
      "epoch: 951300, train loss: 4.046376514434814, val loss: 4.063368701934815, ETA in seconds: 2499893.029\n",
      "epoch: 951400, train loss: 4.048915576934815, val loss: 4.071962451934814, ETA in seconds: 2495016.913\n",
      "epoch: 951500, train loss: 4.053603076934815, val loss: 4.068056201934814, ETA in seconds: 2490135.241\n",
      "epoch: 951600, train loss: 4.048524951934814, val loss: 4.069032764434814, ETA in seconds: 2485264.064\n",
      "epoch: 951700, train loss: 4.044423389434814, val loss: 4.065907764434814, ETA in seconds: 2480378.194\n",
      "epoch: 951800, train loss: 4.049892139434815, val loss: 4.074110889434815, ETA in seconds: 2475488.621\n",
      "epoch: 951900, train loss: 4.054579639434815, val loss: 4.064931201934814, ETA in seconds: 2470595.300\n",
      "epoch: 952000, train loss: 4.050478076934814, val loss: 4.063954639434814, ETA in seconds: 2465699.270\n",
      "epoch: 952100, train loss: 4.045985889434815, val loss: 4.071181201934815, ETA in seconds: 2460803.457\n",
      "epoch: 952200, train loss: 4.050478076934814, val loss: 4.067079639434814, ETA in seconds: 2455924.397\n",
      "epoch: 952300, train loss: 4.053603076934815, val loss: 4.074110889434815, ETA in seconds: 2451043.522\n",
      "epoch: 952400, train loss: 4.051259326934814, val loss: 4.0608296394348145, ETA in seconds: 2446174.018\n",
      "epoch: 952500, train loss: 4.051649951934815, val loss: 4.064149951934814, ETA in seconds: 2441310.782\n",
      "epoch: 952600, train loss: 4.050478076934814, val loss: 4.069423389434815, ETA in seconds: 2436431.658\n",
      "epoch: 952700, train loss: 4.048524951934814, val loss: 4.066298389434815, ETA in seconds: 2431542.677\n",
      "epoch: 952800, train loss: 4.054189014434814, val loss: 4.0686421394348145, ETA in seconds: 2426658.057\n",
      "epoch: 952900, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 2421773.831\n",
      "epoch: 953000, train loss: 4.053603076934815, val loss: 4.067860889434814, ETA in seconds: 2416893.759\n",
      "epoch: 953100, train loss: 4.048524951934814, val loss: 4.070204639434815, ETA in seconds: 2411999.115\n",
      "epoch: 953200, train loss: 4.051259326934814, val loss: 4.065517139434815, ETA in seconds: 2407102.974\n",
      "epoch: 953300, train loss: 4.052235889434814, val loss: 4.069228076934815, ETA in seconds: 2402207.510\n",
      "epoch: 953400, train loss: 4.052821826934815, val loss: 4.067274951934815, ETA in seconds: 2397312.989\n",
      "epoch: 953500, train loss: 4.052431201934814, val loss: 4.065321826934815, ETA in seconds: 2392439.628\n",
      "epoch: 953600, train loss: 4.050673389434815, val loss: 4.068056201934814, ETA in seconds: 2387544.651\n",
      "epoch: 953700, train loss: 4.051259326934814, val loss: 4.068251514434815, ETA in seconds: 2382643.122\n",
      "epoch: 953800, train loss: 4.0549702644348145, val loss: 4.0618062019348145, ETA in seconds: 2377739.256\n",
      "epoch: 953900, train loss: 4.047353076934814, val loss: 4.0754780769348145, ETA in seconds: 2372838.887\n",
      "epoch: 954000, train loss: 4.051649951934815, val loss: 4.0647358894348145, ETA in seconds: 2367932.629\n",
      "epoch: 954100, train loss: 4.059071826934814, val loss: 4.067079639434814, ETA in seconds: 2363029.861\n",
      "epoch: 954200, train loss: 4.050478076934814, val loss: 4.066103076934814, ETA in seconds: 2358131.556\n",
      "epoch: 954300, train loss: 4.056728076934815, val loss: 4.0666890144348145, ETA in seconds: 2353230.971\n",
      "epoch: 954400, train loss: 4.046376514434814, val loss: 4.063954639434814, ETA in seconds: 2348322.379\n",
      "epoch: 954500, train loss: 4.048329639434814, val loss: 4.066298389434815, ETA in seconds: 2343411.785\n",
      "epoch: 954600, train loss: 4.058095264434814, val loss: 4.060439014434815, ETA in seconds: 2338498.801\n",
      "epoch: 954700, train loss: 4.060048389434814, val loss: 4.069032764434814, ETA in seconds: 2333586.543\n",
      "epoch: 954800, train loss: 4.059071826934814, val loss: 4.068251514434815, ETA in seconds: 2328672.292\n",
      "epoch: 954900, train loss: 4.052821826934815, val loss: 4.066884326934814, ETA in seconds: 2323757.780\n",
      "epoch: 955000, train loss: 4.053212451934814, val loss: 4.0676655769348145, ETA in seconds: 2318847.976\n",
      "epoch: 955100, train loss: 4.054384326934814, val loss: 4.0666890144348145, ETA in seconds: 2313929.176\n",
      "epoch: 955200, train loss: 4.0481343269348145, val loss: 4.070399951934815, ETA in seconds: 2309004.857\n",
      "epoch: 955300, train loss: 4.0510640144348145, val loss: 4.062001514434814, ETA in seconds: 2304095.857\n",
      "epoch: 955400, train loss: 4.0452046394348145, val loss: 4.065126514434814, ETA in seconds: 2299180.657\n",
      "epoch: 955500, train loss: 4.0539937019348145, val loss: 4.0637593269348145, ETA in seconds: 2294265.542\n",
      "epoch: 955600, train loss: 4.0510640144348145, val loss: 4.068056201934814, ETA in seconds: 2289351.958\n",
      "epoch: 955700, train loss: 4.0530171394348145, val loss: 4.0637593269348145, ETA in seconds: 2284437.310\n",
      "epoch: 955800, train loss: 4.051649951934815, val loss: 4.065321826934815, ETA in seconds: 2279516.625\n",
      "epoch: 955900, train loss: 4.047939014434815, val loss: 4.070985889434814, ETA in seconds: 2274615.974\n",
      "epoch: 956000, train loss: 4.051454639434814, val loss: 4.069032764434814, ETA in seconds: 2269716.945\n",
      "epoch: 956100, train loss: 4.0510640144348145, val loss: 4.069814014434814, ETA in seconds: 2264813.197\n",
      "epoch: 956200, train loss: 4.0539937019348145, val loss: 4.067079639434814, ETA in seconds: 2259893.167\n",
      "epoch: 956300, train loss: 4.048720264434815, val loss: 4.066493701934815, ETA in seconds: 2254964.865\n",
      "epoch: 956400, train loss: 4.0491108894348145, val loss: 4.067860889434814, ETA in seconds: 2250036.742\n",
      "epoch: 956500, train loss: 4.0452046394348145, val loss: 4.068251514434815, ETA in seconds: 2245106.608\n",
      "epoch: 956600, train loss: 4.0500874519348145, val loss: 4.063173389434814, ETA in seconds: 2240174.753\n",
      "epoch: 956700, train loss: 4.057118701934814, val loss: 4.064931201934814, ETA in seconds: 2235246.253\n",
      "epoch: 956800, train loss: 4.050282764434814, val loss: 4.067470264434815, ETA in seconds: 2230312.595\n",
      "epoch: 956900, train loss: 4.0500874519348145, val loss: 4.071376514434815, ETA in seconds: 2225378.872\n",
      "epoch: 957000, train loss: 4.050282764434814, val loss: 4.068446826934815, ETA in seconds: 2220435.021\n",
      "epoch: 957100, train loss: 4.050868701934815, val loss: 4.067470264434815, ETA in seconds: 2215499.291\n",
      "epoch: 957200, train loss: 4.043642139434814, val loss: 4.065907764434814, ETA in seconds: 2210554.545\n",
      "epoch: 957300, train loss: 4.046376514434814, val loss: 4.071181201934815, ETA in seconds: 2205616.382\n",
      "epoch: 957400, train loss: 4.050868701934815, val loss: 4.068251514434815, ETA in seconds: 2200675.477\n",
      "epoch: 957500, train loss: 4.0481343269348145, val loss: 4.068837451934814, ETA in seconds: 2195726.853\n",
      "epoch: 957600, train loss: 4.0588765144348145, val loss: 4.070985889434814, ETA in seconds: 2190783.397\n",
      "epoch: 957700, train loss: 4.048915576934815, val loss: 4.064540576934815, ETA in seconds: 2185842.920\n",
      "epoch: 957800, train loss: 4.050868701934815, val loss: 4.063954639434814, ETA in seconds: 2180911.410\n",
      "epoch: 957900, train loss: 4.0481343269348145, val loss: 4.072157764434815, ETA in seconds: 2175982.796\n",
      "epoch: 958000, train loss: 4.057509326934815, val loss: 4.063368701934815, ETA in seconds: 2171062.751\n",
      "epoch: 958100, train loss: 4.051259326934814, val loss: 4.066298389434815, ETA in seconds: 2166131.791\n",
      "epoch: 958200, train loss: 4.0461812019348145, val loss: 4.064540576934815, ETA in seconds: 2161200.211\n",
      "epoch: 958300, train loss: 4.051454639434814, val loss: 4.066103076934814, ETA in seconds: 2156252.814\n",
      "epoch: 958400, train loss: 4.051259326934814, val loss: 4.065907764434814, ETA in seconds: 2151301.768\n",
      "epoch: 958500, train loss: 4.053603076934815, val loss: 4.060634326934815, ETA in seconds: 2146353.549\n",
      "epoch: 958600, train loss: 4.047939014434815, val loss: 4.071376514434815, ETA in seconds: 2141400.397\n",
      "epoch: 958700, train loss: 4.053603076934815, val loss: 4.0676655769348145, ETA in seconds: 2136448.898\n",
      "epoch: 958800, train loss: 4.044423389434814, val loss: 4.066103076934814, ETA in seconds: 2131492.538\n",
      "epoch: 958900, train loss: 4.053212451934814, val loss: 4.067274951934815, ETA in seconds: 2126544.301\n",
      "epoch: 959000, train loss: 4.054774951934815, val loss: 4.065321826934815, ETA in seconds: 2121582.257\n",
      "epoch: 959100, train loss: 4.051845264434815, val loss: 4.065126514434814, ETA in seconds: 2116617.852\n",
      "epoch: 959200, train loss: 4.056532764434815, val loss: 4.070790576934814, ETA in seconds: 2111653.472\n",
      "epoch: 959300, train loss: 4.051649951934815, val loss: 4.064540576934815, ETA in seconds: 2106692.147\n",
      "epoch: 959400, train loss: 4.048524951934814, val loss: 4.060634326934815, ETA in seconds: 2101731.317\n",
      "epoch: 959500, train loss: 4.053212451934814, val loss: 4.070985889434814, ETA in seconds: 2096771.350\n",
      "epoch: 959600, train loss: 4.049306201934814, val loss: 4.068251514434815, ETA in seconds: 2091807.442\n",
      "epoch: 959700, train loss: 4.047548389434814, val loss: 4.0715718269348145, ETA in seconds: 2086842.066\n",
      "epoch: 959800, train loss: 4.060048389434814, val loss: 4.0657124519348145, ETA in seconds: 2081876.258\n",
      "epoch: 959900, train loss: 4.048329639434814, val loss: 4.064149951934814, ETA in seconds: 2076906.808\n",
      "epoch: 960000, train loss: 4.053407764434814, val loss: 4.069228076934815, ETA in seconds: 2071937.594\n",
      "epoch: 960100, train loss: 4.053212451934814, val loss: 4.070790576934814, ETA in seconds: 2066971.817\n",
      "epoch: 960200, train loss: 4.050868701934815, val loss: 4.069228076934815, ETA in seconds: 2062000.039\n",
      "epoch: 960300, train loss: 4.052235889434814, val loss: 4.065517139434815, ETA in seconds: 2057029.836\n",
      "epoch: 960400, train loss: 4.0578999519348145, val loss: 4.065517139434815, ETA in seconds: 2052062.881\n",
      "epoch: 960500, train loss: 4.0520405769348145, val loss: 4.071181201934815, ETA in seconds: 2047091.894\n",
      "epoch: 960600, train loss: 4.057314014434814, val loss: 4.072157764434815, ETA in seconds: 2042117.626\n",
      "epoch: 960700, train loss: 4.0510640144348145, val loss: 4.0696187019348145, ETA in seconds: 2037161.527\n",
      "epoch: 960800, train loss: 4.054579639434815, val loss: 4.061610889434815, ETA in seconds: 2032209.704\n",
      "epoch: 960900, train loss: 4.054189014434814, val loss: 4.069814014434814, ETA in seconds: 2027259.220\n",
      "epoch: 961000, train loss: 4.049696826934815, val loss: 4.072939014434814, ETA in seconds: 2022306.044\n",
      "epoch: 961100, train loss: 4.0520405769348145, val loss: 4.071181201934815, ETA in seconds: 2017352.466\n",
      "epoch: 961200, train loss: 4.051649951934815, val loss: 4.068251514434815, ETA in seconds: 2012400.816\n",
      "epoch: 961300, train loss: 4.051454639434814, val loss: 4.0725483894348145, ETA in seconds: 2007439.119\n",
      "epoch: 961400, train loss: 4.055360889434814, val loss: 4.066103076934814, ETA in seconds: 2002457.443\n",
      "epoch: 961500, train loss: 4.046376514434814, val loss: 4.065907764434814, ETA in seconds: 1997477.309\n",
      "epoch: 961600, train loss: 4.051454639434814, val loss: 4.0666890144348145, ETA in seconds: 1992498.909\n",
      "epoch: 961700, train loss: 4.045595264434814, val loss: 4.070009326934814, ETA in seconds: 1987517.956\n",
      "epoch: 961800, train loss: 4.055556201934815, val loss: 4.070985889434814, ETA in seconds: 1982530.068\n",
      "epoch: 961900, train loss: 4.046767139434815, val loss: 4.068251514434815, ETA in seconds: 1977540.180\n",
      "epoch: 962000, train loss: 4.051454639434814, val loss: 4.065321826934815, ETA in seconds: 1972552.894\n",
      "epoch: 962100, train loss: 4.050282764434814, val loss: 4.073134326934815, ETA in seconds: 1967560.774\n",
      "epoch: 962200, train loss: 4.058681201934815, val loss: 4.070790576934814, ETA in seconds: 1962572.842\n",
      "epoch: 962300, train loss: 4.049696826934815, val loss: 4.063954639434814, ETA in seconds: 1957585.185\n",
      "epoch: 962400, train loss: 4.0578999519348145, val loss: 4.069228076934815, ETA in seconds: 1952596.651\n",
      "epoch: 962500, train loss: 4.0588765144348145, val loss: 4.069423389434815, ETA in seconds: 1947604.200\n",
      "epoch: 962600, train loss: 4.043837451934815, val loss: 4.0627827644348145, ETA in seconds: 1942610.090\n",
      "epoch: 962700, train loss: 4.049306201934814, val loss: 4.0676655769348145, ETA in seconds: 1937618.128\n",
      "epoch: 962800, train loss: 4.0510640144348145, val loss: 4.064540576934815, ETA in seconds: 1932625.067\n",
      "epoch: 962900, train loss: 4.055556201934815, val loss: 4.0696187019348145, ETA in seconds: 1927630.362\n",
      "epoch: 963000, train loss: 4.054774951934815, val loss: 4.068251514434815, ETA in seconds: 1922632.902\n",
      "epoch: 963100, train loss: 4.047939014434815, val loss: 4.067079639434814, ETA in seconds: 1917630.948\n",
      "epoch: 963200, train loss: 4.048720264434815, val loss: 4.0666890144348145, ETA in seconds: 1912629.577\n",
      "epoch: 963300, train loss: 4.048329639434814, val loss: 4.066298389434815, ETA in seconds: 1907628.201\n",
      "epoch: 963400, train loss: 4.051845264434815, val loss: 4.063564014434815, ETA in seconds: 1902622.043\n",
      "epoch: 963500, train loss: 4.055360889434814, val loss: 4.0676655769348145, ETA in seconds: 1897620.840\n",
      "epoch: 963600, train loss: 4.054384326934814, val loss: 4.072743701934814, ETA in seconds: 1892624.053\n",
      "epoch: 963700, train loss: 4.0510640144348145, val loss: 4.068837451934814, ETA in seconds: 1887624.584\n",
      "epoch: 963800, train loss: 4.047353076934814, val loss: 4.0666890144348145, ETA in seconds: 1882623.969\n",
      "epoch: 963900, train loss: 4.048524951934814, val loss: 4.071962451934814, ETA in seconds: 1877615.828\n",
      "epoch: 964000, train loss: 4.047353076934814, val loss: 4.066298389434815, ETA in seconds: 1872611.836\n",
      "epoch: 964100, train loss: 4.0491108894348145, val loss: 4.063954639434814, ETA in seconds: 1867598.861\n",
      "epoch: 964200, train loss: 4.0520405769348145, val loss: 4.070204639434815, ETA in seconds: 1862586.711\n",
      "epoch: 964300, train loss: 4.060634326934815, val loss: 4.066103076934814, ETA in seconds: 1857572.759\n",
      "epoch: 964400, train loss: 4.054384326934814, val loss: 4.068251514434815, ETA in seconds: 1852560.336\n",
      "epoch: 964500, train loss: 4.0500874519348145, val loss: 4.065126514434814, ETA in seconds: 1847546.316\n",
      "epoch: 964600, train loss: 4.0530171394348145, val loss: 4.065517139434815, ETA in seconds: 1842530.317\n",
      "epoch: 964700, train loss: 4.042665576934814, val loss: 4.0696187019348145, ETA in seconds: 1837512.390\n",
      "epoch: 964800, train loss: 4.0549702644348145, val loss: 4.074110889434815, ETA in seconds: 1832492.258\n",
      "epoch: 964900, train loss: 4.048720264434815, val loss: 4.068446826934815, ETA in seconds: 1827469.980\n",
      "epoch: 965000, train loss: 4.0520405769348145, val loss: 4.0725483894348145, ETA in seconds: 1822450.027\n",
      "epoch: 965100, train loss: 4.0471577644348145, val loss: 4.072743701934814, ETA in seconds: 1817427.283\n",
      "epoch: 965200, train loss: 4.044032764434815, val loss: 4.066298389434815, ETA in seconds: 1812404.342\n",
      "epoch: 965300, train loss: 4.053798389434815, val loss: 4.066103076934814, ETA in seconds: 1807382.244\n",
      "epoch: 965400, train loss: 4.049501514434814, val loss: 4.0686421394348145, ETA in seconds: 1802359.186\n",
      "epoch: 965500, train loss: 4.044032764434815, val loss: 4.068446826934815, ETA in seconds: 1797331.302\n",
      "epoch: 965600, train loss: 4.049696826934815, val loss: 4.067274951934815, ETA in seconds: 1792296.762\n",
      "epoch: 965700, train loss: 4.0510640144348145, val loss: 4.0696187019348145, ETA in seconds: 1787270.305\n",
      "epoch: 965800, train loss: 4.050673389434815, val loss: 4.0666890144348145, ETA in seconds: 1782238.230\n",
      "epoch: 965900, train loss: 4.054579639434815, val loss: 4.064931201934814, ETA in seconds: 1777205.298\n",
      "epoch: 966000, train loss: 4.049501514434814, val loss: 4.066884326934814, ETA in seconds: 1772168.742\n",
      "epoch: 966100, train loss: 4.051845264434815, val loss: 4.060439014434815, ETA in seconds: 1767135.487\n",
      "epoch: 966200, train loss: 4.057704639434815, val loss: 4.065907764434814, ETA in seconds: 1762101.707\n",
      "epoch: 966300, train loss: 4.049696826934815, val loss: 4.067470264434815, ETA in seconds: 1757066.302\n",
      "epoch: 966400, train loss: 4.045790576934815, val loss: 4.067274951934815, ETA in seconds: 1752029.207\n",
      "epoch: 966500, train loss: 4.052821826934815, val loss: 4.0705952644348145, ETA in seconds: 1746990.399\n",
      "epoch: 966600, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 1741952.384\n",
      "epoch: 966700, train loss: 4.048720264434815, val loss: 4.065907764434814, ETA in seconds: 1736913.847\n",
      "epoch: 966800, train loss: 4.0549702644348145, val loss: 4.0725483894348145, ETA in seconds: 1731873.995\n",
      "epoch: 966900, train loss: 4.048524951934814, val loss: 4.0725483894348145, ETA in seconds: 1726830.866\n",
      "epoch: 967000, train loss: 4.046962451934815, val loss: 4.0686421394348145, ETA in seconds: 1721782.636\n",
      "epoch: 967100, train loss: 4.053798389434815, val loss: 4.070790576934814, ETA in seconds: 1716747.592\n",
      "epoch: 967200, train loss: 4.044423389434814, val loss: 4.0657124519348145, ETA in seconds: 1711700.612\n",
      "epoch: 967300, train loss: 4.055556201934815, val loss: 4.069032764434814, ETA in seconds: 1706657.856\n",
      "epoch: 967400, train loss: 4.054774951934815, val loss: 4.066298389434815, ETA in seconds: 1701612.449\n",
      "epoch: 967500, train loss: 4.058290576934814, val loss: 4.0666890144348145, ETA in seconds: 1696567.866\n",
      "epoch: 967600, train loss: 4.054774951934815, val loss: 4.0705952644348145, ETA in seconds: 1691518.000\n",
      "epoch: 967700, train loss: 4.052431201934814, val loss: 4.0657124519348145, ETA in seconds: 1686470.304\n",
      "epoch: 967800, train loss: 4.053212451934814, val loss: 4.070790576934814, ETA in seconds: 1681416.653\n",
      "epoch: 967900, train loss: 4.0520405769348145, val loss: 4.062196826934814, ETA in seconds: 1676366.867\n",
      "epoch: 968000, train loss: 4.049501514434814, val loss: 4.066298389434815, ETA in seconds: 1671316.133\n",
      "epoch: 968100, train loss: 4.054774951934815, val loss: 4.064345264434815, ETA in seconds: 1666268.712\n",
      "epoch: 968200, train loss: 4.0539937019348145, val loss: 4.065321826934815, ETA in seconds: 1661221.607\n",
      "epoch: 968300, train loss: 4.058290576934814, val loss: 4.063368701934815, ETA in seconds: 1656176.898\n",
      "epoch: 968400, train loss: 4.048329639434814, val loss: 4.067079639434814, ETA in seconds: 1651128.981\n",
      "epoch: 968500, train loss: 4.053798389434815, val loss: 4.068837451934814, ETA in seconds: 1646079.849\n",
      "epoch: 968600, train loss: 4.0510640144348145, val loss: 4.063368701934815, ETA in seconds: 1641025.278\n",
      "epoch: 968700, train loss: 4.0539937019348145, val loss: 4.0676655769348145, ETA in seconds: 1635968.294\n",
      "epoch: 968800, train loss: 4.060048389434814, val loss: 4.066493701934815, ETA in seconds: 1630904.716\n",
      "epoch: 968900, train loss: 4.0549702644348145, val loss: 4.070204639434815, ETA in seconds: 1625846.036\n",
      "epoch: 969000, train loss: 4.0539937019348145, val loss: 4.063954639434814, ETA in seconds: 1620787.050\n",
      "epoch: 969100, train loss: 4.039735889434814, val loss: 4.071962451934814, ETA in seconds: 1615730.431\n",
      "epoch: 969200, train loss: 4.0491108894348145, val loss: 4.069814014434814, ETA in seconds: 1610674.413\n",
      "epoch: 969300, train loss: 4.048720264434815, val loss: 4.0608296394348145, ETA in seconds: 1605610.691\n",
      "epoch: 969400, train loss: 4.059657764434815, val loss: 4.068446826934815, ETA in seconds: 1600555.204\n",
      "epoch: 969500, train loss: 4.0491108894348145, val loss: 4.065126514434814, ETA in seconds: 1595490.006\n",
      "epoch: 969600, train loss: 4.054189014434814, val loss: 4.068251514434815, ETA in seconds: 1590422.436\n",
      "epoch: 969700, train loss: 4.0539937019348145, val loss: 4.067470264434815, ETA in seconds: 1585360.091\n",
      "epoch: 969800, train loss: 4.056337451934814, val loss: 4.066103076934814, ETA in seconds: 1580295.574\n",
      "epoch: 969900, train loss: 4.054189014434814, val loss: 4.0676655769348145, ETA in seconds: 1575224.146\n",
      "epoch: 970000, train loss: 4.053407764434814, val loss: 4.066298389434815, ETA in seconds: 1570157.517\n",
      "epoch: 970100, train loss: 4.050478076934814, val loss: 4.0647358894348145, ETA in seconds: 1565079.510\n",
      "epoch: 970200, train loss: 4.0471577644348145, val loss: 4.066493701934815, ETA in seconds: 1560008.715\n",
      "epoch: 970300, train loss: 4.055751514434815, val loss: 4.068446826934815, ETA in seconds: 1554941.173\n",
      "epoch: 970400, train loss: 4.054189014434814, val loss: 4.0696187019348145, ETA in seconds: 1549862.224\n",
      "epoch: 970500, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 1544779.708\n",
      "epoch: 970600, train loss: 4.048720264434815, val loss: 4.0657124519348145, ETA in seconds: 1539695.227\n",
      "epoch: 970700, train loss: 4.051649951934815, val loss: 4.064931201934814, ETA in seconds: 1534606.805\n",
      "epoch: 970800, train loss: 4.0559468269348145, val loss: 4.062587451934815, ETA in seconds: 1529504.467\n",
      "epoch: 970900, train loss: 4.051845264434815, val loss: 4.0637593269348145, ETA in seconds: 1524418.980\n",
      "epoch: 971000, train loss: 4.049306201934814, val loss: 4.064149951934814, ETA in seconds: 1519337.046\n",
      "epoch: 971100, train loss: 4.049892139434815, val loss: 4.066298389434815, ETA in seconds: 1514260.184\n",
      "epoch: 971200, train loss: 4.051649951934815, val loss: 4.063173389434814, ETA in seconds: 1509176.653\n",
      "epoch: 971300, train loss: 4.049892139434815, val loss: 4.068056201934814, ETA in seconds: 1504093.294\n",
      "epoch: 971400, train loss: 4.051259326934814, val loss: 4.071181201934815, ETA in seconds: 1499015.744\n",
      "epoch: 971500, train loss: 4.051649951934815, val loss: 4.067470264434815, ETA in seconds: 1493931.734\n",
      "epoch: 971600, train loss: 4.051259326934814, val loss: 4.069032764434814, ETA in seconds: 1488842.949\n",
      "epoch: 971700, train loss: 4.0539937019348145, val loss: 4.074110889434815, ETA in seconds: 1483750.863\n",
      "epoch: 971800, train loss: 4.043837451934815, val loss: 4.064149951934814, ETA in seconds: 1478661.821\n",
      "epoch: 971900, train loss: 4.055165576934814, val loss: 4.0637593269348145, ETA in seconds: 1473557.957\n",
      "epoch: 972000, train loss: 4.046376514434814, val loss: 4.070790576934814, ETA in seconds: 1468460.935\n",
      "epoch: 972100, train loss: 4.0530171394348145, val loss: 4.0647358894348145, ETA in seconds: 1463359.877\n",
      "epoch: 972200, train loss: 4.0520405769348145, val loss: 4.064931201934814, ETA in seconds: 1458262.171\n",
      "epoch: 972300, train loss: 4.051259326934814, val loss: 4.063564014434815, ETA in seconds: 1453162.525\n",
      "epoch: 972400, train loss: 4.054774951934815, val loss: 4.068837451934814, ETA in seconds: 1448063.219\n",
      "epoch: 972500, train loss: 4.051259326934814, val loss: 4.067079639434814, ETA in seconds: 1442963.727\n",
      "epoch: 972600, train loss: 4.0500874519348145, val loss: 4.0676655769348145, ETA in seconds: 1437864.828\n",
      "epoch: 972700, train loss: 4.043642139434814, val loss: 4.065517139434815, ETA in seconds: 1432765.739\n",
      "epoch: 972800, train loss: 4.043837451934815, val loss: 4.065907764434814, ETA in seconds: 1427662.432\n",
      "epoch: 972900, train loss: 4.049892139434815, val loss: 4.066103076934814, ETA in seconds: 1422555.101\n",
      "epoch: 973000, train loss: 4.049501514434814, val loss: 4.068056201934814, ETA in seconds: 1417451.370\n",
      "epoch: 973100, train loss: 4.047939014434815, val loss: 4.0745015144348145, ETA in seconds: 1412342.738\n",
      "epoch: 973200, train loss: 4.0598530769348145, val loss: 4.068251514434815, ETA in seconds: 1407232.228\n",
      "epoch: 973300, train loss: 4.050673389434815, val loss: 4.067274951934815, ETA in seconds: 1402123.967\n",
      "epoch: 973400, train loss: 4.060634326934815, val loss: 4.0666890144348145, ETA in seconds: 1397011.786\n",
      "epoch: 973500, train loss: 4.053212451934814, val loss: 4.068251514434815, ETA in seconds: 1391901.164\n",
      "epoch: 973600, train loss: 4.0500874519348145, val loss: 4.066298389434815, ETA in seconds: 1386788.944\n",
      "epoch: 973700, train loss: 4.047743701934815, val loss: 4.069032764434814, ETA in seconds: 1381672.845\n",
      "epoch: 973800, train loss: 4.055360889434814, val loss: 4.076064014434815, ETA in seconds: 1376556.291\n",
      "epoch: 973900, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 1371438.843\n",
      "epoch: 974000, train loss: 4.052235889434814, val loss: 4.0754780769348145, ETA in seconds: 1366324.773\n",
      "epoch: 974100, train loss: 4.0471577644348145, val loss: 4.064931201934814, ETA in seconds: 1361204.713\n",
      "epoch: 974200, train loss: 4.053212451934814, val loss: 4.063173389434814, ETA in seconds: 1356085.146\n",
      "epoch: 974300, train loss: 4.050478076934814, val loss: 4.065126514434814, ETA in seconds: 1350965.201\n",
      "epoch: 974400, train loss: 4.052431201934814, val loss: 4.069228076934815, ETA in seconds: 1345843.747\n",
      "epoch: 974500, train loss: 4.047548389434814, val loss: 4.068446826934815, ETA in seconds: 1340719.686\n",
      "epoch: 974600, train loss: 4.052235889434814, val loss: 4.070790576934814, ETA in seconds: 1335593.369\n",
      "epoch: 974700, train loss: 4.056337451934814, val loss: 4.071767139434814, ETA in seconds: 1330469.605\n",
      "epoch: 974800, train loss: 4.052235889434814, val loss: 4.062978076934814, ETA in seconds: 1325343.021\n",
      "epoch: 974900, train loss: 4.048915576934815, val loss: 4.071962451934814, ETA in seconds: 1320216.681\n",
      "epoch: 975000, train loss: 4.050868701934815, val loss: 4.069032764434814, ETA in seconds: 1315087.126\n",
      "epoch: 975100, train loss: 4.056142139434814, val loss: 4.066884326934814, ETA in seconds: 1309958.178\n",
      "epoch: 975200, train loss: 4.052235889434814, val loss: 4.071767139434814, ETA in seconds: 1304827.589\n",
      "epoch: 975300, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 1299695.960\n",
      "epoch: 975400, train loss: 4.051454639434814, val loss: 4.069032764434814, ETA in seconds: 1294561.384\n",
      "epoch: 975500, train loss: 4.050282764434814, val loss: 4.068446826934815, ETA in seconds: 1289429.905\n",
      "epoch: 975600, train loss: 4.047548389434814, val loss: 4.063368701934815, ETA in seconds: 1284294.585\n",
      "epoch: 975700, train loss: 4.0578999519348145, val loss: 4.069228076934815, ETA in seconds: 1279156.975\n",
      "epoch: 975800, train loss: 4.052431201934814, val loss: 4.0657124519348145, ETA in seconds: 1274021.766\n",
      "epoch: 975900, train loss: 4.047353076934814, val loss: 4.069228076934815, ETA in seconds: 1268883.754\n",
      "epoch: 976000, train loss: 4.051454639434814, val loss: 4.068251514434815, ETA in seconds: 1263745.007\n",
      "epoch: 976100, train loss: 4.048524951934814, val loss: 4.070399951934815, ETA in seconds: 1258604.043\n",
      "epoch: 976200, train loss: 4.050868701934815, val loss: 4.069032764434814, ETA in seconds: 1253462.439\n",
      "epoch: 976300, train loss: 4.059071826934814, val loss: 4.0657124519348145, ETA in seconds: 1248321.472\n",
      "epoch: 976400, train loss: 4.0530171394348145, val loss: 4.0627827644348145, ETA in seconds: 1243181.208\n",
      "epoch: 976500, train loss: 4.0481343269348145, val loss: 4.068251514434815, ETA in seconds: 1238038.413\n",
      "epoch: 976600, train loss: 4.053603076934815, val loss: 4.0657124519348145, ETA in seconds: 1232893.719\n",
      "epoch: 976700, train loss: 4.039149951934815, val loss: 4.0578999519348145, ETA in seconds: 1227745.651\n",
      "epoch: 976800, train loss: 4.046767139434815, val loss: 4.0715718269348145, ETA in seconds: 1222597.612\n",
      "epoch: 976900, train loss: 4.0520405769348145, val loss: 4.0666890144348145, ETA in seconds: 1217448.785\n",
      "epoch: 977000, train loss: 4.050868701934815, val loss: 4.060634326934815, ETA in seconds: 1212299.910\n",
      "epoch: 977100, train loss: 4.050282764434814, val loss: 4.061220264434814, ETA in seconds: 1207149.666\n",
      "epoch: 977200, train loss: 4.0481343269348145, val loss: 4.068251514434815, ETA in seconds: 1202000.117\n",
      "epoch: 977300, train loss: 4.045399951934814, val loss: 4.065126514434814, ETA in seconds: 1196846.283\n",
      "epoch: 977400, train loss: 4.053798389434815, val loss: 4.074306201934815, ETA in seconds: 1191696.290\n",
      "epoch: 977500, train loss: 4.054189014434814, val loss: 4.067860889434814, ETA in seconds: 1186541.725\n",
      "epoch: 977600, train loss: 4.052821826934815, val loss: 4.0676655769348145, ETA in seconds: 1181385.818\n",
      "epoch: 977700, train loss: 4.0569233894348145, val loss: 4.071767139434814, ETA in seconds: 1176225.041\n",
      "epoch: 977800, train loss: 4.056142139434814, val loss: 4.0618062019348145, ETA in seconds: 1171064.372\n",
      "epoch: 977900, train loss: 4.043056201934815, val loss: 4.072939014434814, ETA in seconds: 1165905.074\n",
      "epoch: 978000, train loss: 4.0520405769348145, val loss: 4.063173389434814, ETA in seconds: 1160745.752\n",
      "epoch: 978100, train loss: 4.049696826934815, val loss: 4.065907764434814, ETA in seconds: 1155583.240\n",
      "epoch: 978200, train loss: 4.050478076934814, val loss: 4.0647358894348145, ETA in seconds: 1150420.496\n",
      "epoch: 978300, train loss: 4.052235889434814, val loss: 4.063564014434815, ETA in seconds: 1145258.071\n",
      "epoch: 978400, train loss: 4.0452046394348145, val loss: 4.068056201934814, ETA in seconds: 1140092.760\n",
      "epoch: 978500, train loss: 4.0578999519348145, val loss: 4.062196826934814, ETA in seconds: 1134927.716\n",
      "epoch: 978600, train loss: 4.060634326934815, val loss: 4.064540576934815, ETA in seconds: 1129760.570\n",
      "epoch: 978700, train loss: 4.057118701934814, val loss: 4.071181201934815, ETA in seconds: 1124593.150\n",
      "epoch: 978800, train loss: 4.052626514434815, val loss: 4.067470264434815, ETA in seconds: 1119424.086\n",
      "epoch: 978900, train loss: 4.0510640144348145, val loss: 4.068837451934814, ETA in seconds: 1114256.562\n",
      "epoch: 979000, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 1109087.153\n",
      "epoch: 979100, train loss: 4.045399951934814, val loss: 4.069228076934815, ETA in seconds: 1103913.631\n",
      "epoch: 979200, train loss: 4.041884326934815, val loss: 4.0715718269348145, ETA in seconds: 1098741.652\n",
      "epoch: 979300, train loss: 4.053407764434814, val loss: 4.065517139434815, ETA in seconds: 1093566.683\n",
      "epoch: 979400, train loss: 4.049696826934815, val loss: 4.070399951934815, ETA in seconds: 1088393.176\n",
      "epoch: 979500, train loss: 4.058290576934814, val loss: 4.0666890144348145, ETA in seconds: 1083218.840\n",
      "epoch: 979600, train loss: 4.055165576934814, val loss: 4.071181201934815, ETA in seconds: 1078040.682\n",
      "epoch: 979700, train loss: 4.0549702644348145, val loss: 4.064345264434815, ETA in seconds: 1072863.487\n",
      "epoch: 979800, train loss: 4.0510640144348145, val loss: 4.063173389434814, ETA in seconds: 1067686.042\n",
      "epoch: 979900, train loss: 4.053798389434815, val loss: 4.066298389434815, ETA in seconds: 1062506.380\n",
      "epoch: 980000, train loss: 4.0520405769348145, val loss: 4.071376514434815, ETA in seconds: 1057325.799\n",
      "epoch: 980100, train loss: 4.0530171394348145, val loss: 4.064149951934814, ETA in seconds: 1052141.788\n",
      "epoch: 980200, train loss: 4.047743701934815, val loss: 4.0647358894348145, ETA in seconds: 1046958.816\n",
      "epoch: 980300, train loss: 4.045399951934814, val loss: 4.068251514434815, ETA in seconds: 1041775.411\n",
      "epoch: 980400, train loss: 4.058485889434815, val loss: 4.0647358894348145, ETA in seconds: 1036589.786\n",
      "epoch: 980500, train loss: 4.051454639434814, val loss: 4.062001514434814, ETA in seconds: 1031403.307\n",
      "epoch: 980600, train loss: 4.054384326934814, val loss: 4.067274951934815, ETA in seconds: 1026213.733\n",
      "epoch: 980700, train loss: 4.050868701934815, val loss: 4.0637593269348145, ETA in seconds: 1021025.906\n",
      "epoch: 980800, train loss: 4.048915576934815, val loss: 4.067079639434814, ETA in seconds: 1015837.125\n",
      "epoch: 980900, train loss: 4.053603076934815, val loss: 4.067079639434814, ETA in seconds: 1010646.608\n",
      "epoch: 981000, train loss: 4.0452046394348145, val loss: 4.064345264434815, ETA in seconds: 1005454.291\n",
      "epoch: 981100, train loss: 4.054579639434815, val loss: 4.068446826934815, ETA in seconds: 1000259.777\n",
      "epoch: 981200, train loss: 4.0471577644348145, val loss: 4.0666890144348145, ETA in seconds: 995065.169\n",
      "epoch: 981300, train loss: 4.050282764434814, val loss: 4.071767139434814, ETA in seconds: 989872.451\n",
      "epoch: 981400, train loss: 4.050868701934815, val loss: 4.070204639434815, ETA in seconds: 984676.337\n",
      "epoch: 981500, train loss: 4.0530171394348145, val loss: 4.062587451934815, ETA in seconds: 979478.248\n",
      "epoch: 981600, train loss: 4.062001514434814, val loss: 4.070399951934815, ETA in seconds: 974283.085\n",
      "epoch: 981700, train loss: 4.043642139434814, val loss: 4.068251514434815, ETA in seconds: 969084.959\n",
      "epoch: 981800, train loss: 4.0569233894348145, val loss: 4.064149951934814, ETA in seconds: 963885.989\n",
      "epoch: 981900, train loss: 4.0510640144348145, val loss: 4.0696187019348145, ETA in seconds: 958684.859\n",
      "epoch: 982000, train loss: 4.0608296394348145, val loss: 4.068446826934815, ETA in seconds: 953482.896\n",
      "epoch: 982100, train loss: 4.058095264434814, val loss: 4.072743701934814, ETA in seconds: 948281.699\n",
      "epoch: 982200, train loss: 4.046962451934815, val loss: 4.066884326934814, ETA in seconds: 943076.806\n",
      "epoch: 982300, train loss: 4.0510640144348145, val loss: 4.070790576934814, ETA in seconds: 937875.072\n",
      "epoch: 982400, train loss: 4.052626514434815, val loss: 4.073915576934814, ETA in seconds: 932671.366\n",
      "epoch: 982500, train loss: 4.0442280769348145, val loss: 4.067274951934815, ETA in seconds: 927462.578\n",
      "epoch: 982600, train loss: 4.053407764434814, val loss: 4.068446826934815, ETA in seconds: 922255.625\n",
      "epoch: 982700, train loss: 4.0491108894348145, val loss: 4.063368701934815, ETA in seconds: 917044.918\n",
      "epoch: 982800, train loss: 4.0481343269348145, val loss: 4.066884326934814, ETA in seconds: 911835.128\n",
      "epoch: 982900, train loss: 4.045985889434815, val loss: 4.064149951934814, ETA in seconds: 906624.234\n",
      "epoch: 983000, train loss: 4.0588765144348145, val loss: 4.063564014434815, ETA in seconds: 901411.264\n",
      "epoch: 983100, train loss: 4.0520405769348145, val loss: 4.066298389434815, ETA in seconds: 896197.662\n",
      "epoch: 983200, train loss: 4.0481343269348145, val loss: 4.070399951934815, ETA in seconds: 890981.696\n",
      "epoch: 983300, train loss: 4.050282764434814, val loss: 4.062001514434814, ETA in seconds: 885766.100\n",
      "epoch: 983400, train loss: 4.057118701934814, val loss: 4.063368701934815, ETA in seconds: 880549.241\n",
      "epoch: 983500, train loss: 4.061024951934814, val loss: 4.066298389434815, ETA in seconds: 875331.157\n",
      "epoch: 983600, train loss: 4.052235889434814, val loss: 4.068837451934814, ETA in seconds: 870111.461\n",
      "epoch: 983700, train loss: 4.047548389434814, val loss: 4.066493701934815, ETA in seconds: 864884.491\n",
      "epoch: 983800, train loss: 4.053798389434815, val loss: 4.062001514434814, ETA in seconds: 859662.720\n",
      "epoch: 983900, train loss: 4.055165576934814, val loss: 4.0666890144348145, ETA in seconds: 854442.416\n",
      "epoch: 984000, train loss: 4.051454639434814, val loss: 4.069032764434814, ETA in seconds: 849224.323\n",
      "epoch: 984100, train loss: 4.045790576934815, val loss: 4.0696187019348145, ETA in seconds: 843999.701\n",
      "epoch: 984200, train loss: 4.048915576934815, val loss: 4.066493701934815, ETA in seconds: 838775.062\n",
      "epoch: 984300, train loss: 4.049696826934815, val loss: 4.069814014434814, ETA in seconds: 833548.893\n",
      "epoch: 984400, train loss: 4.050673389434815, val loss: 4.0725483894348145, ETA in seconds: 828321.885\n",
      "epoch: 984500, train loss: 4.045985889434815, val loss: 4.062196826934814, ETA in seconds: 823093.812\n",
      "epoch: 984600, train loss: 4.047353076934814, val loss: 4.074696826934814, ETA in seconds: 817863.908\n",
      "epoch: 984700, train loss: 4.053407764434814, val loss: 4.0666890144348145, ETA in seconds: 812634.120\n",
      "epoch: 984800, train loss: 4.050868701934815, val loss: 4.065907764434814, ETA in seconds: 807403.336\n",
      "epoch: 984900, train loss: 4.044032764434815, val loss: 4.0637593269348145, ETA in seconds: 802170.505\n",
      "epoch: 985000, train loss: 4.0432515144348145, val loss: 4.066103076934814, ETA in seconds: 796937.581\n",
      "epoch: 985100, train loss: 4.050673389434815, val loss: 4.069423389434815, ETA in seconds: 791702.940\n",
      "epoch: 985200, train loss: 4.054384326934814, val loss: 4.0657124519348145, ETA in seconds: 786468.490\n",
      "epoch: 985300, train loss: 4.048720264434815, val loss: 4.0657124519348145, ETA in seconds: 781232.337\n",
      "epoch: 985400, train loss: 4.057704639434815, val loss: 4.065517139434815, ETA in seconds: 775997.073\n",
      "epoch: 985500, train loss: 4.0481343269348145, val loss: 4.074110889434815, ETA in seconds: 770758.484\n",
      "epoch: 985600, train loss: 4.047743701934815, val loss: 4.0745015144348145, ETA in seconds: 765518.241\n",
      "epoch: 985700, train loss: 4.052235889434814, val loss: 4.0666890144348145, ETA in seconds: 760277.319\n",
      "epoch: 985800, train loss: 4.054774951934815, val loss: 4.066493701934815, ETA in seconds: 755035.659\n",
      "epoch: 985900, train loss: 4.057704639434815, val loss: 4.0696187019348145, ETA in seconds: 749793.514\n",
      "epoch: 986000, train loss: 4.056532764434815, val loss: 4.063173389434814, ETA in seconds: 744551.253\n",
      "epoch: 986100, train loss: 4.049892139434815, val loss: 4.062978076934814, ETA in seconds: 739306.789\n",
      "epoch: 986200, train loss: 4.0549702644348145, val loss: 4.070009326934814, ETA in seconds: 734058.913\n",
      "epoch: 986300, train loss: 4.0452046394348145, val loss: 4.0676655769348145, ETA in seconds: 728814.245\n",
      "epoch: 986400, train loss: 4.046962451934815, val loss: 4.066103076934814, ETA in seconds: 723567.869\n",
      "epoch: 986500, train loss: 4.055360889434814, val loss: 4.070399951934815, ETA in seconds: 718319.314\n",
      "epoch: 986600, train loss: 4.0549702644348145, val loss: 4.066884326934814, ETA in seconds: 713070.322\n",
      "epoch: 986700, train loss: 4.053603076934815, val loss: 4.073329639434815, ETA in seconds: 707819.152\n",
      "epoch: 986800, train loss: 4.049501514434814, val loss: 4.062392139434815, ETA in seconds: 702566.657\n",
      "epoch: 986900, train loss: 4.049892139434815, val loss: 4.0666890144348145, ETA in seconds: 697312.637\n",
      "epoch: 987000, train loss: 4.060439014434815, val loss: 4.066298389434815, ETA in seconds: 692060.289\n",
      "epoch: 987100, train loss: 4.048720264434815, val loss: 4.064540576934815, ETA in seconds: 686804.374\n",
      "epoch: 987200, train loss: 4.0510640144348145, val loss: 4.065517139434815, ETA in seconds: 681547.993\n",
      "epoch: 987300, train loss: 4.050868701934815, val loss: 4.063368701934815, ETA in seconds: 676289.956\n",
      "epoch: 987400, train loss: 4.052431201934814, val loss: 4.066298389434815, ETA in seconds: 671030.773\n",
      "epoch: 987500, train loss: 4.044618701934814, val loss: 4.063954639434814, ETA in seconds: 665769.847\n",
      "epoch: 987600, train loss: 4.0481343269348145, val loss: 4.070790576934814, ETA in seconds: 660509.454\n",
      "epoch: 987700, train loss: 4.055165576934814, val loss: 4.063954639434814, ETA in seconds: 655247.501\n",
      "epoch: 987800, train loss: 4.052235889434814, val loss: 4.068251514434815, ETA in seconds: 649983.989\n",
      "epoch: 987900, train loss: 4.050282764434814, val loss: 4.069228076934815, ETA in seconds: 644720.119\n",
      "epoch: 988000, train loss: 4.054579639434815, val loss: 4.074110889434815, ETA in seconds: 639454.459\n",
      "epoch: 988100, train loss: 4.051649951934815, val loss: 4.069423389434815, ETA in seconds: 634188.844\n",
      "epoch: 988200, train loss: 4.051259326934814, val loss: 4.062587451934815, ETA in seconds: 628922.657\n",
      "epoch: 988300, train loss: 4.052626514434815, val loss: 4.0666890144348145, ETA in seconds: 623653.963\n",
      "epoch: 988400, train loss: 4.055360889434814, val loss: 4.068251514434815, ETA in seconds: 618384.056\n",
      "epoch: 988500, train loss: 4.051259326934814, val loss: 4.067470264434815, ETA in seconds: 613114.145\n",
      "epoch: 988600, train loss: 4.054384326934814, val loss: 4.0686421394348145, ETA in seconds: 607843.539\n",
      "epoch: 988700, train loss: 4.0491108894348145, val loss: 4.067470264434815, ETA in seconds: 602570.517\n",
      "epoch: 988800, train loss: 4.044032764434815, val loss: 4.0696187019348145, ETA in seconds: 597297.701\n",
      "epoch: 988900, train loss: 4.051649951934815, val loss: 4.074306201934815, ETA in seconds: 592023.197\n",
      "epoch: 989000, train loss: 4.0569233894348145, val loss: 4.063368701934815, ETA in seconds: 586748.338\n",
      "epoch: 989100, train loss: 4.055360889434814, val loss: 4.065321826934815, ETA in seconds: 581472.805\n",
      "epoch: 989200, train loss: 4.048915576934815, val loss: 4.073915576934814, ETA in seconds: 576193.993\n",
      "epoch: 989300, train loss: 4.049696826934815, val loss: 4.0686421394348145, ETA in seconds: 570915.674\n",
      "epoch: 989400, train loss: 4.051649951934815, val loss: 4.062978076934814, ETA in seconds: 565636.466\n",
      "epoch: 989500, train loss: 4.060048389434814, val loss: 4.063564014434815, ETA in seconds: 560355.260\n",
      "epoch: 989600, train loss: 4.053212451934814, val loss: 4.069814014434814, ETA in seconds: 555073.408\n",
      "epoch: 989700, train loss: 4.0510640144348145, val loss: 4.070399951934815, ETA in seconds: 549791.068\n",
      "epoch: 989800, train loss: 4.053407764434814, val loss: 4.070985889434814, ETA in seconds: 544509.231\n",
      "epoch: 989900, train loss: 4.049892139434815, val loss: 4.069423389434815, ETA in seconds: 539224.589\n",
      "epoch: 990000, train loss: 4.053603076934815, val loss: 4.071181201934815, ETA in seconds: 533938.170\n",
      "epoch: 990100, train loss: 4.055360889434814, val loss: 4.064345264434815, ETA in seconds: 528650.473\n",
      "epoch: 990200, train loss: 4.051649951934815, val loss: 4.073329639434815, ETA in seconds: 523363.412\n",
      "epoch: 990300, train loss: 4.051845264434815, val loss: 4.065517139434815, ETA in seconds: 518074.852\n",
      "epoch: 990400, train loss: 4.055556201934815, val loss: 4.0686421394348145, ETA in seconds: 512785.090\n",
      "epoch: 990500, train loss: 4.049696826934815, val loss: 4.0715718269348145, ETA in seconds: 507493.883\n",
      "epoch: 990600, train loss: 4.049892139434815, val loss: 4.066884326934814, ETA in seconds: 502200.533\n",
      "epoch: 990700, train loss: 4.050282764434814, val loss: 4.0725483894348145, ETA in seconds: 496906.890\n",
      "epoch: 990800, train loss: 4.054579639434815, val loss: 4.066103076934814, ETA in seconds: 491612.214\n",
      "epoch: 990900, train loss: 4.053212451934814, val loss: 4.074892139434814, ETA in seconds: 486316.228\n",
      "epoch: 991000, train loss: 4.051454639434814, val loss: 4.068056201934814, ETA in seconds: 481019.823\n",
      "epoch: 991100, train loss: 4.050673389434815, val loss: 4.0696187019348145, ETA in seconds: 475722.001\n",
      "epoch: 991200, train loss: 4.0500874519348145, val loss: 4.066884326934814, ETA in seconds: 470423.272\n",
      "epoch: 991300, train loss: 4.050868701934815, val loss: 4.068837451934814, ETA in seconds: 465124.119\n",
      "epoch: 991400, train loss: 4.049306201934814, val loss: 4.0696187019348145, ETA in seconds: 459822.356\n",
      "epoch: 991500, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 454520.222\n",
      "epoch: 991600, train loss: 4.050478076934814, val loss: 4.065907764434814, ETA in seconds: 449217.613\n",
      "epoch: 991700, train loss: 4.0510640144348145, val loss: 4.0696187019348145, ETA in seconds: 443912.901\n",
      "epoch: 991800, train loss: 4.049696826934815, val loss: 4.067470264434815, ETA in seconds: 438607.440\n",
      "epoch: 991900, train loss: 4.058485889434815, val loss: 4.068446826934815, ETA in seconds: 433301.070\n",
      "epoch: 992000, train loss: 4.045985889434815, val loss: 4.067079639434814, ETA in seconds: 427993.375\n",
      "epoch: 992100, train loss: 4.055165576934814, val loss: 4.0647358894348145, ETA in seconds: 422684.660\n",
      "epoch: 992200, train loss: 4.0500874519348145, val loss: 4.066493701934815, ETA in seconds: 417375.471\n",
      "epoch: 992300, train loss: 4.058095264434814, val loss: 4.063173389434814, ETA in seconds: 412064.710\n",
      "epoch: 992400, train loss: 4.048720264434815, val loss: 4.0725483894348145, ETA in seconds: 406753.167\n",
      "epoch: 992500, train loss: 4.048915576934815, val loss: 4.060243701934814, ETA in seconds: 401440.199\n",
      "epoch: 992600, train loss: 4.051454639434814, val loss: 4.062978076934814, ETA in seconds: 396127.018\n",
      "epoch: 992700, train loss: 4.0569233894348145, val loss: 4.0686421394348145, ETA in seconds: 390812.211\n",
      "epoch: 992800, train loss: 4.050673389434815, val loss: 4.065907764434814, ETA in seconds: 385496.477\n",
      "epoch: 992900, train loss: 4.054189014434814, val loss: 4.065907764434814, ETA in seconds: 380179.790\n",
      "epoch: 993000, train loss: 4.052821826934815, val loss: 4.070985889434814, ETA in seconds: 374861.582\n",
      "epoch: 993100, train loss: 4.049696826934815, val loss: 4.071376514434815, ETA in seconds: 369542.071\n",
      "epoch: 993200, train loss: 4.052431201934814, val loss: 4.063368701934815, ETA in seconds: 364221.797\n",
      "epoch: 993300, train loss: 4.048720264434815, val loss: 4.0696187019348145, ETA in seconds: 358900.712\n",
      "epoch: 993400, train loss: 4.053603076934815, val loss: 4.067274951934815, ETA in seconds: 353578.952\n",
      "epoch: 993500, train loss: 4.051649951934815, val loss: 4.066103076934814, ETA in seconds: 348256.262\n",
      "epoch: 993600, train loss: 4.054579639434815, val loss: 4.073134326934815, ETA in seconds: 342931.288\n",
      "epoch: 993700, train loss: 4.053407764434814, val loss: 4.065907764434814, ETA in seconds: 337606.352\n",
      "epoch: 993800, train loss: 4.050673389434815, val loss: 4.065321826934815, ETA in seconds: 332279.871\n",
      "epoch: 993900, train loss: 4.047353076934814, val loss: 4.066884326934814, ETA in seconds: 326951.802\n",
      "epoch: 994000, train loss: 4.051454639434814, val loss: 4.067079639434814, ETA in seconds: 321623.023\n",
      "epoch: 994100, train loss: 4.048915576934815, val loss: 4.066493701934815, ETA in seconds: 316293.604\n",
      "epoch: 994200, train loss: 4.051259326934814, val loss: 4.070790576934814, ETA in seconds: 310962.784\n",
      "epoch: 994300, train loss: 4.049306201934814, val loss: 4.067860889434814, ETA in seconds: 305631.252\n",
      "epoch: 994400, train loss: 4.051845264434815, val loss: 4.068446826934815, ETA in seconds: 300298.656\n",
      "epoch: 994500, train loss: 4.0471577644348145, val loss: 4.072939014434814, ETA in seconds: 294964.984\n",
      "epoch: 994600, train loss: 4.052626514434815, val loss: 4.067274951934815, ETA in seconds: 289630.379\n",
      "epoch: 994700, train loss: 4.052821826934815, val loss: 4.064931201934814, ETA in seconds: 284294.952\n",
      "epoch: 994800, train loss: 4.058485889434815, val loss: 4.0647358894348145, ETA in seconds: 278957.818\n",
      "epoch: 994900, train loss: 4.051845264434815, val loss: 4.067079639434814, ETA in seconds: 273619.954\n",
      "epoch: 995000, train loss: 4.052235889434814, val loss: 4.0666890144348145, ETA in seconds: 268281.133\n",
      "epoch: 995100, train loss: 4.044423389434814, val loss: 4.0637593269348145, ETA in seconds: 262941.098\n",
      "epoch: 995200, train loss: 4.0520405769348145, val loss: 4.063173389434814, ETA in seconds: 257600.451\n",
      "epoch: 995300, train loss: 4.0500874519348145, val loss: 4.068837451934814, ETA in seconds: 252259.975\n",
      "epoch: 995400, train loss: 4.052431201934814, val loss: 4.070009326934814, ETA in seconds: 246916.699\n",
      "epoch: 995500, train loss: 4.052821826934815, val loss: 4.069032764434814, ETA in seconds: 241572.909\n",
      "epoch: 995600, train loss: 4.049696826934815, val loss: 4.071767139434814, ETA in seconds: 236228.103\n",
      "epoch: 995700, train loss: 4.052821826934815, val loss: 4.071376514434815, ETA in seconds: 230882.175\n",
      "epoch: 995800, train loss: 4.053212451934814, val loss: 4.065907764434814, ETA in seconds: 225534.819\n",
      "epoch: 995900, train loss: 4.047353076934814, val loss: 4.068056201934814, ETA in seconds: 220186.768\n",
      "epoch: 996000, train loss: 4.0500874519348145, val loss: 4.068056201934814, ETA in seconds: 214837.822\n",
      "epoch: 996100, train loss: 4.050673389434815, val loss: 4.0637593269348145, ETA in seconds: 209487.644\n",
      "epoch: 996200, train loss: 4.053212451934814, val loss: 4.069814014434814, ETA in seconds: 204136.217\n",
      "epoch: 996300, train loss: 4.049696826934815, val loss: 4.071767139434814, ETA in seconds: 198783.546\n",
      "epoch: 996400, train loss: 4.060243701934814, val loss: 4.0676655769348145, ETA in seconds: 193429.929\n",
      "epoch: 996500, train loss: 4.047353076934814, val loss: 4.065321826934815, ETA in seconds: 188074.772\n",
      "epoch: 996600, train loss: 4.051649951934815, val loss: 4.070790576934814, ETA in seconds: 182719.325\n",
      "epoch: 996700, train loss: 4.0539937019348145, val loss: 4.068251514434815, ETA in seconds: 177362.485\n",
      "epoch: 996800, train loss: 4.054384326934814, val loss: 4.070009326934814, ETA in seconds: 172004.937\n",
      "epoch: 996900, train loss: 4.0422749519348145, val loss: 4.066298389434815, ETA in seconds: 166646.272\n",
      "epoch: 997000, train loss: 4.050673389434815, val loss: 4.065907764434814, ETA in seconds: 161286.150\n",
      "epoch: 997100, train loss: 4.049306201934814, val loss: 4.063954639434814, ETA in seconds: 155925.321\n",
      "epoch: 997200, train loss: 4.049892139434815, val loss: 4.064931201934814, ETA in seconds: 150563.040\n",
      "epoch: 997300, train loss: 4.0510640144348145, val loss: 4.066103076934814, ETA in seconds: 145200.089\n",
      "epoch: 997400, train loss: 4.047548389434814, val loss: 4.0666890144348145, ETA in seconds: 139835.978\n",
      "epoch: 997500, train loss: 4.053407764434814, val loss: 4.062587451934815, ETA in seconds: 134470.913\n",
      "epoch: 997600, train loss: 4.051649951934815, val loss: 4.069423389434815, ETA in seconds: 129104.601\n",
      "epoch: 997700, train loss: 4.051649951934815, val loss: 4.070790576934814, ETA in seconds: 123737.186\n",
      "epoch: 997800, train loss: 4.051454639434814, val loss: 4.0715718269348145, ETA in seconds: 118368.938\n",
      "epoch: 997900, train loss: 4.0510640144348145, val loss: 4.066298389434815, ETA in seconds: 112999.605\n",
      "epoch: 998000, train loss: 4.050868701934815, val loss: 4.068251514434815, ETA in seconds: 107629.045\n",
      "epoch: 998100, train loss: 4.051649951934815, val loss: 4.0676655769348145, ETA in seconds: 102257.835\n",
      "epoch: 998200, train loss: 4.052626514434815, val loss: 4.067470264434815, ETA in seconds: 96885.270\n",
      "epoch: 998300, train loss: 4.046767139434815, val loss: 4.068446826934815, ETA in seconds: 91511.898\n",
      "epoch: 998400, train loss: 4.053212451934814, val loss: 4.0696187019348145, ETA in seconds: 86137.038\n",
      "epoch: 998500, train loss: 4.055165576934814, val loss: 4.068837451934814, ETA in seconds: 80761.410\n",
      "epoch: 998600, train loss: 4.048720264434815, val loss: 4.066103076934814, ETA in seconds: 75384.578\n",
      "epoch: 998700, train loss: 4.056142139434814, val loss: 4.066103076934814, ETA in seconds: 70006.819\n",
      "epoch: 998800, train loss: 4.053603076934815, val loss: 4.066493701934815, ETA in seconds: 64627.992\n",
      "epoch: 998900, train loss: 4.047939014434815, val loss: 4.068446826934815, ETA in seconds: 59248.079\n",
      "epoch: 999000, train loss: 4.047548389434814, val loss: 4.066298389434815, ETA in seconds: 53867.118\n",
      "epoch: 999100, train loss: 4.0520405769348145, val loss: 4.065126514434814, ETA in seconds: 48485.208\n",
      "epoch: 999200, train loss: 4.0520405769348145, val loss: 4.0705952644348145, ETA in seconds: 43102.122\n",
      "epoch: 999300, train loss: 4.055360889434814, val loss: 4.071962451934814, ETA in seconds: 37718.054\n",
      "epoch: 999400, train loss: 4.0471577644348145, val loss: 4.067470264434815, ETA in seconds: 32332.881\n",
      "epoch: 999500, train loss: 4.048915576934815, val loss: 4.065907764434814, ETA in seconds: 26946.746\n",
      "epoch: 999600, train loss: 4.062001514434814, val loss: 4.069228076934815, ETA in seconds: 21559.534\n",
      "epoch: 999700, train loss: 4.053407764434814, val loss: 4.0676655769348145, ETA in seconds: 16171.230\n",
      "epoch: 999800, train loss: 4.050282764434814, val loss: 4.063564014434815, ETA in seconds: 10781.872\n",
      "epoch: 999900, train loss: 4.0510640144348145, val loss: 4.067470264434815, ETA in seconds: 5391.460\n",
      "validation loss:  4.067470264434815\n"
     ]
    }
   ],
   "source": [
    "# GPT model\n",
    "## Param\n",
    "num_blocks = 2\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "ff_hidden_dim = embed_dim*4\n",
    "dropout = 0.\n",
    "\n",
    "lr = 3e-4\n",
    "\n",
    "model = GPT(max_seq_len=64,vocab_size=len(vocab),num_blocks=num_blocks,embed_dim=embed_dim,\n",
    "            num_heads=num_heads,ff_hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=lr,betas=(0.9,0.95),weight_decay=0.)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000,eta_min=lr*0.1)\n",
    "losses = train(model,optimizer,scheduler,print_logs=True,use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ke at the heaven with your stave',\n",
       "  'e at the heaven with your staves',\n",
       "  '                                '),\n",
       " ('with your staves as lift them\\nAg',\n",
       "  'ith your staves as lift them\\nAga',\n",
       "  '                                '),\n",
       " ('cracking ten thousand curbs\\nOf m',\n",
       "  'racking ten thousand curbs\\nOf mo',\n",
       "  '                                '),\n",
       " ('f more strong link asunder than ',\n",
       "  ' more strong link asunder than c',\n",
       "  '                                '),\n",
       " ('or your wants,\\nYour suffering in',\n",
       "  'r your wants,\\nYour suffering in ',\n",
       "  '                                '),\n",
       " ('hem\\nAgainst the Roman state, who',\n",
       "  'em\\nAgainst the Roman state, whos',\n",
       "  '                                '),\n",
       " (' your wants,\\nYour suffering in t',\n",
       "  'your wants,\\nYour suffering in th',\n",
       "  '                                '),\n",
       " ('ve the patricians of you. For yo',\n",
       "  'e the patricians of you. For you',\n",
       "  '                                '),\n",
       " ('iends, most charitable care\\nHave',\n",
       "  'ends, most charitable care\\nHave ',\n",
       "  '                                '),\n",
       " (' your staves as lift them\\nAgains',\n",
       "  'your staves as lift them\\nAgainst',\n",
       "  '                                '),\n",
       " ('he Roman state, whose course wil',\n",
       "  'e Roman state, whose course will',\n",
       "  '                                '),\n",
       " ('ants,\\nYour suffering in this dea',\n",
       "  'nts,\\nYour suffering in this dear',\n",
       "  '                                '),\n",
       " ('ans of you. For your wants,\\nYour',\n",
       "  'ns of you. For your wants,\\nYour ',\n",
       "  '                                '),\n",
       " ('uffering in this dearth, you may',\n",
       "  'ffering in this dearth, you may ',\n",
       "  '                                '),\n",
       " (' the Roman state, whose course w',\n",
       "  'the Roman state, whose course wi',\n",
       "  '                                '),\n",
       " ('staves as lift them\\nAgainst the ',\n",
       "  'taves as lift them\\nAgainst the R',\n",
       "  '                                ')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()\n",
    "xs,ys = get_batches(dataset,\"test\",batch_size=BATCH_SIZE,context_window=CONTEXT_SIZE)\n",
    "evaluate_text(model,xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ide o' the city\n",
      "is risen: why st\n",
      "                                                                 \n"
     ]
    }
   ],
   "source": [
    "x_train,_ = get_batches(dataset,\"train\",batch_size=1,context_window=CONTEXT_SIZE)\n",
    "x_val,_ = get_batches(dataset,\"val\",batch_size=1,context_window=CONTEXT_SIZE)\n",
    "\n",
    "print(decode(x_train[0].tolist()))\n",
    "#print(decode(x_val[0].tolist()))\n",
    "pred_sentence = model.generate(x_train,max_new_tokens=100)\n",
    "print(decode(pred_sentence.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_layer.weight tensor(6.6011e-05)\n",
      "embedding_layer.weight 0.01201923076923077\n",
      "blocks.0.mmhsa.query.weight tensor(0.0008)\n",
      "blocks.0.mmhsa.query.weight 0.15869140625\n",
      "blocks.0.mmhsa.query.bias tensor(6.4163e-05)\n",
      "blocks.0.mmhsa.query.bias 0.0\n",
      "blocks.0.mmhsa.key.weight tensor(0.0009)\n",
      "blocks.0.mmhsa.key.weight 0.09765625\n",
      "blocks.0.mmhsa.key.bias tensor(5.8326e-10)\n",
      "blocks.0.mmhsa.key.bias 0.0\n",
      "blocks.0.mmhsa.value.weight tensor(0.0057)\n",
      "blocks.0.mmhsa.value.weight 0.09765625\n",
      "blocks.0.mmhsa.value.bias tensor(0.0005)\n",
      "blocks.0.mmhsa.value.bias 0.0\n",
      "blocks.0.mmhsa.projection.0.weight tensor(0.0027)\n",
      "blocks.0.mmhsa.projection.0.weight 0.140380859375\n",
      "blocks.0.mmhsa.projection.0.bias tensor(0.0006)\n",
      "blocks.0.mmhsa.projection.0.bias 0.0\n",
      "blocks.0.ff.linear1.weight tensor(0.0050)\n",
      "blocks.0.ff.linear1.weight 0.09613037109375\n",
      "blocks.0.ff.linear1.bias tensor(0.0004)\n",
      "blocks.0.ff.linear1.bias 0.0\n",
      "blocks.0.ff.linear2.weight tensor(0.0054)\n",
      "blocks.0.ff.linear2.weight 0.2227783203125\n",
      "blocks.0.ff.linear2.bias tensor(0.0021)\n",
      "blocks.0.ff.linear2.bias 0.0\n",
      "blocks.0.norm1.weight tensor(0.0011)\n",
      "blocks.0.norm1.weight 0.0\n",
      "blocks.0.norm1.bias tensor(0.0011)\n",
      "blocks.0.norm1.bias 10.15625\n",
      "blocks.0.norm2.weight tensor(0.0006)\n",
      "blocks.0.norm2.weight 0.0\n",
      "blocks.0.norm2.bias tensor(0.0005)\n",
      "blocks.0.norm2.bias 9.375\n",
      "blocks.1.mmhsa.query.weight tensor(0.0006)\n",
      "blocks.1.mmhsa.query.weight 0.146484375\n",
      "blocks.1.mmhsa.query.bias tensor(5.1928e-05)\n",
      "blocks.1.mmhsa.query.bias 0.0\n",
      "blocks.1.mmhsa.key.weight tensor(0.0006)\n",
      "blocks.1.mmhsa.key.weight 0.164794921875\n",
      "blocks.1.mmhsa.key.bias tensor(3.3485e-10)\n",
      "blocks.1.mmhsa.key.bias 0.0\n",
      "blocks.1.mmhsa.value.weight tensor(0.0029)\n",
      "blocks.1.mmhsa.value.weight 0.1220703125\n",
      "blocks.1.mmhsa.value.bias tensor(0.0002)\n",
      "blocks.1.mmhsa.value.bias 0.0\n",
      "blocks.1.mmhsa.projection.0.weight tensor(0.0016)\n",
      "blocks.1.mmhsa.projection.0.weight 0.103759765625\n",
      "blocks.1.mmhsa.projection.0.bias tensor(0.0003)\n",
      "blocks.1.mmhsa.projection.0.bias 1.5625\n",
      "blocks.1.ff.linear1.weight tensor(0.0025)\n",
      "blocks.1.ff.linear1.weight 0.1190185546875\n",
      "blocks.1.ff.linear1.bias tensor(0.0002)\n",
      "blocks.1.ff.linear1.bias 0.390625\n",
      "blocks.1.ff.linear2.weight tensor(0.0021)\n",
      "blocks.1.ff.linear2.weight 0.24566650390625\n",
      "blocks.1.ff.linear2.bias tensor(0.0008)\n",
      "blocks.1.ff.linear2.bias 0.0\n",
      "blocks.1.norm1.weight tensor(0.0005)\n",
      "blocks.1.norm1.weight 0.0\n",
      "blocks.1.norm1.bias tensor(0.0005)\n",
      "blocks.1.norm1.bias 7.8125\n",
      "blocks.1.norm2.weight tensor(5.5411e-05)\n",
      "blocks.1.norm2.weight 0.0\n",
      "blocks.1.norm2.bias tensor(0.0001)\n",
      "blocks.1.norm2.bias 13.28125\n",
      "head.weight tensor(0.0007)\n",
      "head.weight 0.07211538461538461\n",
      "head.bias tensor(5.8307e-05)\n",
      "head.bias 0.0\n"
     ]
    }
   ],
   "source": [
    "# 学習が上手くいってない？ので勾配を確認する\n",
    "def show_grads(model,tol=1e-4):\n",
    "    for name,param in model.named_parameters():\n",
    "        value = 100.0 * float(torch.sum(torch.abs(param) <= tol)) / float(param.nelement())\n",
    "        print(name,param.grad.abs().mean())\n",
    "        print(name,value)\n",
    "        # if param.grad is not None:\n",
    "        #     if param.grad.abs().mean() > tol:\n",
    "        #         print(name,param.grad.abs().mean())\n",
    "\n",
    "show_grads(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tinyshakespeare.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: BPE\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 3\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: tinyshakespeare.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=64\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 25670\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24133 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6114 size=20 all=1649 active=1585 piece=it\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3906 size=40 all=2220 active=2156 piece=st\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2392 size=60 all=2991 active=2927 piece=ld\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1864 size=80 all=3604 active=3540 piece=ke\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1429 size=100 all=4152 active=4088 piece=▁his\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1396 min_freq=99\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1253 size=120 all=4718 active=1565 piece=al\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=990 size=140 all=5343 active=2190 piece=ess\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=828 size=160 all=5858 active=2705 piece=▁To\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=717 size=180 all=6278 active=3125 piece=▁are\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=621 size=200 all=6799 active=3646 piece=▁up\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=614 min_freq=91\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=567 size=220 all=7173 active=1366 piece=▁con\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=510 size=240 all=7591 active=1784 piece=▁was\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=478 size=260 all=7897 active=2090 piece=ind\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=451 size=280 all=8260 active=2453 piece=▁br\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=395 size=300 all=8641 active=2834 piece=fore\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=394 min_freq=80\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=359 size=320 all=8934 active=1288 piece=ak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=334 size=340 all=9213 active=1567 piece=▁had\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301 size=360 all=9513 active=1867 piece=▁speak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=286 size=380 all=9783 active=2137 piece=▁heart\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=270 size=400 all=10004 active=2358 piece=▁In\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=270 min_freq=68\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=255 size=420 all=10219 active=1204 piece=oy\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n",
      "60 all=5858 active=2705 piece=▁To\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=717 size=180 all=6278 active=3125 piece=▁are\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=621 size=200 all=6799 active=3646 piece=▁up\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=614 min_freq=91\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=567 size=220 all=7173 active=1366 piece=▁con\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=510 size=240 all=7591 active=1784 piece=▁was\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=478 size=260 all=7897 active=2090 piece=ind\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=451 size=280 all=8260 active=2453 piece=▁br\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=395 size=300 all=8641 active=2834 piece=fore\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=394 min_freq=80\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=359 size=320 all=8934 active=1288 piece=ak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=334 size=340 all=9213 active=1567 piece=▁had\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=301 size=360 all=9513 active=1867 piece=▁speak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=286 size=380 all=9783 active=2137 piece=▁heart\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=270 size=400 all=10004 active=2358 piece=▁In\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=270 min_freq=68\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=255 size=420 all=10219 active=1204 piece=oy\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "# sentence piece tokenizer\n",
    "import sentencepiece as spm\n",
    "#spm.SentencePieceTrainer.Train('--input=tinyshakespeare.txt --model_prefix=m --vocab_size=500 --character_coverage=1.0 --model_type=bpe')\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='tinyshakespeare.txt',\n",
    "    model_prefix='m',\n",
    "    vocab_size=500,\n",
    "    character_coverage=1.0,\n",
    "    model_type='bpe',\n",
    "    pad_id=3,\n",
    "    pad_piece='<pad>',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "<unk>\n",
      "<s>\n",
      "</s>\n",
      "<pad>\n",
      "▁t\n",
      "he\n",
      "▁a\n",
      "ou\n",
      "▁s\n",
      "▁m\n",
      "▁w\n",
      "in\n",
      "re\n",
      "ha\n",
      "▁the\n",
      "nd\n",
      "▁b\n",
      "is\n",
      "or\n",
      "▁f\n",
      "▁I\n",
      "er\n",
      "ll\n",
      "it\n",
      "on\n",
      "▁d\n",
      "▁c\n",
      "▁n\n",
      "▁l\n",
      "▁y\n",
      "es\n",
      "en\n",
      "▁th\n",
      "ar\n",
      "▁h\n",
      "▁o\n",
      "▁to\n",
      "▁you\n",
      "▁p\n",
      "▁T\n",
      "hat\n",
      "▁A\n",
      "▁he\n",
      "st\n",
      "ve\n",
      "ot\n",
      "▁and\n",
      "ow\n",
      "ing\n",
      "▁of\n",
      "an\n",
      "om\n",
      "▁g\n",
      "at\n",
      "▁be\n",
      "▁W\n",
      "se\n",
      "▁my\n",
      "▁in\n",
      "▁ha\n",
      "ce\n",
      "le\n",
      "ay\n",
      "ld\n",
      "ir\n",
      "et\n",
      "ed\n",
      "ut\n",
      "▁B\n",
      "▁me\n",
      "im\n",
      "▁S\n",
      "ith\n",
      "▁not\n",
      "▁H\n",
      "ch\n",
      "▁that\n",
      "▁is\n",
      "▁M\n",
      "gh\n",
      "▁And\n",
      "▁for\n",
      "▁u\n",
      "ke\n",
      "▁C\n",
      "▁we\n",
      "our\n",
      "oo\n",
      "ill\n",
      "▁e\n",
      "▁with\n",
      "her\n",
      "▁it\n",
      "ent\n",
      "▁your\n",
      "ad\n",
      "ri\n",
      "▁O\n",
      "▁thou\n",
      "▁st\n",
      "▁k\n",
      "▁L\n",
      "ome\n",
      "▁his\n",
      "▁F\n",
      "▁G\n",
      "ght\n",
      "EN\n",
      "ord\n",
      "▁re\n",
      "id\n",
      "ra\n",
      "▁The\n",
      "▁have\n",
      "▁him\n",
      "IN\n",
      "ly\n",
      "▁li\n",
      "as\n",
      "▁P\n",
      "▁this\n",
      "ur\n",
      "IO\n",
      "al\n",
      "▁so\n",
      "▁as\n",
      "▁de\n",
      "▁on\n",
      "▁R\n",
      "ore\n",
      "ro\n",
      "▁N\n",
      "hi\n",
      "ould\n",
      "ood\n",
      "AR\n",
      "ck\n",
      "ain\n",
      "ver\n",
      "▁Y\n",
      "est\n",
      "▁sha\n",
      "▁thy\n",
      "ess\n",
      "▁will\n",
      "▁do\n",
      "ea\n",
      "▁no\n",
      "am\n",
      "▁E\n",
      "▁but\n",
      "▁D\n",
      "us\n",
      "▁se\n",
      "US\n",
      "if\n",
      "▁'\n",
      "ge\n",
      "▁all\n",
      "and\n",
      "▁Th\n",
      "▁su\n",
      "ake\n",
      "▁To\n",
      "▁her\n",
      "ru\n",
      "ion\n",
      "▁an\n",
      "▁K\n",
      "▁lo\n",
      "ard\n",
      "ter\n",
      "han\n",
      "▁sp\n",
      "ell\n",
      "ear\n",
      "▁thee\n",
      "▁fa\n",
      "▁shall\n",
      "▁our\n",
      "▁by\n",
      "th\n",
      "UC\n",
      "▁are\n",
      "il\n",
      "ING\n",
      "▁ne\n",
      "▁kn\n",
      "rom\n",
      "ho\n",
      "▁v\n",
      "▁That\n",
      "ER\n",
      "OR\n",
      "ast\n",
      "ct\n",
      "ous\n",
      "▁what\n",
      "▁sh\n",
      "ight\n",
      "ul\n",
      "ET\n",
      "ant\n",
      "▁up\n",
      "sel\n",
      "▁good\n",
      "qu\n",
      "▁But\n",
      "art\n",
      "row\n",
      "ath\n",
      "ine\n",
      "▁com\n",
      "▁mu\n",
      "▁lord\n",
      "hich\n",
      "nt\n",
      "▁pr\n",
      "▁man\n",
      "▁at\n",
      "▁V\n",
      "one\n",
      "▁whe\n",
      "▁con\n",
      "▁What\n",
      "▁am\n",
      "end\n",
      "ic\n",
      "ES\n",
      "ble\n",
      "ry\n",
      "ong\n",
      "▁from\n",
      "ie\n",
      "▁she\n",
      "▁bl\n",
      "ive\n",
      "ven\n",
      "▁go\n",
      "▁For\n",
      "▁more\n",
      "▁them\n",
      "em\n",
      "▁was\n",
      "IC\n",
      "out\n",
      "au\n",
      "▁sir\n",
      "other\n",
      "are\n",
      "oth\n",
      "ol\n",
      "▁if\n",
      "▁He\n",
      "▁now\n",
      "▁hat\n",
      "▁there\n",
      "▁would\n",
      "AN\n",
      "ost\n",
      "▁know\n",
      "▁can\n",
      "LO\n",
      "ind\n",
      "ers\n",
      "IUS\n",
      "ep\n",
      "self\n",
      "ARD\n",
      "▁say\n",
      "▁KING\n",
      "ather\n",
      "ond\n",
      "ate\n",
      "res\n",
      "fe\n",
      "▁sw\n",
      "▁here\n",
      "▁love\n",
      "▁their\n",
      "▁My\n",
      "▁or\n",
      "▁than\n",
      "▁br\n",
      "pp\n",
      "--\n",
      "▁us\n",
      "▁king\n",
      "▁they\n",
      "▁then\n",
      "all\n",
      "▁ar\n",
      "od\n",
      "▁let\n",
      "▁un\n",
      "ig\n",
      "▁wor\n",
      "ure\n",
      "ink\n",
      "▁may\n",
      "hy\n",
      "ame\n",
      "▁come\n",
      "fore\n",
      "ort\n",
      "▁As\n",
      "▁qu\n",
      "ook\n",
      "▁well\n",
      "el\n",
      "ish\n",
      "LI\n",
      "▁Whe\n",
      "▁j\n",
      "KE\n",
      "ves\n",
      "▁one\n",
      "▁hath\n",
      "irst\n",
      "▁make\n",
      "▁You\n",
      "▁gra\n",
      "reat\n",
      "ak\n",
      "ng\n",
      "▁must\n",
      "gain\n",
      "ound\n",
      "▁were\n",
      "▁ho\n",
      "▁see\n",
      "▁should\n",
      "▁like\n",
      "ci\n",
      "▁pro\n",
      "ue\n",
      "eak\n",
      "TIO\n",
      "▁sa\n",
      "ity\n",
      "▁pl\n",
      "▁mad\n",
      "▁father\n",
      "▁had\n",
      "um\n",
      "▁did\n",
      "▁upon\n",
      "OM\n",
      "ime\n",
      "pe\n",
      "▁too\n",
      "ice\n",
      "ON\n",
      "▁J\n",
      "▁dis\n",
      "ence\n",
      "▁death\n",
      "ward\n",
      "▁which\n",
      "▁en\n",
      "▁If\n",
      "ose\n",
      "own\n",
      "▁speak\n",
      "▁po\n",
      "▁when\n",
      "ful\n",
      "▁out\n",
      "▁fri\n",
      "▁how\n",
      "entle\n",
      "io\n",
      "pt\n",
      "un\n",
      "ICH\n",
      "▁again\n",
      "▁RICH\n",
      "▁tru\n",
      "OL\n",
      "de\n",
      "UKE\n",
      "▁DUKE\n",
      "man\n",
      "▁heart\n",
      "▁yet\n",
      "ick\n",
      "▁hand\n",
      "very\n",
      "▁With\n",
      "▁RICHARD\n",
      "▁who\n",
      "ENTIO\n",
      "▁Thou\n",
      "ign\n",
      "▁We\n",
      "▁some\n",
      "ance\n",
      "▁Which\n",
      "▁hon\n",
      "▁mar\n",
      "nce\n",
      "▁r\n",
      "ies\n",
      "▁In\n",
      "▁son\n",
      "ire\n",
      "▁off\n",
      "▁mine\n",
      "ist\n",
      "INC\n",
      "▁al\n",
      "ince\n",
      "har\n",
      "orn\n",
      "ack\n",
      "▁Why\n",
      "wn\n",
      "▁Of\n",
      "▁God\n",
      "EL\n",
      "▁hea\n",
      "▁such\n",
      "▁time\n",
      "oy\n",
      "▁First\n",
      "▁hear\n",
      "▁So\n",
      "ness\n",
      "▁brother\n",
      "ec\n",
      "▁these\n",
      "▁Cl\n",
      "▁blood\n",
      "▁ab\n",
      "▁ex\n",
      "ither\n",
      "▁\n",
      "e\n",
      "t\n",
      "o\n",
      "a\n",
      "h\n",
      "s\n",
      "r\n",
      "n\n",
      "i\n",
      "l\n",
      "d\n",
      "u\n",
      "m\n",
      "y\n",
      ",\n",
      "w\n",
      "f\n",
      "c\n",
      "g\n",
      "I\n",
      "b\n",
      "p\n",
      ":\n",
      ".\n",
      "A\n",
      "v\n",
      "k\n",
      "T\n",
      "'\n",
      "E\n",
      "O\n",
      "N\n",
      "R\n",
      "S\n",
      "L\n",
      "C\n",
      ";\n",
      "W\n",
      "U\n",
      "H\n",
      "M\n",
      "B\n",
      "?\n",
      "G\n",
      "!\n",
      "D\n",
      "-\n",
      "F\n",
      "Y\n",
      "P\n",
      "K\n",
      "V\n",
      "j\n",
      "q\n",
      "x\n",
      "z\n",
      "J\n",
      "Q\n",
      "Z\n",
      "X\n",
      "3\n",
      "&\n",
      "$\n"
     ]
    }
   ],
   "source": [
    "# vocabの確認\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"m.model\")\n",
    "print(sp.GetPieceSize())\n",
    "for i in range(sp.GetPieceSize()):\n",
    "    print(sp.IdToPiece(i))\n",
    "#print(sp.IdToPiece(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115393\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "['▁First', '▁C', 'it', 'i', 'z', 'en', ':', '▁B', 'e', 'fore', '▁we', '▁pro', 'ce', 'ed', '▁an', 'y', '▁f', 'ur', 't', 'her', ',', '▁hear', '▁me', '▁speak', '.', '▁A', 'll', ':', '▁S', 'p', 'eak', ',', '▁speak', '.', '▁First', '▁C', 'it', 'i', 'z', 'en', ':', '▁You']\n"
     ]
    }
   ],
   "source": [
    "# tinyshakespeare.txtを読み込んで、sentence pieceでtokenizeする\n",
    "lines = open(path,\"r\").read()\n",
    "print(len(lines))\n",
    "print(lines[:100])\n",
    "print(sp.encode(lines[:100],out_type=str))\n",
    "a = sp.encode(lines)\n",
    "dataset = torch.tensor(a,dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([424,  84,  23,  ..., 323,  48, 460], dtype=torch.int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "torch.Size([3, 10])\n",
      "tensor([[175, 366,  26, 172, 157, 473, 128, 445,  75,   9],\n",
      "        [142,  10, 155, 437, 481,  84, 193, 122, 471, 258],\n",
      "        [451, 358, 288, 269,  77,  42,  63,  68, 450,  68]])\n",
      "tensor([[366,  26, 172, 157, 473, 128, 445,  75,   9,  31],\n",
      "        [ 10, 155, 437, 481,  84, 193, 122, 471, 258, 154],\n",
      "        [358, 288, 269,  77,  42,  63,  68, 450,  68, 152]])\n"
     ]
    }
   ],
   "source": [
    "def get_batches(data,split,batch_size,context_window):\n",
    "    train = data[:int(.8*len(data))]\n",
    "    val = data[int(.8*len(data)):int(.9*len(data))]\n",
    "    test = data[int(.9*len(data)):]\n",
    "\n",
    "    if split == \"val\":\n",
    "        batch_data = val\n",
    "    elif split == \"test\":\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = train\n",
    "    \n",
    "    # pick random starting points\n",
    "    sp = torch.randint(0,batch_data.size(0) - context_window - 1,(batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in sp]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in sp]).long()\n",
    "\n",
    "    return x,y\n",
    "\n",
    "xs,ys = get_batches(dataset,\"train\",batch_size=3,context_window=10)\n",
    "print(xs.shape)\n",
    "print(ys.shape)\n",
    "print(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset[:10000]\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 6.217187309265137, val loss: 6.2160790920257565, ETA in seconds: 10.284\n",
      "epoch: 10, train loss: 6.21271858215332, val loss: 6.211899185180664, ETA in seconds: 104.569\n",
      "epoch: 20, train loss: 6.209840202331543, val loss: 6.20477032661438, ETA in seconds: 199.331\n",
      "epoch: 30, train loss: 6.20983510017395, val loss: 6.20358567237854, ETA in seconds: 284.687\n",
      "epoch: 40, train loss: 6.209444189071656, val loss: 6.203194618225098, ETA in seconds: 372.679\n",
      "epoch: 50, train loss: 6.207295656204224, val loss: 6.201241111755371, ETA in seconds: 459.677\n",
      "epoch: 60, train loss: 6.206318950653076, val loss: 6.203389310836792, ETA in seconds: 549.121\n",
      "epoch: 70, train loss: 6.20944390296936, val loss: 6.202217531204224, ETA in seconds: 637.461\n",
      "epoch: 80, train loss: 6.21022515296936, val loss: 6.203389263153076, ETA in seconds: 728.113\n",
      "epoch: 90, train loss: 6.209248733520508, val loss: 6.20553765296936, ETA in seconds: 816.377\n",
      "epoch: 100, train loss: 6.208272123336792, val loss: 6.205733108520508, ETA in seconds: 905.119\n",
      "epoch: 110, train loss: 6.20905327796936, val loss: 6.2047566890716555, ETA in seconds: 994.376\n",
      "epoch: 120, train loss: 6.209053325653076, val loss: 6.200459766387939, ETA in seconds: 1079.596\n",
      "epoch: 130, train loss: 6.21120171546936, val loss: 6.207686233520508, ETA in seconds: 1164.185\n",
      "epoch: 140, train loss: 6.207881450653076, val loss: 6.2053422927856445, ETA in seconds: 1247.930\n",
      "epoch: 150, train loss: 6.2098345279693605, val loss: 6.204365921020508, ETA in seconds: 1336.236\n",
      "epoch: 160, train loss: 6.21022515296936, val loss: 6.204170608520508, ETA in seconds: 1422.152\n",
      "epoch: 170, train loss: 6.208076667785645, val loss: 6.205342578887939, ETA in seconds: 1505.716\n",
      "epoch: 180, train loss: 6.208662796020508, val loss: 6.205928325653076, ETA in seconds: 1591.239\n",
      "epoch: 190, train loss: 6.20670976638794, val loss: 6.203194046020508, ETA in seconds: 1680.395\n",
      "epoch: 200, train loss: 6.2108110904693605, val loss: 6.205147218704224, ETA in seconds: 1763.512\n",
      "epoch: 210, train loss: 6.207490730285644, val loss: 6.2029987335205075, ETA in seconds: 1846.322\n",
      "epoch: 220, train loss: 6.20905327796936, val loss: 6.203389263153076, ETA in seconds: 1929.011\n",
      "epoch: 230, train loss: 6.208662605285644, val loss: 6.2033891677856445, ETA in seconds: 2009.676\n",
      "epoch: 240, train loss: 6.2102251052856445, val loss: 6.201045560836792, ETA in seconds: 2093.871\n",
      "epoch: 250, train loss: 6.207295513153076, val loss: 6.203779983520508, ETA in seconds: 2178.445\n",
      "epoch: 260, train loss: 6.208076810836792, val loss: 6.206318998336792, ETA in seconds: 2260.705\n",
      "epoch: 270, train loss: 6.2088579654693605, val loss: 6.204756450653076, ETA in seconds: 2342.754\n",
      "epoch: 280, train loss: 6.209834575653076, val loss: 6.203779840469361, ETA in seconds: 2425.873\n",
      "epoch: 290, train loss: 6.208467292785644, val loss: 6.2035847187042235, ETA in seconds: 2508.516\n",
      "epoch: 300, train loss: 6.209834575653076, val loss: 6.20299882888794, ETA in seconds: 2590.084\n",
      "epoch: 310, train loss: 6.211201763153076, val loss: 6.206514310836792, ETA in seconds: 2673.454\n",
      "epoch: 320, train loss: 6.208272123336792, val loss: 6.203975248336792, ETA in seconds: 2756.142\n",
      "epoch: 330, train loss: 6.210029888153076, val loss: 6.205537748336792, ETA in seconds: 2841.283\n",
      "epoch: 340, train loss: 6.210420417785644, val loss: 6.204170656204224, ETA in seconds: 2924.675\n",
      "epoch: 350, train loss: 6.210615825653076, val loss: 6.20729546546936, ETA in seconds: 3004.648\n",
      "epoch: 360, train loss: 6.211396980285644, val loss: 6.205928325653076, ETA in seconds: 3088.102\n",
      "epoch: 370, train loss: 6.208662748336792, val loss: 6.205537748336792, ETA in seconds: 3168.043\n",
      "epoch: 380, train loss: 6.209834575653076, val loss: 6.204365968704224, ETA in seconds: 3251.541\n",
      "epoch: 390, train loss: 6.209639167785644, val loss: 6.2029987335205075, ETA in seconds: 3331.216\n",
      "epoch: 400, train loss: 6.20710015296936, val loss: 6.2008504390716555, ETA in seconds: 3411.985\n",
      "epoch: 410, train loss: 6.2074908256530765, val loss: 6.201240968704224, ETA in seconds: 3492.770\n",
      "epoch: 420, train loss: 6.211006450653076, val loss: 6.204365825653076, ETA in seconds: 3571.971\n",
      "epoch: 430, train loss: 6.212373542785644, val loss: 6.203779983520508, ETA in seconds: 3651.786\n",
      "epoch: 440, train loss: 6.209443855285644, val loss: 6.202217531204224, ETA in seconds: 3731.814\n",
      "epoch: 450, train loss: 6.20729546546936, val loss: 6.20456109046936, ETA in seconds: 3810.840\n",
      "epoch: 460, train loss: 6.211006450653076, val loss: 6.2039752960205075, ETA in seconds: 3891.894\n",
      "epoch: 470, train loss: 6.208858013153076, val loss: 6.2026080131530765, ETA in seconds: 3968.916\n",
      "epoch: 480, train loss: 6.210225200653076, val loss: 6.206123685836792, ETA in seconds: 4046.177\n",
      "epoch: 490, train loss: 6.209443855285644, val loss: 6.202608108520508, ETA in seconds: 4125.780\n",
      "epoch: 500, train loss: 6.2074908256530765, val loss: 6.204951906204224, ETA in seconds: 4204.703\n",
      "epoch: 510, train loss: 6.208858013153076, val loss: 6.205342388153076, ETA in seconds: 4283.974\n",
      "epoch: 520, train loss: 6.208272123336792, val loss: 6.2045612812042235, ETA in seconds: 4371.105\n",
      "epoch: 530, train loss: 6.209053373336792, val loss: 6.202998685836792, ETA in seconds: 4452.736\n",
      "epoch: 540, train loss: 6.20807671546936, val loss: 6.203975248336792, ETA in seconds: 4538.168\n",
      "epoch: 550, train loss: 6.210811042785645, val loss: 6.205733060836792, ETA in seconds: 4615.898\n",
      "epoch: 560, train loss: 6.209053325653076, val loss: 6.205342435836792, ETA in seconds: 4692.356\n",
      "epoch: 570, train loss: 6.20827202796936, val loss: 6.20710015296936, ETA in seconds: 4772.703\n",
      "epoch: 580, train loss: 6.20846734046936, val loss: 6.204756546020508, ETA in seconds: 4854.549\n",
      "epoch: 590, train loss: 6.209248685836792, val loss: 6.202998685836792, ETA in seconds: 4936.567\n",
      "epoch: 600, train loss: 6.208662652969361, val loss: 6.203389263153076, ETA in seconds: 5014.602\n",
      "epoch: 610, train loss: 6.206709623336792, val loss: 6.20534234046936, ETA in seconds: 5091.984\n",
      "epoch: 620, train loss: 6.2078814029693605, val loss: 6.202608108520508, ETA in seconds: 5169.227\n",
      "epoch: 630, train loss: 6.207881498336792, val loss: 6.203389406204224, ETA in seconds: 5250.159\n",
      "epoch: 640, train loss: 6.210029792785645, val loss: 6.2039751529693605, ETA in seconds: 5335.468\n",
      "epoch: 650, train loss: 6.2104205131530765, val loss: 6.204951763153076, ETA in seconds: 5420.195\n",
      "epoch: 660, train loss: 6.208272123336792, val loss: 6.20221734046936, ETA in seconds: 5501.858\n",
      "epoch: 670, train loss: 6.2112016677856445, val loss: 6.20651421546936, ETA in seconds: 5576.616\n",
      "epoch: 680, train loss: 6.210811042785645, val loss: 6.2035847187042235, ETA in seconds: 5650.930\n",
      "epoch: 690, train loss: 6.210029792785645, val loss: 6.204366016387939, ETA in seconds: 5723.944\n",
      "epoch: 700, train loss: 6.21002984046936, val loss: 6.203389310836792, ETA in seconds: 5797.387\n",
      "epoch: 710, train loss: 6.209834575653076, val loss: 6.202412700653076, ETA in seconds: 5873.484\n",
      "epoch: 720, train loss: 6.210029792785645, val loss: 6.203389263153076, ETA in seconds: 5947.704\n",
      "epoch: 730, train loss: 6.207490873336792, val loss: 6.2055378437042235, ETA in seconds: 6021.359\n",
      "epoch: 740, train loss: 6.2088579654693605, val loss: 6.203584671020508, ETA in seconds: 6094.505\n",
      "epoch: 750, train loss: 6.206904888153076, val loss: 6.2029987335205075, ETA in seconds: 6167.996\n",
      "epoch: 760, train loss: 6.21100640296936, val loss: 6.203193950653076, ETA in seconds: 6241.825\n",
      "epoch: 770, train loss: 6.210615777969361, val loss: 6.204365921020508, ETA in seconds: 6324.029\n",
      "epoch: 780, train loss: 6.208662796020508, val loss: 6.205733156204223, ETA in seconds: 6405.163\n",
      "epoch: 790, train loss: 6.207490730285644, val loss: 6.204951763153076, ETA in seconds: 6486.468\n",
      "epoch: 800, train loss: 6.2088579654693605, val loss: 6.201827049255371, ETA in seconds: 6567.969\n",
      "epoch: 810, train loss: 6.2078814029693605, val loss: 6.207295608520508, ETA in seconds: 6649.419\n",
      "epoch: 820, train loss: 6.209639263153076, val loss: 6.2049518585205075, ETA in seconds: 6733.375\n",
      "epoch: 830, train loss: 6.208662605285644, val loss: 6.203975248336792, ETA in seconds: 6810.219\n",
      "epoch: 840, train loss: 6.208662700653076, val loss: 6.2039751529693605, ETA in seconds: 6886.086\n",
      "epoch: 850, train loss: 6.207881355285645, val loss: 6.200264263153076, ETA in seconds: 6961.776\n",
      "epoch: 860, train loss: 6.2072954177856445, val loss: 6.20299882888794, ETA in seconds: 7036.516\n",
      "epoch: 870, train loss: 6.207881450653076, val loss: 6.2029987335205075, ETA in seconds: 7112.515\n",
      "epoch: 880, train loss: 6.2076863765716555, val loss: 6.204170656204224, ETA in seconds: 7187.044\n",
      "epoch: 890, train loss: 6.20905327796936, val loss: 6.203584671020508, ETA in seconds: 7261.576\n",
      "epoch: 900, train loss: 6.2082719802856445, val loss: 6.205733013153076, ETA in seconds: 7335.637\n",
      "epoch: 910, train loss: 6.209053373336792, val loss: 6.2029987335205075, ETA in seconds: 7409.896\n",
      "epoch: 920, train loss: 6.207686090469361, val loss: 6.2045612812042235, ETA in seconds: 7485.288\n",
      "epoch: 930, train loss: 6.208662605285644, val loss: 6.205147171020508, ETA in seconds: 7558.952\n",
      "epoch: 940, train loss: 6.209443855285644, val loss: 6.203194046020508, ETA in seconds: 7632.678\n",
      "epoch: 950, train loss: 6.209639215469361, val loss: 6.2041707038879395, ETA in seconds: 7705.887\n",
      "epoch: 960, train loss: 6.206709527969361, val loss: 6.202803277969361, ETA in seconds: 7780.735\n",
      "epoch: 970, train loss: 6.209443855285644, val loss: 6.202217435836792, ETA in seconds: 7850.020\n",
      "epoch: 980, train loss: 6.209639215469361, val loss: 6.202412796020508, ETA in seconds: 7921.194\n",
      "epoch: 990, train loss: 6.209443998336792, val loss: 6.203779935836792, ETA in seconds: 7990.418\n",
      "epoch: 1000, train loss: 6.207686042785644, val loss: 6.204170513153076, ETA in seconds: 8059.618\n",
      "epoch: 1010, train loss: 6.209443998336792, val loss: 6.203975248336792, ETA in seconds: 8132.343\n",
      "epoch: 1020, train loss: 6.208662605285644, val loss: 6.204561328887939, ETA in seconds: 8205.127\n",
      "epoch: 1030, train loss: 6.204951667785645, val loss: 6.202608203887939, ETA in seconds: 8277.175\n",
      "epoch: 1040, train loss: 6.208272123336792, val loss: 6.205928468704224, ETA in seconds: 8350.709\n",
      "epoch: 1050, train loss: 6.208662748336792, val loss: 6.206123733520508, ETA in seconds: 8422.476\n",
      "epoch: 1060, train loss: 6.207491016387939, val loss: 6.202803373336792, ETA in seconds: 8494.042\n",
      "epoch: 1070, train loss: 6.20944390296936, val loss: 6.202217531204224, ETA in seconds: 8565.337\n",
      "epoch: 1080, train loss: 6.20710015296936, val loss: 6.206123733520508, ETA in seconds: 8632.557\n",
      "epoch: 1090, train loss: 6.207295513153076, val loss: 6.205537939071656, ETA in seconds: 8703.339\n",
      "epoch: 1100, train loss: 6.20749077796936, val loss: 6.204365825653076, ETA in seconds: 8776.402\n",
      "epoch: 1110, train loss: 6.2094439506530765, val loss: 6.20378007888794, ETA in seconds: 8848.441\n",
      "epoch: 1120, train loss: 6.206904792785645, val loss: 6.202608203887939, ETA in seconds: 8919.273\n",
      "epoch: 1130, train loss: 6.2082719802856445, val loss: 6.202022361755371, ETA in seconds: 8991.136\n",
      "epoch: 1140, train loss: 6.207100248336792, val loss: 6.20417046546936, ETA in seconds: 9061.172\n",
      "epoch: 1150, train loss: 6.20924859046936, val loss: 6.207295608520508, ETA in seconds: 9131.238\n",
      "epoch: 1160, train loss: 6.206709527969361, val loss: 6.205928373336792, ETA in seconds: 9195.659\n",
      "epoch: 1170, train loss: 6.2112016677856445, val loss: 6.202998781204224, ETA in seconds: 9259.482\n",
      "epoch: 1180, train loss: 6.210029888153076, val loss: 6.204365825653076, ETA in seconds: 9325.292\n",
      "epoch: 1190, train loss: 6.2065142631530765, val loss: 6.203389406204224, ETA in seconds: 9388.485\n",
      "epoch: 1200, train loss: 6.2104205131530765, val loss: 6.204170751571655, ETA in seconds: 9453.169\n",
      "epoch: 1210, train loss: 6.2098345279693605, val loss: 6.202217435836792, ETA in seconds: 9517.639\n",
      "epoch: 1220, train loss: 6.2078814029693605, val loss: 6.204170560836792, ETA in seconds: 9582.495\n",
      "epoch: 1230, train loss: 6.208662652969361, val loss: 6.2047566890716555, ETA in seconds: 9650.826\n",
      "epoch: 1240, train loss: 6.20827202796936, val loss: 6.203389310836792, ETA in seconds: 9715.624\n",
      "epoch: 1250, train loss: 6.207100248336792, val loss: 6.2029985904693605, ETA in seconds: 9781.019\n",
      "epoch: 1260, train loss: 6.20846734046936, val loss: 6.204170513153076, ETA in seconds: 9846.542\n",
      "epoch: 1270, train loss: 6.209443998336792, val loss: 6.2022175788879395, ETA in seconds: 9911.021\n",
      "epoch: 1280, train loss: 6.20827202796936, val loss: 6.204756498336792, ETA in seconds: 9978.171\n",
      "epoch: 1290, train loss: 6.206904888153076, val loss: 6.201631546020508, ETA in seconds: 10043.093\n",
      "epoch: 1300, train loss: 6.209248685836792, val loss: 6.2055377006530765, ETA in seconds: 10110.557\n",
      "epoch: 1310, train loss: 6.207295560836792, val loss: 6.205733013153076, ETA in seconds: 10177.361\n",
      "epoch: 1320, train loss: 6.20631890296936, val loss: 6.203194046020508, ETA in seconds: 10243.926\n",
      "epoch: 1330, train loss: 6.2082719802856445, val loss: 6.205147123336792, ETA in seconds: 10309.383\n",
      "epoch: 1340, train loss: 6.2121782302856445, val loss: 6.207295513153076, ETA in seconds: 10372.374\n",
      "epoch: 1350, train loss: 6.209443855285644, val loss: 6.2049517154693605, ETA in seconds: 10435.408\n",
      "epoch: 1360, train loss: 6.2108110904693605, val loss: 6.203389263153076, ETA in seconds: 10498.339\n",
      "epoch: 1370, train loss: 6.208467435836792, val loss: 6.205342435836792, ETA in seconds: 10560.797\n",
      "epoch: 1380, train loss: 6.208662700653076, val loss: 6.2033891677856445, ETA in seconds: 10623.270\n",
      "epoch: 1390, train loss: 6.21002984046936, val loss: 6.203780221939087, ETA in seconds: 10688.144\n",
      "epoch: 1400, train loss: 6.207295513153076, val loss: 6.204365873336792, ETA in seconds: 10749.764\n",
      "epoch: 1410, train loss: 6.208272218704224, val loss: 6.201826810836792, ETA in seconds: 10812.770\n",
      "epoch: 1420, train loss: 6.2078814029693605, val loss: 6.204170656204224, ETA in seconds: 10879.597\n",
      "epoch: 1430, train loss: 6.207881450653076, val loss: 6.2065142631530765, ETA in seconds: 10942.130\n",
      "epoch: 1440, train loss: 6.210420417785644, val loss: 6.200459671020508, ETA in seconds: 11003.653\n",
      "epoch: 1450, train loss: 6.208076906204224, val loss: 6.20729546546936, ETA in seconds: 11070.339\n",
      "epoch: 1460, train loss: 6.2092485427856445, val loss: 6.203389263153076, ETA in seconds: 11134.950\n",
      "epoch: 1470, train loss: 6.207100296020508, val loss: 6.20553765296936, ETA in seconds: 11196.645\n",
      "epoch: 1480, train loss: 6.208662700653076, val loss: 6.206123638153076, ETA in seconds: 11258.699\n",
      "epoch: 1490, train loss: 6.20749077796936, val loss: 6.205342388153076, ETA in seconds: 11318.566\n",
      "epoch: 1500, train loss: 6.20710015296936, val loss: 6.2010456085205075, ETA in seconds: 11383.196\n",
      "epoch: 1510, train loss: 6.21100640296936, val loss: 6.2016314506530765, ETA in seconds: 11446.145\n",
      "epoch: 1520, train loss: 6.208662605285644, val loss: 6.20631890296936, ETA in seconds: 11505.815\n",
      "epoch: 1530, train loss: 6.208076810836792, val loss: 6.203584623336792, ETA in seconds: 11563.478\n",
      "epoch: 1540, train loss: 6.208467292785644, val loss: 6.20631890296936, ETA in seconds: 11623.628\n",
      "epoch: 1550, train loss: 6.208858060836792, val loss: 6.203584671020508, ETA in seconds: 11683.093\n",
      "epoch: 1560, train loss: 6.208662748336792, val loss: 6.204365921020508, ETA in seconds: 11739.934\n",
      "epoch: 1570, train loss: 6.20827202796936, val loss: 6.2059282779693605, ETA in seconds: 11795.955\n",
      "epoch: 1580, train loss: 6.2117876529693605, val loss: 6.205733013153076, ETA in seconds: 11856.340\n",
      "epoch: 1590, train loss: 6.2084673881530765, val loss: 6.203975248336792, ETA in seconds: 11912.169\n",
      "epoch: 1600, train loss: 6.208857917785645, val loss: 6.2045611381530765, ETA in seconds: 11968.110\n",
      "epoch: 1610, train loss: 6.20749077796936, val loss: 6.204170656204224, ETA in seconds: 12024.029\n",
      "epoch: 1620, train loss: 6.21100640296936, val loss: 6.203975439071655, ETA in seconds: 12080.053\n",
      "epoch: 1630, train loss: 6.209248638153076, val loss: 6.203779888153076, ETA in seconds: 12135.332\n",
      "epoch: 1640, train loss: 6.209639215469361, val loss: 6.203389310836792, ETA in seconds: 12190.288\n",
      "epoch: 1650, train loss: 6.207686090469361, val loss: 6.203780031204223, ETA in seconds: 12246.971\n",
      "epoch: 1660, train loss: 6.20944390296936, val loss: 6.20299882888794, ETA in seconds: 12303.279\n",
      "epoch: 1670, train loss: 6.21022515296936, val loss: 6.202803468704223, ETA in seconds: 12359.107\n",
      "epoch: 1680, train loss: 6.2080769538879395, val loss: 6.2026081562042235, ETA in seconds: 12415.221\n",
      "epoch: 1690, train loss: 6.2078815460205075, val loss: 6.2055377006530765, ETA in seconds: 12470.888\n",
      "epoch: 1700, train loss: 6.2094440937042235, val loss: 6.202413415908813, ETA in seconds: 12526.208\n",
      "epoch: 1710, train loss: 6.209834861755371, val loss: 6.201046419143677, ETA in seconds: 12584.584\n",
      "epoch: 1720, train loss: 6.2082723617553714, val loss: 6.202804517745972, ETA in seconds: 12639.478\n",
      "epoch: 1730, train loss: 6.210220432281494, val loss: 6.202619743347168, ETA in seconds: 12692.914\n",
      "epoch: 1740, train loss: 6.208178234100342, val loss: 6.200242233276367, ETA in seconds: 12747.977\n",
      "epoch: 1750, train loss: 6.209823083877564, val loss: 6.204879951477051, ETA in seconds: 12805.546\n",
      "epoch: 1760, train loss: 6.207490730285644, val loss: 6.204561042785644, ETA in seconds: 12866.164\n",
      "epoch: 1770, train loss: 6.209639167785644, val loss: 6.204170417785645, ETA in seconds: 12920.454\n",
      "epoch: 1780, train loss: 6.207881355285645, val loss: 6.206514167785644, ETA in seconds: 12972.128\n",
      "epoch: 1790, train loss: 6.209053230285645, val loss: 6.2033891677856445, ETA in seconds: 13026.642\n",
      "epoch: 1800, train loss: 6.206904792785645, val loss: 6.203193855285645, ETA in seconds: 13079.807\n",
      "epoch: 1810, train loss: 6.207100105285645, val loss: 6.205928230285645, ETA in seconds: 13136.491\n",
      "epoch: 1820, train loss: 6.208662605285644, val loss: 6.204561042785644, ETA in seconds: 13188.066\n",
      "epoch: 1830, train loss: 6.208467292785644, val loss: 6.204561042785644, ETA in seconds: 13241.428\n",
      "epoch: 1840, train loss: 6.208662605285644, val loss: 6.201240730285645, ETA in seconds: 13292.421\n",
      "epoch: 1850, train loss: 6.209834480285645, val loss: 6.2033891677856445, ETA in seconds: 13343.759\n",
      "epoch: 1860, train loss: 6.208467292785644, val loss: 6.205928230285645, ETA in seconds: 13394.138\n",
      "epoch: 1870, train loss: 6.210811042785645, val loss: 6.2053422927856445, ETA in seconds: 13444.499\n",
      "epoch: 1880, train loss: 6.206123542785645, val loss: 6.201240730285645, ETA in seconds: 13494.527\n",
      "epoch: 1890, train loss: 6.209443855285644, val loss: 6.204561042785644, ETA in seconds: 13549.950\n",
      "epoch: 1900, train loss: 6.208076667785645, val loss: 6.203975105285645, ETA in seconds: 13605.264\n",
      "epoch: 1910, train loss: 6.209053230285645, val loss: 6.207881355285645, ETA in seconds: 13654.472\n",
      "epoch: 1920, train loss: 6.208857917785645, val loss: 6.203779792785644, ETA in seconds: 13705.911\n",
      "epoch: 1930, train loss: 6.210615730285644, val loss: 6.203584480285644, ETA in seconds: 13756.848\n",
      "epoch: 1940, train loss: 6.209443855285644, val loss: 6.2033891677856445, ETA in seconds: 13805.686\n",
      "epoch: 1950, train loss: 6.207686042785644, val loss: 6.2043657302856445, ETA in seconds: 13858.087\n",
      "epoch: 1960, train loss: 6.210029792785645, val loss: 6.2024126052856445, ETA in seconds: 13906.852\n",
      "epoch: 1970, train loss: 6.208857917785645, val loss: 6.203193855285645, ETA in seconds: 13955.985\n",
      "epoch: 1980, train loss: 6.207490730285644, val loss: 6.201045417785645, ETA in seconds: 14005.924\n",
      "epoch: 1990, train loss: 6.207100105285645, val loss: 6.207100105285645, ETA in seconds: 14055.272\n",
      "epoch: 2000, train loss: 6.209443855285644, val loss: 6.206709480285644, ETA in seconds: 14106.318\n",
      "epoch: 2010, train loss: 6.2112016677856445, val loss: 6.202998542785645, ETA in seconds: 14155.654\n",
      "epoch: 2020, train loss: 6.206514167785644, val loss: 6.203584480285644, ETA in seconds: 14206.939\n",
      "epoch: 2030, train loss: 6.209639167785644, val loss: 6.204561042785644, ETA in seconds: 14257.519\n",
      "epoch: 2040, train loss: 6.207490730285644, val loss: 6.2024126052856445, ETA in seconds: 14305.857\n",
      "epoch: 2050, train loss: 6.208857917785645, val loss: 6.202803230285644, ETA in seconds: 14353.873\n",
      "epoch: 2060, train loss: 6.207686042785644, val loss: 6.205537605285644, ETA in seconds: 14402.624\n",
      "epoch: 2070, train loss: 6.208076667785645, val loss: 6.205732917785644, ETA in seconds: 14449.925\n",
      "epoch: 2080, train loss: 6.209053230285645, val loss: 6.205537605285644, ETA in seconds: 14498.413\n",
      "epoch: 2090, train loss: 6.210029792785645, val loss: 6.203975105285645, ETA in seconds: 14545.255\n",
      "epoch: 2100, train loss: 6.2092485427856445, val loss: 6.202998542785645, ETA in seconds: 14592.400\n",
      "epoch: 2110, train loss: 6.209639167785644, val loss: 6.205537605285644, ETA in seconds: 14640.988\n",
      "epoch: 2120, train loss: 6.210029792785645, val loss: 6.204170417785645, ETA in seconds: 14687.745\n",
      "epoch: 2130, train loss: 6.206904792785645, val loss: 6.204561042785644, ETA in seconds: 14737.774\n",
      "epoch: 2140, train loss: 6.208662605285644, val loss: 6.204951667785645, ETA in seconds: 14784.915\n",
      "epoch: 2150, train loss: 6.207490730285644, val loss: 6.204170417785645, ETA in seconds: 14832.173\n",
      "epoch: 2160, train loss: 6.207490730285644, val loss: 6.2053422927856445, ETA in seconds: 14878.745\n",
      "epoch: 2170, train loss: 6.208662605285644, val loss: 6.204951667785645, ETA in seconds: 14925.120\n",
      "epoch: 2180, train loss: 6.207881355285645, val loss: 6.202998542785645, ETA in seconds: 14970.876\n",
      "epoch: 2190, train loss: 6.210420417785644, val loss: 6.204561042785644, ETA in seconds: 15017.899\n",
      "epoch: 2200, train loss: 6.207686042785644, val loss: 6.205146980285645, ETA in seconds: 15063.510\n",
      "epoch: 2210, train loss: 6.209053230285645, val loss: 6.203193855285645, ETA in seconds: 15109.012\n",
      "epoch: 2220, train loss: 6.208076667785645, val loss: 6.204561042785644, ETA in seconds: 15157.628\n",
      "epoch: 2230, train loss: 6.210029792785645, val loss: 6.200654792785644, ETA in seconds: 15204.074\n",
      "epoch: 2240, train loss: 6.208467292785644, val loss: 6.204756355285644, ETA in seconds: 15249.838\n",
      "epoch: 2250, train loss: 6.212568855285644, val loss: 6.203779792785644, ETA in seconds: 15294.421\n",
      "epoch: 2260, train loss: 6.2072954177856445, val loss: 6.2033891677856445, ETA in seconds: 15342.126\n",
      "epoch: 2270, train loss: 6.208662605285644, val loss: 6.203779792785644, ETA in seconds: 15391.494\n",
      "epoch: 2280, train loss: 6.208076667785645, val loss: 6.203975105285645, ETA in seconds: 15435.005\n",
      "epoch: 2290, train loss: 6.207881355285645, val loss: 6.204561042785644, ETA in seconds: 15479.367\n",
      "epoch: 2300, train loss: 6.210615730285644, val loss: 6.2072954177856445, ETA in seconds: 15522.275\n",
      "epoch: 2310, train loss: 6.210420417785644, val loss: 6.2053422927856445, ETA in seconds: 15567.116\n",
      "epoch: 2320, train loss: 6.206709480285644, val loss: 6.205537605285644, ETA in seconds: 15610.664\n",
      "epoch: 2330, train loss: 6.209443855285644, val loss: 6.203779792785644, ETA in seconds: 15654.630\n",
      "epoch: 2340, train loss: 6.209053230285645, val loss: 6.2033891677856445, ETA in seconds: 15699.253\n",
      "epoch: 2350, train loss: 6.2092485427856445, val loss: 6.204756355285644, ETA in seconds: 15744.177\n",
      "epoch: 2360, train loss: 6.206123542785645, val loss: 6.206904792785645, ETA in seconds: 15788.222\n",
      "epoch: 2370, train loss: 6.208662605285644, val loss: 6.205928230285645, ETA in seconds: 15830.908\n",
      "epoch: 2380, train loss: 6.209053230285645, val loss: 6.203584480285644, ETA in seconds: 15872.470\n",
      "epoch: 2390, train loss: 6.208662605285644, val loss: 6.204561042785644, ETA in seconds: 15913.583\n",
      "epoch: 2400, train loss: 6.210029792785645, val loss: 6.206904792785645, ETA in seconds: 15957.559\n",
      "epoch: 2410, train loss: 6.209639167785644, val loss: 6.205146980285645, ETA in seconds: 16002.218\n",
      "epoch: 2420, train loss: 6.208857917785645, val loss: 6.205928230285645, ETA in seconds: 16045.739\n",
      "epoch: 2430, train loss: 6.207100105285645, val loss: 6.2053422927856445, ETA in seconds: 16089.426\n",
      "epoch: 2440, train loss: 6.208076667785645, val loss: 6.204561042785644, ETA in seconds: 16130.747\n",
      "epoch: 2450, train loss: 6.2082719802856445, val loss: 6.206904792785645, ETA in seconds: 16173.379\n",
      "epoch: 2460, train loss: 6.2072954177856445, val loss: 6.204951667785645, ETA in seconds: 16214.303\n",
      "epoch: 2470, train loss: 6.209443855285644, val loss: 6.205146980285645, ETA in seconds: 16256.299\n",
      "epoch: 2480, train loss: 6.2072954177856445, val loss: 6.2014360427856445, ETA in seconds: 16298.476\n",
      "epoch: 2490, train loss: 6.209639167785644, val loss: 6.205537605285644, ETA in seconds: 16341.208\n",
      "epoch: 2500, train loss: 6.2082719802856445, val loss: 6.205928230285645, ETA in seconds: 16383.136\n",
      "epoch: 2510, train loss: 6.208857917785645, val loss: 6.203975105285645, ETA in seconds: 16428.303\n",
      "epoch: 2520, train loss: 6.207100105285645, val loss: 6.2014360427856445, ETA in seconds: 16468.450\n",
      "epoch: 2530, train loss: 6.207100105285645, val loss: 6.205928230285645, ETA in seconds: 16508.246\n",
      "epoch: 2540, train loss: 6.207100105285645, val loss: 6.203779792785644, ETA in seconds: 16547.325\n",
      "epoch: 2550, train loss: 6.2112016677856445, val loss: 6.2033891677856445, ETA in seconds: 16586.351\n",
      "epoch: 2560, train loss: 6.208857917785645, val loss: 6.203193855285645, ETA in seconds: 16625.596\n",
      "epoch: 2570, train loss: 6.2072954177856445, val loss: 6.203193855285645, ETA in seconds: 16664.236\n",
      "epoch: 2580, train loss: 6.2102251052856445, val loss: 6.204951667785645, ETA in seconds: 16704.213\n",
      "epoch: 2590, train loss: 6.2072954177856445, val loss: 6.205146980285645, ETA in seconds: 16745.469\n",
      "epoch: 2600, train loss: 6.209053230285645, val loss: 6.202217292785645, ETA in seconds: 16787.623\n",
      "epoch: 2610, train loss: 6.2092485427856445, val loss: 6.2043657302856445, ETA in seconds: 16832.758\n",
      "epoch: 2620, train loss: 6.209443855285644, val loss: 6.204951667785645, ETA in seconds: 16871.646\n",
      "epoch: 2630, train loss: 6.208076667785645, val loss: 6.204951667785645, ETA in seconds: 16911.911\n",
      "epoch: 2640, train loss: 6.2072954177856445, val loss: 6.202607917785644, ETA in seconds: 16952.499\n",
      "epoch: 2650, train loss: 6.209443855285644, val loss: 6.205537605285644, ETA in seconds: 16990.045\n",
      "epoch: 2660, train loss: 6.2072954177856445, val loss: 6.2033891677856445, ETA in seconds: 17027.659\n",
      "epoch: 2670, train loss: 6.211982917785645, val loss: 6.203975105285645, ETA in seconds: 17067.761\n",
      "epoch: 2680, train loss: 6.207490730285644, val loss: 6.203975105285645, ETA in seconds: 17107.929\n",
      "epoch: 2690, train loss: 6.209834480285645, val loss: 6.205732917785644, ETA in seconds: 17149.091\n",
      "epoch: 2700, train loss: 6.208467292785644, val loss: 6.204561042785644, ETA in seconds: 17186.787\n",
      "epoch: 2710, train loss: 6.208467292785644, val loss: 6.201240730285645, ETA in seconds: 17224.011\n",
      "epoch: 2720, train loss: 6.208662605285644, val loss: 6.203779792785644, ETA in seconds: 17261.157\n",
      "epoch: 2730, train loss: 6.208467292785644, val loss: 6.2053422927856445, ETA in seconds: 17298.115\n",
      "epoch: 2740, train loss: 6.209834480285645, val loss: 6.204170417785645, ETA in seconds: 17334.568\n",
      "epoch: 2750, train loss: 6.209443855285644, val loss: 6.202021980285645, ETA in seconds: 17374.678\n",
      "epoch: 2760, train loss: 6.2092485427856445, val loss: 6.204756355285644, ETA in seconds: 17411.279\n",
      "epoch: 2770, train loss: 6.207100105285645, val loss: 6.203779792785644, ETA in seconds: 17447.656\n",
      "epoch: 2780, train loss: 6.209834480285645, val loss: 6.204756355285644, ETA in seconds: 17483.672\n",
      "epoch: 2790, train loss: 6.207881355285645, val loss: 6.203584480285644, ETA in seconds: 17519.479\n",
      "epoch: 2800, train loss: 6.2082719802856445, val loss: 6.204951667785645, ETA in seconds: 17556.178\n",
      "epoch: 2810, train loss: 6.209443855285644, val loss: 6.2043657302856445, ETA in seconds: 17597.117\n",
      "epoch: 2820, train loss: 6.209053230285645, val loss: 6.203193855285645, ETA in seconds: 17632.777\n",
      "epoch: 2830, train loss: 6.210029792785645, val loss: 6.2043657302856445, ETA in seconds: 17668.129\n",
      "epoch: 2840, train loss: 6.207100105285645, val loss: 6.201045417785645, ETA in seconds: 17703.263\n",
      "epoch: 2850, train loss: 6.2092485427856445, val loss: 6.201240730285645, ETA in seconds: 17738.166\n",
      "epoch: 2860, train loss: 6.208857917785645, val loss: 6.205537605285644, ETA in seconds: 17773.859\n",
      "epoch: 2870, train loss: 6.210029792785645, val loss: 6.204170417785645, ETA in seconds: 17810.756\n",
      "epoch: 2880, train loss: 6.207490730285644, val loss: 6.2043657302856445, ETA in seconds: 17848.275\n",
      "epoch: 2890, train loss: 6.2092485427856445, val loss: 6.205732917785644, ETA in seconds: 17882.443\n",
      "epoch: 2900, train loss: 6.207881355285645, val loss: 6.205146980285645, ETA in seconds: 17916.552\n",
      "epoch: 2910, train loss: 6.209443855285644, val loss: 6.200654792785644, ETA in seconds: 17951.010\n",
      "epoch: 2920, train loss: 6.2082719802856445, val loss: 6.203779792785644, ETA in seconds: 17984.861\n",
      "epoch: 2930, train loss: 6.208467292785644, val loss: 6.205146980285645, ETA in seconds: 18018.288\n",
      "epoch: 2940, train loss: 6.209053230285645, val loss: 6.201631355285644, ETA in seconds: 18052.793\n",
      "epoch: 2950, train loss: 6.208467292785644, val loss: 6.204170417785645, ETA in seconds: 18086.170\n",
      "epoch: 2960, train loss: 6.2092485427856445, val loss: 6.204170417785645, ETA in seconds: 18123.318\n",
      "epoch: 2970, train loss: 6.206123542785645, val loss: 6.2072954177856445, ETA in seconds: 18157.228\n",
      "epoch: 2980, train loss: 6.209639167785644, val loss: 6.206123542785645, ETA in seconds: 18190.019\n",
      "epoch: 2990, train loss: 6.209443855285644, val loss: 6.203193855285645, ETA in seconds: 18225.495\n",
      "epoch: 3000, train loss: 6.2092485427856445, val loss: 6.202998542785645, ETA in seconds: 18263.908\n",
      "epoch: 3010, train loss: 6.210420417785644, val loss: 6.201240730285645, ETA in seconds: 18300.019\n",
      "epoch: 3020, train loss: 6.208662605285644, val loss: 6.205146980285645, ETA in seconds: 18332.601\n",
      "epoch: 3030, train loss: 6.209053230285645, val loss: 6.204561042785644, ETA in seconds: 18364.394\n",
      "epoch: 3040, train loss: 6.210420417785644, val loss: 6.203779792785644, ETA in seconds: 18397.142\n",
      "epoch: 3050, train loss: 6.209053230285645, val loss: 6.2033891677856445, ETA in seconds: 18428.637\n",
      "epoch: 3060, train loss: 6.209443855285644, val loss: 6.203193855285645, ETA in seconds: 18459.923\n",
      "epoch: 3070, train loss: 6.2072954177856445, val loss: 6.203779792785644, ETA in seconds: 18490.981\n",
      "epoch: 3080, train loss: 6.209053230285645, val loss: 6.205146980285645, ETA in seconds: 18522.078\n",
      "epoch: 3090, train loss: 6.206709480285644, val loss: 6.201631355285644, ETA in seconds: 18553.531\n",
      "epoch: 3100, train loss: 6.209443855285644, val loss: 6.201826667785644, ETA in seconds: 18586.208\n",
      "epoch: 3110, train loss: 6.208857917785645, val loss: 6.204951667785645, ETA in seconds: 18616.041\n",
      "epoch: 3120, train loss: 6.209053230285645, val loss: 6.203975105285645, ETA in seconds: 18646.122\n",
      "epoch: 3130, train loss: 6.207881355285645, val loss: 6.203975105285645, ETA in seconds: 18677.038\n",
      "epoch: 3140, train loss: 6.206709480285644, val loss: 6.204561042785644, ETA in seconds: 18709.369\n",
      "epoch: 3150, train loss: 6.209443855285644, val loss: 6.2033891677856445, ETA in seconds: 18739.398\n",
      "epoch: 3160, train loss: 6.207490730285644, val loss: 6.202607917785644, ETA in seconds: 18769.395\n",
      "epoch: 3170, train loss: 6.209443855285644, val loss: 6.200654792785644, ETA in seconds: 18800.623\n",
      "epoch: 3180, train loss: 6.210420417785644, val loss: 6.204170417785645, ETA in seconds: 18832.146\n",
      "epoch: 3190, train loss: 6.208467292785644, val loss: 6.203193855285645, ETA in seconds: 18862.269\n",
      "epoch: 3200, train loss: 6.2082719802856445, val loss: 6.205537605285644, ETA in seconds: 18890.384\n",
      "epoch: 3210, train loss: 6.211006355285645, val loss: 6.203779792785644, ETA in seconds: 18919.098\n",
      "epoch: 3220, train loss: 6.2092485427856445, val loss: 6.203779792785644, ETA in seconds: 18949.157\n",
      "epoch: 3230, train loss: 6.210420417785644, val loss: 6.202998542785645, ETA in seconds: 18977.678\n",
      "epoch: 3240, train loss: 6.206709480285644, val loss: 6.203779792785644, ETA in seconds: 19006.090\n",
      "epoch: 3250, train loss: 6.205146980285645, val loss: 6.204951667785645, ETA in seconds: 19034.198\n",
      "epoch: 3260, train loss: 6.209053230285645, val loss: 6.202998542785645, ETA in seconds: 19061.020\n",
      "epoch: 3270, train loss: 6.2082719802856445, val loss: 6.205146980285645, ETA in seconds: 19087.946\n",
      "epoch: 3280, train loss: 6.209053230285645, val loss: 6.2033891677856445, ETA in seconds: 19115.310\n",
      "epoch: 3290, train loss: 6.209639167785644, val loss: 6.206709480285644, ETA in seconds: 19146.360\n",
      "epoch: 3300, train loss: 6.208857917785645, val loss: 6.203584480285644, ETA in seconds: 19174.737\n",
      "epoch: 3310, train loss: 6.207490730285644, val loss: 6.204756355285644, ETA in seconds: 19203.356\n",
      "epoch: 3320, train loss: 6.2043657302856445, val loss: 6.203975105285645, ETA in seconds: 19233.136\n",
      "epoch: 3330, train loss: 6.210615730285644, val loss: 6.205537605285644, ETA in seconds: 19266.860\n",
      "epoch: 3340, train loss: 6.208662605285644, val loss: 6.203975105285645, ETA in seconds: 19294.993\n",
      "epoch: 3350, train loss: 6.208662605285644, val loss: 6.200068855285645, ETA in seconds: 19320.609\n",
      "epoch: 3360, train loss: 6.208467292785644, val loss: 6.202998542785645, ETA in seconds: 19345.812\n",
      "epoch: 3370, train loss: 6.208076667785645, val loss: 6.203584480285644, ETA in seconds: 19370.769\n",
      "epoch: 3380, train loss: 6.2102251052856445, val loss: 6.202998542785645, ETA in seconds: 19395.411\n",
      "epoch: 3390, train loss: 6.210420417785644, val loss: 6.202998542785645, ETA in seconds: 19420.557\n",
      "epoch: 3400, train loss: 6.2063188552856445, val loss: 6.202607917785644, ETA in seconds: 19445.793\n",
      "epoch: 3410, train loss: 6.2092485427856445, val loss: 6.207881355285645, ETA in seconds: 19474.073\n",
      "epoch: 3420, train loss: 6.211982917785645, val loss: 6.203193855285645, ETA in seconds: 19499.093\n",
      "epoch: 3430, train loss: 6.206904792785645, val loss: 6.2053422927856445, ETA in seconds: 19523.720\n",
      "epoch: 3440, train loss: 6.208662605285644, val loss: 6.2043657302856445, ETA in seconds: 19551.527\n",
      "epoch: 3450, train loss: 6.209443855285644, val loss: 6.203975105285645, ETA in seconds: 19577.081\n",
      "epoch: 3460, train loss: 6.206904792785645, val loss: 6.202803230285644, ETA in seconds: 19601.338\n",
      "epoch: 3470, train loss: 6.209834480285645, val loss: 6.204756355285644, ETA in seconds: 19626.384\n",
      "epoch: 3480, train loss: 6.208662605285644, val loss: 6.202998542785645, ETA in seconds: 19650.518\n",
      "epoch: 3490, train loss: 6.211396980285644, val loss: 6.201826667785644, ETA in seconds: 19674.213\n",
      "epoch: 3500, train loss: 6.2092485427856445, val loss: 6.203779792785644, ETA in seconds: 19697.696\n",
      "epoch: 3510, train loss: 6.210029792785645, val loss: 6.201240730285645, ETA in seconds: 19721.433\n",
      "epoch: 3520, train loss: 6.2072954177856445, val loss: 6.204561042785644, ETA in seconds: 19745.126\n",
      "epoch: 3530, train loss: 6.211787605285645, val loss: 6.203975105285645, ETA in seconds: 19768.554\n",
      "epoch: 3540, train loss: 6.209053230285645, val loss: 6.207100105285645, ETA in seconds: 19793.875\n",
      "epoch: 3550, train loss: 6.208467292785644, val loss: 6.2033891677856445, ETA in seconds: 19816.932\n",
      "epoch: 3560, train loss: 6.2072954177856445, val loss: 6.2033891677856445, ETA in seconds: 19839.672\n",
      "epoch: 3570, train loss: 6.209443855285644, val loss: 6.205146980285645, ETA in seconds: 19862.877\n",
      "epoch: 3580, train loss: 6.206709480285644, val loss: 6.205146980285645, ETA in seconds: 19884.654\n",
      "epoch: 3590, train loss: 6.210029792785645, val loss: 6.206123542785645, ETA in seconds: 19908.267\n",
      "epoch: 3600, train loss: 6.211787605285645, val loss: 6.203584480285644, ETA in seconds: 19929.859\n",
      "epoch: 3610, train loss: 6.207881355285645, val loss: 6.2043657302856445, ETA in seconds: 19950.550\n",
      "epoch: 3620, train loss: 6.2082719802856445, val loss: 6.203975105285645, ETA in seconds: 19970.887\n",
      "epoch: 3630, train loss: 6.207490730285644, val loss: 6.202803230285644, ETA in seconds: 19992.178\n",
      "epoch: 3640, train loss: 6.210420417785644, val loss: 6.204756355285644, ETA in seconds: 20014.721\n",
      "epoch: 3650, train loss: 6.210029792785645, val loss: 6.203975105285645, ETA in seconds: 20036.927\n",
      "epoch: 3660, train loss: 6.208662605285644, val loss: 6.203779792785644, ETA in seconds: 20057.262\n",
      "epoch: 3670, train loss: 6.209443855285644, val loss: 6.202607917785644, ETA in seconds: 20077.857\n",
      "epoch: 3680, train loss: 6.206709480285644, val loss: 6.202217292785645, ETA in seconds: 20099.409\n",
      "epoch: 3690, train loss: 6.208857917785645, val loss: 6.202998542785645, ETA in seconds: 20119.256\n",
      "epoch: 3700, train loss: 6.210029792785645, val loss: 6.205732917785644, ETA in seconds: 20140.804\n",
      "epoch: 3710, train loss: 6.209443855285644, val loss: 6.2024126052856445, ETA in seconds: 20161.888\n",
      "epoch: 3720, train loss: 6.210811042785645, val loss: 6.205146980285645, ETA in seconds: 20183.626\n",
      "epoch: 3730, train loss: 6.2072954177856445, val loss: 6.204756355285644, ETA in seconds: 20204.753\n",
      "epoch: 3740, train loss: 6.2092485427856445, val loss: 6.2033891677856445, ETA in seconds: 20224.419\n",
      "epoch: 3750, train loss: 6.205732917785644, val loss: 6.202998542785645, ETA in seconds: 20244.939\n",
      "epoch: 3760, train loss: 6.2092485427856445, val loss: 6.202021980285645, ETA in seconds: 20263.398\n",
      "epoch: 3770, train loss: 6.208076667785645, val loss: 6.205146980285645, ETA in seconds: 20283.763\n",
      "epoch: 3780, train loss: 6.206123542785645, val loss: 6.2004594802856445, ETA in seconds: 20303.027\n",
      "epoch: 3790, train loss: 6.205537605285644, val loss: 6.204561042785644, ETA in seconds: 20321.955\n",
      "epoch: 3800, train loss: 6.207881355285645, val loss: 6.207100105285645, ETA in seconds: 20342.122\n",
      "epoch: 3810, train loss: 6.206123542785645, val loss: 6.203975105285645, ETA in seconds: 20359.872\n",
      "epoch: 3820, train loss: 6.2092485427856445, val loss: 6.202998542785645, ETA in seconds: 20377.472\n",
      "epoch: 3830, train loss: 6.208467292785644, val loss: 6.204756355285644, ETA in seconds: 20399.727\n",
      "epoch: 3840, train loss: 6.207490730285644, val loss: 6.201826667785644, ETA in seconds: 20418.565\n",
      "epoch: 3850, train loss: 6.206123542785645, val loss: 6.203193855285645, ETA in seconds: 20436.005\n",
      "epoch: 3860, train loss: 6.207100105285645, val loss: 6.204170417785645, ETA in seconds: 20454.854\n",
      "epoch: 3870, train loss: 6.207881355285645, val loss: 6.206123542785645, ETA in seconds: 20471.429\n",
      "epoch: 3880, train loss: 6.207881355285645, val loss: 6.205732917785644, ETA in seconds: 20487.496\n",
      "epoch: 3890, train loss: 6.208662605285644, val loss: 6.205928230285645, ETA in seconds: 20507.468\n",
      "epoch: 3900, train loss: 6.207490730285644, val loss: 6.203975105285645, ETA in seconds: 20523.906\n",
      "epoch: 3910, train loss: 6.206709480285644, val loss: 6.2063188552856445, ETA in seconds: 20540.524\n",
      "epoch: 3920, train loss: 6.2102251052856445, val loss: 6.2004594802856445, ETA in seconds: 20556.836\n",
      "epoch: 3930, train loss: 6.209834480285645, val loss: 6.205537605285644, ETA in seconds: 20575.102\n",
      "epoch: 3940, train loss: 6.210420417785644, val loss: 6.204951667785645, ETA in seconds: 20593.569\n",
      "epoch: 3950, train loss: 6.210420417785644, val loss: 6.205537605285644, ETA in seconds: 20613.663\n",
      "epoch: 3960, train loss: 6.2082719802856445, val loss: 6.203193855285645, ETA in seconds: 20633.421\n",
      "epoch: 3970, train loss: 6.208662605285644, val loss: 6.203975105285645, ETA in seconds: 20649.013\n",
      "epoch: 3980, train loss: 6.210420417785644, val loss: 6.203975105285645, ETA in seconds: 20665.977\n",
      "epoch: 3990, train loss: 6.209053230285645, val loss: 6.204561042785644, ETA in seconds: 20681.452\n",
      "epoch: 4000, train loss: 6.2102251052856445, val loss: 6.2024126052856445, ETA in seconds: 20697.602\n",
      "epoch: 4010, train loss: 6.208076667785645, val loss: 6.202803230285644, ETA in seconds: 20713.275\n",
      "epoch: 4020, train loss: 6.208662605285644, val loss: 6.201631355285644, ETA in seconds: 20728.959\n",
      "epoch: 4030, train loss: 6.208857917785645, val loss: 6.205537605285644, ETA in seconds: 20746.212\n",
      "epoch: 4040, train loss: 6.210420417785644, val loss: 6.2024126052856445, ETA in seconds: 20762.815\n",
      "epoch: 4050, train loss: 6.207881355285645, val loss: 6.202607917785644, ETA in seconds: 20776.853\n",
      "epoch: 4060, train loss: 6.206904792785645, val loss: 6.203584480285644, ETA in seconds: 20790.772\n",
      "epoch: 4070, train loss: 6.206709480285644, val loss: 6.204756355285644, ETA in seconds: 20805.900\n",
      "epoch: 4080, train loss: 6.208857917785645, val loss: 6.202021980285645, ETA in seconds: 20821.525\n",
      "epoch: 4090, train loss: 6.2053422927856445, val loss: 6.204170417785645, ETA in seconds: 20837.174\n",
      "epoch: 4100, train loss: 6.208662605285644, val loss: 6.203975105285645, ETA in seconds: 20850.588\n",
      "epoch: 4110, train loss: 6.210811042785645, val loss: 6.206514167785644, ETA in seconds: 20864.237\n",
      "epoch: 4120, train loss: 6.2092485427856445, val loss: 6.2024126052856445, ETA in seconds: 20877.085\n",
      "epoch: 4130, train loss: 6.208076667785645, val loss: 6.205537605285644, ETA in seconds: 20891.457\n",
      "epoch: 4140, train loss: 6.207881355285645, val loss: 6.203193855285645, ETA in seconds: 20905.052\n",
      "epoch: 4150, train loss: 6.2082719802856445, val loss: 6.203193855285645, ETA in seconds: 20918.311\n",
      "epoch: 4160, train loss: 6.2092485427856445, val loss: 6.2033891677856445, ETA in seconds: 20934.960\n",
      "epoch: 4170, train loss: 6.211006355285645, val loss: 6.2033891677856445, ETA in seconds: 20947.831\n",
      "epoch: 4180, train loss: 6.2072954177856445, val loss: 6.2063188552856445, ETA in seconds: 20960.192\n",
      "epoch: 4190, train loss: 6.2092485427856445, val loss: 6.202803230285644, ETA in seconds: 20972.378\n",
      "epoch: 4200, train loss: 6.209834480285645, val loss: 6.205928230285645, ETA in seconds: 20984.782\n",
      "epoch: 4210, train loss: 6.206123542785645, val loss: 6.202607917785644, ETA in seconds: 20995.915\n",
      "epoch: 4220, train loss: 6.208662605285644, val loss: 6.205146980285645, ETA in seconds: 21006.829\n",
      "epoch: 4230, train loss: 6.206904792785645, val loss: 6.202803230285644, ETA in seconds: 21017.781\n",
      "epoch: 4240, train loss: 6.209639167785644, val loss: 6.200654792785644, ETA in seconds: 21028.450\n",
      "epoch: 4250, train loss: 6.209834480285645, val loss: 6.202998542785645, ETA in seconds: 21039.884\n",
      "epoch: 4260, train loss: 6.206514167785644, val loss: 6.201045417785645, ETA in seconds: 21052.931\n",
      "epoch: 4270, train loss: 6.2063188552856445, val loss: 6.204170417785645, ETA in seconds: 21063.908\n",
      "epoch: 4280, train loss: 6.2102251052856445, val loss: 6.202217292785645, ETA in seconds: 21076.233\n",
      "epoch: 4290, train loss: 6.208857917785645, val loss: 6.207100105285645, ETA in seconds: 21086.762\n",
      "epoch: 4300, train loss: 6.207490730285644, val loss: 6.204170417785645, ETA in seconds: 21097.332\n",
      "epoch: 4310, train loss: 6.207490730285644, val loss: 6.204951667785645, ETA in seconds: 21108.164\n",
      "epoch: 4320, train loss: 6.208467292785644, val loss: 6.204170417785645, ETA in seconds: 21122.875\n",
      "epoch: 4330, train loss: 6.208857917785645, val loss: 6.203779792785644, ETA in seconds: 21133.295\n",
      "epoch: 4340, train loss: 6.2092485427856445, val loss: 6.202217292785645, ETA in seconds: 21142.213\n",
      "epoch: 4350, train loss: 6.205928230285645, val loss: 6.202021980285645, ETA in seconds: 21151.086\n",
      "epoch: 4360, train loss: 6.209443855285644, val loss: 6.201826667785644, ETA in seconds: 21159.646\n",
      "epoch: 4370, train loss: 6.208467292785644, val loss: 6.200654792785644, ETA in seconds: 21170.894\n",
      "epoch: 4380, train loss: 6.208662605285644, val loss: 6.2053422927856445, ETA in seconds: 21183.019\n",
      "epoch: 4390, train loss: 6.209443855285644, val loss: 6.205537605285644, ETA in seconds: 21192.096\n",
      "epoch: 4400, train loss: 6.207490730285644, val loss: 6.202607917785644, ETA in seconds: 21200.183\n",
      "epoch: 4410, train loss: 6.207686042785644, val loss: 6.202803230285644, ETA in seconds: 21212.983\n",
      "epoch: 4420, train loss: 6.208857917785645, val loss: 6.208076667785645, ETA in seconds: 21223.079\n",
      "epoch: 4430, train loss: 6.2063188552856445, val loss: 6.205146980285645, ETA in seconds: 21230.736\n",
      "epoch: 4440, train loss: 6.208467292785644, val loss: 6.2024126052856445, ETA in seconds: 21238.854\n",
      "epoch: 4450, train loss: 6.206709480285644, val loss: 6.201045417785645, ETA in seconds: 21246.550\n",
      "epoch: 4460, train loss: 6.207881355285645, val loss: 6.205146980285645, ETA in seconds: 21254.948\n",
      "epoch: 4470, train loss: 6.2082719802856445, val loss: 6.204561042785644, ETA in seconds: 21261.998\n",
      "epoch: 4480, train loss: 6.210029792785645, val loss: 6.205732917785644, ETA in seconds: 21268.811\n",
      "epoch: 4490, train loss: 6.206904792785645, val loss: 6.202021980285645, ETA in seconds: 21275.344\n",
      "epoch: 4500, train loss: 6.2102251052856445, val loss: 6.2043657302856445, ETA in seconds: 21282.553\n",
      "epoch: 4510, train loss: 6.2072954177856445, val loss: 6.2014360427856445, ETA in seconds: 21288.881\n",
      "epoch: 4520, train loss: 6.208662605285644, val loss: 6.2033891677856445, ETA in seconds: 21295.685\n",
      "epoch: 4530, train loss: 6.211006355285645, val loss: 6.2053422927856445, ETA in seconds: 21304.399\n",
      "epoch: 4540, train loss: 6.209639167785644, val loss: 6.202021980285645, ETA in seconds: 21309.899\n",
      "epoch: 4550, train loss: 6.209834480285645, val loss: 6.204561042785644, ETA in seconds: 21317.277\n",
      "epoch: 4560, train loss: 6.207686042785644, val loss: 6.204951667785645, ETA in seconds: 21325.628\n",
      "epoch: 4570, train loss: 6.209834480285645, val loss: 6.203584480285644, ETA in seconds: 21330.677\n",
      "epoch: 4580, train loss: 6.209443855285644, val loss: 6.2033891677856445, ETA in seconds: 21337.690\n",
      "epoch: 4590, train loss: 6.210420417785644, val loss: 6.202803230285644, ETA in seconds: 21342.640\n",
      "epoch: 4600, train loss: 6.2092485427856445, val loss: 6.202998542785645, ETA in seconds: 21347.399\n",
      "epoch: 4610, train loss: 6.208662605285644, val loss: 6.206123542785645, ETA in seconds: 21353.391\n",
      "epoch: 4620, train loss: 6.210029792785645, val loss: 6.204756355285644, ETA in seconds: 21357.902\n",
      "epoch: 4630, train loss: 6.207686042785644, val loss: 6.204756355285644, ETA in seconds: 21362.140\n",
      "epoch: 4640, train loss: 6.209834480285645, val loss: 6.2033891677856445, ETA in seconds: 21366.157\n",
      "epoch: 4650, train loss: 6.209639167785644, val loss: 6.205537605285644, ETA in seconds: 21370.114\n",
      "epoch: 4660, train loss: 6.210420417785644, val loss: 6.205537605285644, ETA in seconds: 21374.557\n",
      "epoch: 4670, train loss: 6.2063188552856445, val loss: 6.202217292785645, ETA in seconds: 21378.913\n",
      "epoch: 4680, train loss: 6.205732917785644, val loss: 6.2033891677856445, ETA in seconds: 21383.997\n",
      "epoch: 4690, train loss: 6.208467292785644, val loss: 6.203584480285644, ETA in seconds: 21387.057\n",
      "epoch: 4700, train loss: 6.208467292785644, val loss: 6.2072954177856445, ETA in seconds: 21389.907\n",
      "epoch: 4710, train loss: 6.2092485427856445, val loss: 6.203584480285644, ETA in seconds: 21392.785\n",
      "epoch: 4720, train loss: 6.206904792785645, val loss: 6.204951667785645, ETA in seconds: 21395.609\n",
      "epoch: 4730, train loss: 6.208857917785645, val loss: 6.204170417785645, ETA in seconds: 21399.989\n",
      "epoch: 4740, train loss: 6.2102251052856445, val loss: 6.205732917785644, ETA in seconds: 21402.389\n",
      "epoch: 4750, train loss: 6.208662605285644, val loss: 6.2072954177856445, ETA in seconds: 21404.465\n",
      "epoch: 4760, train loss: 6.209053230285645, val loss: 6.200850105285644, ETA in seconds: 21409.148\n",
      "epoch: 4770, train loss: 6.208857917785645, val loss: 6.201045417785645, ETA in seconds: 21415.172\n",
      "epoch: 4780, train loss: 6.2072954177856445, val loss: 6.201240730285645, ETA in seconds: 21417.063\n",
      "epoch: 4790, train loss: 6.208662605285644, val loss: 6.202998542785645, ETA in seconds: 21418.734\n",
      "epoch: 4800, train loss: 6.207490730285644, val loss: 6.202803230285644, ETA in seconds: 21422.649\n",
      "epoch: 4810, train loss: 6.2082719802856445, val loss: 6.204756355285644, ETA in seconds: 21424.020\n",
      "epoch: 4820, train loss: 6.2082719802856445, val loss: 6.203975105285645, ETA in seconds: 21425.520\n",
      "epoch: 4830, train loss: 6.206904792785645, val loss: 6.2043657302856445, ETA in seconds: 21428.425\n",
      "epoch: 4840, train loss: 6.208662605285644, val loss: 6.202021980285645, ETA in seconds: 21430.873\n",
      "epoch: 4850, train loss: 6.209443855285644, val loss: 6.2053422927856445, ETA in seconds: 21432.455\n",
      "epoch: 4860, train loss: 6.209053230285645, val loss: 6.203193855285645, ETA in seconds: 21432.848\n",
      "epoch: 4870, train loss: 6.209053230285645, val loss: 6.204951667785645, ETA in seconds: 21435.689\n",
      "epoch: 4880, train loss: 6.209834480285645, val loss: 6.206904792785645, ETA in seconds: 21437.236\n",
      "epoch: 4890, train loss: 6.208857917785645, val loss: 6.2033891677856445, ETA in seconds: 21439.113\n",
      "epoch: 4900, train loss: 6.206904792785645, val loss: 6.2043657302856445, ETA in seconds: 21438.995\n",
      "epoch: 4910, train loss: 6.2082719802856445, val loss: 6.205537605285644, ETA in seconds: 21438.658\n",
      "epoch: 4920, train loss: 6.208467292785644, val loss: 6.2033891677856445, ETA in seconds: 21438.239\n",
      "epoch: 4930, train loss: 6.210811042785645, val loss: 6.204951667785645, ETA in seconds: 21437.609\n",
      "epoch: 4940, train loss: 6.209639167785644, val loss: 6.2004594802856445, ETA in seconds: 21439.724\n",
      "epoch: 4950, train loss: 6.2082719802856445, val loss: 6.203779792785644, ETA in seconds: 21439.173\n",
      "epoch: 4960, train loss: 6.2063188552856445, val loss: 6.204561042785644, ETA in seconds: 21438.495\n",
      "epoch: 4970, train loss: 6.210420417785644, val loss: 6.202998542785645, ETA in seconds: 21438.126\n",
      "epoch: 4980, train loss: 6.2102251052856445, val loss: 6.205146980285645, ETA in seconds: 21439.475\n",
      "epoch: 4990, train loss: 6.206709480285644, val loss: 6.206904792785645, ETA in seconds: 21438.127\n",
      "epoch: 5000, train loss: 6.210420417785644, val loss: 6.2033891677856445, ETA in seconds: 21436.537\n",
      "epoch: 5010, train loss: 6.208076667785645, val loss: 6.202607917785644, ETA in seconds: 21437.056\n",
      "epoch: 5020, train loss: 6.2102251052856445, val loss: 6.203193855285645, ETA in seconds: 21435.634\n",
      "epoch: 5030, train loss: 6.210029792785645, val loss: 6.205928230285645, ETA in seconds: 21433.710\n",
      "epoch: 5040, train loss: 6.210420417785644, val loss: 6.204951667785645, ETA in seconds: 21431.790\n",
      "epoch: 5050, train loss: 6.209443855285644, val loss: 6.204170417785645, ETA in seconds: 21430.286\n",
      "epoch: 5060, train loss: 6.209443855285644, val loss: 6.204170417785645, ETA in seconds: 21429.684\n",
      "epoch: 5070, train loss: 6.208467292785644, val loss: 6.206904792785645, ETA in seconds: 21427.099\n",
      "epoch: 5080, train loss: 6.207881355285645, val loss: 6.204756355285644, ETA in seconds: 21426.332\n",
      "epoch: 5090, train loss: 6.2072954177856445, val loss: 6.2082719802856445, ETA in seconds: 21425.837\n",
      "epoch: 5100, train loss: 6.206123542785645, val loss: 6.2043657302856445, ETA in seconds: 21422.683\n",
      "epoch: 5110, train loss: 6.208662605285644, val loss: 6.201826667785644, ETA in seconds: 21418.955\n",
      "epoch: 5120, train loss: 6.208857917785645, val loss: 6.202607917785644, ETA in seconds: 21416.103\n",
      "epoch: 5130, train loss: 6.210029792785645, val loss: 6.204951667785645, ETA in seconds: 21412.484\n",
      "epoch: 5140, train loss: 6.210615730285644, val loss: 6.204951667785645, ETA in seconds: 21408.670\n",
      "epoch: 5150, train loss: 6.208076667785645, val loss: 6.205146980285645, ETA in seconds: 21408.272\n",
      "epoch: 5160, train loss: 6.209834480285645, val loss: 6.203975105285645, ETA in seconds: 21407.178\n",
      "epoch: 5170, train loss: 6.208662605285644, val loss: 6.2043657302856445, ETA in seconds: 21402.796\n",
      "epoch: 5180, train loss: 6.208662605285644, val loss: 6.200850105285644, ETA in seconds: 21399.230\n",
      "epoch: 5190, train loss: 6.209639167785644, val loss: 6.207686042785644, ETA in seconds: 21395.088\n",
      "epoch: 5200, train loss: 6.208857917785645, val loss: 6.205537605285644, ETA in seconds: 21391.645\n",
      "epoch: 5210, train loss: 6.207686042785644, val loss: 6.2024126052856445, ETA in seconds: 21387.037\n",
      "epoch: 5220, train loss: 6.207686042785644, val loss: 6.205732917785644, ETA in seconds: 21385.433\n",
      "epoch: 5230, train loss: 6.2072954177856445, val loss: 6.205732917785644, ETA in seconds: 21380.372\n",
      "epoch: 5240, train loss: 6.209639167785644, val loss: 6.2024126052856445, ETA in seconds: 21375.549\n",
      "epoch: 5250, train loss: 6.211396980285644, val loss: 6.204756355285644, ETA in seconds: 21371.328\n",
      "epoch: 5260, train loss: 6.208467292785644, val loss: 6.202803230285644, ETA in seconds: 21366.479\n",
      "epoch: 5270, train loss: 6.2102251052856445, val loss: 6.2053422927856445, ETA in seconds: 21361.910\n",
      "epoch: 5280, train loss: 6.210811042785645, val loss: 6.202217292785645, ETA in seconds: 21355.533\n",
      "epoch: 5290, train loss: 6.208662605285644, val loss: 6.203584480285644, ETA in seconds: 21349.384\n",
      "epoch: 5300, train loss: 6.209053230285645, val loss: 6.203584480285644, ETA in seconds: 21342.400\n",
      "epoch: 5310, train loss: 6.211982917785645, val loss: 6.206123542785645, ETA in seconds: 21335.358\n",
      "epoch: 5320, train loss: 6.207490730285644, val loss: 6.207686042785644, ETA in seconds: 21328.234\n",
      "epoch: 5330, train loss: 6.209443855285644, val loss: 6.204170417785645, ETA in seconds: 21320.778\n",
      "epoch: 5340, train loss: 6.210029792785645, val loss: 6.201240730285645, ETA in seconds: 21313.849\n",
      "epoch: 5350, train loss: 6.209443855285644, val loss: 6.2014360427856445, ETA in seconds: 21309.221\n",
      "epoch: 5360, train loss: 6.207490730285644, val loss: 6.206904792785645, ETA in seconds: 21308.313\n",
      "epoch: 5370, train loss: 6.207490730285644, val loss: 6.205928230285645, ETA in seconds: 21299.630\n",
      "epoch: 5380, train loss: 6.209053230285645, val loss: 6.2053422927856445, ETA in seconds: 21290.854\n",
      "epoch: 5390, train loss: 6.2053422927856445, val loss: 6.205928230285645, ETA in seconds: 21283.685\n",
      "epoch: 5400, train loss: 6.206709480285644, val loss: 6.2033891677856445, ETA in seconds: 21274.859\n",
      "epoch: 5410, train loss: 6.209053230285645, val loss: 6.204170417785645, ETA in seconds: 21265.824\n",
      "epoch: 5420, train loss: 6.210029792785645, val loss: 6.200068855285645, ETA in seconds: 21256.856\n",
      "epoch: 5430, train loss: 6.208662605285644, val loss: 6.202803230285644, ETA in seconds: 21247.986\n",
      "epoch: 5440, train loss: 6.2072954177856445, val loss: 6.202803230285644, ETA in seconds: 21238.805\n",
      "epoch: 5450, train loss: 6.2092485427856445, val loss: 6.204951667785645, ETA in seconds: 21229.854\n",
      "epoch: 5460, train loss: 6.2102251052856445, val loss: 6.203779792785644, ETA in seconds: 21221.304\n",
      "epoch: 5470, train loss: 6.2092485427856445, val loss: 6.205537605285644, ETA in seconds: 21211.614\n",
      "epoch: 5480, train loss: 6.210615730285644, val loss: 6.205732917785644, ETA in seconds: 21204.120\n",
      "epoch: 5490, train loss: 6.2053422927856445, val loss: 6.202217292785645, ETA in seconds: 21196.330\n",
      "epoch: 5500, train loss: 6.2102251052856445, val loss: 6.2033891677856445, ETA in seconds: 21186.416\n",
      "epoch: 5510, train loss: 6.211396980285644, val loss: 6.203193855285645, ETA in seconds: 21176.155\n",
      "epoch: 5520, train loss: 6.209443855285644, val loss: 6.199873542785644, ETA in seconds: 21165.838\n",
      "epoch: 5530, train loss: 6.207100105285645, val loss: 6.202217292785645, ETA in seconds: 21156.017\n",
      "epoch: 5540, train loss: 6.208662605285644, val loss: 6.2043657302856445, ETA in seconds: 21147.676\n",
      "epoch: 5550, train loss: 6.207686042785644, val loss: 6.205146980285645, ETA in seconds: 21138.137\n",
      "epoch: 5560, train loss: 6.209053230285645, val loss: 6.203975105285645, ETA in seconds: 21128.312\n",
      "epoch: 5570, train loss: 6.208467292785644, val loss: 6.203975105285645, ETA in seconds: 21119.715\n",
      "epoch: 5580, train loss: 6.206514167785644, val loss: 6.202803230285644, ETA in seconds: 21109.713\n",
      "epoch: 5590, train loss: 6.209834480285645, val loss: 6.203584480285644, ETA in seconds: 21100.270\n",
      "epoch: 5600, train loss: 6.208662605285644, val loss: 6.2053422927856445, ETA in seconds: 21088.441\n",
      "epoch: 5610, train loss: 6.209053230285645, val loss: 6.2053422927856445, ETA in seconds: 21079.664\n",
      "epoch: 5620, train loss: 6.2102251052856445, val loss: 6.2053422927856445, ETA in seconds: 21068.193\n",
      "epoch: 5630, train loss: 6.209053230285645, val loss: 6.206514167785644, ETA in seconds: 21056.580\n",
      "epoch: 5640, train loss: 6.207881355285645, val loss: 6.203779792785644, ETA in seconds: 21044.749\n",
      "epoch: 5650, train loss: 6.2092485427856445, val loss: 6.204561042785644, ETA in seconds: 21032.652\n",
      "epoch: 5660, train loss: 6.208467292785644, val loss: 6.202803230285644, ETA in seconds: 21021.535\n",
      "epoch: 5670, train loss: 6.208857917785645, val loss: 6.202021980285645, ETA in seconds: 21008.945\n",
      "epoch: 5680, train loss: 6.207100105285645, val loss: 6.205732917785644, ETA in seconds: 20995.788\n",
      "epoch: 5690, train loss: 6.206514167785644, val loss: 6.203779792785644, ETA in seconds: 20982.459\n",
      "epoch: 5700, train loss: 6.208076667785645, val loss: 6.201826667785644, ETA in seconds: 20970.077\n",
      "epoch: 5710, train loss: 6.209443855285644, val loss: 6.206709480285644, ETA in seconds: 20957.606\n",
      "epoch: 5720, train loss: 6.209834480285645, val loss: 6.205732917785644, ETA in seconds: 20943.770\n",
      "epoch: 5730, train loss: 6.209443855285644, val loss: 6.203975105285645, ETA in seconds: 20929.800\n",
      "epoch: 5740, train loss: 6.208857917785645, val loss: 6.202998542785645, ETA in seconds: 20915.505\n",
      "epoch: 5750, train loss: 6.205537605285644, val loss: 6.2024126052856445, ETA in seconds: 20900.960\n",
      "epoch: 5760, train loss: 6.208076667785645, val loss: 6.204561042785644, ETA in seconds: 20886.448\n",
      "epoch: 5770, train loss: 6.209834480285645, val loss: 6.203779792785644, ETA in seconds: 20871.601\n",
      "epoch: 5780, train loss: 6.210811042785645, val loss: 6.205537605285644, ETA in seconds: 20857.312\n",
      "epoch: 5790, train loss: 6.2092485427856445, val loss: 6.204170417785645, ETA in seconds: 20842.276\n",
      "epoch: 5800, train loss: 6.210420417785644, val loss: 6.205928230285645, ETA in seconds: 20827.168\n",
      "epoch: 5810, train loss: 6.208467292785644, val loss: 6.201826667785644, ETA in seconds: 20812.028\n",
      "epoch: 5820, train loss: 6.207100105285645, val loss: 6.203975105285645, ETA in seconds: 20796.759\n",
      "epoch: 5830, train loss: 6.207881355285645, val loss: 6.202803230285644, ETA in seconds: 20781.460\n",
      "epoch: 5840, train loss: 6.208857917785645, val loss: 6.2043657302856445, ETA in seconds: 20765.737\n",
      "epoch: 5850, train loss: 6.209053230285645, val loss: 6.201631355285644, ETA in seconds: 20749.894\n",
      "epoch: 5860, train loss: 6.2092485427856445, val loss: 6.204170417785645, ETA in seconds: 20733.755\n",
      "epoch: 5870, train loss: 6.209834480285645, val loss: 6.203193855285645, ETA in seconds: 20717.300\n",
      "epoch: 5880, train loss: 6.209639167785644, val loss: 6.202998542785645, ETA in seconds: 20702.501\n",
      "epoch: 5890, train loss: 6.207100105285645, val loss: 6.201826667785644, ETA in seconds: 20685.825\n",
      "epoch: 5900, train loss: 6.208662605285644, val loss: 6.201631355285644, ETA in seconds: 20671.636\n",
      "epoch: 5910, train loss: 6.210420417785644, val loss: 6.207490730285644, ETA in seconds: 20655.677\n",
      "epoch: 5920, train loss: 6.207686042785644, val loss: 6.205928230285645, ETA in seconds: 20640.029\n",
      "epoch: 5930, train loss: 6.208467292785644, val loss: 6.204170417785645, ETA in seconds: 20623.634\n",
      "epoch: 5940, train loss: 6.207490730285644, val loss: 6.202021980285645, ETA in seconds: 20607.004\n",
      "epoch: 5950, train loss: 6.2072954177856445, val loss: 6.2033891677856445, ETA in seconds: 20589.460\n",
      "epoch: 5960, train loss: 6.205928230285645, val loss: 6.204170417785645, ETA in seconds: 20571.620\n",
      "epoch: 5970, train loss: 6.2102251052856445, val loss: 6.204561042785644, ETA in seconds: 20553.604\n",
      "epoch: 5980, train loss: 6.2082719802856445, val loss: 6.203193855285645, ETA in seconds: 20537.919\n",
      "epoch: 5990, train loss: 6.210420417785644, val loss: 6.203584480285644, ETA in seconds: 20520.817\n",
      "epoch: 6000, train loss: 6.209834480285645, val loss: 6.203779792785644, ETA in seconds: 20503.022\n",
      "epoch: 6010, train loss: 6.207686042785644, val loss: 6.203584480285644, ETA in seconds: 20485.710\n",
      "epoch: 6020, train loss: 6.210811042785645, val loss: 6.203975105285645, ETA in seconds: 20467.856\n",
      "epoch: 6030, train loss: 6.206709480285644, val loss: 6.202217292785645, ETA in seconds: 20449.416\n",
      "epoch: 6040, train loss: 6.204170417785645, val loss: 6.202803230285644, ETA in seconds: 20431.221\n",
      "epoch: 6050, train loss: 6.207881355285645, val loss: 6.2043657302856445, ETA in seconds: 20412.242\n",
      "epoch: 6060, train loss: 6.208857917785645, val loss: 6.201240730285645, ETA in seconds: 20393.306\n",
      "epoch: 6070, train loss: 6.207881355285645, val loss: 6.205732917785644, ETA in seconds: 20374.485\n",
      "epoch: 6080, train loss: 6.2112016677856445, val loss: 6.204756355285644, ETA in seconds: 20354.641\n",
      "epoch: 6090, train loss: 6.207686042785644, val loss: 6.202217292785645, ETA in seconds: 20334.736\n",
      "epoch: 6100, train loss: 6.204756355285644, val loss: 6.205732917785644, ETA in seconds: 20314.781\n",
      "epoch: 6110, train loss: 6.208857917785645, val loss: 6.2053422927856445, ETA in seconds: 20295.230\n",
      "epoch: 6120, train loss: 6.209443855285644, val loss: 6.205928230285645, ETA in seconds: 20275.225\n",
      "epoch: 6130, train loss: 6.2082719802856445, val loss: 6.202217292785645, ETA in seconds: 20254.438\n",
      "epoch: 6140, train loss: 6.2072954177856445, val loss: 6.204170417785645, ETA in seconds: 20233.628\n",
      "epoch: 6150, train loss: 6.2092485427856445, val loss: 6.205537605285644, ETA in seconds: 20212.586\n",
      "epoch: 6160, train loss: 6.207100105285645, val loss: 6.2033891677856445, ETA in seconds: 20191.539\n",
      "epoch: 6170, train loss: 6.2072954177856445, val loss: 6.204561042785644, ETA in seconds: 20172.073\n",
      "epoch: 6180, train loss: 6.2092485427856445, val loss: 6.201631355285644, ETA in seconds: 20151.216\n",
      "epoch: 6190, train loss: 6.211006355285645, val loss: 6.203193855285645, ETA in seconds: 20129.941\n",
      "epoch: 6200, train loss: 6.207881355285645, val loss: 6.203584480285644, ETA in seconds: 20110.096\n",
      "epoch: 6210, train loss: 6.209053230285645, val loss: 6.203779792785644, ETA in seconds: 20089.781\n",
      "epoch: 6220, train loss: 6.210029792785645, val loss: 6.204561042785644, ETA in seconds: 20067.665\n",
      "epoch: 6230, train loss: 6.210420417785644, val loss: 6.204170417785645, ETA in seconds: 20045.278\n",
      "epoch: 6240, train loss: 6.209834480285645, val loss: 6.203975105285645, ETA in seconds: 20022.887\n",
      "epoch: 6250, train loss: 6.211396980285644, val loss: 6.208076667785645, ETA in seconds: 20002.224\n",
      "epoch: 6260, train loss: 6.208857917785645, val loss: 6.202021980285645, ETA in seconds: 19980.899\n",
      "epoch: 6270, train loss: 6.207686042785644, val loss: 6.2063188552856445, ETA in seconds: 19958.516\n",
      "epoch: 6280, train loss: 6.206904792785645, val loss: 6.204561042785644, ETA in seconds: 19935.786\n",
      "epoch: 6290, train loss: 6.208662605285644, val loss: 6.202998542785645, ETA in seconds: 19912.733\n",
      "epoch: 6300, train loss: 6.205732917785644, val loss: 6.2043657302856445, ETA in seconds: 19889.642\n",
      "epoch: 6310, train loss: 6.208857917785645, val loss: 6.203779792785644, ETA in seconds: 19866.381\n",
      "epoch: 6320, train loss: 6.208662605285644, val loss: 6.204756355285644, ETA in seconds: 19842.893\n",
      "epoch: 6330, train loss: 6.210615730285644, val loss: 6.204170417785645, ETA in seconds: 19819.436\n",
      "epoch: 6340, train loss: 6.2092485427856445, val loss: 6.202021980285645, ETA in seconds: 19795.476\n",
      "epoch: 6350, train loss: 6.209443855285644, val loss: 6.204756355285644, ETA in seconds: 19773.085\n",
      "epoch: 6360, train loss: 6.209639167785644, val loss: 6.2072954177856445, ETA in seconds: 19748.802\n",
      "epoch: 6370, train loss: 6.208467292785644, val loss: 6.204170417785645, ETA in seconds: 19724.304\n",
      "epoch: 6380, train loss: 6.2082719802856445, val loss: 6.200264167785645, ETA in seconds: 19699.899\n",
      "epoch: 6390, train loss: 6.209443855285644, val loss: 6.203193855285645, ETA in seconds: 19675.511\n",
      "epoch: 6400, train loss: 6.2092485427856445, val loss: 6.2043657302856445, ETA in seconds: 19650.801\n",
      "epoch: 6410, train loss: 6.206514167785644, val loss: 6.202998542785645, ETA in seconds: 19625.805\n",
      "epoch: 6420, train loss: 6.209834480285645, val loss: 6.204170417785645, ETA in seconds: 19600.745\n",
      "epoch: 6430, train loss: 6.210029792785645, val loss: 6.2014360427856445, ETA in seconds: 19575.514\n",
      "epoch: 6440, train loss: 6.2102251052856445, val loss: 6.203584480285644, ETA in seconds: 19551.338\n",
      "epoch: 6450, train loss: 6.210029792785645, val loss: 6.202021980285645, ETA in seconds: 19526.014\n",
      "epoch: 6460, train loss: 6.208857917785645, val loss: 6.205537605285644, ETA in seconds: 19501.746\n",
      "epoch: 6470, train loss: 6.209443855285644, val loss: 6.202803230285644, ETA in seconds: 19476.645\n",
      "epoch: 6480, train loss: 6.209834480285645, val loss: 6.202803230285644, ETA in seconds: 19450.968\n",
      "epoch: 6490, train loss: 6.2082719802856445, val loss: 6.203584480285644, ETA in seconds: 19424.886\n",
      "epoch: 6500, train loss: 6.207686042785644, val loss: 6.202998542785645, ETA in seconds: 19398.604\n",
      "epoch: 6510, train loss: 6.210420417785644, val loss: 6.204561042785644, ETA in seconds: 19374.706\n",
      "epoch: 6520, train loss: 6.210029792785645, val loss: 6.206709480285644, ETA in seconds: 19347.706\n",
      "epoch: 6530, train loss: 6.2082719802856445, val loss: 6.203779792785644, ETA in seconds: 19320.404\n",
      "epoch: 6540, train loss: 6.207100105285645, val loss: 6.202803230285644, ETA in seconds: 19294.691\n",
      "epoch: 6550, train loss: 6.206123542785645, val loss: 6.202607917785644, ETA in seconds: 19267.176\n",
      "epoch: 6560, train loss: 6.208467292785644, val loss: 6.206904792785645, ETA in seconds: 19239.972\n",
      "epoch: 6570, train loss: 6.207881355285645, val loss: 6.2053422927856445, ETA in seconds: 19212.924\n",
      "epoch: 6580, train loss: 6.208467292785644, val loss: 6.205537605285644, ETA in seconds: 19185.344\n",
      "epoch: 6590, train loss: 6.207881355285645, val loss: 6.2033891677856445, ETA in seconds: 19157.475\n",
      "epoch: 6600, train loss: 6.208076667785645, val loss: 6.204561042785644, ETA in seconds: 19129.637\n",
      "epoch: 6610, train loss: 6.2072954177856445, val loss: 6.2063188552856445, ETA in seconds: 19102.727\n",
      "epoch: 6620, train loss: 6.207490730285644, val loss: 6.204756355285644, ETA in seconds: 19074.638\n",
      "epoch: 6630, train loss: 6.206904792785645, val loss: 6.206514167785644, ETA in seconds: 19047.392\n",
      "epoch: 6640, train loss: 6.2092485427856445, val loss: 6.203779792785644, ETA in seconds: 19018.838\n",
      "epoch: 6650, train loss: 6.208857917785645, val loss: 6.203584480285644, ETA in seconds: 18989.987\n",
      "epoch: 6660, train loss: 6.211006355285645, val loss: 6.202803230285644, ETA in seconds: 18961.127\n",
      "epoch: 6670, train loss: 6.208467292785644, val loss: 6.204170417785645, ETA in seconds: 18931.992\n",
      "epoch: 6680, train loss: 6.2082719802856445, val loss: 6.204756355285644, ETA in seconds: 18903.104\n",
      "epoch: 6690, train loss: 6.209053230285645, val loss: 6.204561042785644, ETA in seconds: 18874.014\n",
      "epoch: 6700, train loss: 6.208857917785645, val loss: 6.205537605285644, ETA in seconds: 18843.923\n",
      "epoch: 6710, train loss: 6.208467292785644, val loss: 6.202998542785645, ETA in seconds: 18815.234\n",
      "epoch: 6720, train loss: 6.208076667785645, val loss: 6.203584480285644, ETA in seconds: 18786.043\n",
      "epoch: 6730, train loss: 6.210029792785645, val loss: 6.203193855285645, ETA in seconds: 18755.382\n",
      "epoch: 6740, train loss: 6.2072954177856445, val loss: 6.204561042785644, ETA in seconds: 18724.551\n",
      "epoch: 6750, train loss: 6.2112016677856445, val loss: 6.207100105285645, ETA in seconds: 18693.675\n",
      "epoch: 6760, train loss: 6.208662605285644, val loss: 6.201826667785644, ETA in seconds: 18663.627\n",
      "epoch: 6770, train loss: 6.206904792785645, val loss: 6.204170417785645, ETA in seconds: 18632.289\n",
      "epoch: 6780, train loss: 6.2092485427856445, val loss: 6.199287605285645, ETA in seconds: 18603.460\n",
      "epoch: 6790, train loss: 6.2072954177856445, val loss: 6.203975105285645, ETA in seconds: 18573.240\n",
      "epoch: 6800, train loss: 6.209443855285644, val loss: 6.202998542785645, ETA in seconds: 18543.238\n",
      "epoch: 6810, train loss: 6.2082719802856445, val loss: 6.2043657302856445, ETA in seconds: 18511.977\n",
      "epoch: 6820, train loss: 6.205928230285645, val loss: 6.2033891677856445, ETA in seconds: 18479.989\n",
      "epoch: 6830, train loss: 6.210029792785645, val loss: 6.2043657302856445, ETA in seconds: 18448.285\n",
      "epoch: 6840, train loss: 6.208467292785644, val loss: 6.205928230285645, ETA in seconds: 18418.116\n",
      "epoch: 6850, train loss: 6.2092485427856445, val loss: 6.2043657302856445, ETA in seconds: 18386.572\n",
      "epoch: 6860, train loss: 6.208662605285644, val loss: 6.202217292785645, ETA in seconds: 18354.000\n",
      "epoch: 6870, train loss: 6.209053230285645, val loss: 6.2033891677856445, ETA in seconds: 18321.852\n",
      "epoch: 6880, train loss: 6.209834480285645, val loss: 6.203584480285644, ETA in seconds: 18288.967\n",
      "epoch: 6890, train loss: 6.2063188552856445, val loss: 6.204951667785645, ETA in seconds: 18255.947\n",
      "epoch: 6900, train loss: 6.208857917785645, val loss: 6.202021980285645, ETA in seconds: 18223.111\n",
      "epoch: 6910, train loss: 6.2102251052856445, val loss: 6.202803230285644, ETA in seconds: 18189.830\n",
      "epoch: 6920, train loss: 6.209053230285645, val loss: 6.203975105285645, ETA in seconds: 18156.343\n",
      "epoch: 6930, train loss: 6.2082719802856445, val loss: 6.202998542785645, ETA in seconds: 18122.917\n",
      "epoch: 6940, train loss: 6.207100105285645, val loss: 6.202021980285645, ETA in seconds: 18089.106\n",
      "epoch: 6950, train loss: 6.211396980285644, val loss: 6.203193855285645, ETA in seconds: 18055.201\n",
      "epoch: 6960, train loss: 6.212373542785644, val loss: 6.202803230285644, ETA in seconds: 18021.833\n",
      "epoch: 6970, train loss: 6.209443855285644, val loss: 6.203193855285645, ETA in seconds: 17987.312\n",
      "epoch: 6980, train loss: 6.2082719802856445, val loss: 6.204951667785645, ETA in seconds: 17953.113\n",
      "epoch: 6990, train loss: 6.206709480285644, val loss: 6.201826667785644, ETA in seconds: 17919.171\n",
      "epoch: 7000, train loss: 6.207686042785644, val loss: 6.202607917785644, ETA in seconds: 17885.249\n",
      "epoch: 7010, train loss: 6.208857917785645, val loss: 6.2043657302856445, ETA in seconds: 17851.471\n",
      "epoch: 7020, train loss: 6.209443855285644, val loss: 6.205146980285645, ETA in seconds: 17818.051\n",
      "epoch: 7030, train loss: 6.209053230285645, val loss: 6.206709480285644, ETA in seconds: 17782.808\n",
      "epoch: 7040, train loss: 6.208076667785645, val loss: 6.202607917785644, ETA in seconds: 17749.752\n",
      "epoch: 7050, train loss: 6.208857917785645, val loss: 6.206709480285644, ETA in seconds: 17717.069\n",
      "epoch: 7060, train loss: 6.210615730285644, val loss: 6.202998542785645, ETA in seconds: 17681.775\n",
      "epoch: 7070, train loss: 6.208662605285644, val loss: 6.202803230285644, ETA in seconds: 17646.055\n",
      "epoch: 7080, train loss: 6.2053422927856445, val loss: 6.204561042785644, ETA in seconds: 17609.818\n",
      "epoch: 7090, train loss: 6.208076667785645, val loss: 6.206123542785645, ETA in seconds: 17573.687\n",
      "epoch: 7100, train loss: 6.208857917785645, val loss: 6.201631355285644, ETA in seconds: 17537.616\n",
      "epoch: 7110, train loss: 6.2092485427856445, val loss: 6.204170417785645, ETA in seconds: 17500.758\n",
      "epoch: 7120, train loss: 6.207686042785644, val loss: 6.205537605285644, ETA in seconds: 17465.103\n",
      "epoch: 7130, train loss: 6.207100105285645, val loss: 6.203975105285645, ETA in seconds: 17428.174\n",
      "epoch: 7140, train loss: 6.2063188552856445, val loss: 6.202607917785644, ETA in seconds: 17391.589\n",
      "epoch: 7150, train loss: 6.209053230285645, val loss: 6.204951667785645, ETA in seconds: 17357.164\n",
      "epoch: 7160, train loss: 6.209639167785644, val loss: 6.2043657302856445, ETA in seconds: 17320.777\n",
      "epoch: 7170, train loss: 6.208857917785645, val loss: 6.2033891677856445, ETA in seconds: 17283.379\n",
      "epoch: 7180, train loss: 6.208467292785644, val loss: 6.205928230285645, ETA in seconds: 17246.136\n",
      "epoch: 7190, train loss: 6.209639167785644, val loss: 6.2063188552856445, ETA in seconds: 17209.741\n",
      "epoch: 7200, train loss: 6.206514167785644, val loss: 6.2024126052856445, ETA in seconds: 17171.411\n",
      "epoch: 7210, train loss: 6.209443855285644, val loss: 6.203193855285645, ETA in seconds: 17133.478\n",
      "epoch: 7220, train loss: 6.2082719802856445, val loss: 6.205537605285644, ETA in seconds: 17095.769\n",
      "epoch: 7230, train loss: 6.206709480285644, val loss: 6.208076667785645, ETA in seconds: 17057.887\n",
      "epoch: 7240, train loss: 6.209834480285645, val loss: 6.204951667785645, ETA in seconds: 17018.776\n",
      "epoch: 7250, train loss: 6.2102251052856445, val loss: 6.203779792785644, ETA in seconds: 16979.473\n",
      "epoch: 7260, train loss: 6.209834480285645, val loss: 6.205146980285645, ETA in seconds: 16940.225\n",
      "epoch: 7270, train loss: 6.209639167785644, val loss: 6.203584480285644, ETA in seconds: 16901.405\n",
      "epoch: 7280, train loss: 6.207100105285645, val loss: 6.2053422927856445, ETA in seconds: 16861.620\n",
      "epoch: 7290, train loss: 6.208076667785645, val loss: 6.2053422927856445, ETA in seconds: 16821.855\n",
      "epoch: 7300, train loss: 6.207686042785644, val loss: 6.202998542785645, ETA in seconds: 16782.793\n",
      "epoch: 7310, train loss: 6.209639167785644, val loss: 6.203193855285645, ETA in seconds: 16742.488\n",
      "epoch: 7320, train loss: 6.2072954177856445, val loss: 6.205537605285644, ETA in seconds: 16701.976\n",
      "epoch: 7330, train loss: 6.210029792785645, val loss: 6.2053422927856445, ETA in seconds: 16661.765\n",
      "epoch: 7340, train loss: 6.2092485427856445, val loss: 6.2033891677856445, ETA in seconds: 16622.536\n",
      "epoch: 7350, train loss: 6.208076667785645, val loss: 6.203584480285644, ETA in seconds: 16581.572\n",
      "epoch: 7360, train loss: 6.2082719802856445, val loss: 6.203779792785644, ETA in seconds: 16540.581\n",
      "epoch: 7370, train loss: 6.206904792785645, val loss: 6.206514167785644, ETA in seconds: 16499.454\n",
      "epoch: 7380, train loss: 6.2063188552856445, val loss: 6.202607917785644, ETA in seconds: 16458.368\n",
      "epoch: 7390, train loss: 6.208857917785645, val loss: 6.202803230285644, ETA in seconds: 16417.136\n",
      "epoch: 7400, train loss: 6.208857917785645, val loss: 6.204561042785644, ETA in seconds: 16375.692\n",
      "epoch: 7410, train loss: 6.209639167785644, val loss: 6.203779792785644, ETA in seconds: 16334.154\n",
      "epoch: 7420, train loss: 6.2082719802856445, val loss: 6.206904792785645, ETA in seconds: 16292.401\n",
      "epoch: 7430, train loss: 6.2072954177856445, val loss: 6.2043657302856445, ETA in seconds: 16250.419\n",
      "epoch: 7440, train loss: 6.206709480285644, val loss: 6.2024126052856445, ETA in seconds: 16208.107\n",
      "epoch: 7450, train loss: 6.2112016677856445, val loss: 6.206904792785645, ETA in seconds: 16165.834\n",
      "epoch: 7460, train loss: 6.2072954177856445, val loss: 6.207100105285645, ETA in seconds: 16123.371\n",
      "epoch: 7470, train loss: 6.2102251052856445, val loss: 6.203975105285645, ETA in seconds: 16081.227\n",
      "epoch: 7480, train loss: 6.209834480285645, val loss: 6.201826667785644, ETA in seconds: 16039.130\n",
      "epoch: 7490, train loss: 6.208076667785645, val loss: 6.204561042785644, ETA in seconds: 15996.820\n",
      "epoch: 7500, train loss: 6.209639167785644, val loss: 6.204756355285644, ETA in seconds: 15955.149\n",
      "epoch: 7510, train loss: 6.206904792785645, val loss: 6.204756355285644, ETA in seconds: 15912.274\n",
      "epoch: 7520, train loss: 6.209834480285645, val loss: 6.206904792785645, ETA in seconds: 15869.281\n",
      "epoch: 7530, train loss: 6.207490730285644, val loss: 6.202217292785645, ETA in seconds: 15825.870\n",
      "epoch: 7540, train loss: 6.208467292785644, val loss: 6.203779792785644, ETA in seconds: 15782.380\n",
      "epoch: 7550, train loss: 6.2092485427856445, val loss: 6.201631355285644, ETA in seconds: 15738.525\n",
      "epoch: 7560, train loss: 6.2102251052856445, val loss: 6.205146980285645, ETA in seconds: 15695.229\n",
      "epoch: 7570, train loss: 6.208467292785644, val loss: 6.2024126052856445, ETA in seconds: 15651.058\n",
      "epoch: 7580, train loss: 6.210615730285644, val loss: 6.204756355285644, ETA in seconds: 15606.727\n",
      "epoch: 7590, train loss: 6.206709480285644, val loss: 6.202998542785645, ETA in seconds: 15562.405\n",
      "epoch: 7600, train loss: 6.208467292785644, val loss: 6.201826667785644, ETA in seconds: 15517.483\n",
      "epoch: 7610, train loss: 6.210615730285644, val loss: 6.2043657302856445, ETA in seconds: 15473.288\n",
      "epoch: 7620, train loss: 6.210420417785644, val loss: 6.203779792785644, ETA in seconds: 15429.643\n",
      "epoch: 7630, train loss: 6.209639167785644, val loss: 6.2053422927856445, ETA in seconds: 15384.541\n",
      "epoch: 7640, train loss: 6.207881355285645, val loss: 6.205928230285645, ETA in seconds: 15340.034\n",
      "epoch: 7650, train loss: 6.210811042785645, val loss: 6.202607917785644, ETA in seconds: 15295.639\n",
      "epoch: 7660, train loss: 6.208662605285644, val loss: 6.202803230285644, ETA in seconds: 15249.781\n",
      "epoch: 7670, train loss: 6.209053230285645, val loss: 6.2024126052856445, ETA in seconds: 15203.789\n",
      "epoch: 7680, train loss: 6.209834480285645, val loss: 6.2053422927856445, ETA in seconds: 15158.485\n",
      "epoch: 7690, train loss: 6.2063188552856445, val loss: 6.202803230285644, ETA in seconds: 15112.068\n",
      "epoch: 7700, train loss: 6.209053230285645, val loss: 6.205537605285644, ETA in seconds: 15067.140\n",
      "epoch: 7710, train loss: 6.209053230285645, val loss: 6.203779792785644, ETA in seconds: 15020.992\n",
      "epoch: 7720, train loss: 6.208467292785644, val loss: 6.2043657302856445, ETA in seconds: 14974.096\n",
      "epoch: 7730, train loss: 6.209639167785644, val loss: 6.203975105285645, ETA in seconds: 14927.556\n",
      "epoch: 7740, train loss: 6.209639167785644, val loss: 6.206123542785645, ETA in seconds: 14880.601\n",
      "epoch: 7750, train loss: 6.207881355285645, val loss: 6.205537605285644, ETA in seconds: 14834.029\n",
      "epoch: 7760, train loss: 6.208076667785645, val loss: 6.203975105285645, ETA in seconds: 14786.335\n",
      "epoch: 7770, train loss: 6.208467292785644, val loss: 6.204756355285644, ETA in seconds: 14738.513\n",
      "epoch: 7780, train loss: 6.208857917785645, val loss: 6.202998542785645, ETA in seconds: 14691.555\n",
      "epoch: 7790, train loss: 6.208662605285644, val loss: 6.204561042785644, ETA in seconds: 14643.758\n",
      "epoch: 7800, train loss: 6.208662605285644, val loss: 6.204170417785645, ETA in seconds: 14595.405\n",
      "epoch: 7810, train loss: 6.209053230285645, val loss: 6.204951667785645, ETA in seconds: 14547.478\n",
      "epoch: 7820, train loss: 6.207100105285645, val loss: 6.204756355285644, ETA in seconds: 14499.031\n",
      "epoch: 7830, train loss: 6.206904792785645, val loss: 6.205146980285645, ETA in seconds: 14450.457\n",
      "epoch: 7840, train loss: 6.2092485427856445, val loss: 6.203975105285645, ETA in seconds: 14401.692\n",
      "epoch: 7850, train loss: 6.207490730285644, val loss: 6.2024126052856445, ETA in seconds: 14353.571\n",
      "epoch: 7860, train loss: 6.209834480285645, val loss: 6.204561042785644, ETA in seconds: 14304.367\n",
      "epoch: 7870, train loss: 6.208857917785645, val loss: 6.204561042785644, ETA in seconds: 14255.724\n",
      "epoch: 7880, train loss: 6.208467292785644, val loss: 6.203193855285645, ETA in seconds: 14206.424\n",
      "epoch: 7890, train loss: 6.210615730285644, val loss: 6.205732917785644, ETA in seconds: 14157.022\n",
      "epoch: 7900, train loss: 6.206514167785644, val loss: 6.2043657302856445, ETA in seconds: 14107.330\n",
      "epoch: 7910, train loss: 6.209053230285645, val loss: 6.201240730285645, ETA in seconds: 14057.429\n",
      "epoch: 7920, train loss: 6.208662605285644, val loss: 6.203193855285645, ETA in seconds: 14007.715\n",
      "epoch: 7930, train loss: 6.209639167785644, val loss: 6.203779792785644, ETA in seconds: 13957.964\n",
      "epoch: 7940, train loss: 6.2092485427856445, val loss: 6.202021980285645, ETA in seconds: 13907.654\n",
      "epoch: 7950, train loss: 6.208662605285644, val loss: 6.202803230285644, ETA in seconds: 13857.296\n",
      "epoch: 7960, train loss: 6.208857917785645, val loss: 6.204951667785645, ETA in seconds: 13806.689\n",
      "epoch: 7970, train loss: 6.207881355285645, val loss: 6.2014360427856445, ETA in seconds: 13756.607\n",
      "epoch: 7980, train loss: 6.2102251052856445, val loss: 6.203975105285645, ETA in seconds: 13706.118\n",
      "epoch: 7990, train loss: 6.209639167785644, val loss: 6.204756355285644, ETA in seconds: 13655.780\n",
      "epoch: 8000, train loss: 6.207100105285645, val loss: 6.206709480285644, ETA in seconds: 13604.160\n",
      "epoch: 8010, train loss: 6.208662605285644, val loss: 6.202998542785645, ETA in seconds: 13552.400\n",
      "epoch: 8020, train loss: 6.209053230285645, val loss: 6.202803230285644, ETA in seconds: 13500.717\n",
      "epoch: 8030, train loss: 6.210029792785645, val loss: 6.205732917785644, ETA in seconds: 13448.866\n",
      "epoch: 8040, train loss: 6.2072954177856445, val loss: 6.203193855285645, ETA in seconds: 13396.971\n",
      "epoch: 8050, train loss: 6.208076667785645, val loss: 6.204170417785645, ETA in seconds: 13344.590\n",
      "epoch: 8060, train loss: 6.209639167785644, val loss: 6.2033891677856445, ETA in seconds: 13292.376\n",
      "epoch: 8070, train loss: 6.2092485427856445, val loss: 6.201826667785644, ETA in seconds: 13239.949\n",
      "epoch: 8080, train loss: 6.208857917785645, val loss: 6.205732917785644, ETA in seconds: 13187.325\n",
      "epoch: 8090, train loss: 6.205732917785644, val loss: 6.205537605285644, ETA in seconds: 13135.121\n",
      "epoch: 8100, train loss: 6.208662605285644, val loss: 6.205732917785644, ETA in seconds: 13082.121\n",
      "epoch: 8110, train loss: 6.209639167785644, val loss: 6.2033891677856445, ETA in seconds: 13029.046\n",
      "epoch: 8120, train loss: 6.2112016677856445, val loss: 6.2072954177856445, ETA in seconds: 12975.656\n",
      "epoch: 8130, train loss: 6.206123542785645, val loss: 6.203584480285644, ETA in seconds: 12921.959\n",
      "epoch: 8140, train loss: 6.208857917785645, val loss: 6.202021980285645, ETA in seconds: 12868.093\n",
      "epoch: 8150, train loss: 6.2102251052856445, val loss: 6.2053422927856445, ETA in seconds: 12815.283\n",
      "epoch: 8160, train loss: 6.208076667785645, val loss: 6.2024126052856445, ETA in seconds: 12761.168\n",
      "epoch: 8170, train loss: 6.208076667785645, val loss: 6.203584480285644, ETA in seconds: 12707.652\n",
      "epoch: 8180, train loss: 6.210029792785645, val loss: 6.204561042785644, ETA in seconds: 12653.451\n",
      "epoch: 8190, train loss: 6.207686042785644, val loss: 6.204170417785645, ETA in seconds: 12599.310\n",
      "epoch: 8200, train loss: 6.209834480285645, val loss: 6.202998542785645, ETA in seconds: 12544.714\n",
      "epoch: 8210, train loss: 6.2053422927856445, val loss: 6.202803230285644, ETA in seconds: 12490.825\n",
      "epoch: 8220, train loss: 6.2082719802856445, val loss: 6.201826667785644, ETA in seconds: 12436.827\n",
      "epoch: 8230, train loss: 6.210615730285644, val loss: 6.204170417785645, ETA in seconds: 12381.879\n",
      "epoch: 8240, train loss: 6.207881355285645, val loss: 6.204170417785645, ETA in seconds: 12327.190\n",
      "epoch: 8250, train loss: 6.205732917785644, val loss: 6.2033891677856445, ETA in seconds: 12272.264\n",
      "epoch: 8260, train loss: 6.210029792785645, val loss: 6.205537605285644, ETA in seconds: 12217.045\n",
      "epoch: 8270, train loss: 6.2082719802856445, val loss: 6.206709480285644, ETA in seconds: 12161.233\n",
      "epoch: 8280, train loss: 6.209053230285645, val loss: 6.2043657302856445, ETA in seconds: 12105.032\n",
      "epoch: 8290, train loss: 6.209443855285644, val loss: 6.2053422927856445, ETA in seconds: 12048.702\n",
      "epoch: 8300, train loss: 6.2112016677856445, val loss: 6.204170417785645, ETA in seconds: 11993.165\n",
      "epoch: 8310, train loss: 6.2063188552856445, val loss: 6.2024126052856445, ETA in seconds: 11936.620\n",
      "epoch: 8320, train loss: 6.208662605285644, val loss: 6.202998542785645, ETA in seconds: 11879.895\n",
      "epoch: 8330, train loss: 6.207100105285645, val loss: 6.2033891677856445, ETA in seconds: 11822.887\n",
      "epoch: 8340, train loss: 6.2092485427856445, val loss: 6.204561042785644, ETA in seconds: 11765.983\n",
      "epoch: 8350, train loss: 6.209053230285645, val loss: 6.2033891677856445, ETA in seconds: 11708.597\n",
      "epoch: 8360, train loss: 6.207490730285644, val loss: 6.202998542785645, ETA in seconds: 11652.039\n",
      "epoch: 8370, train loss: 6.2092485427856445, val loss: 6.204170417785645, ETA in seconds: 11595.155\n",
      "epoch: 8380, train loss: 6.209053230285645, val loss: 6.2053422927856445, ETA in seconds: 11538.168\n",
      "epoch: 8390, train loss: 6.211787605285645, val loss: 6.203975105285645, ETA in seconds: 11480.298\n",
      "epoch: 8400, train loss: 6.2082719802856445, val loss: 6.204561042785644, ETA in seconds: 11422.645\n",
      "epoch: 8410, train loss: 6.210811042785645, val loss: 6.205537605285644, ETA in seconds: 11364.879\n",
      "epoch: 8420, train loss: 6.210420417785644, val loss: 6.205732917785644, ETA in seconds: 11306.862\n",
      "epoch: 8430, train loss: 6.206514167785644, val loss: 6.202607917785644, ETA in seconds: 11249.126\n",
      "epoch: 8440, train loss: 6.207490730285644, val loss: 6.205146980285645, ETA in seconds: 11190.567\n",
      "epoch: 8450, train loss: 6.2082719802856445, val loss: 6.203584480285644, ETA in seconds: 11132.736\n",
      "epoch: 8460, train loss: 6.211592292785644, val loss: 6.203975105285645, ETA in seconds: 11073.746\n",
      "epoch: 8470, train loss: 6.209834480285645, val loss: 6.2063188552856445, ETA in seconds: 11014.635\n",
      "epoch: 8480, train loss: 6.207490730285644, val loss: 6.202021980285645, ETA in seconds: 10955.346\n",
      "epoch: 8490, train loss: 6.208662605285644, val loss: 6.204561042785644, ETA in seconds: 10896.181\n",
      "epoch: 8500, train loss: 6.208662605285644, val loss: 6.203779792785644, ETA in seconds: 10837.018\n",
      "epoch: 8510, train loss: 6.2072954177856445, val loss: 6.204170417785645, ETA in seconds: 10777.065\n",
      "epoch: 8520, train loss: 6.209639167785644, val loss: 6.202217292785645, ETA in seconds: 10717.053\n",
      "epoch: 8530, train loss: 6.2092485427856445, val loss: 6.204951667785645, ETA in seconds: 10657.064\n",
      "epoch: 8540, train loss: 6.209443855285644, val loss: 6.203975105285645, ETA in seconds: 10597.048\n",
      "epoch: 8550, train loss: 6.209834480285645, val loss: 6.205732917785644, ETA in seconds: 10537.063\n",
      "epoch: 8560, train loss: 6.207100105285645, val loss: 6.2063188552856445, ETA in seconds: 10476.792\n",
      "epoch: 8570, train loss: 6.207490730285644, val loss: 6.203193855285645, ETA in seconds: 10415.951\n",
      "epoch: 8580, train loss: 6.211592292785644, val loss: 6.2043657302856445, ETA in seconds: 10354.917\n",
      "epoch: 8590, train loss: 6.208662605285644, val loss: 6.203975105285645, ETA in seconds: 10293.515\n",
      "epoch: 8600, train loss: 6.210811042785645, val loss: 6.2033891677856445, ETA in seconds: 10232.380\n",
      "epoch: 8610, train loss: 6.210029792785645, val loss: 6.201631355285644, ETA in seconds: 10170.914\n",
      "epoch: 8620, train loss: 6.207686042785644, val loss: 6.2043657302856445, ETA in seconds: 10109.285\n",
      "epoch: 8630, train loss: 6.205537605285644, val loss: 6.2033891677856445, ETA in seconds: 10047.572\n",
      "epoch: 8640, train loss: 6.208857917785645, val loss: 6.205537605285644, ETA in seconds: 9985.549\n",
      "epoch: 8650, train loss: 6.209834480285645, val loss: 6.203584480285644, ETA in seconds: 9923.396\n",
      "epoch: 8660, train loss: 6.209053230285645, val loss: 6.2053422927856445, ETA in seconds: 9861.315\n",
      "epoch: 8670, train loss: 6.208662605285644, val loss: 6.205146980285645, ETA in seconds: 9799.020\n",
      "epoch: 8680, train loss: 6.2082719802856445, val loss: 6.205537605285644, ETA in seconds: 9736.318\n",
      "epoch: 8690, train loss: 6.207686042785644, val loss: 6.204951667785645, ETA in seconds: 9674.248\n",
      "epoch: 8700, train loss: 6.207686042785644, val loss: 6.2063188552856445, ETA in seconds: 9611.240\n",
      "epoch: 8710, train loss: 6.208467292785644, val loss: 6.205537605285644, ETA in seconds: 9548.023\n",
      "epoch: 8720, train loss: 6.2092485427856445, val loss: 6.203193855285645, ETA in seconds: 9484.603\n",
      "epoch: 8730, train loss: 6.207100105285645, val loss: 6.204170417785645, ETA in seconds: 9421.080\n",
      "epoch: 8740, train loss: 6.209443855285644, val loss: 6.205146980285645, ETA in seconds: 9357.329\n",
      "epoch: 8750, train loss: 6.209639167785644, val loss: 6.205537605285644, ETA in seconds: 9293.890\n",
      "epoch: 8760, train loss: 6.207881355285645, val loss: 6.204561042785644, ETA in seconds: 9230.204\n",
      "epoch: 8770, train loss: 6.209053230285645, val loss: 6.203584480285644, ETA in seconds: 9166.340\n",
      "epoch: 8780, train loss: 6.207881355285645, val loss: 6.2043657302856445, ETA in seconds: 9101.771\n",
      "epoch: 8790, train loss: 6.206904792785645, val loss: 6.204170417785645, ETA in seconds: 9037.070\n",
      "epoch: 8800, train loss: 6.207100105285645, val loss: 6.202803230285644, ETA in seconds: 8972.395\n",
      "epoch: 8810, train loss: 6.206709480285644, val loss: 6.203975105285645, ETA in seconds: 8907.358\n",
      "epoch: 8820, train loss: 6.207490730285644, val loss: 6.203193855285645, ETA in seconds: 8842.324\n",
      "epoch: 8830, train loss: 6.2082719802856445, val loss: 6.206123542785645, ETA in seconds: 8777.064\n",
      "epoch: 8840, train loss: 6.207881355285645, val loss: 6.204561042785644, ETA in seconds: 8711.454\n",
      "epoch: 8850, train loss: 6.207881355285645, val loss: 6.201826667785644, ETA in seconds: 8646.030\n",
      "epoch: 8860, train loss: 6.209053230285645, val loss: 6.202803230285644, ETA in seconds: 8580.507\n",
      "epoch: 8870, train loss: 6.207881355285645, val loss: 6.204561042785644, ETA in seconds: 8514.449\n",
      "epoch: 8880, train loss: 6.209443855285644, val loss: 6.2043657302856445, ETA in seconds: 8448.269\n",
      "epoch: 8890, train loss: 6.208076667785645, val loss: 6.2043657302856445, ETA in seconds: 8381.913\n",
      "epoch: 8900, train loss: 6.207686042785644, val loss: 6.203193855285645, ETA in seconds: 8315.626\n",
      "epoch: 8910, train loss: 6.206904792785645, val loss: 6.205146980285645, ETA in seconds: 8249.290\n",
      "epoch: 8920, train loss: 6.2092485427856445, val loss: 6.201826667785644, ETA in seconds: 8182.603\n",
      "epoch: 8930, train loss: 6.208857917785645, val loss: 6.2033891677856445, ETA in seconds: 8115.560\n",
      "epoch: 8940, train loss: 6.209053230285645, val loss: 6.203193855285645, ETA in seconds: 8048.367\n",
      "epoch: 8950, train loss: 6.208467292785644, val loss: 6.2043657302856445, ETA in seconds: 7981.360\n",
      "epoch: 8960, train loss: 6.205732917785644, val loss: 6.200850105285644, ETA in seconds: 7913.980\n",
      "epoch: 8970, train loss: 6.2072954177856445, val loss: 6.204756355285644, ETA in seconds: 7846.577\n",
      "epoch: 8980, train loss: 6.2082719802856445, val loss: 6.203584480285644, ETA in seconds: 7778.892\n",
      "epoch: 8990, train loss: 6.209639167785644, val loss: 6.2053422927856445, ETA in seconds: 7711.280\n",
      "epoch: 9000, train loss: 6.2082719802856445, val loss: 6.2072954177856445, ETA in seconds: 7643.370\n",
      "epoch: 9010, train loss: 6.210811042785645, val loss: 6.203193855285645, ETA in seconds: 7575.301\n",
      "epoch: 9020, train loss: 6.210420417785644, val loss: 6.201826667785644, ETA in seconds: 7507.911\n",
      "epoch: 9030, train loss: 6.208662605285644, val loss: 6.202607917785644, ETA in seconds: 7439.221\n",
      "epoch: 9040, train loss: 6.208467292785644, val loss: 6.204561042785644, ETA in seconds: 7370.526\n",
      "epoch: 9050, train loss: 6.208662605285644, val loss: 6.201631355285644, ETA in seconds: 7301.502\n",
      "epoch: 9060, train loss: 6.206709480285644, val loss: 6.2024126052856445, ETA in seconds: 7232.287\n",
      "epoch: 9070, train loss: 6.207686042785644, val loss: 6.203584480285644, ETA in seconds: 7162.925\n",
      "epoch: 9080, train loss: 6.210615730285644, val loss: 6.203975105285645, ETA in seconds: 7093.617\n",
      "epoch: 9090, train loss: 6.207686042785644, val loss: 6.202607917785644, ETA in seconds: 7023.872\n",
      "epoch: 9100, train loss: 6.208076667785645, val loss: 6.203779792785644, ETA in seconds: 6953.987\n",
      "epoch: 9110, train loss: 6.209053230285645, val loss: 6.2043657302856445, ETA in seconds: 6883.902\n",
      "epoch: 9120, train loss: 6.205732917785644, val loss: 6.204951667785645, ETA in seconds: 6813.647\n",
      "epoch: 9130, train loss: 6.208467292785644, val loss: 6.205732917785644, ETA in seconds: 6743.285\n",
      "epoch: 9140, train loss: 6.208467292785644, val loss: 6.206514167785644, ETA in seconds: 6672.982\n",
      "epoch: 9150, train loss: 6.208662605285644, val loss: 6.205146980285645, ETA in seconds: 6602.327\n",
      "epoch: 9160, train loss: 6.210811042785645, val loss: 6.206904792785645, ETA in seconds: 6531.522\n",
      "epoch: 9170, train loss: 6.208857917785645, val loss: 6.204951667785645, ETA in seconds: 6460.884\n",
      "epoch: 9180, train loss: 6.208076667785645, val loss: 6.203779792785644, ETA in seconds: 6389.829\n",
      "epoch: 9190, train loss: 6.209639167785644, val loss: 6.201045417785645, ETA in seconds: 6318.570\n",
      "epoch: 9200, train loss: 6.208857917785645, val loss: 6.202607917785644, ETA in seconds: 6247.286\n",
      "epoch: 9210, train loss: 6.208857917785645, val loss: 6.2053422927856445, ETA in seconds: 6175.947\n",
      "epoch: 9220, train loss: 6.211396980285644, val loss: 6.204170417785645, ETA in seconds: 6104.178\n",
      "epoch: 9230, train loss: 6.210811042785645, val loss: 6.207686042785644, ETA in seconds: 6032.217\n",
      "epoch: 9240, train loss: 6.2092485427856445, val loss: 6.204951667785645, ETA in seconds: 5960.147\n",
      "epoch: 9250, train loss: 6.211592292785644, val loss: 6.2053422927856445, ETA in seconds: 5887.887\n",
      "epoch: 9260, train loss: 6.207881355285645, val loss: 6.203779792785644, ETA in seconds: 5815.866\n",
      "epoch: 9270, train loss: 6.209443855285644, val loss: 6.203779792785644, ETA in seconds: 5743.695\n",
      "epoch: 9280, train loss: 6.207686042785644, val loss: 6.202607917785644, ETA in seconds: 5671.078\n",
      "epoch: 9290, train loss: 6.209834480285645, val loss: 6.203584480285644, ETA in seconds: 5598.204\n",
      "epoch: 9300, train loss: 6.2092485427856445, val loss: 6.203193855285645, ETA in seconds: 5525.197\n",
      "epoch: 9310, train loss: 6.2121782302856445, val loss: 6.206904792785645, ETA in seconds: 5452.015\n",
      "epoch: 9320, train loss: 6.210615730285644, val loss: 6.206123542785645, ETA in seconds: 5378.980\n",
      "epoch: 9330, train loss: 6.206709480285644, val loss: 6.205537605285644, ETA in seconds: 5305.865\n",
      "epoch: 9340, train loss: 6.2082719802856445, val loss: 6.204561042785644, ETA in seconds: 5232.064\n",
      "epoch: 9350, train loss: 6.208467292785644, val loss: 6.2024126052856445, ETA in seconds: 5158.144\n",
      "epoch: 9360, train loss: 6.211982917785645, val loss: 6.201240730285645, ETA in seconds: 5084.283\n",
      "epoch: 9370, train loss: 6.209639167785644, val loss: 6.204561042785644, ETA in seconds: 5010.108\n",
      "epoch: 9380, train loss: 6.2092485427856445, val loss: 6.202998542785645, ETA in seconds: 4935.976\n",
      "epoch: 9390, train loss: 6.207881355285645, val loss: 6.204756355285644, ETA in seconds: 4861.460\n",
      "epoch: 9400, train loss: 6.207881355285645, val loss: 6.204951667785645, ETA in seconds: 4786.916\n",
      "epoch: 9410, train loss: 6.210420417785644, val loss: 6.205537605285644, ETA in seconds: 4712.008\n",
      "epoch: 9420, train loss: 6.209639167785644, val loss: 6.201631355285644, ETA in seconds: 4636.989\n",
      "epoch: 9430, train loss: 6.2092485427856445, val loss: 6.202021980285645, ETA in seconds: 4561.779\n",
      "epoch: 9440, train loss: 6.209443855285644, val loss: 6.205732917785644, ETA in seconds: 4486.385\n",
      "epoch: 9450, train loss: 6.210615730285644, val loss: 6.206123542785645, ETA in seconds: 4410.909\n",
      "epoch: 9460, train loss: 6.208076667785645, val loss: 6.204561042785644, ETA in seconds: 4335.195\n",
      "epoch: 9470, train loss: 6.2102251052856445, val loss: 6.2004594802856445, ETA in seconds: 4259.376\n",
      "epoch: 9480, train loss: 6.2072954177856445, val loss: 6.205537605285644, ETA in seconds: 4183.337\n",
      "epoch: 9490, train loss: 6.208467292785644, val loss: 6.202998542785645, ETA in seconds: 4107.071\n",
      "epoch: 9500, train loss: 6.209443855285644, val loss: 6.203975105285645, ETA in seconds: 4030.688\n",
      "epoch: 9510, train loss: 6.205928230285645, val loss: 6.204561042785644, ETA in seconds: 3954.148\n",
      "epoch: 9520, train loss: 6.208467292785644, val loss: 6.205537605285644, ETA in seconds: 3877.385\n",
      "epoch: 9530, train loss: 6.208467292785644, val loss: 6.2033891677856445, ETA in seconds: 3800.561\n",
      "epoch: 9540, train loss: 6.207881355285645, val loss: 6.205732917785644, ETA in seconds: 3723.455\n",
      "epoch: 9550, train loss: 6.208662605285644, val loss: 6.205928230285645, ETA in seconds: 3646.181\n",
      "epoch: 9560, train loss: 6.206709480285644, val loss: 6.2072954177856445, ETA in seconds: 3568.735\n",
      "epoch: 9570, train loss: 6.209053230285645, val loss: 6.205732917785644, ETA in seconds: 3491.216\n",
      "epoch: 9580, train loss: 6.2092485427856445, val loss: 6.204951667785645, ETA in seconds: 3413.422\n",
      "epoch: 9590, train loss: 6.211787605285645, val loss: 6.202021980285645, ETA in seconds: 3335.453\n",
      "epoch: 9600, train loss: 6.208857917785645, val loss: 6.204951667785645, ETA in seconds: 3257.401\n",
      "epoch: 9610, train loss: 6.206709480285644, val loss: 6.204561042785644, ETA in seconds: 3179.208\n",
      "epoch: 9620, train loss: 6.208076667785645, val loss: 6.206514167785644, ETA in seconds: 3100.780\n",
      "epoch: 9630, train loss: 6.209443855285644, val loss: 6.2024126052856445, ETA in seconds: 3022.194\n",
      "epoch: 9640, train loss: 6.2072954177856445, val loss: 6.204170417785645, ETA in seconds: 2943.603\n",
      "epoch: 9650, train loss: 6.208076667785645, val loss: 6.202803230285644, ETA in seconds: 2864.720\n",
      "epoch: 9660, train loss: 6.208662605285644, val loss: 6.204561042785644, ETA in seconds: 2785.671\n",
      "epoch: 9670, train loss: 6.210615730285644, val loss: 6.204951667785645, ETA in seconds: 2706.480\n",
      "epoch: 9680, train loss: 6.209443855285644, val loss: 6.202217292785645, ETA in seconds: 2627.160\n",
      "epoch: 9690, train loss: 6.2092485427856445, val loss: 6.201826667785644, ETA in seconds: 2547.741\n",
      "epoch: 9700, train loss: 6.2092485427856445, val loss: 6.203193855285645, ETA in seconds: 2468.050\n",
      "epoch: 9710, train loss: 6.209053230285645, val loss: 6.2024126052856445, ETA in seconds: 2388.199\n",
      "epoch: 9720, train loss: 6.208857917785645, val loss: 6.202803230285644, ETA in seconds: 2308.170\n",
      "epoch: 9730, train loss: 6.209053230285645, val loss: 6.203193855285645, ETA in seconds: 2227.973\n",
      "epoch: 9740, train loss: 6.208076667785645, val loss: 6.2033891677856445, ETA in seconds: 2147.747\n",
      "epoch: 9750, train loss: 6.2092485427856445, val loss: 6.205537605285644, ETA in seconds: 2067.247\n",
      "epoch: 9760, train loss: 6.209443855285644, val loss: 6.207881355285645, ETA in seconds: 1986.593\n",
      "epoch: 9770, train loss: 6.208467292785644, val loss: 6.204561042785644, ETA in seconds: 1905.723\n",
      "epoch: 9780, train loss: 6.2092485427856445, val loss: 6.2033891677856445, ETA in seconds: 1824.725\n",
      "epoch: 9790, train loss: 6.209053230285645, val loss: 6.203584480285644, ETA in seconds: 1743.526\n",
      "epoch: 9800, train loss: 6.209639167785644, val loss: 6.202803230285644, ETA in seconds: 1662.137\n",
      "epoch: 9810, train loss: 6.210029792785645, val loss: 6.205146980285645, ETA in seconds: 1580.623\n",
      "epoch: 9820, train loss: 6.205146980285645, val loss: 6.204170417785645, ETA in seconds: 1498.953\n",
      "epoch: 9830, train loss: 6.209443855285644, val loss: 6.206709480285644, ETA in seconds: 1417.139\n",
      "epoch: 9840, train loss: 6.208857917785645, val loss: 6.2033891677856445, ETA in seconds: 1335.142\n",
      "epoch: 9850, train loss: 6.2092485427856445, val loss: 6.204170417785645, ETA in seconds: 1253.009\n",
      "epoch: 9860, train loss: 6.205537605285644, val loss: 6.204951667785645, ETA in seconds: 1170.618\n",
      "epoch: 9870, train loss: 6.208857917785645, val loss: 6.205146980285645, ETA in seconds: 1088.062\n",
      "epoch: 9880, train loss: 6.2063188552856445, val loss: 6.2043657302856445, ETA in seconds: 1005.342\n",
      "epoch: 9890, train loss: 6.207881355285645, val loss: 6.206123542785645, ETA in seconds: 922.458\n",
      "epoch: 9900, train loss: 6.207100105285645, val loss: 6.2004594802856445, ETA in seconds: 839.417\n",
      "epoch: 9910, train loss: 6.207490730285644, val loss: 6.204951667785645, ETA in seconds: 756.258\n",
      "epoch: 9920, train loss: 6.211787605285645, val loss: 6.205146980285645, ETA in seconds: 672.887\n",
      "epoch: 9930, train loss: 6.207881355285645, val loss: 6.203584480285644, ETA in seconds: 589.353\n",
      "epoch: 9940, train loss: 6.2082719802856445, val loss: 6.2043657302856445, ETA in seconds: 505.663\n",
      "epoch: 9950, train loss: 6.208467292785644, val loss: 6.2053422927856445, ETA in seconds: 421.795\n",
      "epoch: 9960, train loss: 6.209639167785644, val loss: 6.203975105285645, ETA in seconds: 337.767\n",
      "epoch: 9970, train loss: 6.207686042785644, val loss: 6.202607917785644, ETA in seconds: 253.573\n",
      "epoch: 9980, train loss: 6.210811042785645, val loss: 6.202998542785645, ETA in seconds: 169.218\n",
      "epoch: 9990, train loss: 6.2082719802856445, val loss: 6.204951667785645, ETA in seconds: 84.692\n",
      "validation loss:  6.204951667785645\n"
     ]
    }
   ],
   "source": [
    "# GPT model\n",
    "## Param\n",
    "num_blocks = 2\n",
    "embed_dim = 512\n",
    "num_heads = 16\n",
    "ff_hidden_dim = embed_dim*4\n",
    "dropout = 0.\n",
    "\n",
    "lr = 1e-5\n",
    "\n",
    "model = GPT(max_seq_len=64,vocab_size=sp.GetPieceSize(),num_blocks=num_blocks,embed_dim=embed_dim,\n",
    "            num_heads=num_heads,ff_hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000,eta_min=lr*0.1)\n",
    "losses = train(model,optimizer,scheduler,print_logs=True,use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 61, 451, 285,  68, 469, 475, 464, 154, 459,  78,  33, 463, 465, 447,\n",
      "         37, 103, 117, 458,  46,  89, 450,  30, 479,  71, 244, 115, 265, 459,\n",
      "        131,  62, 460, 150])\n",
      "le,-- BRUTUS: Mark'd you his lip and eyes? SICINIUS: Nay. but\n",
      "le,-- BR I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "x_train,_ = get_batches(dataset,\"train\",batch_size=1,context_window=CONTEXT_SIZE)\n",
    "x_val,_ = get_batches(dataset,\"val\",batch_size=1,context_window=CONTEXT_SIZE)\n",
    "\n",
    "print(x_train[0])\n",
    "print(sp.decode(x_train[0].tolist()))\n",
    "pred_sentence = model.generate(x_train[:,:5],max_new_tokens=32)\n",
    "print(sp.decode(pred_sentence.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
