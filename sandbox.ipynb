{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "tensor([[[3.1623, 4.1623, 3.1623, 4.1623, 3.1623, 4.1623, 3.1623, 4.1623,\n",
      "          3.1623, 4.1623],\n",
      "         [4.0037, 3.7026, 3.1874, 4.1620, 3.1629, 4.1623, 3.1623, 4.1623,\n",
      "          3.1623, 4.1623],\n",
      "         [4.0716, 2.7461, 3.2125, 4.1610, 3.1635, 4.1623, 3.1623, 4.1623,\n",
      "          3.1623, 4.1623]]])\n"
     ]
    }
   ],
   "source": [
    "# sinusoidal position embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_seq_len: int,embed_model_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len(int) : length of input sequence\n",
    "            embed_model_dim(int) : dimension of embedding\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding,self).__init__()\n",
    "        self.embed_dim = torch.tensor(embed_model_dim).float()\n",
    "        #self.embed_dim = embed_model_dim\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,embed_model_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,embed_model_dim,2):\n",
    "                pe[pos,i] = torch.sin(torch.tensor(pos/(10000**(2*i/embed_model_dim))))\n",
    "                pe[pos,i+1] = torch.cos(torch.tensor(pos/(10000**(2*i/embed_model_dim))))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x(torch.Tensor) : input tensor (B,Length,Dim)\n",
    "        Returns:\n",
    "            torch.Tensor : input tensor + positional embedding\n",
    "        \"\"\"\n",
    "        x = x + torch.sqrt(self.embed_dim) # make embeddings relatively larger\n",
    "        #x = x + math.sqrt(self.embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len],requires_grad=False)\n",
    "        return x\n",
    "\n",
    "test_input = torch.zeros(1,3,10)\n",
    "print(test_input)\n",
    "posembed = PositionalEmbedding(3,10)\n",
    "test_output = posembed(test_input)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,dim: int,num_heads :int=8,qkv_bias: bool=True,dropout: float=0.,\n",
    "                 is_causal: bool=False,quiet_attention: bool=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): 埋め込み次元数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            qkv_bias (bool): MultiHeadAttentionの埋め込み全結合層にbiasを付けるかどうか\n",
    "            dropout (float): ドロップアウト確率\n",
    "            is_causal (bool): Trueの場合、masked multi-head attentionを行う\n",
    "            quiet_attention (bool): Trueの場合、softmaxの分母に1を足す\n",
    "        Note:\n",
    "            quiet attentionのreference\n",
    "            https://www.evanmiller.org/attention-is-off-by-one.html\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.is_causal = is_causal\n",
    "        self.quiet_attention = quiet_attention\n",
    "        self.num_heads = num_heads\n",
    "        assert dim % num_heads == 0, f\"The hidden size {dim} is not a multiple of the number of head attention\"\n",
    "        self.hidden_dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(dim,dim,bias=qkv_bias)\n",
    "        self.key = nn.Linear(dim,dim,bias=qkv_bias)\n",
    "        self.value = nn.Linear(dim,dim,bias=qkv_bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x,mask=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor (B,Length,Dim)\n",
    "            mask (bool): Trueの場合、masked multi-head attentionを行う\n",
    "        \"\"\"\n",
    "        batch_size,num_patches,_ = x.size()\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # マルチヘッドに分割\n",
    "        #multihead_qkv_shape = q.size()[:-1] + (self.num_heads, self.head_dim)\n",
    "        multihead_qkv_shape = torch.Size([batch_size, num_patches, self.num_heads, self.head_dim])\n",
    "        qs = q.view(multihead_qkv_shape)\n",
    "        qs = qs.permute(0, 2, 1, 3)\n",
    "        ks = k.view(multihead_qkv_shape)\n",
    "        ks = ks.permute(0, 2, 1, 3)\n",
    "        ks_T = ks.transpose(2,3)\n",
    "        vs = v.view(multihead_qkv_shape)\n",
    "        vs = vs.permute(0, 2, 1, 3)\n",
    "        \n",
    "        scaled_dot_product = qs@ks_T / np.sqrt(self.head_dim)\n",
    "\n",
    "        # masked multi-head attention\n",
    "        if self.is_causal:\n",
    "            mask = nn.Transformer.generate_square_subsequent_mask(num_patches,device=x.device)\n",
    "            scaled_dot_product = scaled_dot_product + mask\n",
    "\n",
    "        if self.quiet_attention:\n",
    "            self_attention = _softmax_one(scaled_dot_product,dim=-1)\n",
    "        else:\n",
    "            self_attention = nn.functional.softmax(scaled_dot_product,dim=-1)\n",
    "        self_attention = self.dropout(self_attention) # 実装上はあるっぽいけど何なんこれ\n",
    "        \n",
    "        context_layer = self_attention@vs\n",
    "        #context_layer = context_layer.transpose(1,2).reshape(batch_size,num_patchs,self.hidden_dim)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous().reshape(batch_size,num_patches,self.hidden_dim)\n",
    "        out = self.projection(context_layer)\n",
    "        #out = context_layer\n",
    "        \n",
    "        return out\n",
    "\n",
    "def _softmax_one(x,dim=-1):\n",
    "    \"\"\" https://www.evanmiller.org/attention-is-off-by-one.html の実装\n",
    "    Args:\n",
    "        x (torch.Tensor):\n",
    "        dim (int, optional): softmaxを取る次元. Defaults to -1.\n",
    "    Returns:\n",
    "        torch.Tensor: softmaxを取った後のテンソル\n",
    "    \"\"\"\n",
    "    x = x - x.max(dim=dim, keepdim=True).values # subtract the max for stability\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / (1+exp_x.sum(dim=dim,keepdim=True))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,dim: int,hidden_dim: int=768*4,activation=nn.GELU(),dropout: float=0.):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): 埋め込み次元数\n",
    "            hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            activation (torch.nn.modules.activation): pytorchの活性化関数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim,hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim,dim)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 4, 4])\n",
      "torch.Size([4, 4])\n",
      "tensor([[[[ 0.5834,    -inf,    -inf,    -inf],\n",
      "          [ 0.5834,  0.5834,    -inf,    -inf],\n",
      "          [ 1.3026,  1.3026,  2.5177,    -inf],\n",
      "          [ 1.3026,  1.3026,  2.5177,  2.5177]],\n",
      "\n",
      "         [[-0.2067,    -inf,    -inf,    -inf],\n",
      "          [-0.2067, -0.2067,    -inf,    -inf],\n",
      "          [-0.3710, -0.3710, -0.4832,    -inf],\n",
      "          [-0.3710, -0.3710, -0.4832, -0.4832]]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 2, 4, 4])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "          [0.1862, 0.1862, 0.6276, 0.0000],\n",
      "          [0.1144, 0.1144, 0.3856, 0.3856]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5000, 0.5000, 0.0000, 0.0000],\n",
      "          [0.3456, 0.3456, 0.3089, 0.0000],\n",
      "          [0.2640, 0.2640, 0.2360, 0.2360]]]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[ 0.0877, -0.1831,  0.2694, -0.2178, -0.2970, -0.6170, -1.1308,\n",
      "           1.1616, -0.5541, -0.4736],\n",
      "         [ 0.0877, -0.1831,  0.2694, -0.2178, -0.2970, -0.6170, -1.1308,\n",
      "           1.1616, -0.5541, -0.4736],\n",
      "         [ 0.0564, -0.2226,  0.2499, -0.4475, -0.5826, -0.7498, -1.5162,\n",
      "           1.5205, -0.7081, -0.5314],\n",
      "         [ 0.0492, -0.2317,  0.2454, -0.5001, -0.6479, -0.8200, -1.7197,\n",
      "           1.7100, -0.7894, -0.5620]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[\n",
    "    [1,1,1,1,1,1,1,1,1,1],\n",
    "    [1,1,1,1,1,1,1,1,1,1],\n",
    "    [2,2,2,2,2,2,2,2,2,2],\n",
    "    [2,2,2,2,2,2,2,2,2,2],\n",
    "]]).float()\n",
    "\n",
    "batch_size,num_patches,_ = x.size()\n",
    "query = nn.Linear(10,10)\n",
    "key = nn.Linear(10,10)\n",
    "value = nn.Linear(10,10)\n",
    "\n",
    "q = query(x)\n",
    "k = key(x)\n",
    "v = value(x)\n",
    "\n",
    "num_heads = 2\n",
    "head_dim = 10 // num_heads\n",
    "\n",
    "# マルチヘッドに分割\n",
    "#multihead_qkv_shape = q.size()[:-1] + (self.num_heads, self.head_dim)\n",
    "multihead_qkv_shape = torch.Size([1, 4, num_heads, head_dim])\n",
    "qs = q.view(multihead_qkv_shape) # (b, n_patch, n_heads, head_dim)\n",
    "qs = qs.permute(0, 2, 1, 3)\n",
    "ks = k.view(multihead_qkv_shape)\n",
    "ks = ks.permute(0, 2, 1, 3)\n",
    "ks_T = ks.transpose(2,3)\n",
    "vs = v.view(multihead_qkv_shape)\n",
    "vs = vs.permute(0, 2, 1, 3)\n",
    "\n",
    "scaled_dot_product = qs@ks_T / np.sqrt(head_dim) # (b, n_heads, n_patch, n_patch)\n",
    "print(scaled_dot_product.shape)\n",
    "\n",
    "# masked multi-head attention\n",
    "if True:\n",
    "    mask = nn.Transformer.generate_square_subsequent_mask(num_patches,device=x.device)\n",
    "    print(mask.shape)\n",
    "    scaled_dot_product = scaled_dot_product + mask\n",
    "    print(scaled_dot_product)\n",
    "\n",
    "self_attention = nn.functional.softmax(scaled_dot_product,dim=-1)\n",
    "print(self_attention.shape)\n",
    "print(self_attention)\n",
    "\n",
    "context_layer = self_attention@vs\n",
    "#context_layer = context_layer.transpose(1,2).reshape(batch_size,num_patchs,self.hidden_dim)\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous().reshape(batch_size,num_patches,10)\n",
    "#out = self.projection(context_layer)\n",
    "out = context_layer\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): 埋め込み次元数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            ff_hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadSelfAttention(dim=embed_dim,num_heads=num_heads,dropout=dropout)\n",
    "        self.ff = FeedForward(dim=embed_dim,hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        z = self.mhsa(x)\n",
    "        z = self.norm1(z)\n",
    "        x = x + z\n",
    "        z = self.ff(x)\n",
    "        z = self.norm2(z)\n",
    "        x = x + z\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,max_seq_len: int, vocab_size: int, embed_dim: int, \n",
    "                 ff_hidden_dim: int, num_blocks: int, num_heads: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len (int): 入力系列の最大長\n",
    "            vocab_size (int): 語彙数\n",
    "            embed_dim (int): 埋め込み次元数\n",
    "            ff_hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            num_blocks (int): TransformerBlockの数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(max_seq_len,embed_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim=embed_dim,num_heads=num_heads,ff_hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "    \n",
    "    def forward(self,token_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids (torch.Tensor): 入力トークンID (batch_size,seq_len)\n",
    "        Returns:\n",
    "            torch.Tensor: TransformerEncoderの出力 (batch_size,seq_len,embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(token_ids)\n",
    "        x = self.positional_embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "test_input = torch.tensor([[1,2,3,4,5,6,7,8,9,10]])\n",
    "print(test_input.shape)\n",
    "transformer_encoder = TransformerEncoder(max_seq_len=10,vocab_size=30,embed_dim=10,ff_hidden_dim=40,num_blocks=2,num_heads=2,dropout=0.1)\n",
    "test_output = transformer_encoder(test_input)\n",
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self,dim: int,num_heads: int=8,dropout: float=0.,quiet_attention: bool=False, proj: bool=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): 埋め込み次元数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            dropout (float): ドロップアウト確率\n",
    "            quiet_attention (bool): Trueの場合、softmaxの分母に1を足す\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.quiet_attention = quiet_attention\n",
    "        self.num_heads = num_heads\n",
    "        assert dim % num_heads == 0, f\"The hidden size {dim} is not a multiple of the number of head attention\"\n",
    "        self.hidden_dim = dim\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        if proj:\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Linear(dim,dim),\n",
    "                nn.Dropout(p=dropout),\n",
    "            )\n",
    "        else:\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Identity(),\n",
    "                nn.Dropout(p=dropout),\n",
    "            )\n",
    "    \n",
    "    def forward(self,query,key,value):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            query (torch.Tensor): query (batch_size,query_len,hidden_dim)\n",
    "            key (torch.Tensor): key (batch_size,key_len,hidden_dim)\n",
    "            value (torch.Tensor): value (batch_size,value_len,hidden_dim)\n",
    "        \"\"\" \n",
    "        # マルチヘッドに分割\n",
    "        batch_size,query_len,_ = query.size()\n",
    "        batch_size,key_len,_ = key.size()\n",
    "        batch_size,value_len,_ = value.size()\n",
    "\n",
    "        multihead_q_shape = torch.Size([batch_size, query_len, self.num_heads, self.head_dim])\n",
    "        multihead_k_shape = torch.Size([batch_size, key_len, self.num_heads, self.head_dim])\n",
    "        multihead_v_shape = torch.Size([batch_size, value_len, self.num_heads, self.head_dim])\n",
    "\n",
    "        qs = query.view(multihead_q_shape)\n",
    "        qs = qs.permute(0, 2, 1, 3)\n",
    "        ks = key.view(multihead_k_shape)\n",
    "        ks = ks.permute(0, 2, 1, 3)\n",
    "        ks_T = ks.transpose(2,3)\n",
    "        vs = value.view(multihead_v_shape)\n",
    "        vs = vs.permute(0, 2, 1, 3)\n",
    "\n",
    "        scaled_dot_product = qs@ks_T / np.sqrt(self.head_dim)\n",
    "        if self.quiet_attention:\n",
    "            cross_attention = _softmax_one(scaled_dot_product,dim=-1)\n",
    "        else:\n",
    "            cross_attention = nn.functional.softmax(scaled_dot_product,dim=-1)\n",
    "        \n",
    "        context_layer = cross_attention@vs\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous().reshape(batch_size,query_len,self.hidden_dim)\n",
    "        out = self.projection(context_layer)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.4126, -0.1694, -0.3108,  0.4688,  0.2641],\n",
      "         [ 0.4126, -0.1694, -0.3108,  0.4688,  0.2641],\n",
      "         [ 0.4031, -0.1579, -0.3032,  0.4626,  0.2592],\n",
      "         [ 0.4031, -0.1579, -0.3032,  0.4626,  0.2592]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.2384,  0.0984,  0.4646,  0.5792,  0.3071],\n",
      "         [-0.2384,  0.0984,  0.4646,  0.5792,  0.3071],\n",
      "         [-0.1494,  0.1263,  0.5072,  0.4239,  0.2393],\n",
      "         [-0.1059,  0.1399,  0.5280,  0.3479,  0.2061]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# causal attention test \n",
    "test_input = torch.tensor([[\n",
    "    [0,0,0,0,0],\n",
    "    [0,0,0,0,0],\n",
    "    [1,1,1,1,1],\n",
    "    [1,1,1,1,1],\n",
    "]]).float()\n",
    "#print(test_input.shape)\n",
    "\n",
    "# normal attention\n",
    "attn = MultiHeadSelfAttention(dim=5,num_heads=1,dropout=0.0,quiet_attention=False)\n",
    "test_output = attn(test_input)\n",
    "print(test_output)\n",
    "\n",
    "# causal attention\n",
    "attn = MultiHeadSelfAttention(dim=5,num_heads=1,dropout=0.0,is_causal=True,quiet_attention=False)\n",
    "test_output = attn(test_input)\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): 埋め込み次元数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            ff_hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderBlock,self).__init__()\n",
    "\n",
    "        self.mmhsa = MultiHeadSelfAttention(dim=embed_dim,num_heads=num_heads,dropout=dropout,is_causal=True)\n",
    "        self.mhca = MultiHeadCrossAttention(dim=embed_dim,num_heads=num_heads,dropout=dropout)\n",
    "        self.ff = FeedForward(dim=embed_dim,hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self,x,encoder_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): 入力トークン (batch_size,seq_len)\n",
    "            encoder_output (torch.Tensor): TransformerEncoderの出力 (batch_size,seq_len,embed_dim)\n",
    "        \"\"\"\n",
    "        z = self.mmhsa(x)\n",
    "        z = self.norm1(z)\n",
    "        x = x + z\n",
    "        z = self.mhca(query=x,key=encoder_output,value=encoder_output)\n",
    "        z = self.norm2(z)\n",
    "        x = x + z\n",
    "        z = self.ff(x)\n",
    "        z = self.norm3(z)\n",
    "        x = x + z\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,max_seq_len: int, vocab_size: int, embed_dim: int, \n",
    "                 ff_hidden_dim: int, num_blocks: int, num_heads: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len (int): 入力系列の最大長\n",
    "            vocab_size (int): 語彙数\n",
    "            embed_dim (int): 埋め込み次元数\n",
    "            ff_hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            num_blocks (int): TransformerBlockの数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder,self).__init__()\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(max_seq_len,embed_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerDecoderBlock(embed_dim=embed_dim,num_heads=num_heads,ff_hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.head = nn.Linear(embed_dim,vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,x,encoder_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): 入力トークン (batch_size,seq_len)\n",
    "            encoder_output (torch.Tensor): TransformerEncoderの出力 (batch_size,seq_len,embed_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: TransformerDecoderの出力 (batch_size,seq_len,vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x,encoder_output)\n",
    "        x = self.head(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO transformerの実装\\n実装したところで何に使うのかは知らんけど\\n\\nclass Transformer(nn.Module):\\n    def __init__(self,vocab_size: int, embed_dim: int, max_seq_len: int,\\n                 num_encoder_blocks: int, num_decoder_blocks: int, embed_dim: int,\\n                 num_heads: int, ff_hidden_dim: int, dropout: float)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO transformerの実装\n",
    "実装したところで何に使うのかは知らんけど\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size: int, embed_dim: int, max_seq_len: int,\n",
    "                 num_encoder_blocks: int, num_decoder_blocks: int, embed_dim: int,\n",
    "                 num_heads: int, ff_hidden_dim: int, dropout: float)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlocks(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim (int): 埋め込み次元数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            ff_hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super(GPTBlocks,self).__init__()\n",
    "\n",
    "        self.mmhsa = MultiHeadSelfAttention(dim=embed_dim,num_heads=num_heads,dropout=dropout,is_causal=True)\n",
    "        self.ff = FeedForward(dim=embed_dim,hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        z = self.mmhsa(x)\n",
    "        z = self.norm1(z)\n",
    "        x = x + z\n",
    "        z = self.ff(x)\n",
    "        z = self.norm2(z)\n",
    "        x = x + z\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, vocab_size: int, num_blocks: int,\n",
    "                 embed_dim: int, num_heads: int, ff_hidden_dim: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_seq_len (int): 入力系列の最大長\n",
    "            vocab_size (int): 語彙数\n",
    "            embed_dim (int): 埋め込み次元数\n",
    "            num_blocks (int): TransformerBlockの数\n",
    "            num_heads (int): MultiHeadAttentionのHead数\n",
    "            ff_hidden_dim (int): FeedForward Networkの隠れ層次元数\n",
    "            dropout (float): ドロップアウト確率\n",
    "        \"\"\"\n",
    "        super(GPT,self).__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(max_seq_len,embed_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GPTBlocks(embed_dim=embed_dim,num_heads=num_heads,ff_hidden_dim=ff_hidden_dim,dropout=dropout)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.head = nn.Linear(embed_dim,vocab_size) # embeddingの逆行列を使う方法もある\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,x,target=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): 入力トークン (batch_size,seq_len)\n",
    "            target (torch.Tensor): 教師トークン (batch_size,seq_len)\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: GPTの出力 (batch_size,seq_len,vocab_size), 損失 (1,)\n",
    "        \"\"\"\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.head(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        if target is not None:\n",
    "            loss = nn.functional.cross_entropy(x.view(-1,x.size(-1)),target.view(-1))\n",
    "            return x,loss\n",
    "        else:\n",
    "            return x,None\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 30])\n",
      "tensor([[20, 20,  4, 20,  4, 20, 20, 20, 20, 20]])\n"
     ]
    }
   ],
   "source": [
    "# GPT test\n",
    "test_input = torch.tensor([[1,2,3,4,5,6,7,8,9,10]])\n",
    "print(test_input.shape)\n",
    "gpt = GPT(max_seq_len=10,vocab_size=30,num_blocks=2,embed_dim=10,num_heads=2,ff_hidden_dim=40,dropout=0.1)\n",
    "test_output = gpt(test_input)\n",
    "print(test_output.shape)\n",
    "print(test_output.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 4])\n",
      "tensor([[[ 0.8219, -0.9072, -0.7282,  2.3748],\n",
      "         [ 1.0498,  0.5476, -0.4132,  0.2733],\n",
      "         [-0.0970,  0.2545, -1.4792, -0.9674],\n",
      "         [ 0.9543,  1.2970,  0.2506, -1.0072],\n",
      "         [ 0.0949,  1.2535,  0.2068,  0.5078]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([4, 5])\n",
      "tensor([[[ 1.0000e+00, -9.2654e-08, -5.3197e-08,  3.7330e-07],\n",
      "         [-5.3914e-08,  1.0000e+00,  2.1766e-08, -2.6786e-07],\n",
      "         [-1.5445e-08, -4.7437e-08,  1.0000e+00,  6.1151e-08],\n",
      "         [-5.1538e-08,  2.5637e-08,  4.2161e-09,  1.0000e+00]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 別の用事のやつ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embed = nn.Embedding(5,4)\n",
    "test_input = torch.tensor([[0,1,2,3,4]])\n",
    "test_output = embed(test_input)\n",
    "print(test_output.shape)\n",
    "print(test_output)\n",
    "\n",
    "# embedの疑似逆行列をかけて元の値に戻す\n",
    "weight = embed.weight.data.detach()\n",
    "weight_inv = (weight.T@weight).inverse()@weight.T\n",
    "print(weight_inv.shape)\n",
    "\n",
    "test = weight_inv@test_output#@weight_inv\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
